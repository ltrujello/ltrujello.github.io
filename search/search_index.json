{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hi, my name is Luke and you've found my website. Here you can find stuff I've made that I hope can be useful. </p> <p>I enjoy working on things that make it faster and easier for others to dessiminate and understand technical information. </p> <p>My undergraduate education focused on pure math, specifically in relation to algebra,  topology, and category theory, and I studied these areas deeply under many wonderfully patient  math profs. However, these days my interests reside more in neural networks,  specifically with regard to neural translation and computer vision. </p>"},{"location":"algebra/","title":"Algebra Notes","text":"<p>These are notes on general algebra that I wrote and maintained throughout my math undergraduate education. </p> <p>This is the html version rendered via MathJax. If you'd like the original LaTeX PDF version, see here.</p> <p>The material here  covers about a semester and a half worth of algebra courses, starting from group theory and ending in module theory. A lot of this material was inspired from lecture notes, homework assignments, and course exams I took (of course no actual duplication of exam material).</p>"},{"location":"algebra/Groups/Conjugation%2C%20The%20Class%20Equation%2C%20and%20Cauchy%27s%20Theorem./","title":"1.10. Conjugation, The Class Equation, and Cauchy's Theorem.","text":"<p>\\textcolor{blue}{We now touch on a very deep example of a group action, known as conjugation.} Let \\(G\\) act on itself \"by conjugation\", which we define as follows. Let \\(g, h \\in G\\). Then </p> \\[ g * h = ghg^{-1} \\] <p>is the group action of conjugation. Let's show that this is a group action.  \\begin{description} \\item[Composition.] Let \\(g_1, g_2, h \\in G\\). Then observe that</p> \\[\\begin{align*} g_1 * (g_2 * h) = g_1 * (g_2hg_2^{-1}) &amp;= g_1g_2 h g_2^{-1}g_1^{-1} \\\\ &amp; = (g_1g_2) h (g_1g_2)^{-1} \\\\ &amp; = (g_1g_2) * h \\end{align*}\\] <p>so that the first axiom of a group action is satisfied. \\item[Identity.] Observe also that for \\(e \\in G\\), the identity of \\(G\\), </p> \\[ e * h = ehe^{-1} = h. \\] <p>Therefore this is a group action. \\end{description}</p> <p>We'll now show that this group action is very special and important. Conjugation itself is important in math. In Linear Algebra, two matrices which are similar (i.e., \\(A\\) is similar to \\(B\\) if there exists \\(P\\) such that \\(A = P^{-1}BP\\)) have \\textbf{the same  rank, determinants, trace, eigenvalues, and much more}. Basically, they represent the same linear transformation, just in different bases. To learn more about conjugation, we make a few definitions with this group action.</p> <p> Let \\(G\\) be a group, and let \\(G\\) act on itself by conjugation. For any \\(h \\in G\\), the orbit of this group action \\[\\begin{align*} Gh &amp; = \\{g * h \\mid g \\in G\\} \\\\ &amp; = \\{ghg^{-1} \\mid g \\in G\\} \\end{align*}\\] <p>is known as a conjugacy class of \\(G\\). </p> <p>\\textcolor{purple}{Previously we discussed how orbits of a group action partition the set \\(X\\) which is being acted on. Since \\(G\\) acts on itself in this example, we see that \\textbf{the conjugacy classes form a partition of \\(G\\)!}} \\ \\ Remark. Recall the definition of a centralizer \\(G\\) for a set \\(A \\subset G\\): </p> \\[\\begin{align*} C_G(A) &amp; = \\{g \\in G \\mid gs = sg \\text{ for all } s \\in S\\}\\\\ &amp; = \\{g \\in G \\mid gsg^{-1} = s  \\text{ for all } s \\in S\\}. \\end{align*}\\] <p>Therefore for a single point \\(x \\in G\\), \\(C_G(x) = \\{g \\in G \\mid gxg^{-1} = x \\} = \\{g \\in G \\mid g * x = x\\}\\), where in the last equation we are speaking in terms of group actions. But note that this last set is exactly the stabilizer of \\(G\\) under this group action. \\textcolor{NavyBlue{Therefore, \\(C_G(x) = G_x\\) for any \\(x \\in G\\) under this group action.}}</p> <p>Furthermore, let \\(x \\in Z(G) = \\{z \\in G \\mid z = gzg^{-1} \\text{ for all } g \\in G\\},\\) the center of \\(G\\). Then we see that \\(Gx = \\{gxg^{-1} \\mid g \\in G\\} = \\{x\\}\\). \\textcolor{NavyBlue{So for any \\(x \\in Z(G)\\), the orbit is of size one. The sole element it contains is just \\(x\\).}} (We can go even further: the conjugacy classes of an abelian group are all of size one.) \\ \\ Let's put all of these results together. In general, if \\(G\\) acts on itself via conjugation, then we know its orbits, or conjugacy classes, partition \\(G\\). Moreover, let \\(R \\subset X\\) be a set of orbit representatives (or conjugacy class representatives, if you like). Then </p> \\[ |G| = \\sum_{x \\in R}|Gx| \\] <p>Recall that \\(|Gx| = 1\\) if \\(x \\in Z(G)\\). Thus we can write this further as </p> \\[\\begin{align*} |G| =  \\sum_{x \\in Z(G)}|Gx| + \\sum_{x \\in R\\setminus Z(G)} |Gx| &amp; = \\sum_{x \\in Z(G)}1 + \\sum_{x \\in R\\setminus Z(G)} |Gx|\\\\ &amp; = |Z(G)| + \\sum_{x \\in R\\setminus Z(G)} |Gx| \\end{align*}\\] <p>By the Orbit-Stabilizer theorem, we can write \\(|Gx| = |G|/|G_x|\\). Substituting this in, we get </p> \\[\\begin{align*} |G| &amp;= |Z(G)| + \\sum_{x \\in R\\setminus Z(G)} |G|/|G_x| \\end{align*}\\] <p>and since \\(C_G(x) = G_x\\),</p> \\[\\begin{align*} |G| = |Z(G)| + \\sum_{x \\in R\\setminus Z(G)} |G|/|C_G(x)| \\end{align*}\\] <p>which is known as the class equation. This equation is pretty badass, as it gives us a way to understand the cardinality of a group. This equation is also useful in proofs, as we shall see in the following examples. First, we begin with a lemma. </p> <p> Let \\(G\\) be a group. Then \\(C_G(x) = G\\) if and only if \\(x \\in Z(G)\\). </p> <p> Suppose \\(C_G(x) = G\\). Then for all \\(g \\in G\\), \\(gx = xg\\). However, \\(Z(G)\\) is the set of all \\(G\\) which commutes with every member of \\(G\\), so \\(x \\in Z(G)\\).  Now suppose \\(x \\in Z(G)\\). Then \\(gx = xg\\) for all \\(g \\in G\\). Therefore, \\(C_G(x) = \\{g \\in G \\mid gx = xg\\} = G\\). </p> <p> Let \\(G\\) be a group such that \\(|G| = p^n\\) for some prime \\(p\\) and \\(n \\in \\mathbb{N}\\). Then \\(|Z(G)| &gt; 1\\). That is, \\(|Z(G)| \\in \\{p, p^2, \\dots, p^{n}\\}\\).  \\textcolor{Purple}{Equivalently, this theorem says that \\(Z(G)\\) is nontrivial. Moreover, this implies that \\textbf{there exists non identity elements of \\(\\mathbf{G}\\) which commute with every element of \\(\\mathbf{G}\\).}}</p> <p> First observe that \\(Z(G)\\) is a subgroup of \\(G\\). Therefore, by Lagrange's Theorem, we know that \\(|Z(G)|\\) divides \\(G\\). Thus \\(|Z(G)| \\in \\{1, p, p^2, \\dots, p^{n}\\}\\). Our goal is to show that \\(|Z(G)|\\) cannot equal 1. <p>\\textcolor{NavyBlue}{For the sake of contradicton, suppose \\(|Z(G)| = 1\\)}. Then by the previous lemma, we see that  there is no nontrivial element \\(g\\) of \\(G\\) such \\(C_G(g) = G\\).</p> <p>Let \\(R\\) be the set of conjugacy class representatives.  Then \\(|G|/|C_G(r)| \\in \\{p, p^2, \\cdots, p^n\\}\\) for \\(r \\in R\\setminus Z(G)\\) (since \\(|Z(G)| = 1\\), \\(R\\setminus Z(G)\\) simply removes \\(e\\), the identtiy, from \\(R\\)).</p> <p>\\textcolor{red!40!purple!100}{Why can't \\(|G|/|C_G(r)| = 1\\) for any \\(r \\in R\\setminus Z(G)\\)? Well, because for such an \\(r\\), \\(r \\not\\in Z(G)\\). Therefore \\(C_G(r) \\ne G\\), so \\(|G|/|C_G(r)| \\ne 1\\).}</p> <p>Now by the class equation, we see that </p> \\[ \\underbrace{ \\hspace{.2cm}|G|\\hspace{.2cm}   }_{\\text{divisible by } p} \\hspace{-.5cm} = |Z(G)| + \\overbrace{\\sum_{r \\in R\\setminus Z(G)} |G|/|C_G(r)|}^{\\text{divisible by } p} \\] <p>since \\(|G|/|C_G(r)| \\in \\{p, p^2, \\dots, p^n\\}\\) for all \\(r \\in R\\setminus Z(G)\\). \\textcolor{NavyBlue}{Therefore we see that \\(|Z(G)|\\) must be divisible by \\(p\\). But this is a contradiction since we said \\(|Z(G)| = 1\\)}. Therefore, we see that \\(|Z(G)| \\in \\{p, p^2, \\dots, p^n\\}\\).</p> <p></p> <p>The above theorem can be used to prove the next theorem, whose signifiance demonstrates the power of the class equation. The theorem below is generally proved by proving the above theorem first in the special case for when \\(|G| = p^2\\). But it will be helpful to other proofs later on to consider the more general case as we presented it above.</p> <p> Let \\(G\\) be a group, and suppose \\(|G| = p^2\\) where \\(p \\ge 2\\) is prime. Then \\(G\\) is abelian.  </p> <p> \\textcolor{green!50!black}{By the previous theorem, we see that \\(|Z(G)| \\in \\{p, p^2\\}\\).)} We'll proceed by considering two cases. <p>\\begin{description} \\item[\\(\\mathbf{|Z(G)| = p^2}\\).]  In this case \\(|G| = |Z(G)|\\). Since we also have that \\(Z(G)\\) is a subgroup of \\(G\\), we can conclude that \\(G = Z(G)\\).  Therefore, \\(G\\) is abelian. </p> <p>\\item[\\(\\mathbf{|Z(G)| = p}\\).]  Recall that \\(Z(G) \\normal G\\) from Proposition \\ref{normal_center}. Therefore, we can speak of the quotient group \\(G/Z(G)\\), which has size \\(|G|/|Z(G)| = p^2/p = p\\). By the corollary to Lagrange's Theorem, this implies that \\(G/Z(G)\\) is cyclic, since it has prime order. Thus there exists a \\(g \\in G\\) such that we can represent \\(G/Z(G)\\) as </p> \\[ G/Z(G) = \\{Z(G), Z(G)g, Z(G)g^2, \\dots, Z(G)g^{p-1}\\}. \\] <p>As we already know, cosets partition \\(G\\). Therefore, let \\(a, b \\in G\\), and suppose \\(a \\in Z(G)g^i\\) and \\(b \\in Z(G)g^j\\). Then there exist \\(x, y \\in Z(G)\\) such that  \\(a = xg^i\\) and \\(b = yg^j\\). Thus observe that </p> \\[\\begin{align*} ab = xg^i yg^j = xyg^ig^j = xyg^{i+j} = xyg^jg^i = yg^jxg^i = ba \\end{align*}\\] <p>where we used the commutavity of \\(x,y\\) since \\(x, y \\in Z(G)\\). Since \\(a, b\\) were arbitrary members of \\(G\\), this proves that \\(G\\) is abelian. \\end{description} </p> <p>Thus we see that the class equation is useful in proving more general facts about group theory. The class equation can also be used to prove the following important theorem in group theory, known as Cauchy's Theorem. </p> <p>[ (Cauchy's Theorem)] Let \\(G\\) be a finite group and \\(p \\ge 2\\) be a prime. If \\(p\\) divides the order of \\(G\\), then \\(G\\) has an element of order \\(p\\).  </p> <p>\\noindent\\textcolor{NavyBlue}{So consider a group \\(G\\) with order \\(n\\), and suppose </p> \\[ n = p_1^{i_1}\\cdot p_2^{i_2} \\cdots p_n^{i_n} \\] <p>is its prime factorization. Then there exist elements \\(g_1, g_2, \\dots, g_n\\) such that \\(|g_i| = p_i\\) for \\(i = 1, 2, \\dots, n\\).}</p> <p>Another way to visualize this as follows. Consider a group \\(G\\) consisting of 10 elements. </p> \\[ \\{e, \\hspace{.1cm}g_1,\\hspace{.1cm} g_2,\\hspace{.1cm} g_3,\\hspace{.1cm} g_4,\\hspace{.1cm} g_5,\\hspace{.1cm} g_6,\\hspace{.1cm} g_7,\\hspace{.1cm} g_8,\\hspace{.1cm} g_{9}\\} \\] <p>By Cauchy's theorem, there exists elements of order \\(2\\) and \\(5\\). So suppose \\(g_1\\) and \\(g_2\\) are such elements, i.e., \\(g_1^2 = 3\\) and \\(g_2^5 = e\\). Then we can really rewrite this as </p> \\[ \\{e,\\hspace{.1cm} \\textcolor{red}{g_1},\\hspace{.1cm} \\textcolor{blue}{g_2},\\hspace{.1cm} \\textcolor{blue}{g_2^2},\\hspace{.1cm} \\textcolor{blue}{g_2^3},\\hspace{.1cm} \\textcolor{blue}{g_2^4},\\hspace{.1cm} g_6,\\hspace{.1cm} g_7,\\hspace{.1cm} g_8,\\hspace{.1cm} g_9\\}.    \\] <p>However, we know \\(\\textcolor{red}{g_1}\\textcolor{blue}{g_2}, \\textcolor{red}{g_1}\\textcolor{blue}{g_2^2}, \\textcolor{red}{g_1}\\textcolor{blue}{g_2^3}\\) and \\(\\textcolor{red}{g_1}\\textcolor{blue}{g_2^4}\\) are all in \\(G\\). Thus we can really write this as </p> \\[ \\{e, \\hspace{.1cm} \\textcolor{red}{g_1},\\hspace{.1cm} \\textcolor{blue}{g_2},\\hspace{.1cm} \\textcolor{blue}{g_2^2},\\hspace{.1cm} \\textcolor{blue}{g_2^3},\\hspace{.1cm} \\textcolor{blue}{g_2^4}, \\hspace{.1cm}\\textcolor{red}{g_1}\\textcolor{blue}{g_2},\\hspace{.1cm} \\textcolor{red}{g_1}\\textcolor{blue}{g_2^2},\\hspace{.1cm} \\textcolor{red}{g_1}\\textcolor{blue}{g_2^3},\\hspace{.1cm} \\textcolor{red}{g_1}\\textcolor{blue}{g_2^4}\\}. \\] <p>Thus we can understand the structure of every single group of order 10. But this can be done for all finite groups!</p> <p> \\textcolor{Plum}{In this proof, we'll prove this in a very clevery way by letting a subgroup of a permutation group act on a special set \\(X\\) (both of which we will define). This will then prove the existence of elements of order \\(p\\).} <p>Let \\(p\\) be a prime which divides \\(|G|\\).  Define \\(H\\) to be the cyclic subgroup of \\(S_p\\) generated by \\((1\\hspace{.1cm}2\\hspace{.1cm}\\cdots\\hspace{.1cm}p)\\). </p> <p>We can picture \\(H\\) as the group </p> \\[ \\{(1\\hspace{.1cm}2\\hspace{.1cm}\\cdots\\hspace{.1cm}p), (2\\hspace{.1cm}3\\hspace{.1cm}\\cdots\\hspace{.1cm}p, \\hspace{.1cm}1), \\cdots, (p\\hspace{.1cm}1\\hspace{.1cm}\\cdots\\hspace{.1cm}p-1)\\}. \\] <p>Now let \\(H\\) act on the set \\(X\\) defined as </p> \\[ X = \\{ (g_1, g_2, \\dots, g_p) \\mid g_1, g_2, \\dots, g_p \\in G \\text{ and } g_1g_2\\cdots g_p = e \\} \\] <p>where the \\(\\sigma \\in H\\) acts on \\(g \\in X\\) as </p> \\[ \\sigma \\cdot (g_1, g_2, \\dots, g_p) = (g_{\\sigma(1)}, g_{\\sigma(2)}, \\dots, g_{\\sigma(p)}). \\] <p>This \\(H\\) takes a \\(p\\)-tuple in \\(X\\) and permutates the elements. Since \\(H\\) is generated by \\((1\\hspace{.1cm}2\\hspace{.1cm}\\cdots\\hspace{.1cm}p)\\), it \"pushes\" the elements \\(g_i\\) in the tuple over to the right, and the elements that are pushed out of the right end of the tuple are pushed back in on the left side.</p> <p>\\textcolor{NavyBlue}{First we'll show that this is a group action.} \\begin{description} \\item[This is a Group Action.]  Let \\(x \\in X\\) and \\(\\sigma \\in H\\). If \\(x = (g_1, g_2, \\dots , g_p)\\), observe that </p> \\[ \\sigma * x = (g_{\\sigma(1)}, g_{\\sigma(2)}, \\dots, g_{\\sigma(p)}). \\] <p>Suppose \\(h(1) = n\\). Then in general \\(h(i) = (i + n) \\mbox{ mod }p.\\) Therefore, we see that </p> \\[ (g_{\\sigma(1)}, g_{\\sigma(2)}, \\dots, g_{\\sigma(p)})  =  (g_{n}, g_{n+1}, \\dots, g_p, g_1, \\dots, g_{n-1}). \\] <p>However, observe that </p> \\[ g_1g_2\\cdots g_p = g_1g_2 \\cdots g_{n-1}g_n g_{n+1} \\cdots g_p = e \\implies (g_1g_2 \\cdots g_{n-1})(g_n g_{n+1} \\cdots g_p) = e. \\] <p>Thus the elements \\(g_1g_2 \\cdots g_{n-1}\\) and \\(g_n g_{n+1} \\cdots g_p\\) in \\(G\\) are inverses of each other. But know that if two group elements are inverses, either order of their product returns \\(e\\). Therefore </p> \\[ (g_ng_{n+1} \\cdots g_{p})(g_1g_2 \\cdots g_{n-1})  = g_ng_{n+1} \\cdots g_{p}g_1g_2 \\cdots g_{n-1} = e. \\] <p>We therefore see that \\((g_{n}, g_{n+1}, \\dots, g_p, g_1, \\dots, g_{n-1}) = \\sigma *x \\in X\\). </p> <p>Now we verify associativity. For any \\(\\sigma_1, \\sigma_2 \\in H\\), we see that </p> \\[\\begin{align*} \\sigma_1 * \\sigma_2*x &amp;= \\sigma_1 * (g_{\\sigma_2(1)}, g_{\\sigma_2(2)}, \\dots, g_{\\sigma_2(p)})\\\\ &amp;= (g_{\\sigma_1(\\sigma_2(1))}, g_{\\sigma_1(\\sigma_2(2))}, \\dots, g_{\\sigma_1(\\sigma_2(p))})\\\\ &amp;= (\\sigma_1 \\sigma_2) * (g_1, g_2, \\dots, g_p). \\end{align*}\\] <p>Thus \\(*\\) is associative. Finally, if \\(\\sigma\\) is the trivial element, </p> \\[ \\sigma * x = (g_{\\sigma(1)}, g_{\\sigma(2)}, \\cdots g_{\\sigma(p)}) = (g_1, g_2, \\dots, g_p) = x. \\] <p>Therefore this is a group action. \\end{description} \\textcolor{NavyBlue}{Now that we've shown that this is a group action, we'll argue that the orbits are either of size 1 or \\(p\\).}</p> <p>\\begin{description} \\item[The Orbits.]  For any \\(x \\in X\\) such that \\(x = (g_1, g_2, \\dots, g_p)\\), we see that the orbit \\(Hx\\) will simply be all of the permutations of the \\(p\\)-tuple \\((g_1, g_2, \\dots, g_p)\\). Note however that there are only \\(p\\) many ways to rearrange this tuple, so that any orbit \\(Hx\\) will be of size \\(p\\).</p> <p>Of course, the exception to this is if \\(g_1 = g_2 = \\dots = g_p\\). In this case, there are no other ways to reorganize the tuple. Hence the orbit will have size 1. \\end{description}</p> <p>\\textcolor{NavyBlue}{Finally, we will show that there exists a nontrivial orbit of size 1. This is equivalent to show that there exists a nontrivial element of \\(G\\) of order \\(p\\), which we'll eloaborate later.}</p> <p>\\begin{description} \\item[Orbit of Size 1.] First let's count the elements of \\(X\\). Observe that for any \\((g_1, g_2, \\dots, g_p) \\in X\\), the last element \\(g_p\\) is always determined by the first \\(p-1\\) elements. This is because if we know the first \\(p-1\\) elements, then </p> \\[ g_p = (g_1g_2 \\cdots g_{p-1})^{-1} \\] <p>in order for \\(g_1g_2\\cdots g_p = e\\). Since there are \\(|G|^{p-1}\\) many ways to pick the first \\(p-1\\) elements in any \\(p\\)-tuple of \\(X\\), we see that \\(X = |G|^{p-1}\\). </p> <p>Now by hypothesis, \\(p\\) divides \\(|G|\\). Therefore \\(p\\) divides \\(|X|\\) so we may write \\(|X| = np\\) for some integer \\(n\\).</p> <p>Since the orbits of \\(X\\) form a partition, the orbits partition a set \\(np\\) elements into orbits of size \\(1\\) or size \\(p\\). We know one orbit of size 1 exists (namely, the trivial orbit \\(He = \\{(e, e, \\dots, e)\\}\\)), so there must exist at least \\(p-1\\) nontrivial other orbits of size 1. </p> <p>Let \\(Hx'\\) be one of those orbits. Then for some \\(g \\in G\\) we have that \\(Hx = \\{(g, g, \\dots, g)\\}\\). However since \\(Hx \\subset G\\), what we have prove is the existence of a nontrivial element \\(g \\in G\\) such that \\(gg\\cdots g = g^p = e\\), which set out to show. \\end{description} This completes the proof.  Cacuhy's Theorem is an incredibly useful tool one can use in finite group theory. Here's an amazing and useful theorem who's proof is eased via Cauchy's Theorem.</p> <p> Let \\(G\\) be a group \\(p \\ge 2\\) a prime. If \\(|G| = p^n\\) for some \\(n \\in \\mathbb{N}\\), then \\(G\\) has a subgroup of order \\(p^k\\) for all \\(0 &lt; k &lt; n\\).  Note we didn't write $ 0 \\le k \\le n$. We could have, but we already know that there exists a subgroup of order \\(p^n\\) (namely, \\(G\\) itself) and that there exists a subgrou of order \\(p^0 = 1\\) (namely, the trivial group).</p> <p> \\textcolor{NavyBlue}{To prove this, we'll use strong induction on the statement. Specifically, we'll induct on the powers of \\(n\\).} <p>Let us induct on \\(n\\) in the statement above. Then for \\(n = 1\\), there is no such \\(k &lt; n\\). Hence the statement is vacuously true. </p> <p>Next suppose that the statement is true up to order \\(p^n\\),  and let \\(G\\) be a group of order \\(p^{n+1}\\). </p> <p>By Theorem 1.\\ref{center_lemma}, we already note that \\(|Z(G)| &gt; 1\\) and hence is a multiple of \\(p\\). By Cauchy's Theorem, we then know that \\(Z(G)\\) contains an element \\(g\\) of order \\(p\\). Note that (1) \\(\\left&lt; g\\right&gt;\\) is a subgroup of \\(Z(G)\\) and (2) \\(h\\left&lt; g \\right&gt; = \\left&lt; g \\right&gt;h\\) for all \\(h \\in G\\) (since, by definition of the center, every element of \\(Z(G)\\) commutes with elements of \\(G\\)). Therefore \\(\\left&lt; g \\right&gt; \\normal G\\). </p> <p>Let \\(H = \\left&lt; g \\right&gt;\\). Since we just showed \\(H \\normal G\\) we can appropriately discuss the quotient group \\(G/H\\).</p> <p>Observe that \\(|G/H| = |G|/|H| = p^{n+1}/p = p^n\\). \\textcolor{purple}{Thus by hypothesis, \\(G/H\\) has a subgroup of order \\(p^k\\) for all \\(0 &lt; k &lt; n\\). Denote these such subgroups of \\(G/H\\) as}</p> \\[ \\{N_1/H, N_2/H, \\cdots N_{n-1}/H\\} \\] <p>\\textcolor{purple}{where \\(|N_k/H| = p^k\\).} Since \\(H \\normal G\\), we know by the Fourth Isomorphism Theorem that every subgroup of \\(G/H\\) is of the form \\(N/H\\) where \\(H \\le N \\le G\\). Thus we see that </p> \\[ H \\le N_k \\le G \\] <p>for all \\(0 &lt; k &lt; n\\). But since \\(|N_k/H| = p^k\\), and \\(|H| = p\\), we see that each such \\(N_k\\) will now have order \\(p^{k+1}\\). Thus what we have shown is that \\(G\\) itself contains subgroups of order \\(k\\) for all \\(1 &lt; k &lt; n+1\\). The subgroup \\(H\\) of order \\(p\\) is the final piece to this puzzle, and allows us to confirm that \\(G\\) has a subgroup of order \\(p^k\\) for all \\(0 &lt; k &lt; n\\). By strong induction this holds for all \\(\\mathbb{N}\\), which completes the proof. </p>"},{"location":"algebra/Groups/Cyclic%20Groups./","title":"1.4. Cyclic Groups.","text":"<p>Cyclics groups are a special type of group that are easy to recognize as group structures. In a cyclic group, one can always pinpoint a single element which can \"generate\" every other element of the group. For example, the subgroup of rotations \\(\\{e, r, r^2, \\dots, r^{n-1}\\}\\) in dihedral groups is cyclic. In such a subgroup, every element is simply a fininte power of \\(r\\). We can then think of this subgroup as being generated by a single element, namely \\(r\\).</p> <p>It will turn out later that, in every group, there will always be a subset of its elements such that the subset generates the whole group. Here, we're starting small, by just considering groups whose elements can be generated by one element.</p> <p> A group \\(G\\) is cyclic if there exists an element \\(g \\in G\\) such that  \\[ G = \\{g^n \\mid n \\in \\mathbb{N}\\}. \\] <p></p> <p>A very trivial example of a cyclic group is the integers under addition. This is because every element in \\((\\mathbb{Z}, +)\\) can be generated by the number 1 (e.g., 3 = 1 + 1 + 1, -2 \\(= -1 -1\\)). With the definition of cyclic groups at hand, we can introduce theorems about cyclic groups.</p> <p> Let \\(G\\) be a cyclic group, and suppose \\(G = \\{g^n  | n \\in \\mathbb{N}\\}\\) for some \\(g \\in G\\). Then \\(|G| = |g|\\). </p> <p> Suppose \\(|g| = k\\) where \\(k\\) is some positive integer. Then  since \\(G = \\{g^n | n \\in \\mathbb{N}\\}\\),  \\[ G = \\{e, g, g^2, \\dots, g^{k-1}\\}. \\] <p>Therefore \\(|G| = k = |g|\\). Now if \\(|g| = \\infty\\), then  by the same exact reasoning \\(|G| = \\infty\\).  A consequence of this theorem is that if \\(|g| = \\infty\\), then we know that \\(g^a \\ne g^b\\) for any \\(a, b \\in \\mathbb{N}\\) such that \\(a \\ne b\\). This would imply that \\(g^{a - b} = 1\\), which otherwise  imply the group \\(G\\) to have finite order; a contradiction if \\(|g| = \\infty\\) since this implies \\(|G| = \\infty\\).</p> <p> Any two cyclic groups of the same order are isomorphic. </p> <p> Let \\(G\\) and \\(G'\\) be two cyclic groups and suppose \\(|G| = |G'|\\). Furthermore suppose that \\(G = \\left&lt; g\\right&gt;\\) and \\(G' = \\left&lt; g' \\right&gt;\\). Construct the homomorphism \\(\\phi: G \\to G'\\) where  \\[ \\phi(g^n) = (g')^n \\] <p>for any \\(n \\in \\mathbb{N}\\). Observe that this is surjective, as the groups are of equal order so for any \\((g')^n\\) there we can identify the preimage to be \\(g^n\\). This is also injective, since if \\(g_1' = g_2'\\) are both elements in \\(G'\\), then \\(g_1' = g_2' = (g')^n\\) for some \\(n\\) which corresponds to one and only one element in \\(G\\); namely, \\(g^n\\). As the homomorphism we constructed is surjective and injective, we have that \\(\\phi\\) is an isomorphism so that the groups are isomorphic.</p> <p>Note we could have also utilized Theorem 1.3 here, by constructing the homomorphism \\(\\psi: G' \\to G\\) where \\(\\psi((g')^n) = g^n\\).</p> <p></p> <p>We'll now move onto a more useful theorem concerning cyclic groups.</p> <p> Let \\(G\\) be a cyclic group. Then every subgroup of \\(G\\) is cyclic. </p> <p> Let \\(H\\) be a subgroup of \\(G\\). Let \\(m\\) be the smallest integer such that \\(g^m \\in H\\). Suppose towards a contradiction that \\(H\\) is not cyclic. That is, there exists an element \\(h \\in H\\) such that \\(h \\ne g^{mn}\\) for any \\(m \\in \\mathbb{N}\\).  <p>Since \\(h \\in G\\), and \\(G\\) is a cyclic group, it must be some integer power of \\(g\\). Since \\(h \\ne g^m\\) for any \\(m \\in \\mathbb{N}\\),  we know that there must exist \\(q, r \\in \\mathbb{Z}\\) such that \\(h = g^{qm + r}\\) where \\(0 &lt; r &lt; m\\). </p> <p>Now since \\(H\\) is a group, \\(h^{-1} = g^{-(qm + r)} \\in H\\). Furthermore, \\(g^{(q+1)m} \\in H\\) since \\(H\\) is closed under products. By the same reasoning,</p> \\[ g^{(q+1)m}g^{-(qm + r)} = g^{m-r} \\] <p>is in \\(H\\). However, this contradicts our assumption that \\(m\\) is the smallest positive integer such that \\(g^m \\in H\\). Thus by contradiction \\(H\\) must be cyclic.   Remark. This proof utilizes the important idea that, if you know \\(G\\) is a group, and \\(g_1, g_2\\) are any two elements of \\(G\\), then the product \\(g_1g_2 \\in G\\). Furthermore, if \\(g_1, g_2, \\dots, g_n \\in G\\), then \\(g_1^{n_1}g_2^{n_2}\\cdots g_k^{n_k} \\in G\\) for literally any powers \\(n_1, n_2 \\dots n_k \\in \\mathbb{Z}\\). </p> <p>We used this idea by (1) observing that \\(g^m \\in H\\), so that \\(g^{(q+1)m} \\in H\\) for any \\(q \\in \\mathbb{Z}\\) and (2) reasoning that \\(g^{(q+1)m}g^{-(qm + r)} \\in H\\).</p> <p>In addition, we'll offer a way to think about subgroups of a cyclic group \\(G\\):</p> \\[\\begin{gather} G = \\{{\\color{blue}e}, g, {\\color{blue}g^2}, g^3, {\\color{blue}g^4}, g^5, {\\color{blue}g^6}, g^7, {\\color{blue}g^8}, g^9, g^10, \\dots\\}\\\\ \\hspace{0.5cm}= \\{{\\color{red}e}, g, g^2, {\\color{red}g^3}, g^4, g^5, {\\color{red}g^6}, g^7, g^8, {\\color{red}g^9}, g^{10}, \\dots\\}\\\\ \\hspace{0.5cm}= \\{{\\color{magenta}e}, g, g^2, g^3, {\\color{magenta}g^4}, g^5, g^6, g^7, {\\color{magenta}g^8}, g^9, g^{10}, \\dots\\} \\end{gather}\\] <p>In the above representations, the color-highlighted powers of \\(g\\) form subgroups of \\(G\\). Thus \\(\\{{\\color{blue}e}, {\\color{blue}g^2}, {\\color{blue}g^4}, {\\color{blue}g^6}, \\dots\\}\\),  \\(\\{{\\color{red}e}, {\\color{red}g^3}, {\\color{red}g^6}, {\\color{red}g^9}, \\dots \\}\\) and \\(\\{{\\color{magenta}e}, {\\color{magenta}g^4}, {\\color{magenta}g^8}, \\dots\\}\\) are all subgroups of \\(G\\). Of course, this process will eventually terminate if \\(G\\) is finite, but we represent the most general case above with the \\(\\dots\\) terms. In addition, if \\(G\\) is finite one of the subgroups may turn out to be just the entire group \\(G\\). </p> <p>To find the exact subgroups of a cyclic group we can use the following theorem. </p> <p> Let \\(G\\) be a finite cyclic group of order \\(n\\). For every positive integer \\(d\\) such that \\(d|n\\), there is exactly one subgroup of order \\(d\\) of \\(G\\). These are all the subgroups of \\(G\\).   </p> <p> Let \\(H\\) be a subgroup of \\(G\\) and suppose \\(|H| = m \\le n\\). We'll first show that \\(n \\mid m\\) and then show that for every \\(d \\mid n\\), there exists a subgroup of order \\(d\\). <p>By the previous theorem, we know that \\(H\\) must be cyclic. Therefore \\(H = \\{h^n \\mid n \\in \\mathbb{N}\\}\\) for some \\(h \\in G\\). However, by Theorem 1.4, we also know that \\(|H| = |h|\\), so that \\(h^m = e\\). Therefore, </p> \\[ H = \\{e, h, h^2, \\dots, h^{m-1}\\}. \\] <p>However, if \\(G = \\{e, g, g^2, \\dots, g^{n-1}\\}\\) for some \\(g \\in G\\), then \\(h = g^k\\) for some positive integer \\(k &lt; n\\). Therefore </p> \\[ H = \\{g^{jk} \\mid j \\in \\mathbb{N}\\}. \\] <p>Since \\(g^k\\) generates \\(H\\), we can apply Theorem 1.4 to conclude that \\(|g^k| = m\\). In order for this to be true, there must exist some positive integer \\(j\\) such that \\(g^{jk} = e\\).</p> <p>On one hand, we already know \\(m\\) is the smallest such integer, as we specified \\(h^m = (g^k)^m = e\\). On the other, we also know that \\(|g| = n\\); that is, \\(n\\) is the smallest integer such that \\(g^n = e\\). Therefore, we have that </p> \\[ n = mk \\] <p>which proves that \\(m \\mid n\\). </p> <p>To show that for each \\(d \\mid n\\) there exists a subgroup of order \\(d\\), simply observe that if \\(G = \\{e, g, g^2, \\dots , g^n\\}\\) then the set \\(\\{e, g^{n/d}, g^{2n/d}, \\dots, g^{(d-1)n/d}\\}\\) is a subgroup of order \\(d\\). This completes the proof. </p>"},{"location":"algebra/Groups/Definitions/","title":"1.1. Definitions","text":"<p> A group is a set \\(G\\), equipped with a binary operation \\(\\cdot : G \\times G \\to G\\) which satisfies the three following axioms: <p>\\begin{enumerate} \\item[(G1)] The operation \\(\\cdot\\) is associative. Specifically, \\(a \\cdot (b \\cdot c) = (a \\cdot b) \\cdot c\\) for all \\(a, b, c \\in G\\)</p> <p>\\item[(G2)] There exists an element \\(e \\in G\\), known as the identity of \\(G\\), such that \\(a \\cdot e = a = e \\cdot a\\) for all \\(a \\in G\\) </p> <p>\\item[(G3)] For each \\(a \\in G\\), there exists an element \\(a^{-1} \\in G\\),  known as the inverse of \\(a\\), such that \\(a \\cdot a^{-1} = a^{-1} \\cdot a = e\\).  \\end{enumerate} In this case, we say that \\(G\\) is a group under \\(\\cdot\\), and denote this  as \\((G, \\cdot)\\). </p> <p> <ul> <li> <p>Note that in order to have a group, we require a set \\(G\\), and a binary operation. Hence we cannot say \"the set \\(X\\) is a group.\" This makes no sense, although as we will see, sometimes this is written when the operation is obvious or stated.</p> </li> <li> <p>It goes without really explicitly stating that  \\(\\cdot\\) must also be closed; that is, it cannot map elements anywhere outside of \\(G\\). This is due to our  definition that \\(\\cdot : G \\times G \\to G\\). That is, the codomain, or range, is always within \\(G\\).</p> </li> </ul> <p></p> <p> Let \\(G\\) be a group. Suppose that, for any two \\(g, h \\in G\\) we have  \\[ g \\cdot h = h \\cdot g   \\] <p>then \\(G\\) is an abelian or commutative group. </p> <p>Notation.  First observe that we use \\(\\cdot\\) in our definition of a group. This is unfortunately the same notation used in modern-day numerical multiplication (i.e., \\(5 \\cdot 3 = 15\\)). Here, this is not the case; it's just a placeholder for some operator. You'll get used to this as you go in group theory.</p> <p>In group theory, we denote the multiplication of group elements \\(g\\) and \\(h\\) as \\(g \\cdot h\\). However, if the operator \\(\\cdot\\) is already understood, then we will just write \\(gh\\). If there is possibility for confusion (i.e., perhaps in a situation in where there are two operators in play) we will be more explicit and clear about our operator. Buts for the most part we'll just write \\(gh\\). \\ \\ \\noindent Example. Consider the set \\(\\mathbb{Z}\\), and let the operator on the elements of \\(\\mathbb{Z}\\) simply be standard addition. This is a group, which we'll demonstrate by showing that this set equipped with the addition operator satisfy the three axioms. \\begin{description} \\item[(1) Closed.] From elementary mathematics, we know that if \\(m, n \\in \\mathbb{Z}\\) then \\(m + n \\in \\mathbb{Z}\\). Thus this set is closed under addition.</p> <p>\\item[(2) Associativity.] Observe that for any integers $n, m $ and \\(p\\), </p> \\[ n + (m + p) = (n + m) + p. \\] <p>This is just a basic fact of elementary arithmetic.</p> <p>\\item[(3) Identity.] Observe that for any \\(n \\in \\mathbb{Z}\\), </p> \\[ n + 0 = 0 + n = n.         \\] <p>Thus 0 is an appropriate choice of an identity. </p> <p>\\item[(4) Inverse.] Consider any \\(n \\in \\mathbb{N}\\). Observe that  (1) \\(-n \\in \\mathbb{Z}\\) and (2) </p> \\[ n + (-n) = (-n) + n = 0. \\] <p>Thus every element has an inverse. Note we specified that \\(-n \\in \\mathbb{Z}\\), as we wanted to emphasize that not only \\(-n\\) exists, but \\(-n\\) is in our set \\(\\mathbb{Z}\\).    </p> <p>\\end{description} With all three properties satisfied, we have that \\(\\mathbb{Z}\\) is a group with addition. More generally, we'd say that \\(\\mathbb{Z}\\) is a group under addition, and denote it as \\((\\mathbb{Z}, +)\\). \\ \\ {\\color{Plum}  Note that \\(\\mathbb{Z}\\) is not a group under multiplication. Suppose we try to say it is one anyways. Then the most natural step is to first note that \\(1\\) should be our identity. After all, for any \\(n \\in \\mathbb{Z}\\), \\(1 \\cdot n = n \\cdot 1 = n\\). If we then consider any \\(m \\in \\mathbb{Z}\\), what is the inverse of \\(m\\)? We'd need a \\(p \\in \\mathbb{Z}\\) such that </p> \\[ m\\cdot p = p\\cdot m = 1. \\] <p>This has no solution if \\(m &gt; 1\\); for example, there is no integer \\(p\\)  such that \\(5 \\cdot p = 1\\). In fact, \\(p = \\dfrac{1}{5}\\), can only satisfy this in the set of real numbers, but \\(\\dfrac{1}{5}\\) is not in \\(\\mathbb{Z}\\). Thus \\(\\ZZ\\) is not a group under multiplication, but it is a group under addition. }</p> <p>\\textcolor{red}{We reiterate again that these two examples highlight the fact that a group requires two things: a set, and a well-defined operator that acts on the set.} \\</p> <p>It turns out that \\(\\mathbb{Q}\\setminus\\{0\\}\\) (not including zero) is a group under multiplication. Also, \\(\\mathbb{R}\\) is a group under multiplication and a group under addition. We wont show this (it's not much work) and will instead move onto more interesting examples which capture how versatile the definition of a group really is. </p> <p>\\noindent Example. Consider the set of \\(n \\times n\\) matrices with determinant 1 and entries in \\(\\mathbb{R}\\), where the multiplication is standard matrix multiplication. This is known as the Special Linear Group and is denoted \\(SL_n(\\mathbb{R})\\). We'll show that this set is a group. \\begin{description} \\item[(1) Closed.] First we need to check if this operation is closed. That is, for \\(A, B \\in SL_n(\\mathbb{R})\\), is it true that \\(AB \\in SL_n(\\mathbb{R})\\)?</p> <p>We know products of \\(n \\times n\\) matrices give back \\(n \\times n\\) matrices. So the real question is,  if two matrices both have determinant 1, will their product necessarily be a matrix whose determinant is also 1?  The answer is yes. From linear algebra, we know that </p> \\[ \\det(AB) = \\det(A)\\det(B). \\] <p>Now if \\(A, B\\) have determinant 1, </p> \\[ \\det(AB) = \\det(A)\\det(B) = 1 \\cdot 1 = 1.    \\] <p>Therefore, \\(AB \\in SL_n(\\mathbb{R})\\), since \\(AB\\) is \\(n \\times n\\) and it has determinant 1.</p> <p>\\item[(2) Associativity.] For matricies \\(A, B, C \\in SL_n(\\mathbb{R})\\), we know from linear algebra that </p> \\[ (AB)C = A(BC). \\] <p>That is, matrix multiplication is associative.</p> <p>\\item[(3) Identity.] Naturally, the identity matrix \\(I\\) serves as our group identity. This is because for any \\(A \\in SL_n(\\mathbb{R})\\), \\(AI = IA = A\\).</p> <p>\\item[(4) Inverses.] For any \\(A \\in SL_n(\\mathbb{R})\\), \\(\\det(A) = 1\\). Specifically observe that \\(\\det(A) \\ne 0\\). Therefore by the  invertible matrix theorem, \\(A\\) has an inverse element \\(A^{-1}\\) such that \\(AA^{-1} = A^{-1}A = I\\). But the real question is: is it true that \\(A^{-1} \\in SL_n(\\mathbb{R})\\)? </p> <p>To answer this, observe that \\(AA^{-1} = I\\) and that \\(\\det(I) = 1\\). Thus </p> \\[  \\det(AA^{-1}) = \\det(I) = 1. \\] <p>However, since \\(\\det(AA^{-1}) = \\det(A)\\det(A^{-1})\\),</p> \\[ \\det(AA^{-1}) = 1 \\implies \\det(A)\\det(A^{-1}) = 1. \\] <p>But \\(A \\in SL_n(\\mathbb{R})\\), so \\(\\det(A) = 1\\). Therefore, \\(\\det(A)^{-1} = 1\\), so that \\(A^{-1} \\in SL_n(\\mathbb{R})\\).  \\end{description} Thus \\(SL_n(\\mathbb{R})\\) does form a group. </p> <p>\\textcolor{MidnightBlue}{ You may be wondeirng: Why did we focus on matrices with determinat 1? Why not consider all matrices in general?  \\ \\indent At first, the set of all \\(n \\times n\\) matrices with coefficients in \\(\\mathbb{R}\\), denoted by \\(M_{n\\times n}(\\mathbb{R})\\), may seem like a group. But recall from linear algebra that some matrices are not invertible under matrix multiplication. Thus we can't form a group since some matrices would just not have an inverse.  }</p> <p>However, consider the following set:</p> \\[ G = \\left\\{  \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix} : a \\in \\mathbb{R}-\\{0\\} \\right\\}. \\] <p>Clearly, these elements are not invertible matrices under matrix multiplication. However, we can still form a group out of this!</p> <p>Naturally, we'd want to make the identity element as \\(\\displaystyle  \\begin{pmatrix} 1 &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix}\\), since this is the natural identity when it comes to matrix multiplication. However, this isn't in the above set, which may make you wonder if this really is a group.</p> <p>\\begin{description} \\item[(1) Closed.] First we show that this is closed. Let </p> \\[ A = \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix},  B =  \\begin{pmatrix} b &amp; b\\\\ b &amp; b \\end{pmatrix} \\] <p>where \\(a, b \\in \\mathbb{R}-\\{0\\}\\). Now observe that </p> \\[ AB =  \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix} \\begin{pmatrix} b &amp; b\\\\ b &amp; b \\end{pmatrix} =  \\begin{pmatrix} 2ab &amp; 2ab\\\\ 2ab &amp; 2ab \\end{pmatrix}. \\] <p>Since \\(2ab \\in \\mathbb{R}-\\{0\\}\\), we see that \\(AB \\in G\\). Hence, the set is closed.</p> <p>\\item[(2) Associativity.] Again, from linear algebra, we already know that matrix multiplication is associative.</p> <p>\\item[(3) Identity.] What should we make our identity? One can realize that </p> \\[ e= \\begin{pmatrix} 1 &amp; 1\\\\ 1 &amp; 1 \\end{pmatrix} \\] <p>suffices for an identity. That is, for any \\(A =  \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix}\\), we see that </p> \\[ Ae =  \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1\\\\ 1 &amp; 1 \\end{pmatrix} = A \\] <p>and </p> \\[ eA =  \\begin{pmatrix} 1 &amp; 1\\\\ 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1\\\\ 1 &amp; 1 \\end{pmatrix} = A. \\] <p>Hence \\(Ae = A = eA\\), so that it works correctly as an identity.</p> <p>\\item[(4) Inverses.] For any \\(A =  \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix}\\), we can write \\(A^{-1} =  \\begin{pmatrix} \\frac{1}{2a} &amp; \\frac{1}{2a}\\\\ \\frac{1}{2a} &amp; \\frac{1}{2a} \\end{pmatrix}\\). Note that since \\(a \\in \\mathbb{R} -\\{0\\}\\) implies that \\(\\frac{1}{2a} \\in \\mathbb{R} - \\{0\\}\\), so that \\(A^{-1} \\in G\\). Now we see that </p> \\[ AA^{-1}  =  \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2a} &amp; \\frac{1}{2a}\\\\ \\frac{1}{2a} &amp; \\frac{1}{2a} \\end{pmatrix} =  \\begin{pmatrix} 1&amp; 1\\\\ 1 &amp; 1 \\end{pmatrix} \\] <p>and </p> \\[ A^{-1}A = \\begin{pmatrix} \\frac{1}{2a} &amp; \\frac{1}{2a}\\\\ \\frac{1}{2a} &amp; \\frac{1}{2a} \\end{pmatrix}   \\begin{pmatrix} a &amp; a\\\\ a &amp; a \\end{pmatrix} =  \\begin{pmatrix} 1&amp; 1\\\\ 1 &amp; 1 \\end{pmatrix}. \\] <p>Thus we see that for every \\(A \\in G\\), there exists an element \\(A^{-1} \\in G\\) such that \\(AA^{-1} = e = A^{-1}A\\). \\end{description} With all four axioms satisfied, we see that \\(G\\) forms a group.</p> <p>Example. Consider an equilateral triangle. The set of rigid transformations which preserve symmetry form a group. This is actually just a special case of the more general Dihedral Group \\(D_{2n}\\).</p> <p>There are many ways we can think of transforming a triangle. But we can always break them down into rotations, denoted by \\(r\\), of 120\\(^\\circ\\), and reflections across a a diagonal of symmetry, denoted by \\(s\\).  It turns out that the full list of unique rotations we can come up with are </p> \\[ \\{e, r, r^2, s, rs, r^2s\\}   \\] <p>which we can illustrate visually with the triangles below. \\begin{figure}[h] \\centering \\begin{tikzpicture}[black, scale = 1.5]</p> <p>\\draw (-4.1,0) -- (-3.1,1.5) node[above] {\\(1\\)}; \\draw (-3.1,1.5) -- (-2.1,0) node[right] {\\(3\\)}; \\draw (-2.1, 0) -- (-4.1, 0) node[left] {\\(2\\)}; \\draw[dashed] (-3.1, 1.5) -- (-3.1,0);</p> <p>\\filldraw[green!50] (-4.1, 0) circle(0.5mm); \\filldraw[blue!70] (-3.1, 1.5) circle (0.5mm); \\filldraw[red!50] (-2.1, 0) circle (0.5mm);</p> <p>\\draw[thick, &lt;-] (-2.3,0.8) to (-1,0.8) node at (-1.6, 1.0) {\\(s\\)};</p> <p>\\draw (-4.1,-3) -- (-3.1,-1.5) node[above] {\\(3\\)}; \\draw (-3.1,-1.5) -- (-2.1,-3) node[right] {\\(2\\)}; \\draw (-2.1, -3) -- (-4.1, -3) node[left] {\\(1\\)}; \\draw[dashed] (-4.1, -3) -- (-2.6,-2.2);</p> <p>\\filldraw[blue!70] (-4.1, -3) circle(0.5mm); \\filldraw[red!50] (-3.1, -1.5) circle (0.5mm); \\filldraw[green!50] (-2.1, -3) circle (0.5mm);</p> <p>\\draw[thick, -&gt;] (-3.1, -0.1) to (-3.1, -1) node at (-3.4, -0.5) {\\(rs\\)};</p> <p>\\draw (-1,-3) -- (0,-1.5) node[above] {\\(2\\)}; \\draw (0,-1.5) -- (1,-3) node[right] {\\(1\\)}; \\draw (1, -3) -- (-1, -3) node[left] {\\(3\\)}; \\draw[dashed] (1, -3) -- (-0.5,-2.2);</p> <p>\\filldraw[red!50] (-1, -3) circle(0.5mm); \\filldraw[green!50] (0, -1.5) circle (0.5mm); \\filldraw[blue!70] (1, -3) circle (0.5mm);</p> <p>\\draw[thick, -&gt;] (-2.3,-2.2) to (-1,-2.2) node at (-1.6, -2) {\\(r^2s\\)};</p> <p>\\draw (-1,0) -- (0,1.5) node[above] {\\(1\\)}; \\draw (0,1.5) -- (1,0) node[right] {\\(2\\)}; \\draw (1, 0) -- (-1, 0) node[left] {\\(3\\)}; \\draw[dashed] (0, 1.5) -- (0,0);</p> <p>\\filldraw[red!50] (-1, 0) circle(0.5mm); \\filldraw[blue!70] (0, 1.5) circle (0.5mm); \\filldraw[green!50] (1, 0) circle (0.5mm);</p> <p>\\draw[thick, -&gt;] (0.7,0.8) to (2,0.8) node at (1.4, 1.0) {\\(r\\)};</p> <p>\\draw (2.1,0) -- (3.1,1.5) node[above] {\\(2\\)}; \\draw (3.1,1.5) -- (4.1,0) node[right] {\\(3\\)}; \\draw (4.1, 0) -- (2.1, 0) node[left] {\\(1\\)}; \\draw[dashed] (2.1,0) -- (3.6, 0.75);</p> <p>\\filldraw[blue!70] (2.1, 0) circle(0.5mm); \\filldraw[green!50] (3.1, 1.5) circle (0.5mm); \\filldraw[red!50] (4.1, 0) circle (0.5mm);</p> <p>\\draw[thick, -&gt;] (3.1,-0.1) to (3.1,-1) node at (3.4, -0.5) {\\(r^2\\)};</p> <p>\\draw (2.1,-3) -- (3.1,-1.5) node[above] {\\(3\\)}; \\draw (3.1,-1.5) -- (4.1,-3) node[right] {\\(1\\)}; \\draw (4.1, -3) -- (2.1, -3) node[left] {\\(2\\)}; \\draw[dashed] (4.1,-3) -- (2.6, -2.25);</p> <p>\\filldraw[green!50] (2.1, -3) circle(0.5mm); \\filldraw[red!50] (3.1, -1.5) circle (0.5mm); \\filldraw[blue!70] (4.1, -3) circle (0.5mm);</p> <p>\\end{tikzpicture} \\end{figure}</p> <p> Let \\((G, \\cdot)\\) be a group. Then the following hold: <ul> <li> <p>[1.] The identity \\(e \\in G\\) is unique</p> </li> <li> <p>[2.] The inverse \\(g^{-1} \\in G\\) is unique for every \\(g \\in G\\).</p> </li> <li> <p>[3.] For any \\(g \\in G\\), \\((g^{-1})^{-1} = g\\). </p> </li> <li> <p>[4.] Let \\(g, h \\in G\\). Then \\((g \\cdot h)^{-1} = h^{-1} \\cdot g^{-1}\\).   </p> </li> <li> <p>[5.] Let \\(g_1, g_2, \\dots, g_n \\in G\\). The product \\(g_1 \\cdot g_2 \\cdot \\vspace{0.01mm} \\dots \\vspace{0.01mm} \\cdot g_n\\) is independent of its bracketing. </p> </li> <li> <p>[6.] Let \\(g, h \\in G\\). There always exist \\(x, y\\) such that \\(g \\cdot x = h\\) and \\(h \\cdot y = g\\).</p> </li> </ul> <p></p> <p> \\begin{description} \\item[1.] Suppose there exists another identity  element \\(f\\), different from \\(e\\), such that \\(g \\cdot f = f \\cdot g = g\\) for all \\(g \\in G\\). Then  \\[ e = e \\cdot f = f \\] <p>so that \\(e = f\\). Therefore, the identity element is unique.</p> <p>\\item[2.] Suppose \\(h_1\\) and \\(h_2\\) are both inverses of \\(g \\in G\\). Then by definition, \\(h_2 \\cdot g = e = g \\cdot h_2\\) and \\(h_1 \\cdot g = e = g \\cdot h_1\\). Therefore, </p> \\[ h_1 = (h_2 \\cdot g) \\cdot h_1 = \\underbrace{h_2 \\cdot (g \\cdot h_2)}_{\\text{by associativity of G}} = h_2 \\cdot e = h_2. \\] <p>Thus \\(h_1 = h_2\\), so that the inverse of \\(g\\) os unique. </p> <p>\\item[3.] Observe that for any \\(g \\in G\\),</p> \\[ g^{-1}\\cdot (g^{-1})^{-1} = e \\] <p>by defintion. Multiplying on the left by \\(g\\) on both sides of the equation, we get </p> \\[\\begin{align*} g \\cdot (g^{-1} \\cdot (g^{-1})^{-1}) = g \\cdot e \\implies (g \\cdot g^{-1}) \\cdot (g^{-1})^{-1} = g  \\end{align*}\\] <p>by associativity. Since \\(g \\cdot g^{-1} = e\\), this then leads to </p> \\[\\begin{align*} e \\cdot (g^{-1})^{-1} = g \\implies (g^{-1})^{-1} = g \\end{align*}\\] <p>as desired.</p> <p>\\item[4.] Note that \\((g \\cdot h)^{-1} \\cdot (g \\cdot h) = e.\\) Therefore,</p> \\[ (g \\cdot h)^{-1} \\cdot (g \\cdot h) = e \\implies (g \\cdot h)^{-1} \\cdot g = h^{-1} \\implies (g \\cdot h)^{-1} = h^{-1} \\cdot g^{-1} \\] <p>by first multiplying on the right by \\(g^{-1}\\) and then by \\(h^{-1}\\), which proves the formula.</p> <p>\\item[5.] We can demonstrate this by induction. First write our proposition as</p> \\[ P(n) = \\begin{cases} \\text{For any } g_1, g_2, \\dots, g_n \\in G\\text{ we have that }\\\\ g_1\\cdot g_2 \\cdots g_n \\text{ is independent of its bracketing. } \\end{cases} \\] <p>\\begin{description} \\item[Base Case.] For the base case \\(n = 1\\), there is nothing to check. \\item[Inductive Step.] Now suppose that \\(P(n)\\) is true for all positive integers \\(n \\le n_0\\). Then let \\(g_1, g_2, \\dots g_{n+1} \\in G\\) and consider </p> \\[ g_1\\cdot g_2 \\cdots \\cdot g_{n+1}. \\] <p>Observe that we clearly have that </p> \\[ g_1\\cdot g_2 \\cdots \\cdot g_{n+1}. = (g_1\\cdot g_2 \\cdots \\cdot g_{i})\\cdot(g_{i+1} \\cdots \\cdot g_{n+1}). \\] <p>for all \\(1 \\le i le n + 1\\). Hence we can apply the inductive hyptohesis to each of the subproducts \\((g_1\\cdot g_2 \\cdots \\cdot g_{i})\\) and \\((g_{i+1} \\cdots \\cdot g_{n+1})\\) generated in each case. Since this exhausts all possible subproducts, and the values do not change by our inductive hypothesis, we see that \\(P(n + 1)\\) is true. Hence \\(P(n)\\) holds for all \\(n \\in \\mathbb{N}\\).  \\end{description}</p> <p>\\item[6.] Observe that if we have the equation \\(g \\cdot x = h\\), then we can multiply both sides on the right by \\(g^{-1}\\) to observe that </p> \\[ (g^{-1} \\cdot g) \\cdot x = g^{-1} \\cdot h \\implies x = g^{-1} \\cdot h. \\] <p>Since \\(x\\) is the product of elements of \\(G\\) (namely, \\(g^{-1} \\cdot h\\)) and because \\(G\\) is closed under \\(\\cdot\\), we have that \\(x \\in G\\). Thus a solution exists in \\(G\\). The proof for the existence of \\(y \\in G\\) such that \\(h \\cdot y = g\\) is exactly the same. \\end{description} </p> <p>In our study of group theory, many of the groups we'll deal with will actually turn out to be finite. We'll also be interested in breaking down the structures of finite groups (a lot of things can happen).  A couple things should be noted about finite groups.</p> <p>{\\color{MidnightBlue} Consider \\(g \\in G\\), where \\(G\\) is a finite group. Since \\(G\\) must be closed under its product, we note that \\(g^2 \\in G\\), \\(g^3 \\in G\\), and so on. That is, all powers of \\(g\\) must be in \\(G\\). But since \\(G\\) is finite, there must exist some \\(m \\in \\mathbb{N}\\) such that \\(g^m = e\\). If not, you could keep raising the power of \\(g\\), and keep getting new elements. Since you'd never come back to \\(e\\), you could then generate an infinite set \\(\\{g, g^2, g^3, g^4, \\dots\\}\\) entirely contained in \\(G\\). But this would imply \\(G\\) is infinite, which it isn't.}</p> <p>\\noindent</p> <p> Let \\(G\\) be a group. The order of an element \\(g \\in G\\) is the smallest integer \\(n\\) such that \\(g^n = e\\).  <p>In addition, if \\(G\\) is a finite group, then we can also talk about the order of a group \\(|G|\\), which we define as the number of elements within the group.</p> <p>The order is denoted as \\(|g|\\); thus we'd say that \\(|g| = n\\) if \\(g\\) has order \\(n\\). If \\(G\\) is infinite, it may be possible that \\(|g| = \\infty\\). On the topic of order, we define that \\(g^0 = e\\).  \\subsection*{Subgroups.}</p> <p> Let \\(G\\) be a group, and consider a subset \\(H\\) of \\(G\\). We define \\(H\\) to be subgroup of \\(G\\) if \\(H\\) is also a group. </p> <p>{\\color{blue}The definition is exactly what it sounds like: \\(H\\) is a subgroup if \\(H \\subset G\\) and \\(H\\) is a group. You might note that the definition is clear, but determining if a set is a subgroup of \\(G\\) sounds like a lot of work. Fortunately there's the subgroup test. }</p> <p> Let \\(H\\) be nonempty and suppose \\(H \\subset G\\). Then \\(H\\) is a subgroup if and only if for all \\(x, y \\in H\\), \\(xy^{-1} \\in H\\). </p> <p>\\noindent **If \\(\\mathbf{H**\\) is a subgroup, we usually write \\(\\mathbf{H \\le G}\\) if we are trying to be concise.}</p> <p> <p>(\\(\\implies\\)) Suppose \\(H \\le G\\). Then since \\(H\\) is a group, for any \\(x, y \\in H\\), \\(xy^{-1} \\in H\\) since it is closed under multiplication of its elements. This proves the forward direction. \\</p> <p>(\\(\\impliedby\\)) Suppose \\(H\\) is nonempty and \\(H \\subset G\\) such that for all \\(x, y \\in H\\), \\(xy^{-1} \\in H\\). We just need to prove \\(H\\) is a group. We already know group multiplication is an associative, binary operation on \\(G\\), so it is still associative on elements of \\(H\\). Thus we only need to prove closedness, existence of identity and inverses.</p> <p>By the definition of \\(H\\), for all \\(x, y \\in H\\), \\(xy^{-1} \\in H\\).  \\begin{description} \\item[Identity.] Let \\(x \\in H\\). Then clearly \\(xx^{-1} = e \\in H\\). Thus \\(H\\) has the identity. \\item[Inverses.] Since \\(x, e \\in H\\), we see that \\(ex^{-1} = x^{-1} \\in H\\). Thus for all \\(x \\in H\\), \\(x^{-1} \\in H\\). \\item[Closedness.] Now let \\(y \\in H\\); hence, \\(y^{-1} \\in H\\), as just proven. Then \\(x(y^{-1})^{-1} = xy\\in H\\), so that \\(H\\) is closed under multiplication of its elements. \\end{description}  Therefore \\(H\\) is (1) a group and (2) a subset of \\(G\\) so that \\(H \\le G\\), as desired. </p> <p>It turns out the intersection of two subgroups is still a subgroup. In fact, the arbitrary intersection of subgroups always produces a subgroup. </p> <p> Let \\(G\\) be a group and \\(\\{H_\\alpha\\}_{\\alpha \\in \\lambda}\\) be a family of subgroups of \\(G\\). Then the set \\(H = \\bigcap_{\\alpha \\in \\lambda} H_\\alpha\\) is a subgroup of \\(G\\). </p> <p> First, observe that  \\[ H = \\bigcap_{\\alpha \\in \\lambda} H_\\alpha \\] <p>is nonempty. This is because each \\(H_\\alpha \\le G\\) and thus the identity of \\(G\\) is contained in each \\(H_\\alpha\\) for all \\(\\alpha \\in \\lambda\\). So the identity is in \\(H\\) as well.</p> <p>Thus let \\(x, y \\in H\\). Then \\(x, y \\in H_\\alpha\\) for all \\(\\alpha \\in \\lambda\\). Since each \\(H_\\alpha\\) is a group, \\(y^{-1}\\) exists and is contained in each \\(H_\\alpha\\). Hence, \\(xy^{-1} \\in H_\\alpha\\) for all \\(\\alpha \\in \\lambda\\), so we have that \\(xy^{-1} \\in H\\). Therefore, we see by the subgroup text that \\(H \\le G\\).   With the basic properties of a group introduced, we now introduce two more group definitions. </p> <p> Let \\(G\\) be a group and \\(S \\subset G\\). The centralizer of \\(S\\) in \\(G\\) is defined to be the set \\(C_G(S)\\) \\[ \\textcolor{NavyBlue}{C_G(S)} = \\{g \\in G \\mid gs = sg \\text{ for all } s \\in S\\}. \\] <p> In the case where \\(G\\) is abelian, we \\(C_G(S) = G\\) for any nonempty subset \\(S\\) of \\(G\\). This definition is close to the \\textbf{center of a group}, which is as follows.</p> <p> Let \\(G\\) be a group. Then the center of a group \\(G\\) is defined as  \\[ Z(G) = \\{z \\in G \\mid zg = gz \\text{ for all } g \\in G\\}. \\] <p></p> <p>In this case, we note that \\(C_G(G) = Z(G)\\) and if \\(G\\) is abelian \\(Z(G) = G\\). Finally, we introduce the definition of the normalizer.</p> <p> Let \\(G\\) be a group and \\(S \\subset G\\). The normalizer of \\(S\\) in \\(G\\) is defined as  \\[ \\textcolor{purple}{N_G(S)} = \\{g \\in G \\mid gS = Sg\\} \\] <p></p> <p>The centralizer and the normalizer are closely related definitions. However, these two concepts highlight the important distinction one must understand between term-by-term equality and set equality. \\textcolor{NavyBlue}{Firstly, we can think of \\(C_G(S)\\) as all \\(g \\in G\\) which commutes with each and every single element of \\(S\\).}  \\textcolor{purple}{On the other hand, if \\(g \\in N_G(S)\\), it is not necessarily true that \\(gs = sg\\) for all \\(s \\in S\\). The only requirement is that \\(gS\\) creates the same set as \\(Sg\\). Of course, one way to do this is if \\(gs = sg\\) for all \\(s \\in G\\). In that case, \\(gS = Sg\\). But there are other ways to do this, so this definition is more versatile than \\(C_G(S)\\).}</p> <p>One interesting fact is that \\(C_G(S)\\) and \\(N_G(S)\\) are subgroups of \\(G\\) for any \\(S \\subset G\\). </p> <p> Let \\(G\\) be a group and \\(S \\subset G\\). Then \\(C_G(S)\\) and \\(N_G(S)\\) are both subgroups of \\(G\\).  \\textcolor{blue}{Note: if we let \\(S = G\\), we see that \\(C_G(S) = Z(G)\\).  Therefore, an immediate corollay of this theorem is that \\(Z(G)\\) is also a subgroup of \\(G\\)!}</p> <p> Let \\(G\\) be a group and \\(S \\subset G\\). To show that \\(C_G(S) \\le G\\), we can use the subgroup test.  \\begin{description} \\item[Nonempty.] First we have to show the set is nonempty. But note that for any \\(S\\), \\(e \\in C_G(S)\\)  since \\(gs = sg\\) for any \\(s \\in S\\). <p>\\item[Inverses.] We now show that if \\(x, y \\in C_G(S)\\) then so is \\(xy^{-1}\\). We know that for all \\(s \\in S\\), \\(xs = sx\\) and \\(ys = sy\\). Therefore \\(s = y^{-1}sy\\) and \\(s = ysy^{-1}\\) by solving for \\(s\\) in the last equation. Plugging this into the first equation with \\(x\\), we get </p> \\[ xs = sx \\implies x(y^{-1}sy) = (ysy^{-1})x  \\implies xy^{-1}sy = ysy^{-1}x. \\] <p>Multiplying both sides on the right by \\(y^{-1}\\) leads to </p> \\[ xy^{-1}s = ysy^{-1}xy^{-1}  \\implies xy^{-1}s = syy^{-1}xy^{-1} \\implies xy^{-1}s = sxy^{-1} \\] <p>where in the second step we used the fact that \\(ys = sy\\). Thus \\(xy^{-1} \\in C_G(S)\\), so by subgroup test we have that \\(C_G(S) \\le G\\).  \\end{description}</p> <p>The proof for \\(N_G(S)\\) is the exact same; simply replace \\(s\\) with \\(S\\). </p>"},{"location":"algebra/Groups/Fundamental%20Theorem%20of%20Finite%20Abelian%20Groups./","title":"1.12. Fundamental Theorem of Finite Abelian Groups.","text":"<p>Due to Sylow's Theorem, it is now an easy task to classify groups of small orders up to an isomorphism by hand. However, abelian groups are even easier to understand. Abelian groups have a simple enough structure that we can actually generalize the structure of every ablian group with the following theorems.</p> <p>First we begin with a lemma. </p> <p> Let \\(G\\) be a finite abelian group. Then \\(G\\) is isomorphic to a direct product of its Sylow \\(p\\)-subgroups. </p> <p> Since \\(G\\) is finite, suppose \\(|G| = p_1^{n_1}p_2^{n_2}\\cdots p_n^{n_k}\\) where \\(p_i\\) are distinct primes and  \\(n_i\\) are positive integers for \\(i = 1, 2, \\dots, k\\). <p>By Sylow's Theorem, there exist Sylow \\(p_i\\)-subgroups for each \\(i = 1, 2, \\dots, k\\). Denote these subgroups as \\(H_i\\) (and hence \\(|H_i| = p_i^{n_i}\\)). Observe that \\(\\mbox{gcd}(p_i^{n_i}, p_j^{n_j}) = 1\\) for any \\(i \\ne j\\). Hence, no \\(H_i\\) is a subgroup of any other \\(H_j\\) for \\(i \\ne j\\). </p> <p>\\textcolor{Plum}{(Otherwise, Lagrange would tell us that that's nonsense because the order of subgroup always divides the order of the bigger group; and in this case, \\(\\mbox{gcd}(p_i^{n_i}, p_j^{n_j}) = 1\\).)}</p> <p>We can equivalently state that \\(H_i \\cap H_j = \\{e\\}\\) for \\(i \\ne j\\), where \\(e\\) is the identity of \\(G\\).</p> <p>Now observe that (1) \\(H_i \\normal G\\) for all \\(i\\) since \\(G\\) is abelian and (2) \\(H_i \\cap H_j = \\{e\\}\\) and (3) </p> \\[  |G| = |H_1|\\cdot|H_2|\\cdots|H_k|. \\] <p>Therefore, we can repeatedly apply Theorem 1.\\ref{product_theorem} to conclude that </p> \\[ G \\cong H_1 \\times H_2 \\times \\cdots \\times H_k.            \\] <p>So \\(G\\) is a product of its Sylow subgroups. </p> <p> Let \\(G\\) be an abelian group and \\(p\\) a prime. Then if \\(G = p^n\\) for some positive integer \\(n\\), then \\(G\\) is isomorphic to a direct product of its cyclic groups. </p> <p> \\textcolor{NavyBlue}{We'll proceed with strong induction.} Consider our base case with \\(n = 1\\). Then we see that \\(G = p\\), and by the corollary to Lagrange's Theorem we know that this is cyclic.  <p>For the inductive case, suppose this statement holds up to \\(p^n\\). Let \\(G\\) be a group such that \\(|G| = p^{n+1}\\). Let \\(g\\) be an nontrivial element of \\(G\\), and consider the cyclic subgroup \\(\\left&lt; g \\right&gt;\\).</p> <p>Now define \\(H\\) as follows:</p> \\[ H = (G\\setminus\\left&lt; g\\right&gt;) \\cup \\{e\\} = \\{h \\in G \\mid h \\ne g^i \\text{ for } i = 1, 2, \\dots, m-1\\}.   \\] <p>We will show that this is a subgroup via the subgroup test.  First observe that \\(H\\) is nonempty, since we supposed that \\(|g| \\ne k+1\\). Therefore, let \\(h, h' \\in H\\). \\textcolor{purple}{Suppose for the sake of contradiction that \\(h^{-1} \\not\\in H\\).} That is, </p> \\[ h^{-1} = g^{j}  \\] <p>for some \\(j = 1, 2, \\dots, m-1\\). \\textcolor{purple}{Then \\(e  = hg^{j}\\).} But since the order of \\(g\\) is \\(m\\), we see that this implies that \\(h = g^{m - j} \\implies h \\in H\\). This is our contradiction so \\(h^{-1} \\in H\\). </p> <p>Since \\(h^{-1} \\in H\\), we see that \\(h^{-1} \\ne g^i\\) for any \\(i = 1, 2, \\dots, m-1\\). Since \\(h' \\in H\\) we see that </p> \\[ h'h^{-1} \\ne g^{i} \\text{ for any } i = 1, 2, \\dots, m-1. \\] <p>Thus \\(h'h^{-1} \\in H\\), and by the subgroup test we see that \\(H\\) is in fact a subgroup of \\(G\\). </p> <p>\\textcolor{NavyBlue}{The result follows immediately after this, but we will elaborate on why. }</p> <p>Note that \\(|\\left&lt; g\\right&gt;| = m \\ne 0\\) and \\(H \\cup \\left&lt;g \\right&gt; = G\\). Therefore, we see that \\(|H| &lt; |G|\\). Since \\(H\\) is a subgroup of \\(G\\), we know by Lagrange's Theorem that \\(|H|\\) divides \\(|G| = p^{k+1}\\). Hence, \\(|H| = p^j\\) for some \\(j &lt; k+1\\).</p> <p>By construction, we see that (1) \\(\\left&lt; g \\right&gt; \\cap H = \\{e\\}\\). Therefore </p> \\[ |H \\cdot K| =    \\] <p>By our inductive hypothesis, we know that \\(H\\) is isomorphic to a direct product of cyclic groups. </p> <p></p> <p>[ (Fundamental Theorem of Finite Abelian Groups)] Let \\(G\\) be a finite group. Then \\(G\\) is a direct product of cyclic groups. (Furthermore, these cyclic groups are Sylow \\(p\\)-groups.) </p> <p> The result follows immediately from the previous two lemmas.  <p>Note that any finite ablein group \\(G\\) is isomorphic to a direct product of its Sylow \\(p\\)-groups by Lemma \\ref{fund_ab_lemma_1}.  However, each Sylow \\(p\\)-group is isomorphic to a product of cyclic groups by Lemma \\ref{fund_ab_lemma_2}. Therefore, we have that \\(G\\) itself is isomorphic to a product of cyclic groups.  </p> <p>The Fundamental Theorem of Finite Abelian Groups is analagous to the fundamental theorem of arithmetic (hence the name). While the fundamental theorem of arithmetic allows us to completely factorize integers, the fundamental theorem of finite abelian groups allows us to factorize finite abelian groups. \\ Example. \\ Suppose we have an abelian group \\(G\\) of order 16. Then, up to an isomophism, \\(G\\) is isomorphic to one of the following:</p> \\[\\begin{gather*} \\ZZ/16\\ZZ\\\\ \\ZZ/8\\ZZ \\times \\ZZ2\\ZZ\\\\ \\ZZ4\\ZZ \\times \\ZZ4\\ZZ\\\\ \\ZZ4\\ZZ \\times \\ZZ2\\ZZ \\times \\ZZ2\\ZZ\\\\ \\ZZ2\\ZZ \\times \\ZZ2\\ZZ \\times \\ZZ2\\ZZ \\times \\ZZ2\\ZZ \\end{gather*}\\] <p>\\ Example. \\ Observe that \\(9000= 9\\cdot 5^3 \\cdot 2^3\\). We know that all abelian groups of order 9000 are going to be direct products of cyclic subgroups. In this case, we can represent the isomorphism with \\(\\mathbb{Z}/m\\mathbb{Z}\\) groups. Now because of the size of \\(G\\), we know that there are Sylow 9-, 5- and 2-subgroups of \\(G\\). Thus we can view \\(G\\) as a product of \\(\\mathbb{Z}/n\\mathbb{Z}\\) groups.\\ \\ For the sake of notation, we'll write that \\(\\mathbb{Z}/n\\mathbb{Z} = \\mathbb{Z}_n\\). We can then lists the groups as  \\setcounter{equation}{0}</p> \\[\\begin{gather} \\ZZ_9 \\times \\ZZ_{5^3} \\times \\ZZ_{2^3}\\\\ \\ZZ_9 \\times \\ZZ_{5^3} \\times (\\ZZ_{2^2} \\times \\ZZ_2)\\\\ \\ZZ_9 \\times \\ZZ_{5^3} \\times (\\ZZ_{2} \\times \\ZZ_2 \\times \\ZZ_2)\\\\ \\ZZ_9 \\times (\\ZZ_{5^2} \\times \\ZZ_5) \\times \\ZZ_{2^3} \\\\ \\ZZ_9 \\times (\\ZZ_{5^2} \\times \\ZZ_5) \\times (\\ZZ_{2^2} \\times \\ZZ_2)\\\\ \\ZZ_9 \\times (\\ZZ_{5^2} \\times \\ZZ_5) \\times (\\ZZ_{2} \\times \\ZZ_2 \\times \\ZZ_2)\\\\ \\ZZ_9 \\times (\\ZZ_{5} \\times \\ZZ_5 \\times \\ZZ_5) \\times \\ZZ_{2^3} \\\\ \\ZZ_9 \\times (\\ZZ_{5} \\times \\ZZ_5 \\times \\ZZ_5) \\times (\\ZZ_{2^2} \\times \\ZZ_2)\\\\ \\ZZ_9 \\times (\\ZZ_{5} \\times \\ZZ_5 \\times \\ZZ_5) \\times (\\ZZ_{2} \\times \\ZZ_2 \\times \\ZZ_2)\\\\ (\\ZZ_3 \\times \\ZZ_3) \\times \\ZZ_{5^3} \\times \\ZZ_{2^3} \\end{gather}\\] \\[\\begin{gather} (\\ZZ_3 \\times \\ZZ_3) \\times \\ZZ_{5^3} \\times (\\ZZ_{2^2} \\times \\ZZ_2)\\\\ (\\ZZ_3 \\times \\ZZ_3) \\times \\ZZ_{5^3} \\times (\\ZZ_{2} \\times \\ZZ_2 \\times \\ZZ_2)\\\\ (\\ZZ_3 \\times \\ZZ_3) \\times (\\ZZ_{5^2} \\times \\ZZ_5) \\times \\ZZ_{2^3} \\\\ (\\ZZ_3 \\times \\ZZ_3) \\times (\\ZZ_{5^2} \\times \\ZZ_5) \\times (\\ZZ_{2^2} \\times \\ZZ_2)\\\\ (\\ZZ_3 \\times \\ZZ_3) \\times (\\ZZ_{5^2} \\times \\ZZ_5) \\times (\\ZZ_{2} \\times \\ZZ_2 \\times \\ZZ_2)\\\\ (\\ZZ_3 \\times \\ZZ_3) \\times (\\ZZ_{5} \\times \\ZZ_5 \\times \\ZZ_5) \\times \\ZZ_{2^3} \\\\ (\\ZZ_3 \\times \\ZZ_3) \\times (\\ZZ_{5} \\times \\ZZ_5 \\times \\ZZ_5) \\times (\\ZZ_{2^2} \\times \\ZZ_2)\\\\ (\\ZZ_3 \\times \\ZZ_3) \\times (\\ZZ_{5} \\times \\ZZ_5 \\times \\ZZ_5) \\times (\\ZZ_{2} \\times \\ZZ_2 \\times \\ZZ_2) \\end{gather}\\] <p>(It's a christmas tree!) Recall the fact that \\(\\mathbb{Z}/mn\\mathbb{N} \\cong \\mathbb{Z}/n\\mathbb{Z} \\times \\mathbb{Z}/m\\mathbb{Z}\\) iff \\(\\mbox{gcd}(m, n) = 1\\). Thus we see that </p> <ul> <li> <p>[1.] \\(\\ZZ_9 \\not\\cong \\ZZ_3 \\times \\ZZ_3\\)</p> </li> <li> <p>[2.] \\(\\ZZ_{5^3} \\not\\cong \\ZZ_5 \\times \\ZZ_{5^2}\\) and \\(\\not\\cong \\ZZ_5 \\times \\ZZ_5 \\times \\ZZ_5\\)</p> </li> <li> <p>[3.] \\(\\ZZ_{2^3} \\not\\cong \\ZZ_2 \\times \\ZZ_{2^2}\\) and \\(\\not\\cong \\ZZ_2 \\times \\ZZ_2 \\times \\ZZ_2\\).</p> </li> </ul> <p>Therefore, we see that none of the groups (1) - (18) are isomorphic to each other, so this exhaustive list of abelian groups of order 9000 up to isomorphism is complete. \\ \\ It turns out that our fundamental theorem for finite abelian groups can actually be strengthened. This strengthened version isn't that useful, since it is sufficiently useful to know that every finite abelian group is a product of cyclic groups. Nevertheless its proof is fun. </p> <p> Let \\(G\\) be a finite abelian group. Then there exist integers \\(a_1, a_2, \\dots, a_k\\) such that  \\[ G \\cong \\ZZ/a_1\\ZZ \\times \\ZZ/a_2\\ZZ \\times \\dots \\ZZ/a_k\\ZZ \\] <p>where \\(a_i \\mid a_{i+1}\\). </p> <p> <p>Let \\(G\\) be a finite abelian group and suppose \\(|G| = p_1^{k_1}p_2^{k_2} \\cdots p_n^{k_n}\\). Since \\(G\\) is abelian,  we know by Lemma \\ref{fund_ab_lemma_1} that it is isomorphic to a  product of Sylow subgroups. Therefore, we see that </p> \\[ G \\cong H_1 \\times H_2 \\times \\cdots \\times H_n   \\] <p>where for some \\(H_1, H_2, \\dots H_n\\) Sylow subgroups, and  \\(|H_i| = p_i^{k_i}\\). However, observe that for each \\(i \\le n\\),  </p> \\[ H_i \\cong \\underbrace{**(**\\mathbb{Z}/p_i\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_i\\mathbb{Z}**)** \\times \\cdots \\times **(**\\mathbb{Z}/p_i\\mathbb{Z}**)**.}_{k_i\\text{-many times}} \\] <p>Substituting for each \\(H_i\\), we then have that </p> \\[\\begin{align*} G \\cong \\overbrace{**(**\\mathbb{Z}/p_1\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_1\\mathbb{Z}**)** \\times \\cdots \\times **(**\\mathbb{Z}/p_1\\mathbb{Z}**)**}^{k_1\\text{-many times}} \\times  \\overbrace{**(**\\mathbb{Z}/p_2\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_2\\mathbb{Z}**)** \\times \\cdots \\times **(**\\mathbb{Z}/p_2\\mathbb{Z}**)**}^{k_2\\text{-many times}} \\times\\\\  \\cdots \\times  \\underbrace{**(**\\mathbb{Z}/p_n\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_n\\mathbb{Z}**)** \\times \\cdots \\times **(**\\mathbb{Z}/p_n\\mathbb{Z}**)**}_{k_n\\text{-many times}}. \\end{align*}\\] <p>Therefore, we can rewrite \\(G\\) as </p> \\[\\begin{align*} G \\cong  **(**\\mathbb{Z}/p_1\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_1\\mathbb{Z}**)** \\times \\cdots \\times \\Big(**(**\\mathbb{Z}/p_1\\mathbb{Z}**)** \\times  **(**\\mathbb{Z}/p_2\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_2\\mathbb{Z}**)** \\times \\cdots \\Big)\\\\ \\times \\Big( **(**\\mathbb{Z}/p_2\\mathbb{Z}**)** \\times **(**\\mathbb{Z}/p_3\\mathbb{Z}**)** \\times **(**\\mathbb{Z}/p_3\\mathbb{Z}**)** \\times \\cdots \\Big) \\\\ \\times \\cdots \\Big(**(**\\ZZ/p_{n-2}\\ZZ**)** \\times **(**\\ZZ/p_{n-1}\\ZZ**)** \\times \\cdots \\times **(**\\ZZ/p_{n-1}\\ZZ**)** \\Big)\\\\  \\times \\Big(**(**\\ZZ/p_{n-1}\\ZZ**)** \\times **(**\\mathbb{Z}/p_n\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_n\\mathbb{Z}**)** \\times \\cdots \\times **(**\\mathbb{Z}/p_n\\mathbb{Z}**)** \\Big). \\end{align*}\\] <p>That is, we can factor it into a product where the \\(i\\)-th factor includes one \\(\\ZZ/p_i\\ZZ\\) factor and \\(k_{i+1}-1\\) many factors of \\(\\ZZ/p_{i+1}\\ZZ\\).</p> <p>Let us make the following observation. By Theorem 1.\\ref{zmod_iso_thm} we know that  \\setcounter{equation}{0}</p> \\[\\begin{align} \\ZZ/hm\\ZZ \\cong \\ZZ/h\\ZZ \\times \\ZZ/m\\ZZ        \\end{align}\\] <p>since \\(\\mbox{gcd}(h, m) = 1\\). Thus we can collapse the products (in the last equation of \\(G\\))  back together to observe that </p> \\[\\begin{align*} G \\cong (\\ZZ/p_1^{k_1 - 1}\\ZZ) \\times (\\ZZ/p_1p_2^{k_2-1}\\ZZ) \\times \\cdots \\times (\\ZZ/p_{n-1}p_n^{k_n}\\ZZ) \\end{align*}\\] <p>since by repeated application of equation (1), </p> \\[ \\ZZ/p_ip_{i+1}^{k_{i+1} - 1} \\cong \\ZZ/p_i\\ZZ\\times \\overbrace{\\ZZ/p_{i + 1}\\ZZ \\times \\cdots \\times \\ZZ/p_{i + 1}\\ZZ}^\\text{$(k_{i+1} - 1)-$many times}         \\] <p>for \\(1 &lt; i &lt; n -2\\), and </p> \\[ \\ZZ/p_{n-1}p_n^{k_n}\\ZZ \\cong **(**\\ZZ/p_{n-1}\\ZZ**)** \\times \\overbrace{**(**\\mathbb{Z}/p_n\\mathbb{Z**)**} \\times **(**\\mathbb{Z}/p_n\\mathbb{Z}**)** \\times \\cdots \\times **(**\\mathbb{Z}/p_n\\mathbb{Z}**)**}^\\text{$k_n-$many times}. \\] <p>Since we have that </p> \\[ G \\cong (\\ZZ/p_1^{k_1 - 1}\\ZZ) \\times (\\ZZ/p_1p_2^{k_2-1}\\ZZ) \\times \\cdots \\times (\\ZZ/p_{n-1}p_n^{k_n}\\ZZ) \\] <p>if we let \\(a_1 = p_1^{k_1 - 1}\\) and  \\(a_i = p_{i-1}p_{i}^{k_{i} - 1}\\) for \\(1 &lt; i &lt; n\\) and \\(a_k = p_{n-1}p_n^{k_n}\\), then we see that </p> \\[ G \\cong  \\mathbb{Z}/a_1\\mathbb{Z} \\times \\mathbb{Z}/a_2\\mathbb{Z} \\times \\cdots \\mathbb{Z} / a_k \\mathbb{Z}  \\] <p>where \\(a_i \\big| a_{i + 1}\\) for \\(0 &lt; i &lt; n\\), as desired. </p>"},{"location":"algebra/Groups/Group%20Actions./","title":"1.9. Group Actions.","text":"<p>As we shall see, a group action is a special type of mapping one can formulate involving a group \\(G\\) and an arbitrary set of objects \\(X\\). Specifically, it is a mapping from \\(G \\times X \\to X\\). Thus, a group action is said to make a group \\(G\\) \"act\" on a set \\(X\\). It is through this perspective that one can then view group actions as permutations of a set \\(X\\). This becomes more clear with the formal definition. </p> <p> Let \\(G\\) be a group and \\(X\\) an arbitrary set. A \\textbf{group action} of \\(G\\) on \\(X\\) is a mapping \\(* : G \\times X \\to X\\) that  <ul> <li> <p>[1.] \\(g_1 * (g_2 * x) = (g_1 \\cdot g_2) * x\\) for all \\(g_1, g_2 \\in G, x \\in X\\).</p> </li> <li> <p>[2.] \\(e * x = x\\) where \\(e \\in G\\) is the identity.</p> </li> </ul> <p> Note that \\(\\cdot\\) is the group multiplication in \\(G\\). For notational convenience, we will surpress \\(\\cdot\\) in the cases for where it's obvious or implied, as usual. \\ We also note that we could have defined \\(*:X \\times G \\to X\\). For simplicity, we let \\(G\\) act on the left. \\  </p> <p>Let's breakdown what this is really saying. \\textcolor{red}{For a group action \\(*\\) of \\(G\\) acting on \\(X\\), we have for all \\(g \\in G\\), \\(x \\in X\\), the product \\(g * x\\) is mapped to some element \\(x' \\in X\\).}</p> <p>Now observe that if we replaced \\(X\\) with \\(G\\), then we get \\(* : G \\times G \\to G\\). Thus \\(*\\) would just permutate the elements of \\(G\\). Furthermore, if we let \\(*\\) be the group multiplication \\(\\cdot\\) which is already defined in \\(G\\), then we just get back the definition of a group! </p> <p>\\textcolor{NavyBlue}{This fits with the intuition that, group multiplication of elements (e.g., \\(g \\cdot g'\\) where \\(g, g' \\in G\\)) simply permutates the elements of a group. That is, if  you placed the elements of \\(G\\) in a tuple such as </p> \\[ (g_1, g_2, \\dots, g_n) \\] <p>and multiplied this by some \\(g' \\in G\\), you would get a tuple </p> \\[ (g_1, g_2, \\dots, g_n) \\cdot g'  = (g_1\\cdot g', g_2 \\cdot g' , \\dots, g_n\\cdot g') = (g_i, g_j, \\dots, g_k) \\] <p>containing all the elements of \\(G\\), but just in a different order. (In this case we supposed \\(g_1 \\cdot g' = g_i, g_2, \\cdot g' = g_j\\), and so on.) } </p> <p>\\begin{minipage}{0.7\\textwidth} \\vspace{.3cm} This permutation phenonmenon can be found in more general group actions.  For a fixed \\(g \\in G\\), define \\(\\sigma_g: X \\to X\\) as </p> \\[ \\sigma_g(x) = g * x. \\] <p>So \\(\\sigma_g\\) maps each \\(x\\) to some other element \\(x' \\in X\\). Therefore, a group action can be thought of as a set of maps \\(\\sigma_g\\), one for every element \\(g \\in G\\), each of which can appropriately be composed with one another. </p> <p>That's why it can be thought of as a permutation. The diagram on the right gives an illustration how this plays out for one particular \\(g \\in G\\) acting on a set \\(X\\) with five elements. \\end{minipage}\\hfill \\begin{minipage}{0.2\\textwidth} \\begin{tikzpicture} \\draw (-0.8,0.5)--(1.8,0.5); \\draw[-&gt;] (0,0) -- (1,0); \\draw[-&gt;] (0,-1) -- (1,-1); \\draw[-&gt;] (0,-2) -- (1,-2); \\draw[-&gt;] (0,-3) -- (1,-3); \\draw[-&gt;] (0,-4) -- (1,-4);</p> <p>\\node at (-0.4, .8) {\\(X\\)}; \\node at (-0.4, 0) {\\(x_1\\)}; \\node at (-0.4, -1) {\\(x_2\\)}; \\node at (-0.4, -2) {\\(x_3\\)}; \\node at (-0.4, -3) {\\(x_4\\)}; \\node at (-0.4, -4) {\\(x_5\\)};</p> <p>\\node at (0.5, 0.2) {\\(\\sigma_g\\)}; \\node at (0.5, -0.8) {\\(\\sigma_g\\)}; \\node at (0.5, -1.8) {\\(\\sigma_g\\)}; \\node at (0.5, -2.8) {\\(\\sigma_g\\)}; \\node at (0.5, -3.8) {\\(\\sigma_g\\)};</p> <p>\\node at (1.4, 0.8){\\(X\\)}; \\node at (1.4, 0) {\\(x_2\\)}; \\node at (1.4, -1) {\\(x_3\\)}; \\node at (1.4, -2) {\\(x_5\\)}; \\node at (1.4, -3) {\\(x_1\\)}; \\node at (1.4, -4) {\\(x_4\\)};</p> <p>\\end{tikzpicture}</p> <p>\\end{minipage}  \\vspace{.5cm}</p> <p>\\textcolor{NavyBlue}{Here's another way to think about a group action. If \\(G\\) acts on \\(X\\), then the group action \\(*\\) turns each  and every element of \\(g \\in G\\) into a function, which maps \\(X\\) to \\(X\\). This agrees with our intuition, since a permutation is exactly a function of \\(X\\) to itself. }</p> <p> A finite group of order \\(n\\) is isomorphic to a subgroup of \\(S_n\\). </p> <p>This theorem is a powerful theorem that gives us a new way to think about finite groups. It states that every finite group is basically the same as a subgroup of a symmetric group up to an isomorphism. </p> <p> \\textcolor{NavyBlue}{To prove this, we'll first construct a group action of \\(G\\) on itself. Then we'll } <p>Consider the group action of \\(G\\) acting on itself, whereby we define \\(g_1 \\cdot g_2 = g_1g_2\\) for \\(g_1, g_2 \\in G\\). That is, the group action  mapping is simply the multiplication used between the elements of \\(G\\).  \\begin{description} \\item[This is a Group Action.]  (Note: we already pointed out that if we replace \\(X\\) with \\(G\\) in the definition of a group action, and let \\(\\cdot\\) be the group multiplication in \\(G\\), then we just get the definition of a group. Thus a group is a special, but boring, type group action.)</p> <p>To show this is a group action, let \\(x \\in G\\). Then </p> \\[ g_1 \\cdot (g_2 \\cdot x) = g_1 \\cdot (g_2x) = g_1g_2x = (g_1g_2)x = (g_1g_2) \\cdot x. \\] <p>for \\(g_1, g_2 \\in G\\). Therefore, \\(g_1 \\cdot (g_2 \\cdot x) = (g_1g_2) \\cdot x.\\) The second axiom is satisfied, since if \\(e\\) is the identity of \\(G\\), then clearly \\(e \\cdot x = ex = x\\). We have both axioms satisfied. So this is a group action.  \\end{description} </p> <p>Before we lead up to a powerful theorem involving group actions, we must define a few definitions. </p> <p> Suppoe \\(G\\) acts on a set \\(X\\), and let \\(x \\in X\\). Then we define the set  \\[ Gx = \\{g * x \\mid g \\in G \\} \\] <p>as the orbirt of \\(x\\).  </p> <p>The orbit basically considers the set of all images one obtains when one grabs a single element of \\(x \\in X\\), and multiplies it by every element \\(g \\in G\\). {\\color{purple}{Note that since \\(g \\cdot x \\in X\\) for every \\(g \\in G, x \\in X\\), we have that \\(Gx \\subset X\\).}}</p> <p>\\textcolor{NavyBlue}{Orbits are rather interesting since \\textbf{they partition their acting set \\(X\\)}. That is, if \\(X = \\{x_1, x_2, \\dots, x_n\\}\\), then  \\(Gx_1 \\cup Gx_2 \\cup \\cdots \\cup Gx_n = X\\). Note that \\(x_i \\in Gx_i\\) for \\(i = 1, 2, \\dots, n\\), so this definitely makes sense. \\ \\ However, it is possible that \\(Gx_i = Gx_j\\) for some \\(i, j.\\) In such a case we note that for each \\(g \\in G\\) there exists a \\(g' \\in G\\) such that \\(gx_i = g'x_j \\implies g^{-1}g'x_j = x_i\\). Since \\(g^{-1}g' \\in G\\),  our condition boils down to the following:  \\(Gx_i = Gx_j\\) for some \\(i, j\\) if there exists a \\(g \\in G\\) such that  \\(gx_j = x_i\\). So \\(Gx_i = Gx_j\\) if \\(x_j \\in Gx_i\\).} \\ \\ Thus, these things are behaving like cosets (recall that \\(Gh = Gh'\\) if and only if \\(h' \\in Gh\\).) and they partition the acting set \\(X\\)! This understanding will become helpful in the future.  \\ \\ Since orbits form partitions, and it is possible that the set of all orbits will be redundant (i.e., it's possible that \\(Gx_i = Gx_j\\) for some \\(i, j\\)), we offer the following definition.</p> <p> Let \\(G\\) be a group, and suppose it acts on a set \\(X\\).  Let \\(Gx_1, Gx_2, \\cdots, Gx_n\\) be a distinct set of orbits  such that  \\[ Gx_1 \\cup Gx_2 \\cup \\cdots G_n = X. \\] <p>Then each \\(x_1, x_2, \\dots, x_n\\) are called representatives of an orbit of \\(G\\). We generally denote \\(R = \\{x_1, x_2, \\dots x_n\\}\\) to be the set of representatives of the orbits.  </p> <p>Thus for some orbit \\(Gx_i\\), we say that \\(x_i\\) \"represents\" this orbit. We make this definition since we just showed that  it doesn't really matter what representative we pick, since if  \\(x_j \\in Gx_j\\), \\(Gx_j = Gx_i\\), so \\(x_j\\) could have equally represented this orbit. Thus given this arbitrary-ness, the definition allows us to talk about orbits more easily.</p> <p>We now offer another definition regarding group actions.</p> <p> Suppose \\(G\\) acts on \\(X\\), and \\(x \\in X\\). Then the set  \\[ G_x = \\{g \\in G \\mid g * x = x\\}. \\] <p>is defined to be the stabilizer of \\(x\\). </p> <p>The stabilizer considers the elements of \\(g \\in G\\) which act as an identity to \\(x\\). \\textcolor{purple}{Since \\(G_x\\) considers elements of \\(G\\), we see that \\(G_x \\subset G\\). Furthermore, we have the following proposition.}</p> <p> Suppose \\(G\\) acts on \\(X\\), and let \\(x \\in X\\). Then \\(G_x \\le G\\). </p> <p> Observe first that this is nonempty, since \\(e * x = x\\) for all \\(x \\in X\\), where \\(e \\in G\\) is the identity. Therefore \\(e \\in G_x\\). Next, observe that associativity is inherited from the set \\(G\\) itself. To check for inverses, we note that for any \\(g \\in G\\), \\(g \\cdot x = x\\),  so we can multiply both sides by \\(g^{-1}\\) to get \\[   g^{-1} * g * x = g^{-1} * x \\implies (g^{-1}g) * x = g^{-1} * x  \\implies x = g^{-1} * x. \\] <p>Thus \\(g^{-1} * x = x\\) so \\(g^{-1} \\in G\\). Finally, observe that the set is closed. Given \\(g, g' \\in G_x\\), we see that </p> \\[ (gg') * x = g * (g' * x) = g *`' (x) = x. \\] <p>Therefore \\(G_x\\) is (1) a subset of \\(G\\) and (2) a group so it is a subgroup of \\(G\\).</p> <p> We now move onto one of the useful theorems that arises once one realizes the definitions of the orbit and stabilizers.</p> <p> Let \\(G\\) be a finite group, and suppose \\(G\\) acts on a set \\(X\\). Then for any \\(x \\in X\\) we have that  \\[ |G| = |Gx| \\cdot |G_x|. \\] <p></p> <p> To show this, we'll construct a bijection between \\(Gx\\) and \\(G/G_x\\).  <p>Let \\(g \\in G\\) so that \\(gG_x \\in G/G_x\\). Then construct the map \\(\\psi: G/G_x \\to Gx\\) by</p> \\[ \\psi(gG_x) = g * x. \\] <p>Note that there is only one element in \\(G/G_x\\) which gets to \\(x\\); namely, \\(G_x\\). The calculation is as follows:</p> \\[ \\psi(G_x) = e * x = x. \\] <p>This map is obviously surjective, since for any \\(x' \\in Gx\\), we we know that there exists a \\(g \\in G\\) such that \\(g * x = x'\\). Thus \\(g \\not\\in G_x\\), so that \\(gG_x\\) is nontrivial and \\(\\psi(gG_x) = g * x = x'\\).</p> <p>Now to show that this is injective, suppose that \\(g*x = h*x\\). we have that \\(g^{-1}h * x = x\\). Therefore, \\(gh^{-1} \\in G_x\\). Furthermoresee that </p> \\[ g^{-1}hG_x = G_x \\implies hG_x = gG_x. \\] <p>Thus this can only happen if the input is the same. Therefore this is a one-to-one and onto mapping. </p> <p>Since this is a bijection, we can conclude that </p> \\[ |Gx| = |G/G_x| = |G|/|G_x| \\implies |G| = |Gx||G_x|     \\] <p>as desired. </p>"},{"location":"algebra/Groups/Homomorphism%20and%20Isomorphisms./","title":"1.3. Homomorphism and Isomorphisms.","text":"<p>{\\color{BlueViolet}As with all mathematical objects, now that we have a well defined abstract concept (i.e., a group) we'll now be interested attempting to understand mappings between different groups. Mappings of abstract concepts simply helps mathematicians get a better sense of what they're dealing with, and most often provides new insight into understand their objects. </p> <p>The most important utility of the following definition is that it not only leads one to have a better understanding of groups, but it also helps us understand when two groups are equivalent. For example, \\(D_3\\) and \\(S_3\\) equivalent, since one could view \\(D_3\\) as simply all the permutations of 1, 2, and 3, if we assigned these numbers to the vertices of a triangle. }</p> <p> Let \\((G, \\cdot)\\) and \\((G', *)\\) be groups. A homomorphism is a mapping \\(\\phi: G \\to G'\\) such that, for all \\(a, b \\in G\\),  \\[ \\phi(a \\cdot b) = \\phi(a) * \\phi(b). \\] <p>{\\color{red}Again, here \\(*\\) is the group operation of \\(G'\\).}</p> <p></p> <p>Example. Consider the two groups \\(GL_n(\\mathbb{R})\\) and \\(\\mathbb{R}\\setminus\\{0\\}\\). If we define \\(\\phi\\) such that, for \\(A \\in GL_n(\\mathbb{R})\\) </p> \\[ \\phi(A) = \\det(A) \\] <p>then \\(\\phi\\) defines a homomorphism. </p> <p>Recall that for for any \\(n \\times n\\) matrices \\(A, B\\) that \\(\\det(AB) = \\det(A)\\det(B)\\). Therefore </p> \\[ \\phi(AB) = \\det(AB) = \\det(A)\\det(B) = \\phi(A)\\phi(B). \\] <p>Since \\(\\phi(AB) = \\phi(A)\\phi(B)\\), we see that \\(\\phi\\) satisfies the condition to be a homomorphism.</p> <p> Let \\(\\phi: G \\to G'\\) be a homomorphism. Then all of the following hold. <ul> <li> <p>[1.] If \\(e_G\\) is the identity of \\(G\\) and \\(e_{G'}\\) is the identity of \\(G'\\), then \\(\\phi(e_G) = e_{G'}\\).</p> </li> <li> <p>[2.] For all \\(g \\in G\\), \\(\\phi(g^{-1}) = \\phi(g)^{-1}\\). </p> </li> <li> <p>[3.] For \\(g_1, g_2, \\dots, g_n \\in G\\), then \\(\\phi(g_1 \\cdot g_2 \\cdot \\dots \\cdot g_n) = \\phi(g_1)\\phi(g_2)\\cdots\\phi(g_n)\\). Consequently, if \\(g = g_1 = g_2 = \\cdots = g_n\\), then \\(\\phi(g^n) = \\phi(g)^{n}\\).</p> </li> </ul> <p></p> <p> Let \\(g \\in G\\), and suppose \\(\\phi: G \\to G'\\) is a homomorphism.  <ul> <li>[1.] Since \\(e_G = e_G \\cdot e_G\\), we have that </li> </ul> \\[ \\phi(e_G) = \\phi(e_G \\cdot e_G) = \\phi(e_G)\\phi(e_G). \\] <p>We also know that \\(\\phi(e_G) \\in G'\\), and becuase \\(G'\\) is a group, there exists an inverse \\(\\phi(e_G)^{-1} \\in G\\) of \\(\\phi(e_G)\\). Multiplying this on the left (or right) yields</p> \\[ e_{G'} = \\phi(e_G)   \\] <p>as desired.</p> <ul> <li>[2.] Since \\(gg^{-1} = e_G\\), and by (1.) we know that \\(\\phi(e_G) = e_{G'}\\). Hence </li> </ul> \\[ \\phi(e_G) = e_{G'} \\implies \\phi(gg^{-1}) = e_{G'}  \\implies \\phi(g)\\phi(g^{-1}) = e_{G'}. \\] <p>Again, \\(\\phi(g) \\in G'\\), and since \\(G'\\) is a group there exist an inverse \\(\\phi(g)^{-1} \\in G\\) of \\(\\phi(g)\\). Multiplying on the left by this inverse, we get </p> \\[ \\phi(g)\\phi(g^{-1}) = e_{G'} \\implies \\phi(g^{-1}) = \\phi(g)^{-1} \\] <p>as desired.</p> <ul> <li>[3.] This is just repeated application of the homomorphism property.  For \\(g_1, g_2, \\dots g_n \\in G\\), \\(g_1 \\cdot g_2 \\cdot \\hspace{0.01mm} \\dots \\hspace{0.01mm} \\cdot g_n = g_1 \\cdot (g')\\)  where \\(g' = g_2 \\cdot g_3 \\cdot \\hspace{0.01mm} \\dots \\hspace{0.01mm} \\cdot g_n\\). Applying the homomorphism property, </li> </ul> \\[ \\phi(g_1 \\cdot g_2 \\cdot \\hspace{0.01mm} \\dots \\hspace{0.01mm} \\cdot g_n) = \\phi(g_1 \\cdot g') = \\phi(g_1) \\phi(g'). \\] <p>Repeatedly applying the same idea, starting again with the product \\(g_2 \\cdot g_3 \\cdot \\hspace{0.01mm} \\dots \\hspace{0.01mm} \\cdot g_n\\) yields the result. The fact that \\(\\phi(g^n) = \\phi(g)^n\\) is follows immediately.</p> <p> {\\color{Plum}  If \\(\\phi\\) is a bijective homomorphism (i.e., one-to-one and onto) then we say that \\(\\phi\\) is an isomorphism. Furthermore, if there exists an isomorphism between two spaces \\(G\\) and \\(G'\\), then we say these spaces are isomorphic and that \\(G \\cong G'\\). As we'll soon see, isomorphisms gives us really nice results (hence the special terminology and notation). In addition, it can sometimes be difficult to tell when two groups \\(G\\) and \\(G'\\) are the same or different. Isomorphisms can help determine when there isn't such an equivalence.</p> <p>As we'll see, the concept of an isomorphism is very powerful. However, proving it may not be that simple, and in ceratin cases the following theorem will be very useful. }</p> <p> Let \\(G\\) and \\(H\\) be groups. The homomorphism \\(\\phi: G \\to H\\) is an isomorphism if and only if there exists a homomorphism \\(\\psi: H \\to G\\) such that \\(\\psi \\circ \\phi\\) is the identity map on \\(G\\) and \\(\\phi \\circ \\psi\\) is the identity map on \\(H\\). </p> <p> (\\(\\implies\\)) Suppose \\(\\phi: G \\to H\\) is an isomorphism. Since \\(\\phi\\) is bijective, define the inverse map \\(\\phi^{-1}: H \\to G\\) such that if \\(\\phi(g) = g'\\) then \\(\\phi^{-1}(g') = g\\).  <p>Note that this is a well defined map due to the surjectivity and injectivity of \\(\\phi\\). To show it is a homomorphism, we need to demonstrate that \\(\\phi^{-1}(h_1\\cdot h_2) = \\phi^{-1}(h_1)\\phi^{-1}(h_2)\\). Thus  observe that for \\(h_1, h_2 \\in H\\) there exist \\(g_1, g_2 \\in G\\) such that \\(\\phi(g_1) = h_1\\) and \\(\\phi(g_2) = h_2\\). Therefore</p> \\[ \\phi(g_1 \\cdot g_2) = h_1 \\cdot h_2 \\implies \\phi^{-1}(h_1 \\cdot h_2) = g_1\\cdot g_2 = \\phi^{-1}(h_1)\\cdot\\phi^{-1}(h_2). \\] <p>Thus \\(\\phi^{-1}\\) is a homomorphism.</p> <p>Now observe that for all \\(g \\in G\\) we have that \\(\\phi^{-1} \\circ \\phi(g) = g\\) and for all \\(h \\in H\\), \\(\\phi \\circ \\phi^{-1}(h) = h.\\) Thus \\(\\phi^{-1} \\circ \\phi\\) is the identity on \\(G\\) while \\(\\phi \\circ \\phi^{-1}\\) is the identity on \\(H\\), which proves this direction.</p> <p>(\\(\\impliedby\\)) Now suppose \\(\\phi: G \\to H\\) is a homomorphism and that there exists a homomorphism \\(\\psi: H \\to G\\) such that \\(\\psi \\circ \\phi\\) is the identity map on \\(G\\) and \\(\\phi \\circ \\psi\\) is the identity map in \\(H\\). In other words, \\(\\psi\\) and \\(\\phi\\) are inverses of each other.  Thus \\(\\phi\\) is a bijection function from \\(G \\to H\\), which implies that \\(\\phi\\) is an isomorphism.  </p> <p>We also introduce the following criteria which is frequently used to evaluate if a homomorphism is one-to-one and/or onto. </p> <p> Let \\(\\phi: G \\to G'\\) be a homomorphism. Then  <ul> <li> <p>[1.] \\(\\phi\\) is one-to-one if and only if \\(\\mbox{ker}(\\phi)\\) is trivial. That is, \\(\\mbox{ker}(\\phi) = \\{e_G\\}\\), where \\(e_G\\) is the identity of \\(G\\).</p> </li> <li> <p>[2.] \\(\\phi\\) is onto if and only if \\(\\mbox{im}(\\phi) = G'\\). </p> </li> </ul> <p>Therefore, \\(\\phi\\) is an isomorphism if and only if (1) and (2) hold. </p> <p> <ul> <li>[1.] Suppose \\(\\phi\\) is one-to-one. By proposition 1.1.1, we know that \\(\\phi(e_G) = e_{G'}\\). But since \\(\\phi\\) is injective we know \\(e_G\\) is the only element in \\(G\\) which is mapped to \\(e_{G'}\\). Therefore \\(\\mbox{ker}(\\phi) = \\{e_G\\}\\).</li> </ul> <p>Now suppose \\(\\mbox{ker}(\\phi) = \\{e_G\\}\\). To show \\(\\phi\\) is one-to-one, consider \\(g, h \\in G\\) such that</p> \\[ \\phi(g) = \\phi(h). \\] <p>Multiplying both sides by \\(\\phi(h)^{-1}\\) we get </p> \\[ \\phi(g)\\phi(h)^{-1} = e_{G'}. \\] <p>By proposition 1.1.2, we know that \\(\\phi(h)^{-1} = \\phi(h^{-1})\\). Since \\(\\phi\\) is a homomorphism, we can then combine the terms to get </p> \\[ \\phi(gh^{-1}) = e_{G'}. \\] <p>Since \\(\\mbox{ker}(\\phi) = \\{e_G\\}\\), we see that </p> \\[ gh^{-1} = e_G \\implies g = h.                 \\] <p>Therefore \\(\\phi\\) is one to one.</p> <ul> <li>[2.] Suppose \\(\\phi\\) is onto. Then \\(\\mbox{im}(\\phi) = G'\\) is just another way of stating this fact. </li> </ul> <p>Suppose \\(\\mbox{im}(\\phi) = G'\\). Then for every element \\(g' \\in G'\\), there exists \\(g \\in G\\) such that \\(\\phi(g) = g'\\). That is, \\(\\phi\\) covers every value in \\(G'\\) so that it is onto.</p> <p>Thus, we have that a function is isomorphic if and only if it is one to one and onto. Hence, it is isomorphic if and only if (1) and (2) hold. </p> <p>We also make two common definitions for special homomorphisms. </p> <p> Let \\(G\\) be a group. <ul> <li> <p>[1.] If \\(\\phi: G \\to G\\) is a group homomorphism, then we say that \\(\\phi\\) is a endomorphism.</p> </li> <li> <p>[2.] If \\(\\phi\\) is a bijective endomorphism (an isomophic endomorphism) then we say that \\(\\phi\\) is an automorphism.</p> </li> </ul> <p></p> <p> The set of all automorphisms of a group \\(G\\), denoted as \\(\\text{Aut}(G)\\), forms a group with an operation \\(\\circ\\) of function composition. </p> <p> We can prove this directly.  \\begin{description} \\item[Closure.] Let \\(\\phi\\) and \\(\\psi\\) be automorphisms. Then \\(\\phi \\circ \\psi\\) is (1) a homomorphism from \\(G \\to G\\) and (2) a bijection (as the composition of bijections is a bijetion). <p>\\item[Associativity.] In general, function composition is associative. </p> <p>\\item[Identity.] Let \\(i:G \\to G\\) be the identity map. The (1) \\(i\\) is a group homomorphism and (2) a bijection. Therefore \\(i \\in \\text{Aut}(G)\\) and we can set \\(i\\) as the identity of the group. Note that </p> \\[ i \\circ \\phi = \\phi = \\phi \\circ i    \\] <p>for any \\(\\phi \\in \\text{Aut}(G)\\). </p> <p>\\item[Inverse.] Let \\(\\phi \\in \\text{Aut}(G)\\). Construct the function \\(\\phi^{-1}\\) as follows. If \\(\\phi(g) = g'\\) for some \\(g, g' \\in G\\), then write \\(\\phi^{-1}(g') = g\\). Such an assignment is well-defined since  \\(\\phi\\) is a bijection. Hence we see that </p> \\[ \\phi \\circ \\phi^{-1} = i = \\phi^{-1} \\circ \\phi. \\] <p>Finally, observe that \\(\\phi^{-1}\\) is (1) a homomorphism and (2) a bijection, so we see that \\(\\phi^{-1} \\in \\text{Aut}(G)\\). Therefore this forms a group. \\end{description} </p>"},{"location":"algebra/Groups/Isomorphism%20Theorems/","title":"1.8. Isomorphism Theorems","text":"<p>With our knowledge of homomorphisms, normality and quotient groups, we are now able to develop four important theorems, known as the isomorphism theorems, which are indispensible tools in group theory. The isomorphism theorems give isomorphic relations which we can use to our advntage to understand groups and aid our proofs. </p> <p>The isomorphism theorems are very deep theorems in abstract algeba. While one may go deeper into algebra, they will come across isomorphism theorems analagous to the ones below again and again.</p> <p>[ (First Isomorphism Theorem)]  Let \\(\\phi: G \\to G'\\) be a homomorphism. Then  \\[ G/\\mbox{ker}(\\phi) \\cong \\mbox{im}(\\phi). \\] <p>\\vspace{-5mm}  This is one of the more useful isomorphism theorems, and says something that matches out intuition. That is, if we quotient out the \\(\\mbox{ker}(\\phi)\\), i.e., the set of all elements which get mapped to 0, then we should obtain something isomorphic to \\(\\mbox{im}(\\phi)\\).</p> <p> \\textcolor{Plum}{We'll prove this directly. That is, we'll create a homomorphism between \\(G/\\ker(\\phi)\\) and \\(\\im(\\phi)\\), and then show that this homomorphism is one-to-one and onto, and therefore bijective. Thus the groups will be isomorphic.} <p>Let \\(\\phi: G' \\to G\\) be a homorphism. Write \\(K = \\ker(\\phi)\\).  Define \\(\\psi: G/K \\to \\im(\\phi)\\) as </p> \\[ \\psi(gK) = \\phi(g) \\] <p>where \\(gK \\in G/K\\) and \\(g \\in G\\). </p> <p>(\\textcolor{red}{We'll use left cosets (\\(gK\\)) to talk about elements in \\(G/K\\) to remind the reader that left cosets can be used to characterize a quotient group just as as right cosets can.})</p> <p>\\textcolor{NavyBlue}{We want this to be a homomorphism. But we pulled this function out of nowhere, so let's check if this is well-defined.} \\begin{description} \\item[Well-Defined.] Suppose \\(g' \\in gK\\). Then \\(gK = g'K\\), and our goal will be to show that \\(\\psi(gK) = \\psi(g'K)\\). Since \\(g' \\in gK\\), there exists a \\(k \\in K\\) such that  \\(gk = g'\\). Then  </p> \\[ \\psi(g'K) = \\psi((gk)K) = \\psi(gK) = \\phi(g) \\] <p>while </p> \\[ \\psi(gK) = \\phi(g). \\] <p>Therefore \\(\\psi(g'K) = \\psi(gK)\\), so the representative \\(g\\) or \\(g'\\) does not matter. \\end{description} \\textcolor{NavyBlue}{Now that we know this function is not nonsense, we move on to showing it is a homomorphism.} \\begin{description} \\item[It's a Homomorphism.] Let's justify that this is a homorphism. For \\(gK, g'K \\in G/K\\), </p> \\[\\begin{align*} \\psi(gK \\cdot g'K) =  \\psi((gg')K) = \\phi(gg')\\\\ = \\phi(g)\\phi(g') = \\psi(gK)\\psi(g'K) \\end{align*}\\] <p>where in the second step we used the fact that \\(\\phi\\) itself is a homomorphism. Thus we have that \\(\\psi\\) is a homomorphism.  \\end{description} \\textcolor{NavyBlue}{We'll now show this is a bijective homomorphism, thereby proving the desired isomorphism.} \\begin{description} \\item[One-to-One.] To show this is one-to-one, we can use Theorem 1.\\ref{theorem_isomorph}. Thus our goal will be to show that \\(\\ker(\\psi) = \\{e_G\\}\\), the identity element of \\(G\\).</p> <p>Suppose </p> <p>[ \\psi(gK) = e ] which is the identity in \\(\\im(\\phi)\\) (technically, the identity in \\(G'\\)). Then by construction \\(\\phi(g) = e.\\)  However, this holds for all \\(g \\in K\\) (as this is the kernal of \\(\\phi\\)). Therefore \\(\\ker(\\psi) = \\{gK \\mid g \\in K\\} = \\{K\\}\\). But \\(K\\) is the identity in \\(G/K\\). Thus by Theorem 1.\\ref{theorem_isomorph}, we have that \\(\\psi\\) is one-to-one.</p> <p>\\item[Onto.] To show this is onto, we'll simply show that for any \\(h \\in \\im(\\phi)\\), there exists a \\(gK \\in G/K\\) such that \\(\\psi(gK) = h\\). </p> <p>So consider any \\(h \\in \\im(\\phi)\\). By definition, \\(h = \\phi(g)\\) for some \\(g \\in G\\). Now observe that for the element \\(gK \\in G/K\\), </p> \\[ \\psi(gK) = \\phi(g) = h. \\] <p>Thus \\(\\psi\\) is onto. \\end{description} \\textcolor{Plum}{In total, we have showed the following: there exists a bijective homomorphism (i.e., an isomorphism) between \\(G/K = G/\\ker(\\phi)\\) and \\(\\im(\\phi)\\). Therefore \\(G/\\ker(\\phi) \\cong \\im(\\phi)\\) as desired.}  The second isomorphism theorem summarizes a great deal of useful information concerning groups. </p> <p>[ (Second Isomorphism Theorem)] Let \\(G\\) be a group, \\(H\\) a subgroup of \\(G\\) and \\(N\\) a normal subgroup of \\(G\\). Then  <ul> <li> <p>[1.] \\(NH\\) is a subgroup of \\(G\\) (\\(HN\\) is also a subgroup)</p> </li> <li> <p>[2.] \\(N \\normal NH\\) (and \\(N \\normal HN\\))</p> </li> <li> <p>[3.] \\(H \\cap N \\normal H\\) </p> </li> <li> <p>[4.] \\(H/(H \\cap N) \\cong NH/N\\) (and \\(H/(H \\cap N) \\cong HN/N\\)).</p> </li> </ul> <p></p> <p>We put parenthesis in some of the statements because while they are true, most people state the second isomorphism theorem by either removing the text in parenthesis or only keeping the text in parenthesis. However, we don't want the reader to get the impression that, for example, \\(NH \\le G\\) but \\(HN \\not\\le G\\). We think it is fair to be thorough and precise. \\begin{minipage}{0.25 \\textwidth} \\begin{figure}[H] \\begin{tikzcd}[column sep=small]  &amp; NH \\ H  \\arrow[ur, dash] &amp;&amp; N \\arrow[ul,swap,\"\\normal\"] \\ &amp; H\\cap N  \\arrow{ul}{\\normal} \\arrow[ur, dash] \\end{tikzcd} \\end{figure} \\end{minipage} \\hfill \\begin{minipage}{0.7\\textwidth} The diagram to the left demonstrates why the Second Isomorphism Theorem is also known as the diamond isomorphism theorem since a relationship between the four main objects in play can be created.  \\ The lemma below will clean up the proof of this theorem. \\end{minipage} \\ \\</p> <p>\\noindentLemma 1.7.1 Let \\(G\\) be a group. Suppose \\(N \\normal G\\). Then for any \\(n \\in N\\), \\(h \\in G\\), there exists \\(n' \\in N\\) such that \\(hn = n'h\\) and \\(nh = hn'\\).</p> <p>Proof: Since \\(N\\) is normal, we know that for any \\(n \\in N\\), \\(hnh^{-1} \\in N\\) for any \\(h \\in G\\). In particular, this means that \\(hnh^{-1} = n'\\) for some \\(n' \\in N\\). This implies that \\(hn = n'h\\), which is what we set out to show. \\</p> <p>Now we prove the theorem itself. We'll only include proofs for the statments not in paranthesis, because the proofs of the statements in paranthesis are basically identical to the ones we'll offer for those not in paranthesis (e.g., for the proof that \\(NH \\le G\\), only small tweaks are needed to show that \\(HN \\le G\\)).</p> <p> We'll prove one statement at a time.  <ul> <li>[1.] \\textcolor{NavyBlue}{Consider \\(NH = \\{nh \\mid n \\in N, h \\in N\\}\\). This is clearly nonempty (\\(N, H\\) are both nonempty), so we can use the Theorem 1.\\ref{subgroup_test}, the subgroup test, to prove this.}</li> </ul> <p>Let \\(n_1h_1, n_2h_2 \\in NH\\). Our goal is to show that \\(n_1h_1(n_2h_2)^{-1} \\in NH\\). Thus observe that </p> \\[ n_1h_1(n_2h_2)^{-1} = n_1h_1h_2^{-1}n_2^{-1} = n_1hn_2^{-1} \\] <p>where in the last step we know that \\(h_1h_2^{-1} = h\\) for some \\(h \\in H\\).  Since \\(N\\) is normal, we have by Lemma 1.7.1 that there an \\(n^* \\in N\\) such that \\(hn_2^{-1} = n^*h\\). Therefore,</p> \\[ n_1(hn_2^{-1}) = n_1(n^*h) = (n_1n^*)h \\in NH \\] <p>Therefore we have that \\(n_1h_1(n_2h_2)^{-1} \\in NH\\), proving that \\(NH \\le G\\).</p> <ul> <li>[2.] \\textcolor{NavyBlue}{We can prove this directly.} Let \\(nh \\in NH\\). By Lemma 1.7.1, we know that \\(nh = hn'\\) for some \\(n' \\in N\\). Therefore </li> </ul> \\[\\begin{align*} (nh)N(n^{-1}h^{-1}) = (hn')N(n^{-1}h^{-1}) = h(n'Nn^{-1})h^{-1}\\\\ = hNh^{-1} = N \\end{align*}\\] <p>where in the last step we used the fact that \\(N\\) is normal and invoked Theorem 1.10.2. By the same theorem, we can then conclude that \\(N \\normal NH\\).</p> <ul> <li>[3.] \\textcolor{NavyBlue}{To prove this, first recall that \\(H \\cap N\\) is a a subgroup of \\(G\\) since \\(H\\) and \\(N\\) are both subgroups.} Now let \\(a \\in H \\cap N\\) and \\(h \\in H\\). </li> </ul> <p>We can prove normality by Theorem 1.10.3, speficially, that \\(hah^{-1} \\in H\\) for all \\(a \\in H \\cap N\\) and \\(h \\in H\\). But since \\(a \\in H\\), we already know that \\(hah^{-1} \\in H\\). So  By Theorem 1.10.3, we thus have that \\(N \\cap H \\normal H\\).</p> <ul> <li>[4.] \\textcolor{NavyBlue}{To prove this last statement, we first construct a homomorphism \\(\\phi: H \\to NH/N\\) by defining \\(\\phi(nh) = Nh\\).} This is a homomorphism since for \\(h, h' \\in H\\),</li> </ul> \\[ \\phi(nhnh') = Nhnh' = (Nh)(Nnh') = (Nh)(Nh') = \\phi(h)\\phi(h') \\] <p>where in the second step we used the fact that (1) \\(N \\normal NH\\) and (2) \\((Nh)(Nh') = N(hh')\\) by Theorem 1.10.4.</p> <p>\\textcolor{Plum}{Note that \\(\\phi\\) is onto.} For any \\(Nh \\in NH/N\\), we note that any \\(nh \\in NH\\) maps to this element via \\(\\phi\\) for any \\(n\\). Since this is onto, \\(\\im(\\phi) = NH/N\\). </p> <p>\\textcolor{Plum}{Also, observe that \\(\\ker(\\phi) = H \\cap N\\)}, since for any \\(h \\in (H \\cap N)\\) we have that \\(\\phi(h) = Nh = N\\), which the identity in \\(NH/N\\).</p> <p>Now by the First Isomorphism Theorem, we have that </p> \\[ H/\\ker(\\phi) \\cong \\im(\\phi) \\implies H/(H \\cap N) \\cong NH/N \\] <p>as desired.</p> <p></p> <p>We now move onto the third isomorphism theorem, which matches our intution for when we form a quotient of quotient groups.</p> <p>[ (Third Isomorphism Theorem)] Let \\(K, N\\) be normal subgroups of \\(G\\), with \\(N \\le K\\). Then \\(K/N \\normal G/N\\) and  \\[ (G/N)/(K/N) \\cong G/K.    \\] <p>\\vspace{-0.5cm} </p> <p> \\textcolor{NavyBlue}{First we'll show that \\(K/N \\normal G/N\\). Consider any \\(Nk \\in K/N\\), where \\(k \\in K\\), and any \\(Ng \\in G/N\\), where \\(g \\in G\\). Our goal will be to show that \\((Ng)(Nk)(Ng)^{-1} \\in K/N\\).} <p>\\begin{description} \\item[\\phantom{1}] \\hspace{0.5cm} Observe that </p> \\[ (Ng)(Nk)(Ng)^{-1} = (Ng)(Nk)(Ng^{-1}) = N(gkg^{-1}) \\] <p>where we used the fact that \\(N \\normal G\\). Since \\(K \\normal G\\), we know that \\(gkg^{-1} \\in K\\). That is, \\(gkg^{-1} = k'\\) for some \\(k' \\in G\\). Therefore, \\(N(gkg^{-1}) \\in K/N\\) so that \\((Ng)(Nk)(Ng^{-1}) \\in K/N\\). Since \\(g, k\\) were arbitrary, we have by Theorem 1.10.3 that \\(K/N \\normal G/N\\) as desired.</p> <p>\\end{description}</p> <p>\\textcolor{NavyBlue}{Next, we'll show that \\((G/N)/(K/N) \\cong G/K\\). We'll do his by constructing an isomorphism between the two groups.} \\begin{description} \\item[\\phantom{1}] \\hspace{0.5cm}  Construct a homomorphism \\(\\phi: G/N \\to G/K\\) defined as \\(\\phi(Ng) = Kg\\) where \\(Ng \\in G/N\\). First, we'll show this is a homomorphism. For any \\(Ng, Ng' \\in G/N\\), we have that </p> \\[\\begin{align*} \\phi\\big((Ng)(Ng')\\big) = \\phi(N(gg')) = Kgg' \\\\ = (Kg)(Kg') = \\phi(Ng)\\phi(Ng') \\end{align*}\\] <p>where in the third step we used the fact that \\(K \\normal G\\). Therefore, this is a homomorphism. </p> <p>Next, observe that this is onto, since for any \\(Kg \\in G/K\\), we know that the element \\(Ng \\in G/N\\) maps to \\(Kg\\) via \\(\\phi\\). Therefore \\(\\im(\\phi) = G/K.\\)</p> <p>We'll now show that \\(\\ker(\\phi) = K/N\\).  Observe that </p> \\[ \\ker(\\phi) = \\{Ng: \\phi(Ng) = K\\} = \\{Ng: Kg = K\\} = \\{Ng: g \\in K\\} = K/N. \\] <p>Therefore, \\(\\ker(\\phi) = K/N\\).</p> <p>Finally, we can use the First Isomorphism Theorem to conclude that </p> \\[ (G/N)/\\ker(\\phi) \\cong \\im(\\phi) \\implies (G/N)/(K/N) \\cong G/K \\] <p>as desired.           \\end{description}</p> <p></p> <p>We now move onto the Fourth Isomorphism Theorem, which is one of the more powerful isomorphism theorems along with the First Isormorphism Theorem. </p> <p>[ (Fourth Isormorphism Theorem)] Let \\(N \\normal G.\\) Then every subgroup of \\(G/N\\) is of the form \\(H/N\\) where \\(N \\le H \\le G\\). Moreover, if \\(H, K\\) are subgroups of \\(G\\) and they contain \\(N\\), then  <ul> <li> <p>[1.] \\(H \\le K\\) if and only if \\(H/N \\le K/N\\)</p> </li> <li> <p>[2.] \\(H \\normal G\\) if and only if \\(H/N \\normal G/N\\) </p> </li> <li> <p>[3.] if \\(H \\le K\\) then \\([K:H] = [K/N:H/N]\\)</p> </li> <li> <p>[4.] \\((H \\cap K)/N \\cong (H/N) \\cap (K/N)\\).    </p> </li> </ul> <p></p> <p>\\begin{minipage}{0.4\\textwidth}</p> <p>\\begin{tikzpicture} \\draw[-&gt;] (0,0) -- (2, 0); \\draw[-&gt;] (0,-1) -- (2, -1); \\draw[-&gt;] (0,-2) -- (2, -2);</p> <p>\\node at (-0.4, 0) {\\(H_1\\)}; \\node at (-0.4, -1) {\\(H_2\\)}; \\node at (-0.4, -2) {\\(H_3\\)};</p> <p>\\node at (3.4, 0) {\\(M_1 \\le G/N\\)}; \\node at (3.4, -1) {\\(M_2 \\le G/N\\)}; \\node at (3.4, -2) {\\(M_3 \\le G/N\\)};</p> <p>\\node at (-1.2, 0) {$G \\ge \\(}; \\node at (-1.2, -1) {\\)G \\ge \\(}; \\node at (-1.2, -2) {\\)G \\ge $};</p> <p>\\node at (1, -2.4) {\\(\\vdots\\)};</p> <p>\\node at (-0.4, -3) {\\(H_n\\)}; \\node at (3.4, -3) {\\(M_n \\le G/N\\)}; \\node at (-1.2, -3) {$G \\ge $}; \\draw[-&gt;] (0,-3) -- (2, -3);</p> <p>\\end{tikzpicture}</p> <p>\\end{minipage}\\hfill \\begin{minipage}{0.55\\textwidth} The Fourth Isomorphism Theorem is also commonly known as the correspondence theorem, since what it effectively states is that there is a one-to-one correspondence between subgroups \\(H\\) of \\(G\\) which contain \\(N\\) and the subgroups of \\(G/N\\).</p> <p>Thus, if \\(G\\) has \\(n\\) subgroups \\(H_i\\) which contain \\(N\\), then \\(G/N\\) has \\(n\\) subgroups. \\end{minipage}</p> <p> We first prove the first statement. <p>\\textcolor{NavyBlue}{Our goal here will be to show that \\(M \\le G/N \\implies M = H/N\\) where \\(M\\) is some subgroup of \\(G/N\\) and \\(N \\le H \\le G\\).}  \\begin{description} \\item[\\phantom{1}] \\hspace{0.5cm} Consider a subgroup \\(M\\) of \\(G/N\\). Let \\(H\\) be the set of all \\(h \\in G\\) such that \\(Nh \\in M\\). Then observe that \\(N \\subset H\\), since the smallest subgroup of \\(G/N\\) is the trivial group, namely \\(\\{N\\}\\). Therefore \\(N \\subset H \\subset G\\).</p> <p>\\textcolor{NavyBlue}{Now we show that \\(N \\le H \\le G\\). To do this, we just need to show that \\(H \\le G\\), which we will do by the subgroup test.}</p> <p>Let \\(h, h' \\in H\\). Since \\(M \\le G/N\\), we know that for any \\(Nh, Nh' \\in M\\),  </p> \\[ \\underbrace{(Nh')(Nh)^{-1} \\in M}_{\\text{by the Subgroup Test}} \\implies (Nh')(Nh^{-1}) \\in M  \\implies N(h'h^{-1}) \\in M. \\] <p>However, in order for \\(N(h' h^{-1}) \\in M\\), we have that \\(h'h^{-1} \\in H\\). Since \\(h, h'\\) were arbitrary elements of \\(H\\), we have by the subgroup test we have that \\(H \\le G\\).  </p> <p>But since we have that \\(N \\le G\\), \\(H \\le G\\) and \\(N \\subset H \\subset G\\), we all together have that \\(N \\le H \\le G\\).  \\end{description} Next, we prove the the statements \\((1)-(4).\\) \\textcolor{NavyBlue}{To prove (1), we'll show that \\(H/N \\le K/N \\implies H \\le K\\) and \\(H \\le K \\implies H/N \\le K/N\\) for any subgroups \\(H, K\\) of \\(G\\) which contain \\(N\\) where \\(N \\normal G\\).}</p> <p>\\begin{description} \\item[\\phantom{1}] \\hspace{0.5cm} Let \\(H, K\\) be subgroups of \\(G\\) such that \\(N \\subset H\\) and \\(N \\subset K\\). Furthermore, suppose that  \\(H/N \\le K/N\\).</p> <p>\\end{description} </p> <p>The Isomophism Theorems are extremely powerful. The following an application to something which matches our intuiton, but extremely difficult to prove without the isomophism theorems.    </p> <p> Let \\(G\\) be a group and \\(H\\) and \\(K\\) be normal subgroups of \\(G\\). Then  <ul> <li> <p>[1.] \\(HK\\) is a subgroup of \\(G\\) </p> </li> <li> <p>[2.] If \\(\\gcd(|H|, |K|) = 1\\) then \\(H \\times K \\cong HK\\). </p> </li> </ul> <p></p> <p> <ul> <li>[1.] Observe that since \\(H \\unlhd G\\) and \\(K \\unlhd G\\), then obviously  \\(H \\le G\\) and hence we can apply the Second Isomorphism Theorem to conclude  that \\(HK \\le G\\). Thus we see that for this statement to be true in general  we really only need one of the subgroups, either \\(H\\) or \\(K\\), to be normal  to \\(G\\).</li> </ul> <p>\\textcolor{NavyBlue}{To prove this, we'll construct an isomorphism between the two groups. In constructing the homomorphism, we'll have to do a bit of work to show our proposed homorphism is in fact a homomorphism, the work which lies in showing elements of \\(H\\) and \\(K\\) commute. Thus we will show this first.}</p> <ul> <li>[2.]The fact that \\(\\gcd(|H|,|K|)=1\\) allows us to concldue that neither  \\(H \\not \\le K\\) and \\(K \\not \\le H\\), since otherwise by Lagrange's theorem  the order of one group would divide the other, and obviously we don't have  that case here. Thus we know that \\(H \\cap K = \\{e\\}\\), as by our previous  argument it would be impossible for them to share any other nontrivial element. \\ \\ Since \\(H, K\\) are  normal to \\(G\\) we'll have that </li> </ul> \\[\\begin{align*} hkh^{-1} \\in K\\\\ kh^{-1}k^{-1} \\in H \\end{align*}\\] <p>because \\(h\\) and \\(k\\) are both elements in \\(G\\), and we know  for all \\(a \\in G\\) that \\(aha^{-1} \\in H\\) for \\(h \\in H\\) and  \\(aka^{-1} \\in K\\) for \\(k \\in K\\). We can then state that </p> \\[\\begin{align*} \\overbrace{(hkh^{-1})}^{\\text{A member of }K} \\hspace{-0.3cm}k^{-1} = hkh^{-1}k^{-1} \\in K\\\\ h\\underbrace{(kh^{-1}k^{-1})}_{\\text{A member of } H} = hkh^{-1}k^{-1} \\in H \\end{align*}\\] <p>by using the fact that \\(H, K\\) are subgroups and are therefore closed under products  of their elements. But we showed earlier that \\(H \\cap K = \\{e\\}\\); hence  $$ hkh^{-1}k^{-1} \\in H \\cap K = {e} \\implies hkh^{-1}k^{-1} = e \\implies hk = kh. $$ But \\(h, k\\) were arbitrary elements of \\(H, K\\), so this shows that products of  their elements commute. \\ \\ Next, consider the function \\(\\phi: H \\times K \\rightarrow HK\\) defined as $$ \\phi((h, k)) = hk. $$ which we will  show to be a homomorphism. Observe that if \\((h_1, k_1)\\) and \\((h_2, k_2)\\) are in \\(H \\times K\\), then  $$ \\phi((h_1, k_1)\\cdot(h_2, k_2)) = \\phi((h_1h_2, k_1k_2)) = h_1h_2k_1k_2. $$ However, we showed that products of elements between \\(H\\) and \\(K\\) can commute, so that  we can rewrite \\(h_2k_1\\) as \\(k_1h_2\\) to write  $$ \\phi((h_1, k_1)\\cdot(h_2, k_2)) =  h_1h_2k_1k_2 = h_1k_1h_2k_2 = \\phi((h_1, k_2))\\phi((h_2, k_2)). $$ Thus \\(\\phi\\) is a homomorphism. \\ \\ Observe now that \\(\\text{ker}(\\phi) = \\{(e, e)\\}\\). This is because we  know that \\(H \\cap K = \\{e\\}\\), so that if  $$ \\phi((h, k)) = hk = e $$ we know it is impossible that this could be because \\(h = k^{-1}\\); otherwise,  \\(H \\cap K \\ne \\{e\\}\\), which we know is not the case.  Hence the only time when \\(hk = e\\) is if both \\(h\\) and \\(k\\)  are \\(e\\), so that \\(\\text{ker}(\\phi) = \\{(e, e)\\}\\). \\ \\ Observe that \\(\\text{im}(\\phi) = HK\\). This is because for any \\(hk \\in  HK\\), we can simply observe that \\(h \\in H, k \\in K\\), and therefore there  exists a \\((h, k) \\in H \\times K\\) such that  $$ \\phi(h, k) = hk. $$ Thus every element of \\(HK\\) is covered by our mapping, so \\(\\phi\\) is injective  and hence \\(\\text{im}(\\phi) = HK\\). \\ \\ Finally, what we have shown is that (1) \\(\\phi\\) is a homomorphism and  (2) it is a bijection from \\(H \\times K\\) to \\(HK\\). We can now apply the  First Isomorphism Theorem to conclude that  $$  H \\times K/\\text{ker}(\\phi) \\cong \\text{im}(\\phi) \\implies H\\times K \\cong HK $$ because \\(\\text{ker}(\\phi) = \\{(e, e)\\}, H \\times K/\\{(e, e)\\} = H \\times K\\),  and \\(\\text{im}(\\phi) = HK\\). This completes the proof.</p> <p></p> <p>The First Isomophism Theorem has a lot of fun applications, one of which we present here. </p> <p> Let \\(G\\) and \\(H\\) be groups such that \\(|G|\\) and \\(|H|\\) are coprime. If \\(\\phi: G \\to H\\) is a homomorphism, then \\(\\phi\\) is zero homomorphism.  </p> <p> By the First Isomorphism Theorem, we see that  \\[ G/\\ker(\\phi) \\cong \\im(\\phi).    \\] <p>Therefore \\(|G/\\ker(\\phi)| = |\\im(\\phi)|\\). However, </p> \\[\\begin{align*} |G/\\ker(\\phi)| = |G|/|\\ker(\\phi)| &amp;= |\\im(\\phi)|\\\\  \\implies |G| &amp;= |\\ker(\\phi)| \\cdot  |\\im(\\phi)|. \\end{align*}\\] <p>Note that \\(|\\ker(\\phi)| \\mid |G|\\) and \\(|\\im(\\phi)| \\mid |H|\\) by Lagrange's Theorem. However, we said that \\(|G|\\) and \\(|H|\\) are corpime which means that \\(|\\im(\\phi)| = 1\\). Hence we must have that \\(|\\ker(\\phi)| = G\\), and since \\(\\ker(\\phi) \\le G\\) we have that \\(\\ker(\\phi) = G\\). Therefore \\(\\phi\\) sends every element of \\(G\\) to the identity of \\(H\\), which is what we set out to show. </p>"},{"location":"algebra/Groups/Left%20and%20Right%20Cosets%2C%20Lagrange%27s%20Theorem/","title":"1.5. Left and Right Cosets, Lagrange's Theorem","text":"<p>The result of the previous proof is a special case of a more general theorem we'll come across, know as Lagrange's Theorem. The theorem states that if \\(G\\) is a finite group and \\(H\\) is a subgroup, then \\(|H| \\mid |G|\\). That is, the order of \\(H\\) divides \\(G\\). This is a remarkable and useful result, aiding proofs as we move on from it. But in order to reach Lagrange's Theorem we first discuss the extremely important concept of a coset of a group.</p> <p>Before defining a coset, we first recall the definition of an equivalence relation. </p> <p> An equivalence relation on a set \\(G\\) is a binary relation \\(\\sim\\) that satisfies the following properties. \\begin{description} \\item[Reflexive.] For all \\(a \\in G\\), \\(a \\sim a\\). \\item[Symmetric.] If \\(a \\sim b\\) then \\(b \\sim a\\). \\item[Transitive.] If \\(a \\sim b\\) and \\(b \\sim c\\) then \\(a \\sim c\\).  \\end{description} </p> <p>\\textcolor{Plum}{Equivalence classes are a useful concept since they tend to break up a set of objects \\(G\\) into distinct, disjoint sets \\(A_i\\). More specifically, they partition \\(G\\). These sets, \\(A_i\\), are known as equivalence classes since their criteria for membership requires that \\(a \\in A_i\\) if and only if \\(a \\sim a'\\) for all \\(a' \\in A_i\\).  This is a general strategy in mathematics: to define equivalence classes from some equivalence relation to break up a set into disjoint partitions. However, we use the concept of an equivalence class to partition a group \\(G\\). We use the following relation to do this.} \\</p> <p>\\textcolor{blue!90!black!100}{The Relation.}</p> <p>Let \\(G\\) be a group and \\(H\\) be a subgroup of \\(G\\). If \\(a, b \\in G\\), then the relation \\(\\sim\\) on \\(G\\) such that \\(a \\sim b\\) if and only if \\(ab^{-1} \\in H\\) is an equivalence relation.</p> <p> \\begin{description} \\item[Reflexive.] Observe that \\(a \\sim a\\), since \\(aa^{-1} = e \\in H\\). <p>\\item[Symmetric.] First, if \\(a \\sim b\\) then \\(ab^{-1} \\in H\\). Since \\(H\\) is a group, we know that \\((ab^{-1})^{-1} = ba^{-1} \\in H\\). Thus by our definition we see that \\(b \\sim a\\), so that our relation is also symmetric. </p> <p>\\item[Transitive.] Now suppose \\(a \\sim b\\) and \\(b \\sim c\\) for \\(a, b, c \\in G\\). Then by definition \\(ab^{-1} \\in H\\) and \\(bc^{-1} \\in H\\). Since \\(H\\) is a group, and it is closed under products of its elements. Therefore</p> \\[ (ab^{-1})(bc^{-1}) = ab^{-1}bc^{-1} = ac^{-1} \\in H. \\] <p>Thus we see that \\(a \\sim c\\), which proves that our relation is transitive. \\end{description}  As our relation is reflexive, symmetric and transitive, we see that it is an equivalence relation. Note however that we could have defined our relation as \\(a \\sim b\\) if and only if \\(a^{-1}b \\in H\\); such a relation is equivalent to what we just worked with.</p> <p>First we require a quick definition.</p> <p> Consider a subgroup \\(H\\) of \\(G\\). For any \\(a \\in G\\), we define \\[\\begin{align*} Ha = \\{ha \\mid \\text{ for all }h \\in H\\} \\end{align*}\\] <p>to be the right coset of \\(H\\). We also define</p> \\[\\begin{align*}  aH = \\{ah \\mid \\text{ for all }h \\in H\\}.  \\end{align*}\\] <p>to be the left coset of \\(H\\). Note that since \\(H\\) is a group, it is closed under products of its elements. Therefore for any \\(h\\in H\\)</p> \\[ hH = Hh = H. \\] <p></p> <p>\\textcolor{red!80}{Don't confuse the above equality; take special note that  equality above is set equality, not term by term equality. Of course we have no idea if \\(ha = ah\\) where \\(h \\in H\\) for \\(a \\in G\\) unless \\(G\\) is abelian or we have other information.} \\</p> <p>\\textcolor{blue!90!black!100}{The Big Idea of Cosets.}</p> <p>Now consider the relation introduced earlier, which we  proved is in fact an equivalence relation. Consider an equivalence class of an element \\(a \\in G\\), denoted \\([a]\\). Then we can describe \\([a]\\) as </p> \\[\\begin{gather*} [a] = \\{g \\in G \\mid g \\sim a \\}\\\\ \\hspace{1.35cm} = \\{g \\in G \\mid ga^{-1} \\in H \\}\\\\ \\hspace{4.2cm} = \\{g \\in G \\mid ga^{-1} = h \\text{ for some } h \\in H \\}\\\\ \\hspace{3.75cm} = \\{g \\in G \\mid g = ha \\text{ for some } h \\in H \\}\\\\ \\hspace{-1.7cm}= Ha. \\end{gather*}\\] <p>Thus the equivalence classes of the elements of \\(G\\) with respect to some subgroup \\(H\\) are simply just the right cosets of \\(H\\). (We could have alternatively defined our equivalence relation to be \\(g \\sim a\\) if and only if \\(a^{-1}g \\in H\\), in which case our above description of \\([a]\\) would have resulted in being equal to \\(aH\\). Since both formulations are equivalent, we will simply work with the right cosets of \\(H\\), namely the sets \\(Ha\\).)</p> <p>\\begin{figure}[h] \\centering \\begin{tikzpicture} \\node at (0,0) {Cosets}; \\draw[-&gt;] (0.3, 0.3) -- (2, 1.7); \\node at (2, 2) {Equivalence Relations}; \\draw[-&gt;] (0.9, 0) -- (2.4, 0); \\draw[-&gt;] (2.2, 1.7) -- (3.6, 0.3); \\node at (4,0) {Properties of \\(G\\)}; \\end{tikzpicture} \\end{figure} Once we understand cosets, we can understand a lot about a group, because they're really </p> <p>just equivalence classes! \\vspace{0.8cm}</p> <p>\\textcolor{NavyBlue!100!black!100}{Since equivalence classes are mathematical objects which partition a set, what we have is the following beautiful idea: We can take a subgroup \\(H\\) of a set \\(G\\) and partition our group \\(G\\) via the right (or left) cosets of \\(H\\). This is because our cosets are equivalence classes, and as we said before equivalence classes partition sets which they are defined on.} \\ \\ Example\\ Consider the group \\(\\ZZ\\) and the subgroup \\(5\\ZZ = \\{5n \\mid n \\in \\ZZ\\}\\).  We can calculate the cosets of this \\(\\ZZ\\) with respect to \\(5\\ZZ\\) as </p> \\[\\begin{align*} 5\\ZZ + 1 &amp;= \\{5n + 1 \\mid n \\in \\ZZ\\}\\\\ 5\\ZZ + 2 &amp;= \\{5n + 2 \\mid n \\in \\ZZ\\}\\\\ 5\\ZZ + 3 &amp;= \\{5n + 3 \\mid n \\in \\ZZ\\}\\\\ 5\\ZZ + 4 &amp;= \\{5n + 4 \\mid n \\in \\ZZ\\}\\\\ 5\\ZZ + 5 &amp;= \\{5n + 5 \\mid n \\in \\ZZ \\} = 5\\ZZ \\end{align*}\\] <p>Note that we didn't list any other cosets. Well, that's because these are all of the possible distinct cosets of \\(\\ZZ\\) with respect to \\(5\\ZZ\\). For example, the coset \\(5\\ZZ + 37\\) is equivalent to \\(5\\ZZ + 2\\), since </p> \\[\\begin{align*} 5\\ZZ + 37 = \\{5n + 37 \\mid n \\in \\ZZ\\} &amp;= \\{5n + 5\\cdot 7 + 2 \\mid n \\in \\ZZ\\}\\\\ &amp;=\\{5(n+7) + 2 \\mid n \\in \\ZZ\\}\\\\ &amp;= 5\\ZZ + 2. \\end{align*}\\] <p>Thus, any other coset we propose is equivalent to one of the five we listed. This is demonstrated in the figure below.</p> <p>\\begin{figure}[h] \\centering \\begin{tikzpicture} \\node at (-1,3.2) {\\(5\\ZZ + 1\\)}; \\node at (-1,2.5) {\\(5\\ZZ + 2\\)}; \\node at (-1,1.8) {\\(5\\ZZ + 3\\)}; \\node at (-1,1) {\\(5\\ZZ + 4\\)}; \\node at (-0.7,.3) {\\(5\\ZZ\\)};</p> <p>\\draw (0,0) rectangle (7.3,3.6); \\node at (0.5, 3.2) {\\(\\cdots\\)}; \\node at (1.4, 3.2) {\\(-14,\\)}; \\node at (2.3, 3.2) {\\(-9,\\)}; \\node at (3.2, 3.2) {\\(-4,\\)}; \\node at (4.1, 3.2) {\\(1,\\)}; \\node at (5.0, 3.2) {\\(6,\\)}; \\node at (5.9, 3.2) {\\(11,\\)}; \\node at (6.8, 3.2) {\\(\\cdots\\)};</p> <p>\\node at (0.5, 2.5) {\\(\\cdots\\)}; \\node at (1.4, 2.5) {\\(-13,\\)}; \\node at (2.3, 2.5) {\\(-8,\\)}; \\node at (3.2, 2.5) {\\(-3,\\)}; \\node at (4.1, 2.5) {\\(2,\\)}; \\node at (5.0, 2.5) {\\(7,\\)}; \\node at (5.9, 2.5) {\\(12,\\)}; \\node at (6.8, 2.5) {\\(\\cdots\\)};</p> <p>\\node at (0.5, 1.8) {\\(\\cdots\\)}; \\node at (1.4, 1.8) {\\(-12,\\)}; \\node at (2.3, 1.8) {\\(-7,\\)}; \\node at (3.2, 1.8) {\\(-2,\\)}; \\node at (4.1, 1.8) {\\(3,\\)}; \\node at (5.0, 1.8) {\\(8,\\)}; \\node at (5.9, 1.8) {\\(13,\\)}; \\node at (6.8, 1.8) {\\(\\cdots\\)};</p> <p>\\node at (0.5, 1.0) {\\(\\cdots\\)}; \\node at (1.4, 1.0) {\\(-11,\\)}; \\node at (2.3, 1.0) {\\(-6,\\)}; \\node at (3.2, 1.0) {\\(-1,\\)}; \\node at (4.1, 1.0) {\\(4,\\)}; \\node at (5.0, 1.0) {\\(9,\\)}; \\node at (5.9, 1.0) {\\(14,\\)}; \\node at (6.8, 1.0) {\\(\\cdots\\)};</p> <p>\\node at (0.5, .3) {\\(\\cdots\\)}; \\node at (1.4, .3) {\\(-10,\\)}; \\node at (2.3, .3) {\\(-5,\\)}; \\node at (3.2, .3) {\\(0,\\)}; \\node at (4.1, .3) {\\(5,\\)}; \\node at (5.0, .3) {\\(10,\\)}; \\node at (5.9, .3) {\\(15,\\)}; \\node at (6.8, .3) {\\(\\cdots\\)}; \\end{tikzpicture} \\end{figure}</p> <p>Note that in this figure we can identify every integer in \\(\\ZZ\\). This assures us that our above list of cosets is in fact complete. In addition, this demonstrates the fact that cosets partition a group. Note that each above coset is disjoint, yet the union of all of the cosets is the entire group \\(G\\). \\ \\</p> <p>As cosets can partition a group, we define \\([G:H]\\), called the index, to be the number of distinct right (or equivalently left) cosets of \\(G\\). If \\(G\\) is finite, then \\([G:H]\\) is of course finite. However, \\(G\\) can still be infinite while \\([G:H]\\) is finite.</p> <p> If \\(G\\) is a group and \\(H\\) is a subgroup, then for \\(a, b \\in G\\), \\(Ha \\cap Hb = \\emptyset\\) or \\(Ha = Hb\\). </p> <p>This proves the observation we made beforehand in the example with the cosets of \\(\\ZZ\\) with respect to \\(5\\ZZ\\). We saw that the 5 cosets we came up with were distinct and disjoint, which is what this proposition proves is true in general.</p> <p> This is simply a consequence of the connection between cosets and  equivalence relations of \\(G\\). Equivalence classes form partitions, so by definition they are disjoint. However, equivalence classes can also be equal to one another (namely, if \\(a, b\\) belong to the same equivalence class \\(A\\), then \\([a] = [b] = A\\). This is why equivalence classes, which in our case are cosets, are awesome.) Therefore cosets \\(Ha\\) and \\(Hb\\) are either disjoint or equal to each other. <p>This, however, can be proven directly. Consider such \\(Ha\\) and \\(Hb\\). Suppose \\(Ha \\cap Hb \\ne \\emptyset\\). Then by definition of cosets, there exists a \\(h_1\\) and \\(h_2\\) such that \\(h_1a = h_2b\\). Therefore \\(a = h_1^{-1}h_2b\\). Since \\(H\\) is a group, and it is closed under products of its elements, there exists a \\(h'\\) such that \\(h'= h_1^{-1}h_2\\). Thus \\(a = h'b\\). Consequently, we see that </p> \\[\\begin{align*} Ha = \\{ha \\mid h \\in H\\} = \\{h(h'b) \\mid h \\in H\\} = H(h'b). \\end{align*}\\] <p>However, recall earlier that \\(Hh = H\\) for any \\(h \\in H\\). Since \\(h' \\in H\\), we then have that </p> \\[ H(h'b) = (Hh')b = Hb \\] <p>which proves that \\(Ha = Hb\\) as well as the proposition. </p> <p> Let \\(G\\) be finite and \\(a, b \\in G\\). If \\(H\\) is a subgroup, and  \\(Ha\\) and \\(Hb\\) are distinct cosets, then \\(|Ha| = |Hb|\\).  \\textcolor{red}{Hence, cosets of \\(G\\) with respect to some subgroup \\(H\\) are always of the same size.}</p> <p> Construct a bijection \\(f: Ha \\to Hb\\) given by \\(f(ha) = hb\\). Observe that this is surjective. It is also injective since \\(hb = hb'\\)  if and only if \\(b = b'\\), but since we assumed \\(Ha\\) and \\(Hb\\) are distinct, we know by the previous proposition that distinctness implies disjointness. Since we can formulate a bijection the two sets, the sets have the same sizes. </p> <p>The next theorem, credited to Lagrange, demonstrates the usefulness of studying cosets to study finite groups. Our equivalence classes not only parition our group \\(G\\), but they are also the same size. Therefore, we can always partition a finite group \\(G\\) into equally sized cosets. </p> <p> Let \\(G\\) be a finite group, and suppose \\(H\\) is a subgroup of \\(G\\). Then \\(|H|\\) divides \\(|G|\\). </p> <p> Since \\(G\\) is finite, there are a distinct set of cosets \\(Ha_1, Ha_2, \\dots , Ha_n\\) which partition \\(G\\). By Proposition 1.3, each set is of equal size; call it \\(k\\). Therefore, we see that  \\[ |Ha_1| + |Ha_2| + \\cdots + |Ha_n| = |G| \\implies kn = |G|. \\] <p>Therefore \\(|G|\\) will always be a multiple of \\(|H|\\). Or, in other words, \\(|H|\\) divides \\(|G|\\). </p> <p>This is the theorem we said was a more general case of Theorem 1.6.  The above theorem enables us to understand all the possible subgroups of any finite group \\(G\\). In fact, the theorem implies more useful consequences of Theorem 1.7.</p> <p> Let \\(G\\) be a finite group and \\(H\\) a subgroup of \\(G\\). Then we have the following consequences:  \\begin{enumerate} \\item If \\(G\\) is a finite group and \\(g \\in G\\), then \\(|g|\\) divides \\(|G|\\) and \\(g^{|G|} = e\\). <p>\\item Let \\(p\\) be a prime number.  If \\(G\\) is a group of order \\(p\\), then \\(G\\) is a cyclic group.</p> <p>\\item If $ \\phi :G \\to G'$ is a homomorphism between finite groups, then \\(|\\mbox{ker } \\phi|\\) divides \\(G\\) and \\(|\\mbox{im }\\phi|\\) divides \\(G'\\).</p> <p>\\item \\(|G| = |H|\\cdot[G:H]\\) for any subgroup \\(H\\) of \\(G\\). \\end{enumerate} </p> <p> \\begin{enumerate} \\item Consider the cyclic subgroup \\(H = \\left&lt;g\\right&gt;\\) of \\(G\\). By Lagrange's theorem, we know that \\(|H|\\) divides \\(|G|\\) isnce \\(H\\) is a subgroup of \\(G\\). However \\(|g| = |H|\\) since \\(H\\) is cyclic. Therefore \\(|g|\\) divides the order of \\(|G|\\). This implies that \\(|G| = n|g|\\) for some \\(n \\in \\mathbb{N}\\). Therefore  \\[ g^{|G|} = g^{n|g|} = (g^{|g|})^n = e^n = e \\] <p>which is what we set out to show.</p> <p>\\item If \\(|G| = p\\), we know by Lagrange's Theorem we know that there are exactly two subgroups of \\(G\\), namely the trivial group and the whole group \\(G\\).</p> <p>Thus let \\(g \\in G\\), where \\(g\\) is not the identity,  and consider the subgroup \\(H = \\left&lt; g \\right&gt;\\). Since \\(g\\) is not the identity, \\(H\\) is not the trivial group. But since it is a nontrivial subgroup, and the only nontrivial subgroup of \\(G\\) is itself, we see that our only choice is to let \\(H = G\\). However, \\(H\\) is cyclic, which proves that \\(G\\) is cyclic as well.</p> <p>\\item This result immediately follows from the fact that \\(\\mbox{ker } \\phi\\) is a subgroup of \\(G\\) and \\(\\mbox{im }\\phi\\) is a subgroup of \\(G'\\). Applying Lagrange's theorem leads to the result.</p> <p>\\item For any subgroup \\(H\\) of \\(G\\), we know that \\([G:H]\\) is the number of left or right cosets of \\(G\\). Since each such set is of size \\(|H|\\), and because they all together partition \\(G\\), we see that \\(|G| = |H| \\cdot [G:H]\\). \\end{enumerate} </p>"},{"location":"algebra/Groups/Normal%20subgroups/","title":"1.6. Normal subgroups","text":"<p>Normal subgroups are special subgroups which exhibit properties of interest for when we go on to later define the idea of quotient groups, a concept we have touched upon slightly in considering \\(\\mathbb{Z}/2\\mathbb{Z}\\) and other modulo groups. They are a bit abstract at first, since they have to do with cosets. Once you work with normal subgroups for a bit, it will s eventually click and the reasoning behind their definitions becomes clear. </p> <p> Let \\(G\\) be a group and suppose \\(H\\) is a subgroup of \\(G\\). We say that \\(H\\) is normal if and only if for every \\(g \\in G\\), we have that \\(Hg = gH\\). We denote such a relation as \\(H \\unlhd G\\).  \\noindent We make two remarks here. \\begin{description} \\item[Commutative Groups.] </p> <p>Note that if \\(G\\) is commutative, then \\(H\\), a subgroup of \\(G\\), is also commutative. In fact, \\(H\\) commutes with all elements of \\(G\\). That is, if \\(H = \\{h_1, h_2, \\dots \\}\\) then</p> \\[ gH = \\{gh_1, gh_2, \\dots\\} = \\{h_1g, h_2g, \\dots\\} = Hg \\] <p>for all \\(g \\in G\\). Thus what we're trying to say here is if \\(G\\) is commutative, every subgroup \\(H\\) of \\(G\\) is normal.</p> <p>\\item[Set Equality.] If \\(H\\) is normal to \\(G\\), then \\(gH = Hg\\) all \\(g \\in G\\). Be careful with this equation, since what this is not saying is that \\(gh=hg\\) for all \\(g\\in G\\) and \\(h \\in H\\); that would imply commutativity, and it may be the case that \\(G\\) and \\(H\\) are not commutative groups. That is, the above equation is set equality, not term-by-term equality.</p> <p>What this does say, however, is if \\(gH = Hg\\), then for each \\(g\\in G\\), and for every \\(h_1 \\in H\\), there exists an \\(h_2 \\in H\\) such that </p> \\[ gh_1 = h_2g. \\] <p>Note here that commutative groups satisfy this because in their case, \\(h_1 = h_2\\) satisfies the equation.  \\end{description}</p> <p>Since our current definition of normality would be exhausting to use directly if we wanted to check if a subgroup is normal, we have the following theorem that helps us check for normality. </p> <p> Let \\(G\\) be a group and \\(H\\) a subgroup of \\(G\\). The following are equivalent: <ul> <li> <p>[1.] \\(H \\normal G\\) for all \\(g \\in G\\)</p> </li> <li> <p>[2.] \\(gHg^{-1} = H\\) for all \\(g \\in G\\)</p> </li> <li> <p>[3.] \\(gHg^{-1} \\subset H\\) for all \\(g\\in G\\).</p> </li> <li> <p>[4.] \\((Hg)(Hh) = H(gh)\\) for all \\(g, h \\in G\\)</p> </li> </ul> <p></p> <p> We'll prove this by producing a chain of imply statements that can traverse in both directions. Let \\(G\\) be a group and \\(H\\) be a subgroup.  <p>\\noindent \\(\\mathbf{(1 \\iff 2)}\\) If \\(H \\normal G\\), then \\(gH = Hg\\) for all \\(g \\in G\\). Multiplying on the left by \\(g^{-1}\\), we then see that \\(gHg^{-1} = H\\) for all \\(g \\in G\\).</p> <p>Proving the reverse direction, if \\(gHg^{-1} = H\\) for all \\(g \\in G\\) then \\(gH = Hg\\) for all \\(g \\in G\\), which means that \\(H\\) is normal by defintion. </p> <p>\\noindent \\(\\mathbf{(2 \\iff 3)}\\) If \\(gHg^{-1} = H\\) for all \\(g \\in G\\) then it is certainly true that \\(gHg^{-1} \\subset H\\) for all \\(g \\in G\\). </p> <p>Now we prove the other direction. Suppose \\(gHg^{-1} \\subset H\\) for all \\(g \\in G\\). Then</p> \\[ gHg^{-1} \\subset H \\implies gH \\subset Hg  \\implies H \\subset g^{-1}Hg \\] <p>by multiplying on the right by \\(g\\) and on the left by \\(g^{-1}\\). However, since we have assumed (3) is true we know that </p> \\[ (g^{-1})H(g^{-1})^{-1} \\subset H \\implies g^{-1}Hg \\subset H.  \\] <p>By the above equations we then have that \\(H = g^{-1}Hg\\), and multiplying by \\(g^{-1}\\) on the right and \\(g\\) on the left yields that \\(H = gHg^{-1}\\) as desired.</p> <p>\\noindent\\(\\mathbf{(2 \\iff 4)}\\) Suppose (2). Then observe that \\(gHg^{-1} = H \\implies gH = Hg\\) for all \\(g \\in G\\). Therefore for \\(h \\in G\\), </p> \\[ (Hg)(Hh) = H(gH)h = H(Hg)h = H(gh). \\] <p>In the first step we used associativity and in the second step we used the fact that \\(gH = Hg\\). </p> <p>To prove the other direction, suppose \\((Hg)(Hh) = H(gh)\\) for all \\(g, h \\in G\\). Let \\(h = e\\). Then   To show a subgroup \\(H\\) of \\(G\\) is normal, condition (3) of this theorem generally the fastest and easy way to take advtange of. It is usually the least complicated one to show.  \\ \\ \\noindent Example. \\ Consider the group \\(GL_n(\\mathbb{R})\\) and its subgroup \\(SL_n(\\mathbb{R})\\). It turns out that \\(SL_n(\\mathbb{R}) \\normal GL_n(\\mathbb{R})\\), which we will show using condition (3).</p> <p>Let \\(A \\in GL_n(\\mathbb{R})\\) and suppose \\(T \\in SL_n(\\mathbb{R})\\). We must show that \\(ATA^{-1} \\in SL_n(\\mathbb{R})\\) for all \\(A \\in GL_n(\\mathbb{R})\\) and \\(T \\in SL_n(\\mathbb{R})\\). Observe that </p> \\[\\begin{align*} \\det(ATA^{-1}) = \\det(A)\\det(T)\\det(A^{-1}) = \\det(A)(1)\\det(A)^{-1} = 1 \\end{align*}\\] <p>where we used the basic properties of the determinant for the calculation. Since \\(\\det(ATA^{-1}) = 1\\), we have that \\(ATA^{-1} \\in SL_n(\\mathbb{R})\\) for all \\(A\\) and \\(T\\) in \\(GL_n(\\mathbb{R})\\) and \\(SL_n(\\mathbb{R})\\), respectively. Therefore \\(SL_n(\\mathbb{R})\\) is normal to \\(GL_n(\\mathbb{R})\\).  \\ \\ Example. \\ One important example is the following: for any group homomorphism \\(\\phi\\) between two groups \\(G\\) and \\(G'\\), recall that \\(\\mbox{ker}(\\phi)\\) is a subgroup of \\(G\\). However, we also have that \\(\\mbox{ker}(\\phi) \\normal G\\), which we'll show as follows.</p> <p> Let \\(G, G'\\) be groups and \\(\\phi: G \\to G'\\) be a group homomorphism. Then \\(\\ker(\\phi) \\normal G\\). </p> <p> We need to show that for all \\(g \\in G\\), \\(h \\in \\mbox{ker}(\\phi)\\) that \\(ghg^{-1} \\in \\mbox{ker}(\\phi)\\). Thus observe that  \\[ \\phi(ghg^{-1}) = \\phi(g)\\phi(h)\\phi(g^{-1}) = \\phi(g)\\cdot 0 \\cdot \\phi(g^{-1}) = 0. \\] <p>Since \\(\\phi(ghg^{-1}) = 0\\), we thus see that \\(ghg^{-1} \\in \\mbox{ker}(\\phi)\\) for all \\(g \\in G\\) and \\(h \\in \\mbox{ker}(\\phi)\\), which proves \\(\\mbox{ker}(\\phi) \\normal G\\).   </p> <p>Another important example of normality is the fact that the center of a group \\(Z(G)\\) is normal to \\(G\\) for any group \\(G\\).</p> <p> Let \\(G\\) be a group. Then \\(Z(G) \\normal G\\). </p> <p> Recall that \\(Z(G)\\) is a subgroup of \\(G\\), consisting of all the elements of \\(G\\) which commute with every element in \\(G\\). More precisely,  \\[ Z(G) = \\{z \\in G \\mid gz = zg \\text{ for all } g \\in G\\}. \\] <p>Now for any \\(g \\in G\\) and \\(z \\in Z(G)\\), we have that \\(gzg^{-1} = gg^{-1}z = z\\), since \\(z\\) commutes with all elements of \\(G\\). Therefore \\(gzg^{-1} \\in Z(G) \\implies gZ(G)g^{-1} \\subset Z(G)\\). By the previous theorem, we can conclude that \\(Z(G) \\normal G\\) as desired. </p> <p>Next, we introduce a small theorem that allows us to quickly and easily identify if a subgroup \\(H\\) of \\(G\\) is normal. </p> <p> If \\(G\\) is a group and \\(H\\) is a subgroup, and \\([G:H] = 2\\), then \\(H \\normal G\\). </p> <p> Since \\(G\\) has two right (and equivalently two left) cosets, we see that they must be of the form \\(H\\) and \\(Hg\\) where \\(g \\in G\\setminus H\\) (that is, all of the elements of \\(G\\) which are not in \\(H\\)).    <p>As we said before, there are equivalently two left cosets \\(H\\) and \\(gH\\) where \\(g \\in G\\setminus H\\). Since the cosets partition \\(G\\), we see that for any \\(g \\in G\\setminus H\\) two partitions of \\(G\\) are </p> \\[ \\{H, Hg\\} \\hspace{0.2cm}\\text{and}\\hspace{0.2cm} \\{H, gH\\}. \\] <p>Since these partition the same set we see that \\(gH = Hg\\) for all \\(g \\in G\\setminus H\\). Note that we already know that for \\    \\(g \\in H\\), \\(Hg = H\\) and \\(gH = H\\) so \\(gH = Hg\\). Therefore, we have all together that \\(Hg = gH\\) for all \\(g \\in G\\). </p> <p>\\noindent In working with normal subgroups, one may form the following questions.  \\</p> <p>\\textcolor{ForestGreen}{Q: If \\(K\\) is a normal subgroup of \\(H\\) and \\(H\\) is a normal subgroup of \\(G\\), is \\(K\\) normal to \\(G\\)?} \\</p> <p>A: Not always. If \\(H \\normal K\\), then \\(khk^{-1} \\in K\\) for all \\(k \\in K\\) but there is nothing allowing for us to extend this further and state that \\(ghg^{-1} \\in K\\) for all \\(g \\in G\\).  \\ However, a special case for when this is true involves \\(Z(G)\\). We know that \\(Z(G) \\normal G\\). But if \\(K \\normal Z(G)\\) then it turns out \\(K \\normal G\\), </p>"},{"location":"algebra/Groups/Permutation%20Groups./","title":"1.2. Permutation Groups.","text":"<p>One of the most important and well-known types of groups are the permutation groups, which we introduce formally as follows. </p> <p>Consider a finite set of elements \\(X = \\{1, 2, \\dots, n\\}\\). We define a  permutation to be a reordering of the elements of \\(X\\). More formally, a permutation is a bijective mapping \\(\\sigma: X \\to X\\), similar to one that follows. </p> <p>How can we represent this information? We generally don't use sets to represent permutations, since sets  don't care about order. That is, \\(\\{1, 2, 3\\} = \\{3,2,1\\}\\), etc.  </p> <p>Thus for a set \\(\\{1, 2, \\dots, n\\}\\), we can represent a permutation \\(\\sigma\\) of the set of elements as follows:</p> \\[ \\sigma =   \\begin{pmatrix} 1 &amp; 2 &amp; \\cdots &amp; n\\\\ \\sigma(1) &amp; \\sigma(2) &amp; \\cdots &amp; \\sigma(n) \\end{pmatrix} \\] <p>where we read this as \\(1\\) is assigned to \\(\\sigma(1)\\), 2 is assigned to \\(\\sigma(2)\\). For example, a permutation that just shifts the elements down the line is </p> \\[ \\sigma =  \\begin{pmatrix} 1 &amp; 2 &amp; \\cdots &amp; n\\\\ 2 &amp; 3 &amp; \\cdots &amp; 1 \\end{pmatrix}. \\] <p>That is, \\(\\sigma\\) sends 1 to 2, 2 to 3 and eventually \\(n\\) to 1.  Here we'll denote the set of all permutations of the set \\(\\{1, 2, \\dots, n\\}\\), or more generally a set of \\(n\\) objects (since we can always enumerate objects with natural numbers) as \\(S_n\\).</p> <p>\\textcolor{NavyBlue}{What is interesting about this is that if we define \"multiplication\" of elements of \\(S_n\\) to be function composition, then the  \\textbf{set of permutations of \\(X\\) form a group} which we show as follows.}</p> <p>Let \\(X = \\{1, 2, \\dots, n\\}\\).  \\begin{description} \\item[Closed.] For any \\(\\sigma_1, \\sigma_2 \\in S_n\\), we see that \\(\\sigma_2 \\circ \\sigma_1\\) is (1) a composition of bijective functions and therefore is bijective and (2) a permutation of \\(X\\). One way to think about the composition is that \\(\\sigma_1\\) rearranges \\(X\\), and \\(\\sigma_2\\) rearranges \\(X\\) again. Thus \\(\\sigma_2 \\circ \\sigma_1 \\in S_n\\). </p> <p>\\item[Associativity.] Associativity is obvious since function composition is associative. </p> <p>\\item[Identity.] Observe that the permutation \\(\\sigma_e: X \\to X\\) for which  \\(\\sigma_e(i) = i\\) is technically a permutation of \\(X\\). Therefore \\(\\sigma_e\\) acts as the identity element in \\(S_n\\). </p> <p>\\item[Inverse.] Consider a permutation \\(\\sigma \\in S_n\\). Define \\(\\sigma^{-1}\\) to be the function where if \\(\\sigma(i) = j\\), then \\(\\sigma^{-1}(j) = i\\). Then by construction, (1) \\(\\sigma^{-1}\\) is a permutation of \\(X\\) and (2) \\(\\sigma \\circ \\sigma^{-1} = \\sigma^{-1} \\circ \\sigma = \\sigma_e\\). Thus \\(S_n\\) contains inverses and composition of the inverses returns \\(\\sigma_e\\), the identity. \\end{description}</p> <p> For all \\(n \\ge 1\\), \\(|S_n| = n!\\) </p> <p> This is counting the number of ways to rearrange a set of size \\(n\\), which we know from combinatorics to simply be \\(n!\\) </p> <p>Now that we know that \\(S_n\\) is a group, we'll study the properties of this group. </p> <p>Recall earlier our notation for representing a permutation \\(\\sigma \\in S_n\\):</p> \\[ \\sigma =   \\begin{pmatrix} 1 &amp; 2 &amp; \\cdots &amp; n\\\\ \\sigma(1) &amp; \\sigma(2) &amp; \\cdots &amp; \\sigma(n) \\end{pmatrix} \\] <p>This notation sucks, since it includes more information than we actually need to. For instance, the top row is always going to be the same. </p> <p>A better way to write this is through cycle decomposition, which we will soon define.</p> <p> Let \\(\\sigma \\in S_n\\) and suppose \\(X = \\{1, 2, \\dots, n\\}.\\)  Suppose that there exists a subset \\(\\{n_1, n_2, \\dots, n_k\\}\\) of \\(X\\) such that  \\[\\begin{align*} \\sigma(n_1) = n_2, \\sigma(n_2) = n_3, \\dots, \\sigma(n_k) = n_1. \\end{align*}\\] <p>Then \\(\\{n_1, n_2, \\dots, n_k\\}\\) is called a  cycle, and we denote this cycle as </p> \\[ \\sigma = \\begin{pmatrix} n_1 &amp; n_2 &amp; \\cdots &amp; n_k \\end{pmatrix}. \\] <p>We then read this as \"\\(n_1 \\to n_2\\), \\(n_2 \\to n_3, \\dots, n_k \\to n_1\\)\". </p> <p>\\textcolor{Blue}{Why do we care about cycles?}  \\ Well, consider an arbitrary cycle $            \\sigma = \\begin{pmatrix} n_1 &amp; n_2 &amp; \\cdots &amp; n_k \\end{pmatrix}.$ Then again, \\(\\sigma(n_1) = n_2, \\sigma(n_2) = n_3, \\dots, \\sigma(n_k) = n_1.\\) However, what this is really saying is that </p> \\[ \\sigma(n_1) = n_2, \\sigma^2(n_1) = n_3, \\dots, \\sigma^{{k-1}}(n_1) = n_k, \\sigma^k(n_1) = n_1. \\] <p>However, also take a note to observe that </p> \\[ \\sigma(n_2) = n_3, \\sigma^2(n_2) = n_4, \\dots, \\sigma^{{k-1}}(n_2) = n_1, \\sigma^k(n_2) = n_2. \\] <p>More generally, we see that \\textcolor{blue}{the element \\(\\sigma \\in S_n\\) has order \\(n_k\\)}, which is why the cycle length is \\(k\\). </p> <p>\\textcolor{Blue}{We care about cycles} since, given the fact that \\(S_n\\) is always a finite group, each of its elements will have finite order. Thus, in some way, we can always represent the elements of \\(S_n\\) in this form. \\ \\ More definitions. \\ If $            \\begin{pmatrix} n_1 &amp; n_2 &amp; \\cdots &amp; n_k \\end{pmatrix}$ and $            \\begin{pmatrix} n'_1 &amp; n'_2 &amp; \\cdots &amp; n'_k \\end{pmatrix}$  share no elements in common, i.e., </p> \\[ \\{n_1, n_2, \\dots, n_k\\} \\cap \\{n_1', n_2',  \\dots, n_k'\\} = \\varnothing \\] <p>then the cycles are defined as disjoint cycles. \\</p> <p>Note that if \\(\\sigma(i) = i\\) for some \\(i \\in X\\), then this is technically a cycle and we represent the cycle as $            \\begin{pmatrix} i \\end{pmatrix}.$ In this case, we say that \\(\\sigma\\) fixes \\(i\\). \\</p> <p>For example, suppose we have a permutation \\(\\sigma \\in S_5\\) where \\(\\sigma(1) = 2, \\sigma(2) = 4, \\sigma(4) = 2\\). Then we have a cycle of length 4 and we denote this as </p> \\[ \\begin{pmatrix} 1 &amp; 2 &amp; 4 \\end{pmatrix}. \\] <p>Since \\(\\sigma \\in S_5\\), suppose further that \\(\\sigma(3) = 5\\) and \\(\\sigma(5) = 3\\). Then we see that we have another cycle, disjoint with the previous cycle, and we write this one as</p> \\[ \\begin{pmatrix} 3 &amp; 5 \\end{pmatrix}. \\] <p>To write the entire permutation, we then can then express \\(\\sigma\\) as </p> \\[ \\sigma =  \\begin{pmatrix} 1 &amp; 2 &amp; 4 \\end{pmatrix} \\begin{pmatrix} 3 &amp; 5 \\end{pmatrix} \\] <p>which gives us all the information we need to know on how \\(\\sigma\\) rearranges the elements of \\(X\\). Such a representation of a permutation is called a disjoint cycle decomposition.  It will turn out that we can actually express every permutation \\(\\sigma \\in S_n\\) in a product of disjoint cycles.  Remark. In general, 1-cycles are omitted in the representation of a disjoint cycle decomposition. Thus if we have a permutation \\(\\sigma \\in S_3\\) such that \\(\\sigma(1) = 2\\), \\(\\sigma(2) = 1\\) and \\(\\sigma(3) = 3\\), then we would write this as </p> \\[ \\sigma = \\begin{pmatrix} 1 &amp; 2 \\end{pmatrix}. \\] <p>Such a statement leads us to conclude that \\(\\sigma(3) = 3\\). And if \\(\\sigma \\in S_5\\), we would furthermore conclude that not only \\(\\sigma(3) = 3\\), but also \\(\\sigma(4) = 4\\) and \\(\\sigma(5) = 5\\). \\ \\ Nonuniqueness. One thing to note is that cycles are not unique. For example, we could have written the cycle $\\textcolor{ForestGreen}{\\begin{pmatrix} 1 &amp; 2 &amp; 4 \\end{pmatrix}} $ as \\(\\textcolor{OrangeRed}{\\begin{pmatrix} 2 &amp; 4 &amp; 1 \\end{pmatrix}}\\) or \\(\\textcolor{Cyan}{\\begin{pmatrix} 4 &amp; 1 &amp; 2 \\end{pmatrix}}\\), since the other expressions still capture the fact that 1 is sent to 2, 2 is sent to 4, and 4 is sent to 1. </p> <p> Note that the colors correspond to where the cycle starts. Clearly in the diagram, there are three ways to start the cycle, and hence why there are three nonunique representations for the cycle.  \\ More generally, for any cycle $            \\begin{pmatrix} i_1 &amp; i_2 &amp; \\cdots &amp; i_n \\end{pmatrix}$ we have that </p> \\[ \\begin{pmatrix} i_1 &amp; i_2 &amp; \\cdots &amp; i_n \\end{pmatrix} = \\begin{pmatrix} i_2 &amp; i_3 &amp; \\cdots &amp; i_n &amp; i_1 \\end{pmatrix} =  \\cdots  =  \\begin{pmatrix} i_n &amp; i_1 &amp; \\cdots &amp; i_{n-1} \\end{pmatrix}. \\]"},{"location":"algebra/Groups/Quotient%20Groups./","title":"1.7. Quotient Groups.","text":"<p>The work done in the previous section on Normal subgroups now leads to the formulation of the Quotient Group. Up to this point we've studied groups which have familiar, concerete objects, but now we're going to get a little bit abstract. We're going to look at the useful concept of the quotient group, \\(G/H\\), which is a  group whose elements are \\(H\\) cosets. That is, the elements of our group are going to be sets themselves. The operation on the elements of the quotient group can only make sense if the cosets are from a subgroup \\(H\\) which is normal to \\(G\\).</p> <p> Let \\(G\\) be a group and \\(H \\normal G\\). Define \\(G/H\\) to be the set consisting of all the possible right (or equivalently left) \\(H\\) cosets. If we equip this set with a product \\(\\cdot\\) such that  \\[ (Ha)\\cdot(Hb) = H(ab) \\] <p>then \\(G/H\\) forms a group, called the Quotient Group.  Let's review what this is saying. Basically, if we have a normal subgroup \\(H\\) of \\(G\\), the set of cosets \\(\\{Hg_1, Hg_2, \\dots \\}\\) with the product \\(Hg_1 \\cdot Hg_2 = H(g_1g_2)\\) forms a group.</p> <p> \\begin{description} \\item[Identity.] To show that this set is a group, we first define the identity element to simply be \\(H\\). This is a \"trivial\" coset, and for any \\(Ha\\), where \\(a \\in G\\),  \\[\\begin{align*} (Ha)(H) = Ha \\\\ (H)(Ha) = Ha \\end{align*}\\] <p>so \\(H\\) is a natural and apporopriate choice for an identity as it has the property of an identity element.  </p> <p>\\item[Associativity.] Associativity is derived from the associativity of our group \\(G\\) itself. Observe that for any \\(a, b, c \\in G\\) we have </p> \\[\\begin{align*} (Ha)[ (Hb)(Hc)] = Ha[H(bc)] = H(abc)\\\\ [(Ha)(Hb)](Hc) = [H(ab)]Hc = H(abc). \\end{align*}\\] <p>Therefore \\((Ha)[ (Hb)(Hc)] = [(Ha)(Hb)](Hc)\\) for all \\(a, b, c \\in G\\), so the product relation is associative.</p> <p>\\item[Closedness.] The result of our proposed product is always a coset itself (\\(Ha \\cdot Hb = H(ab)\\)), and since  \\(G/H\\) is a set of all \\(H\\) cosets we see that this set is closed under \\(\\cdot\\).</p> <p>\\item[Inverses.] For any \\(Ha \\in G/H\\), where \\(a \\in G\\), we see that the inverse element is \\(Ha^{-1}\\), since </p> \\[\\begin{align*} (Ha)(Ha^{-1}) = H(aa^{-1}) = H\\\\ (Ha^{-1})(Ha) = H(a^{-1}a) = H \\end{align*}\\] <p>and we already defined \\(H\\) to be our identity element. So our proposed inverse makes sense. Note that \\(Ha^{-1} \\in G/H\\) since \\(a^{-1} \\in G\\), so an inverse element not only exists but it also exists in \\(G/H\\) \\end{description} All together, this allows us to observe that we have a group structure, so long as \\(H \\normal G\\).  {\\color{purple}(Why do we need this the condition that \\(H \\normal G\\)? Well, because the only way we can make damn sure that \\((Ha)(Hb) = H(ab)\\) is by Theorem 1.10, which requires that \\(H \\normal G\\).) }</p> <p>{\\color{NavyBlue} Note that there is another way to think about \\(G/H\\). The elements of the quotient group are cosets, right? However, let us not forget that cosets are simply \\textbf{\\textit{equivalence classes which respect the following equivalence relation}} }: {\\color{Black} if \\(G\\) is a group, \\(H\\) is a subgroup, then for any \\(a, b \\in G\\) we say that \\(a \\sim b\\) if and only if \\(ab^{-1} \\in H\\).} {\\color{NavyBlue} Thus we can recast our definition follows:} \\</p> <p>\\begingroup \\par \\leftskip25pt \\rightskip\\leftskip \\noindent Let \\(H \\normal G\\). Then the set \\(G/H\\) is defined to consist of all of the \\sout{right (or left) cosets of \\(H\\) in \\(G\\)} equivalence classes of the elements of \\(G\\) (under the equivalence relation stated in the previous paragraph).  \\par \\endgroup \\vspace{1cm}</p> <p>{\\color{Violet}We thus have two equivalent ways to interpret the meaning of a quotient group. One involves equivalence classes, while the other involves cosets. In our case it seems more complicated to think about equivalence classes. However, in different applications of group theory (such as to algebraic geometry and topology) it will be convenient to interpret quotient groups as equivalence classes. For now, we'll stick with the coset interpretation, since it's the easiest way to understand a quotient group. } \\ </p> <p>Example. Recall that we showed \\(SL_n(\\mathbb{R}) \\normal GL_n(\\mathbb{R})\\). Thus the quotient group \\(GL_n(\\mathbb{R})/SL_n(\\mathbb{R})\\) makes sense by Theorem 1.11, so let's see what this group looks like.</p> <p>First, the identity element of our group is \\(SL_n(\\mathbb{R})\\). \\ \\ \\indent In dealing with quotient groups, you may be wondering the following questions:\\ \\textcolor{ForestGreen}{Q: If \\(H\\) is a normal subgroup of \\(G\\), and \\(G\\) is abelian, is \\(G/H\\) abelian? If \\(G/H\\) is abelian, is \\(G\\) abelian?} \\ A: The answer to the first question is yes.  Observe that by definition, \\(G/H = \\{aH \\mid a \\in G\\}.\\) But since \\(H\\)  is normal, we know that \\(gH = Hg\\) for all \\(g \\in G\\).  Thus observe that for \\(aH, bH \\in G/H\\), we have that </p> \\[\\begin{align*} (aH)(bH) =(ab)H &amp;= (ba)H \\text{ (since } G \\text{ is abelian) }\\\\ &amp;= (bH)(aH). \\end{align*}\\] <p>Thus the set \\(G/H\\) must be abelian. \\ \\ The answer to the second question is \\textbf{no, not always}. If \\(G/H\\) is abelian,  we know that  $$ (aH)(bH) = (bH)(aH) \\implies (ab)H = (ba)H. $$  for all \\(a, b \\in G\\). However, this only guarantees set equality,  not a term-by-term equality (in which case the group would be abelian).  An example of this is \\(D_{6}\\) with the subgroup \\(H = \\{1, r, r^2\\}.\\) In this case \\(H \\unlhd D_6\\) because all the left cosets are \\(H, sH\\) and therefore  \\([D_{2n}: H] = 2\\) (Hence \\(H \\normal G\\) by the previous proposition). In addition,  \\(H(sH) = sH=  sH(H)\\), \\(sH(sH) = s^2H = (sH)sH\\), so \\(G/H\\) is abelian, but the set \\(D_{2n}\\) is itself not an abelian group. Thus, \\textbf{it is possible for \\(G/H\\) to be ableian while \\(G\\) itself is not abelian } \\ \\ Another fun example for when the quotient group \\(G/H\\) is abelian, even though the group \\(G\\) is abelian, is the following. \\ \\ Example. Let </p> \\[ G = \\left\\{ \\begin{pmatrix} a &amp; b \\\\ 0 &amp; 1     \\end{pmatrix} \\mid a, b \\in \\mathbb{R}, a \\ne 0\\right\\},  \\quad H =  \\left\\{ \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1 \\end{pmatrix} \\mid c \\in \\mathbb{R}  \\right\\}.    \\] <p>\\(G\\) is subset of \\(GL_2(\\mathbb{R})\\) and \\(H\\) is a subgroup of \\(G\\). \\begin{description} \\item[\\(\\bm{H \\normal G}\\).] First we'll show that \\(H\\) is normal to \\(G\\). Thus let \\(x \\in G\\), so that  $ x =         \\begin{pmatrix} a &amp; b \\ 0 &amp; 1   \\end{pmatrix} $ for some \\(a, b \\in \\mathbb{R}\\) where \\(a \\ne 0\\). Now let \\(h \\in H\\) so that  $ h = \\begin{pmatrix} 1 &amp; c \\ 0 &amp; 1   \\end{pmatrix} $ for some \\(c \\in \\mathbb{R}\\). Then observe that </p> \\[\\begin{align*} xhx^{-1} &amp;=  \\begin{pmatrix} a &amp; b \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} 1/a &amp; -b/a \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} a &amp; b \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} 1/a &amp; -b/a + c \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} 1 &amp; (-b + ca) + b \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;= \\begin{pmatrix} 1 &amp; ca \\\\ 0 &amp; 1     \\end{pmatrix} \\in H. \\end{align*}\\] <p>Therefore, we have that \\(xhx^{-1} \\in H\\) for all \\(H\\), which implies that \\(H\\) is a normal subgroup of \\(G\\). </p> <p>\\item[\\(\\bm{G/H}\\) is abelian.] Now we'll show that \\(G/H\\) is an abelian group. Firstly, what does it mean for a quotient group to abelian? Well, it would mean that for any \\(x, y \\in G\\) we have that </p> \\[ (Hx)\\cdot(Hy) = (Hy)\\cdot(Hx). \\] <p>Or, in other words, </p> \\[ H(xy) = H(yx).    \\] <p>Thus we need some kind of set equality to be happening. Thus consider \\(h =             \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1     \\end{pmatrix}\\), where again \\(x \\in \\mathbb{R}\\), and suppose \\(x =             \\begin{pmatrix} a_x &amp; b_x \\\\ 0 &amp; 1     \\end{pmatrix}\\) and \\(y =             \\begin{pmatrix} a_y &amp; b_y \\\\ 0 &amp; 1     \\end{pmatrix}\\) where \\(a_x,a_y,b_x,b_y \\in \\mathbb{R}\\) and \\(a_y, a_x \\ne 0\\). Then observe that </p> <p>\\begin{minipage}{0.40\\textwidth}</p> \\[\\begin{align*} hxy &amp;=  \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} a_x &amp; b_x \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} a_y &amp; b_y \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;=  \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} a_xa_y &amp; a_xb_y + b_x \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;=  \\begin{pmatrix} a_xa_y &amp; a_xb_y + b_x + c \\\\ 0 &amp; 1     \\end{pmatrix} \\end{align*}\\] <p>\\end{minipage} \\hfill \\begin{minipage}{0.5\\textwidth}</p> \\[\\begin{align*} hyx &amp;=  \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} a_y &amp; b_y \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} a_x &amp; b_x \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;=  \\begin{pmatrix} 1 &amp; c \\\\ 0 &amp; 1     \\end{pmatrix} \\begin{pmatrix} a_ya_x &amp; a_yb_x+ b_y \\\\ 0 &amp; 1     \\end{pmatrix}\\\\ &amp;=  \\begin{pmatrix} a_ya_x &amp; a_yb_x + b_y + c \\\\ 0 &amp; 1     \\end{pmatrix}. \\end{align*}\\] <p>\\end{minipage} \\textcolor{purple}{Note that the (1,1) entry in both matrices are equal; that is, \\(a_xa_y = a_ya_x\\) since they are members of \\(\\mathbb{R}\\).} Therefore, we see that </p> \\[\\begin{align*} Hxy =  \\left\\{  \\begin{pmatrix} a_xa_y &amp; a_xb_y + b_x + c \\\\ 0 &amp; 1     \\end{pmatrix} \\mid  a_x,a_y,b_x,b_y, c \\in \\mathbb{R}, a_x, a_y \\ne 0 \\right \\}\\\\ Hyx = \\left\\{ \\begin{pmatrix} a_xa_y &amp; a_yb_x + b_y + c \\\\ 0 &amp; 1     \\end{pmatrix}. \\mid  a_x,a_y,b_x,b_y, c \\in \\mathbb{R}, a_x, a_y \\ne 0 \\right\\}. \\end{align*}\\] <p>Since \\(b_x, b_y, c\\) are arbitrary members of \\(\\mathbb{R}\\), we can replace their sums with another arbitrary \\(c', c'' \\in \\mathbb{R}\\). Then we see that </p> \\[\\begin{align*} Hxy =  \\left\\{  \\begin{pmatrix} a_xa_y &amp; a_xb_y +c' \\\\ 0 &amp; 1     \\end{pmatrix} \\mid  a_x,a_y,b_y, c' \\in \\mathbb{R}, a_x, a_y \\ne 0 \\right \\}\\\\ Hyx = \\left\\{ \\begin{pmatrix} a_xa_y &amp; a_yb_x + c'' \\\\ 0 &amp; 1     \\end{pmatrix} \\mid  a_x,a_y,b_x,c'' \\in \\mathbb{R}, a_x, a_y \\ne 0, \\right\\}. \\end{align*}\\] <p>After cleaning up the sets, we can now see they are equal, which wasn't as obvious as it was before. They're equal because their criteria for set memberships are identical; they just have different variables, but that of course does not change their members. Therefore we see that \\(Hxy = Hyx\\) for all \\(x, y \\in G\\), which proves that \\(G/H\\) is an abelian group, even though \\(G\\) nor \\(H\\) are abelian. </p> <p>\\end{description}</p>"},{"location":"algebra/Groups/Sylow%20Theorems./","title":"1.11. Sylow Theorems.","text":"<p>Lagrange's Theorem states that \\(H \\le G\\), then \\(|H|\\) divides \\(|G|\\). However, you may wonder if there is some kind of converse. If \\(k\\) divides \\(|G|\\), is there a subgroup of order \\(k\\)? </p> <p>By Cauchy's Theorem, we know that if \\(p\\) is a prime which divides then there exists an element of order \\(p\\). Can we generalize this result further (for example, state how many such elements satisfy this)? </p> <p>The answer to both questions is yes and is achieved through Sylow's Theorem. It's a foundational theorem in finite group theory, as it strengthens our two most power theorems for finite groups: Lagrange's Theorem and Cauchy's Theorem.</p> <p> \\(H\\) is a \\(p\\)-subgroup of a group \\(G\\) if \\(H\\) is a subgroup of \\(G\\) and \\(|H| = p^n\\) for some \\(n \\ge 1\\). </p> <p> Let \\(G\\) be a group and let \\(p\\) be a prime such that \\(p\\mid |G|\\). Suppose \\(p^k\\) is the largest power such that \\(p^k \\mid |G|\\). That is, \\(|G| = p^km\\) for some integer \\(m \\in G\\), \\(\\mbox{gcd}(p, m) = 1\\). Then any subgroup \\(H\\) of \\(G\\) with \\(|H| = p^k\\) is called a Sylow \\(p\\)-subgroup. \\ \\ An equivalent definition is the following: \\(H\\) is a  Sylow-\\(p\\) subgroup if \\(H\\) is a \\(p\\)-subgroup where \\(|H| = p^k\\). </p> <p>\\textcolor{purple}{A Sylow \\(p\\)-subgroup is nothing more than a subgroup \\(H\\) where \\(|H| = p^k\\) and \\(|G| = p^km\\) where \\(\\mbox{gcd(p, m)} = 1\\).}</p> <p> Let \\(G\\) be a group and \\(P\\) and \\(Q\\) be subgroups of \\(G\\). If there exists an element \\(g \\in G\\) such that  \\[ gPg^{-1} = Q \\] <p>then \\(P\\) is conjugate to \\(Q\\).  Recall that if \\(H \\normal G\\), then for any \\(g \\in G\\) we see that \\(gHg^{-1} = H\\). Thus \\(H\\) is conjugate to itself. Also note that if \\(P\\) is a subgroup then so is \\(gPg^{-1}\\).</p> <p>[ (Sylow Theorem)] Let \\(G\\) be a finite group and \\(p\\) a prime such that \\(p \\mid |G|\\). Suppose further that \\(|G| = p^km\\) where \\(\\mbox{gcd}(p, m) = 1\\). Then  <ul> <li> <p>[1.] There exists a Sylow \\(p\\)-subgroup (equivalently, there exists a subgroup \\(H\\) of \\(G\\) where \\(H = p^k\\)) and every \\(p\\)-subgroup of \\(G\\) is contained in some Sylow \\(p\\)-subgroup </p> </li> <li> <p>[2.] All Sylow \\(p\\)-subgroups are conjugate to each other, and the conjugate of any Sylow \\(p\\)-subgroup is also a Sylow \\(p\\)-subgroup</p> </li> <li> <p>[3.] If \\(n_p\\) is the number of Sylow \\(p\\)-subgroups, then </p> </li> </ul> \\[ n_p \\mid m \\hspace{0.5cm}\\text{and}\\hspace{0.5cm} n_p = 1 \\mbox{ mod } m. \\] <p>\\vspace{-.4cm} </p> <p> \\textcolor{NavyBlue}{We can prove the first part by letting \\(G\\) act on a special set \\(\\Omega\\). It will turn out that the stabilizer of our action will be the desired Sylow \\(p\\)-subgroup.} <ul> <li>[1.] Define </li> </ul> \\[ \\Omega = \\{ X \\subset G \\mid |X| = p^k\\} \\] <p>and let \\(G\\) act on \\(\\Omega\\) from the left. Observe that for \\(X \\in \\Omega\\), \\(g * X = \\{gx \\mid \\text{ for all } x \\in X\\} = gX.\\) Since \\(|gX| = |X| = p^k\\), we see that \\(gX \\in \\Omega\\). Associativity and identity applications are trivial, so we get that this is a group action.</p> <p>\\textcolor{NavyBlue}{Now that we have shown that this is a group action, we will consider the orbits of the group aciton.}</p> <p>Since \\(|G| = p^km\\), there are \\(\\displaystyle \\binom{p^km}{p^k}\\) many ways for us to choose a subset \\(X\\) of \\(G\\) with size \\(p^k\\). Hence \\(|\\Omega| = \\displaystyle \\binom{p^km}{p^k}\\). Note that since this is a group action, the orbits form a partition of \\(X\\). Now from number theory, we know that </p> \\[ \\binom{p^km}{p^k} = m \\mbox{ mod } p. \\] <p>Since the orbits must partition \\(\\Omega\\), the above result tells us that we cannot partition \\(G\\) with sets which are divisible by \\(p\\). In other words, there must exist some orbit \\(\\mathcal{O}\\) such that \\(|\\mathcal{O}|\\) is not divisible by \\(p\\). </p> <p>\\textcolor{NavyBlue}{Now that we know that there exists an orbit not divisible by \\(p\\), we will anaylze the corresponding stabilizer of this orbit. This stabilizer will turn out to be our Sylow \\(p\\)-subgroup. }</p> <p>Let \\(H\\) be the orbit corresponding to \\(\\mathcal{O}\\). Then by the Orbit-Stabilizer Theorem </p> \\[ |G| = |\\mathcal{O}||H| \\implies p^km = |\\mathcal{O}||H|.                 \\] <p>By the last equation, we see that \\(p^k\\) must divide both sides. However, \\(|\\mathcal{O}|\\) is not divisible by \\(p\\). Hence \\(|H|\\) must be divisible by \\(p^k\\).</p> <p>However, by Lagrange's Theorem, \\(|H|\\) divides \\(|G| = p^km\\). Therefore \\(|H| = m\\) or \\(|H| \\in \\{1, p, p^2, \\dots, p^k\\}\\). In either case \\(|H| \\le p^k\\) (since \\(m \\le p\\)). But we just showed that \\(p^k\\) divides \\(|H|\\), which proves that \\(|H| = p^k\\). </p> <p>Since \\(H\\) is a stabilizer, \\(H \\le G\\), so we have effectively proved the existence of a subgroup of order \\(p^k\\); or, in other words, a Sylow \\(p\\)-subgroup. </p> <ul> <li> <p>[2.] Suppose \\(H\\) and \\(K\\) are Sylow \\(p\\)-subgroups of \\(G\\).  Then observe that </p> </li> <li> <p>[3.] </p> </li> </ul> <p></p> <p>The consequences of this theorem are immediate. </p> <p> Let \\(G\\) be a finite group and suppose \\(|G| = p^km\\) for some prime \\(p\\) where \\(\\mbox{gcd}(p, m) = 1\\). Then \\(G\\) has a normal subgroup of order \\(p^k\\) if and only if \\(n_p = 1\\). </p> <p> (\\(\\implies\\)) Suppose \\(G\\) has a normal subgroup \\(H\\) of order \\(p^k\\). By Sylow's Theorem, we know that all  other Sylow \\(p\\)-subgroups are conjugate to \\(H\\). Thus let \\(g \\in G\\) and observe that  \\[ gHg^{-1} = H \\] <p>since \\(H\\) is normal. Therefore, there are no other Sylow \\(p\\)-subgroups so \\(n_p = 1.\\)</p> <p>(\\(\\impliedby\\)) Now suppose that \\(n_p = 1\\). Let \\(H\\) be a sole Sylow \\(p\\)-subgroup of \\(G\\). Since it is the only Sylow \\(p\\)-subgroup, we see that </p> \\[ gHg^{-1} = H \\] <p>for all \\(g \\in G\\). However this exactly the definition for \\(H\\) to be a normal subgroup of \\(G\\). This proves the result. </p> <p>Once you use Sylow's Theorem and study finite groups more, you'll realize that some groups aren't that complicated. For example, consider any subgroup of order 4. This can be any wild group you want, but at the end of the day, it turns out one of the following options is true:</p> \\[ G \\cong \\mathbb{Z}/4\\mathbb{Z} \\hspace{0.2cm}\\text{ or }\\hspace{.2cm}  G \\cong \\mathbb{Z}/2\\mathbb{Z} \\times \\mathbb{Z}/2\\mathbb{Z}. \\] <p>The process leading to such a conclusion is known as classifying groups up to an isomorphism. That is, you start with a group with a fixed order, and then determine much simpler groups that your group could be isomorphic to. In our example, we say that any group of order 4 can only be two things up to an isomorphism.</p> <p>The cool thing about Sylow's Theorem is that it is so strong that it allows us to classify groups up to an isomorphism. </p> <p>In general, when classifying groups up to an isomorphism, it is convenient to do in terms of integer groups \\(\\mathbb{Z}\\) or modulo integer groups, as we saw above. This isn't always possible, but when it is, the following theorem comes in handy.</p> <p> Let \\(m, n\\) be positive integers. Then  \\[ \\mathbb{Z}/mn\\mathbb{Z} \\cong \\mathbb{Z}/m\\mathbb{Z} \\times \\mathbb{Z}/n\\mathbb{Z} \\] <p>if and only if \\(m\\) and \\(n\\) are coprime. </p> <p>\\noindent Example. \\ \\textcolor{NavyBlue}{Suppose we want to classify all groups of order 1225 up to an isomorphism.} \\ Let \\(G\\) be a group such that \\(|G| = 1225 = 5^27^2\\). Then observe \\(\\mbox{gcd}(5, 7^2) = 1\\). By Sylow's theorem, we know that  if \\(n_5\\) is the number of Sylow \\(5\\)-subgroups of \\(G\\), then </p> \\[ n_5 \\big| 7^2 \\quad \\text{ and }\\quad n_5 \\equiv 1 \\mbox{ mod } 5. \\] <p>Observe that \\(n_5\\) can only equal 1. Since \\(n_5 = 1\\), we know by Propsition \\ref{sylow_normal} that for the unique Sylow 5-subgroup \\(H\\) that \\(H \\unlhd G\\). Also note that \\(|H| = 5^2\\). \\ \\ Now observe that \\(\\mbox{gcd}(7, 5^2) = 1\\). By Sylow's Theorem, we know that if \\(n_7\\) is the number of Sylow 7-subgroups of \\(G\\) that </p> \\[ n_7 \\big| 5^2 \\quad \\text{ and }\\quad n_7 \\equiv 1 \\mbox{ mod } 7. \\] <p>Note that \\(n_7\\) must also equal 1. Thus again for the unique Sylow 7-subgroup \\(K\\), we must have that \\(K \\unlhd G\\) and \\(|K| = 7^2\\). Now we can observe that (1) \\(\\mbox{gcd}(|H|, |K|) = 1\\) and (2) \\(|G| = |H||K|\\) so that</p> \\[ G \\cong H \\times K      \\] <p>by Theorem 1.\\ref{product_theorem}.  Now observe that since \\(|H| = 5^2\\), \\(H \\cong \\mathbb{Z}/25\\mathbb{Z}\\) and \\(H \\cong \\mathbb{Z}/5\\mathbb{Z} \\times \\mathbb{Z}/5\\mathbb{Z}\\). Since \\(K = 7^2\\), \\(K \\cong  \\mathbb{Z}/49\\mathbb{Z}\\) and \\(H \\cong (\\mathbb{Z}/7\\mathbb{Z} \\times \\mathbb{Z}/7\\mathbb{Z})\\). Therefore, we see that the groups of order 1225 are, up to isomorphism, </p> <ul> <li> <p>[(1)] \\((\\mathbb{Z}/25\\mathbb{Z}) \\times (\\mathbb{Z}/49\\mathbb{Z})\\)</p> </li> <li> <p>[(2)] \\((\\mathbb{Z}/25\\mathbb{Z}) \\times (\\mathbb{Z}/7\\mathbb{Z} \\times \\mathbb{Z}/7\\mathbb{Z})\\)</p> </li> <li> <p>[(3)] \\((\\mathbb{Z}/5\\mathbb{Z} \\times \\mathbb{Z}/5\\mathbb{Z}) \\times \\mathbb{Z}/49\\mathbb{Z}\\)</p> </li> <li> <p>[(4)] \\((\\mathbb{Z}/5\\mathbb{Z} \\times \\mathbb{Z}/5\\mathbb{Z}) \\times (\\mathbb{Z}/7\\mathbb{Z} \\times \\mathbb{Z}/7\\mathbb{Z})\\).</p> </li> </ul> <p>\\textcolor{purple}{We suspect that these are all the groups of order 1225 up to an isomorphism. However, we double check that none of these groups are actually equivalent to each other, i.e., that we have no redundancies.}</p> <p>Observe that (1) is not isomorphic to any of the the other groups, since \\((1, 1) \\in (\\mathbb{Z}/25\\mathbb{Z}) \\times (\\mathbb{Z}/49\\mathbb{Z})\\), has order 1225 but none of the other groups have an element of order 1225. \\ \\ In addition, (3) is not isomorphic to (2) or (3) since \\((0, 1) \\in (\\mathbb{Z}/5\\mathbb{Z} \\times \\mathbb{Z}/5\\mathbb{Z}) \\times \\mathbb{Z}/49\\mathbb{Z}\\) and has order 49 but no element of either (2) or (3) has an element of either 49.  \\ \\ Finally, we see that (2) is not isomorphic to (4) because \\((1, 0) \\in (\\mathbb{Z}/25\\mathbb{Z}) \\times (\\mathbb{Z}/7\\mathbb{Z} \\times \\mathbb{Z}/7\\mathbb{Z})\\) is an element of order 25 but there is no element of order 25 in (4). Thus we see that (1) these subgroups are isomorphic to \\(G\\) and (2) none of them are isomorphic to each other. Therefore, this an exhaustive list of all the groups of order 1225 up to isomorphism.</p> <p>Here's another example in which Sylow's Theorem helps us classify a specific type of group.</p> <p> Let \\(p,q\\) be primes with \\(p&lt;q\\) and suppose \\(p\\) does not divide \\(q-1\\).  If \\(G\\) is a group such that \\(|G| = pq\\), then  \\(G \\cong \\mathbb{Z}/pq\\mathbb{Z}\\).  </p> <p> Let \\(G\\) be a group and \\(|G| = pq\\). Since \\(\\mbox{gcd}(p, q) = 1,\\) by the Sylow Theorem,  there exists a Sylow \\(p\\)-subgroup and Sylow \\(q\\)-subgroup of \\(G\\).  \\ Now let \\(n_p\\) and \\(n_q\\) be the number of Sylow \\(p\\) and \\(q\\)-subgroups, respectively. Then observe that  \\[ n_p \\big|q \\qquad n_p \\equiv 1 \\mbox{ mod } p    \\] <p>so that \\(n_p = 1\\) and </p> \\[ n_q \\big|p   \\qquad n_q \\equiv 1 \\mbox{ mod } q. \\] <p>Now observe that \\(n_p = 1\\) or \\(q\\). However, since \\(p\\) does not divide \\(q - 1\\), we know that </p> \\[ q \\not\\equiv 1 \\mbox{ mod } p.   \\] <p>Thus \\(n_p = 1\\). Again, either \\(n_p = 1\\) or \\(p\\) but \\(p &lt; q\\) so </p> \\[ n_q \\not\\equiv 1 \\mbox{ mod } q  \\] <p>unless \\(n_q = 1\\). Thus there is one and only one Sylow \\(p\\)-subgroup and Sylow \\(q\\)-subgroup, which we can call \\(H\\) and \\(K\\) respectively. By proposition \\ref{sylow_normal}, </p> \\[ H \\unlhd G \\qquad K \\unlhd G.   \\] <p>Note that (1) \\(\\mbox{gcd}(|H|, |K|) = \\mbox{gcd}(p, q) = 1\\) and (2) \\(|G| = |H||K| = pq\\). Thus \\(G \\cong H \\times K\\) by Theorem 1.\\ref{product_theorem}. Now observe that \\(H\\) and \\(K\\) are of prime order, so that \\(H \\cong \\mathbb{Z}/p\\mathbb{Z}\\) and \\(K \\cong \\mathbb{Z}/q\\mathbb{Z}\\). We then see that </p> \\[ G \\cong  \\mathbb{Z}/p\\mathbb{Z} \\times \\mathbb{Z}/q\\mathbb{Z}. \\] <p>From theorem ???, we know that if \\(m, n\\) are positive integers and \\(\\mbox{gcd}(m, n) = 1\\), then </p> \\[ \\mathbb{Z}/m\\mathbb{Z} \\times \\mathbb{Z}/n\\mathbb{Z}  \\cong \\mathbb{Z}/mn\\mathbb{Z}. \\] <p>Obviously, \\(\\mbox{gcd}(p, q) = 1\\), so that </p> \\[ \\mathbb{Z}/p\\mathbb{Z} \\times \\mathbb{Z}/q\\mathbb{Z} \\cong \\mathbb{Z}/pq\\mathbb{Z}. \\] <p>Now isomorphic relations are transitive, so we can finally state that </p> \\[ G \\cong \\mathbb{Z}/pq\\mathbb{Z} \\] <p>as desired. </p>"},{"location":"algebra/Modules/Cartesian%20Products%20and%20Direct%20Sums.%20/","title":"3.4. Cartesian Products and Direct Sums.","text":"<p>In group and ring theories, we can make sense of the idea of a cartesian product of groups or rings. Thus it is again no surprise that we can construct cartesian products of modules. </p> <p>However, we shall see a theme that is common in most areas of mathematics: infinite products behave differently than finite products. In fact, it usually turns out that our intuitive definiton for the products of our objects (in our case, modules) is usually wrong and cumbersome, even though it feels intuitive.  That is, we generally want to define products using the cartesian notion, but this usually just gives us more problems. </p> <p>The alternative is to come up with a definition of multiplication that is cartesian for finite products, but is not exactly cartesian for infinite products. This will make sense once we are more specific by what we mean.</p> <p> Let \\(M_1, M_2, \\dots, M_n\\) be a set of \\(R\\)-modules. We define \\[ \\prod_{i = 1}^{n}M_i =  M_1\\times M_2 \\times \\cdots \\times M_n  \\] <p>as the cartesian product of these \\(R\\)-modules whose elements are of the form \\((x_1, x_2, \\dots, x_n)\\) where \\(x_i \\in M_i\\) for \\(i \\in \\{1, 2, \\dots, n\\}\\). </p> <p>More generally, if \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) is an arbitrary family of \\(R\\)-modules then \\(\\displaystyle \\prod_{\\alpha \\in \\lambda} M_\\alpha\\) is the arbitrary cartesian product. </p> <p>\\textcolor{purple}{You may now ask how we notate, or even describe these elements. We can't put them in a tuple, since they're not finite. We could put them in a tuple like \"\\((x_1, x_2, \\dots)\\)\", where the ellipsis implies an infinite list of elements, but that would only take care of at most countable families of \\(R\\)-modules. } \\textcolor{MidnightBlue}{ \\ \\ \\indent Instead, we use the following idea as elements of \\(\\displaystyle \\prod_{\\alpha \\in \\lambda} M_\\alpha\\) being \"functions.\" This is an abstract, yet quite useful strategy used in different areas of mathematics to deal with arbitrary products.  \\ \\ \\indent Let us first literally describe our elements. An element \\(\\displaystyle a \\in \\prod_{\\alpha \\in \\lambda} M_\\alpha\\) is uniquely determined by selecting one element \\(m_\\alpha \\in M_\\alpha\\) for each \\(\\alpha \\in \\lambda\\). This is how a tuple works. For example, in \\(\\mathbb{R}^3\\), we separately pick 3 elements out of 3 separate copies of \\(\\RR\\) to form a tuple \\((x_1, x_2, x_3) \\in \\mathbb{R}^3\\). \\ \\ Thus for each \\(\\displaystyle a \\in \\prod_{\\alpha \\in \\lambda}\\) we may associate \\(a\\) with a function \\(f_a: \\lambda \\to \\prod_{\\alpha \\in \\lambda}M_\\alpha\\) which iterates through all \\(\\alpha \\in \\lambda\\) and picks out an element \\(M_\\alpha\\). For example, if we know that, for \\(i \\in \\lambda\\), the \\(i\\)-th coordinate of \\(a\\) is \\(x\\), then \\(f_a(i) = x\\).  }</p> <p> \\textcolor{NavyBlue}{ The above diagram illustrates our descriptions so far, where in the case above we have that the \\(\\alpha\\)-th element of \\(a\\) is \\(x_\\alpha\\), the \\(\\beta\\)-th element of \\(a\\) is \\(x_\\beta\\), and so on. With that said, we can now restate that </p> \\[ \\prod_{\\alpha \\in \\lambda} M_\\alpha = \\{\\text{All functions } f \\mid  f(\\alpha) \\in M_\\alpha \\text{ where } \\alpha \\in \\lambda \\}. \\] <p>and move onto understanding why we want to adjust our definition for multiplication of \\(R\\)-modules.  }</p> <p>It turns out that we can make the arbitrary cartesian product into an \\(R\\)-module. </p> <p> If \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) is a family of \\(R\\)-modules, then \\(\\displaystyle \\prod_{\\alpha \\in  \\lambda}M_{\\alpha}\\) is an \\(R\\)-module. </p> <p> \\begin{description} \\item[Abelian Group.] First observe that \\(\\displaystyle \\prod_{\\alpha \\in \\lambda} M_\\alpha\\) is an abelian group if we realize the identity is the zero map \\(f\\) (i.e., the \"tuple\" of all zeros) and endow an operation of addition as follows. For \\(f_1, f_2 \\in \\displaystyle \\prod_{\\alpha \\in \\lambda} M_\\alpha\\) we have that  \\[ (f_1 + f_2)(\\alpha) = f_1(\\alpha) + f_2(\\alpha) \\] <p>for all \\(\\alpha \\in \\lambda\\). Note that this makes sense since \\(f_1(\\alpha), f_2(\\alpha) \\in M_\\alpha\\). Hence the sum will be an element in \\(M_\\alpha\\). Also, if \\(f \\in \\displaystyle \\prod_{\\alpha \\in \\lambda} M_\\alpha\\), we define the inverse to be \\(f^{-1}\\) where \\(f^{-1}(\\alpha) = -f(\\alpha)\\). Commutativity is inherited from commutativity of all \\(M_\\alpha\\), and so we have an abelian group.</p> <p>\\item[Ring Multiplication.] Let \\(a \\in R\\). Then define </p> \\[ (af)(\\alpha) = a(f(\\alpha))   \\] <p>for all \\(\\alpha \\in \\lambda\\). Observe that, since each \\(M_\\alpha\\) is an \\(R\\)-module, we have that \\(f(\\alpha) \\in M_\\alpha \\implies af(\\alpha) \\in M_\\alpha\\) for all \\(\\alpha \\in \\lambda\\). Thus our multiplcation is well-defined. It is then a simple exercise to check that the axioms of an \\(R\\)-module are satisfied via our operations. \\end{description} </p> <p>Since our above argument was a bit abstract, we reintroduce it in the language of finite products. Again, we can turn a finite cartesian product of \\(R\\)-modules into an \\(R\\)-module with the following operations.</p> <ul> <li>[1.] Let \\((m_1, m_2, \\dots, m_n), (p_1, p_2 ,\\dots, p_n) \\in M_1 \\times M_2 \\times \\cdots \\times M_n\\). Then let us define addition of elements as</li> </ul> \\[ (m_1, m_2, \\dots, m_n) + (p_1, p_2 ,\\dots, p_n) = (m_1 + p_1, m_2 + p_2, \\dots, m_n + p_n). \\] <ul> <li>[2.] For any \\(a \\in R\\) and \\((m_1, m_2, \\dots, m_n) \\in M_1 \\times M_2 \\times \\cdots \\times M_n\\) we define scalar multiplication as </li> </ul> \\[ a(m_1, m_2, \\dots, m_n) = (am_1, am_2, \\dots, am_n). \\] <p> Again, it is then simple to check that this satisfies the axioms for an \\(R\\)-module. </p> <p>\\textcolor{NavyBlue}{When we think of multiplying sets together, cartesian products usually come to mind. They are the most natural to us since it has been ingrained in us to think this way since primary school. However, it turns out in many areas of mathematics that the cartesian approach to defining multiplication of objects leads to undersirable properties, and objects often misbehave under a cartesian definition.  \\ \\ As we said earlier, the problems arise when the products get infinite. Hence the solution involves defining a new kind of multiplication which is the same as a cartesian product for finite products, but is different for infinite products.}</p> <p>This leads to the concept of direct sums, which we will use instead of cartesian products (we will soon see why).</p> <p> Let \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) be a family of \\(R\\)-modules. Then we define the direct sum of \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) as  \\[ \\bigoplus_{\\alpha \\in \\lambda}M_\\alpha = \\{\\text{All functions } f \\mid f(\\alpha) \\in M_\\alpha ** and ** f(\\alpha) = 0 \\text{ except for finitely many } \\alpha \\in \\lambda\\}. \\] <p></p> <p>The only difference between the direct sum and the cartesian product is that, for any point \\(\\displaystyle a \\in \\bigoplus_{\\alpha \\in \\lambda} M_\\alpha\\), all indices of \\(a\\) are zero except for finitely many indices. So only finitely many indices are nonzero for a direct sum, while in a cartesian product there may be finite, countable or uncountably many nonzero indices.</p> <p>\\textcolor{purple}{Thus, note that for a finite product, the direct sum and the cartesian product are the exact same thing}. There is no difference when the product is finite. In other words, </p> \\[ M_1 \\times M_2 \\times \\cdots \\times M_n = M_1 \\oplus M_2 \\oplus \\cdots \\oplus M_n. \\] <p> The direct sum of a family \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) of \\(R\\)-modules is an \\(R\\)-module. In fact, \\(\\displaystyle \\bigoplus_{\\alpha \\in  \\lambda} M_\\alpha\\) is an \\(R\\)-submodule of \\(\\displaystyle \\prod_{\\alpha \\in \\lambda} M_{\\alpha}\\). </p> <p> Note that \\(\\displaystyle \\bigoplus_{\\alpha \\in \\lambda} M_\\alpha \\subset \\prod_{\\alpha \\in \\lambda}M_\\alpha\\). Thus we can use the submodule test to check if is in fact an \\(R\\)-module. Observe that for any \\(a, b \\in R\\) and \\(\\displaystyle f_1, f_2 \\in \\bigoplus_{\\alpha \\in \\lambda}M_{\\alpha}\\), we have that  \\[ a(f_1)(\\alpha) + b(f_2)(\\alpha) \\in \\bigoplus_{\\alpha \\in \\lambda}M_{\\alpha}  \\] <p>since the function \\(a(f_1)(\\alpha) + b(f_2)(\\alpha)\\) will be nonzero for only finitely many values. (In fact, if \\(f_1\\) is nonzero for \\(k\\)-many values and \\(f_2\\) is nozero for \\(l\\) many values, then \\(a(f_1)(\\alpha) + b(f_2)(\\alpha)\\) is nonzero for at most \\(k + l\\)-many values). Hence this passes the submodule test. </p> <p>\\noindentWhy do we prefer direct sums over cartesian products?  \\</p> <p>The answer lies in the following observation. Suppose \\(\\{M_{\\alpha}\\}_{\\alpha \\in \\lambda}\\) is a family of \\(R\\)-modules and that for each \\(\\alpha \\in \\lambda\\) there exists a homomorphism \\(\\phi_\\alpha : M_\\alpha \\to N\\). Let \\(a \\in \\displaystyle \\prod_{\\alpha \\in \\lambda}M_\\alpha\\) and represent \\(a\\) with the map \\(f_a:\\lambda \\to \\displaystyle \\prod_{\\alpha \\in \\lambda}M_{\\alpha}\\). Thus \\(f_a(\\alpha) \\in M_\\alpha\\) is the \\(\\alpha\\)-th coordinate of our point \\(a\\).</p> <p>If we try to define a homomorphism \\(\\displaystyle \\phi : \\prod_{\\alpha \\in \\lambda}M_\\alpha \\to N\\) in a natural, linear way such as </p> \\[ \\phi(a) = \\sum_{\\alpha \\in \\lambda}\\phi_\\alpha(f_a(\\alpha)) \\] <p>where \\(\\displaystyle a \\in \\prod_{\\alpha \\in \\lambda}M_{\\alpha}\\), then observe that the above sum is nonsense. What the hell is an infinite sum of module elements of \\(N\\) supposed to represent? Also, there's no way to make sure this is even well-defined!</p> <p>However, if we instead consider \\(\\displaystyle \\bigoplus_{\\alpha \\in \\lambda}M_{\\alpha}\\), then creating a natural homomorphism \\(\\displaystyle \\phi: \\bigoplus_{\\alpha \\in  \\lambda}M_{\\alpha} \\to N\\) where again </p> \\[ \\phi(a) = \\sum_{\\alpha \\in \\lambda}\\phi_\\alpha(f_a(\\alpha)) \\] <p>works out fine. We see that \\(\\phi\\) is valid because \\(f_a(\\alpha) = 0\\) for all but finitely many \\(\\alpha \\in \\lambda\\). Hence, the above sum will only ever consist of a sum of finite elements.</p> <p>The next important two theorems demonstrate the importance of the direct sum.</p> <p> Let \\(M\\) be an \\(R\\)-module and suppose \\(M_1, M_2, \\dots, M_n\\) are submodules such that  <ul> <li> <p>[1.] \\(M = M_1 + M_2 + \\cdots + M_n\\)</p> </li> <li> <p>[2.] \\(M_j \\cap (M_1 + M_2 + \\cdots + M_{j-1} + M_{j + 1} + \\cdots + M_n) = \\{0\\}\\) for all \\(j \\in \\{1, 2, \\dots, n\\}\\). </p> </li> </ul> <p>Then </p> \\[ M \\cong M_1 \\oplus M_2 \\oplus \\cdots \\oplus M_n. \\] <p>\\vspace{-0.8cm} </p> <p> Construct the map \\(f:M_1 \\oplus M_2 \\oplus \\cdots \\oplus M_n \\to M\\) as  \\[ f(x_1, x_2, \\dots, x_n) = x_1 + x_2 + \\cdots + x_n. \\] <p>It is simple to check that this is an \\(R\\)-module homomorphism. Observe that by (1) \\(\\im(f) = M\\). Now suppose \\((x_1, x_2, \\dots, x_n) \\in \\ker(f)\\). Then we see that </p> \\[ x_1 + x_2 + \\cdots + x_n = 0 \\implies x_i = -(x_1 + x_2 + \\cdots + x_{i-1} + x_{i+1} + \\cdots + x_n) \\] <p>for all \\(i \\in \\{1, 2, \\dots, n\\}\\). But by (2), we know that no such \\(x_i\\) can exist. Therefore \\(x_1 = x_2 = \\cdots = x_n = 0\\). Hence, \\(f\\) is an isomorphism, which yields the desired result. </p> <p>The above result can be generalized to arbitrary direct sums. However, if we were dealing with cartesian products, we would not be able to generalize the above theorem to arbitrary direct sums. </p> <p> Let \\(M\\) be an \\(R\\)-module and suppose \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) is a family of \\(R\\)-modules such that  <ul> <li> <p>[1.] \\(\\displaystyle M = \\sum_{\\alpha \\in \\lambda} M_\\alpha\\) </p> </li> <li> <p>[2.] \\(M_\\beta \\bigcap \\displaystyle  \\sum_{\\alpha \\in \\lambda\\setminus\\{\\beta\\}}M_\\alpha = \\{0\\}\\) for all \\(\\beta \\in \\lambda\\)</p> </li> </ul> <p>then </p> \\[ M \\cong \\bigoplus_{\\alpha \\in \\lambda}M_{\\alpha} \\] <p>\\vspace{-0.7cm} </p> <p>The proof is the exact same as before, although the notation is annoying. </p>"},{"location":"algebra/Modules/Definitions./","title":"3.1. Definitions.","text":"<p>In group theory, we started with a set \\(G\\) equipped with a bilinear operation \\(\\cdot : G \\times G \\to G\\) which mapped \\(G\\) to itself. The operation was required to be associative, and there needed to be inverses and an identity element. </p> <p>In ring theory, we went further to assume \\(R\\) was not only an abelian group, we placed the group operation with \\(+: R\\times R \\to R\\) and then defined a multiplication \\(\\cdot: R \\times R \\to R\\) which is was associative and left- and right- distributive.</p> <p>Finally, we reach module theory, which considers again an abelian group \\(M\\) with operation \\(+:M \\times M \\to M\\) but lets a ring \\(R\\) act on \\(M\\), whose addition \\(+R\\times R \\to R\\) agrees with the one which acts on \\(M\\) but whose multiplication \\(\\cdot: R \\times M \\to M\\) acts on \\(R\\) and \\(M\\). </p> <p>Note how abelian groups and rings are special cases of modules. This will be more clear once we introduce the axioms. </p> <p> Let \\(R\\) be a ring with identity, and \\(M\\) an abelian group equipped with \\(+:M \\times M \\to M\\). Then \\(M\\) is an left \\(R\\)-module if we equip \\(R \\times M\\) with multiplication \\(\\cdot : R \\times M \\to M\\) and for all \\(m \\in M\\) and \\(a, b \\in R\\) \\begin{enumerate} \\item \\(a(m_1 + m_2)= am_1 + am_2\\) \\item \\((a + b)m = am + bm\\) \\item \\((ab)m = a(bm)\\) \\item \\(1_Rm = m\\) where \\(1_R\\) is the identity of \\(R\\). \\end{enumerate}  <p>Alternatively, an abelian group \\(M\\) is a right \\(R\\)-module if we equip \\(M \\times R \\to M\\) with multiplication \\(\\cdot: M \\times R \\to M\\) and for all \\(m \\in M\\) and \\(a, b \\in R\\) \\begin{enumerate} \\item \\((m + n)a = ma + na\\)  \\item \\(m(a + b) = ma + mb\\) \\item \\(m(ab) = (ma)b\\) \\item \\(m1_R = m\\) where \\(1_R\\) is the identity of \\(R\\). \\end{enumerate} </p> <p>Notice that we can think of these products as a group action, or sort of a \"ring action\" acting on \\(M\\). That is, an \\(R\\)-module \\(M\\) is just an abelian group that a ring \\(R\\) can act on. If you have an abelian group \\(N\\) that \\(R\\) simply cannot act on and satisfy the above axioms, then \\(N\\) is not an \\(R\\)-module. \\ For convenience, we will develop the theory of \\(R\\)-modules by solely working with left \\(R\\)-modules, since all proofs and statements will be equivalent up to a swap of variables for right \\(R\\)-modules.  \\ Examples.\\</p> <ul> <li>[1.] \\textcolor{NavyBlue}{Note that if \\(R\\) is commutative, then a left \\(R\\)-module coincides with a right \\(R\\)-module. To see this, let \\(M\\) be a left \\(R\\)-module. Then construct the right \\(R\\)-module by defining the multiplication as</li> </ul> \\[ m\\cdot r = rm. \\] <p>Then we see that for all \\(m \\in M\\), \\(a,b \\in R\\),  \\begin{enumerate}</p> <ul> <li> <p>\\((m_1 + m_2)\\cdot a = a(m_1 + m_2) = am_1 + am_2 = m_1 \\cdot a + m_2 \\cdot a\\) \\checkmark</p> </li> <li> <p>\\(m(a + b)= (a + b)m = am + bm = m \\cdot a + m \\cdot b\\) \\checkmark </p> </li> <li> <p>\\(m\\cdot (a b) = (ab)m = (ba)m = b(am) = b(m \\cdot a) = (m \\cdot a )\\cdot b\\) \\checkmark </p> </li> <li> <p>\\(m \\cdot 1_R = 1_Rm = m\\). \\checkmark \\end{enumerate} Note that in part \\((c)\\) is where we used the fact that \\(R\\) is commutative. So whenever \\(R\\) is commutative, the existence of a left \\(R\\)-module automatically implies that existence of a right \\(R\\)-module, and vice versa. }</p> </li> <li> <p>[2.] Let \\(R\\) be a ring. Then if we substitute \\(M =R\\) in the above definition, and let the multiplication $ \\cdot$ be the multiplication on \\(R\\) then \\(R\\) is a left and a right \\(R\\)-module. This is because \\(R\\) is an abelian group which is associative and left- and right-distributive. Hence, it satisfies all of the above axioms. </p> </li> </ul> <p>So keep in mind that a ring \\(R\\) is just a left- and right-\\(R\\) module that acts on \\(R\\). </p> <p>Here's another example which shows that abelian groups are simply \\(\\ZZ\\) modules. </p> <p> Let \\(G\\) be an abelian group. Then \\(G\\) is a left and right \\(\\ZZ\\)-module.  </p> <p> Let \\(\\ZZ\\) act on \\(G\\) as follows. Define  \\[ ng =  \\begin{cases} g + g + \\cdots + g \\text{ ($n$ times)} &amp; \\text{ if } n  &gt; 0\\\\  0 &amp; \\text{ if } n = 0\\\\ (-g) + (-g) \\cdots (-g) \\text{ ($n$ times) } &amp; \\text{ if } n &lt; 0 \\end{cases} \\] <p>and </p> \\[ gn =  \\begin{cases} g + g + \\cdots + g \\text{ ($n$ times)} &amp; \\text{ if } n  &gt; 0\\\\  0 &amp; \\text{ if } n = 0\\\\ (-g) + (-g) \\cdots (-g) \\text{ ($n$ times) } &amp; \\text{ if } n &lt; 0. \\end{cases} \\] <p>Then with this definition of multiplication, it is easy to show that the axioms (a)-(d) are satisfied. </p> <ul> <li> <p>[3.] If \\(R\\) is a ring and \\(I\\) is a left (right) ideal of \\(R\\), then \\(I\\) is a left (right) \\(R\\)-module. </p> </li> <li> <p>[4.] Let \\(V\\) be a vector space defined over a field \\(F\\). Then \\(V\\) is an \\(F\\)-module. (Now it is clear why there are a million axioms in the definition of a vector space!)</p> </li> </ul> <p>With \\(R\\)-modules introduced and understood, we can jump right into homomorphisms. </p> <p> Let \\(R\\) be a ring and \\(M\\) and \\(N\\) be \\(R\\)-modules. We define \\(f: M \\to N\\) to be an \\(R\\)-module homomorphism if  <ul> <li> <p>[1.] \\(f(m_1 + m_2) = f(m_1) + f(m_2)\\) for any \\(m_1, m_2 \\in M\\) </p> </li> <li> <p>[2.] \\(f(am) = af(m)\\) for all \\(a \\in R\\) and \\(m \\in M\\).</p> </li> </ul> <p>If \\(f\\) is a bijective \\(R\\)-module homomorphism, then we say that \\(f\\) is an isomorphism and that \\(M \\cong N\\).  Thus we see that \\(R\\)-module homomorphisms must not only be linear over the elements of \\(M\\), but they must also pull out scalar multiplication by elements of \\(R\\).</p> <p>Recall earlier that we said a vector space \\(V\\) over a field \\(F\\) is an \\(F\\)-module. Now if \\(W\\) is another vector space and \\(T: V \\to W\\) is a linear transformation, then we see that \\(T\\) is also an \\(F\\)-module homomorphism! </p> <p>In the language of linear algebra, a linear transformation is usually defined as a function \\(T: V \\to W\\) such that for any \\(\\bf{v}_1, \\bf{v}_2, \\bf{v} \\in V\\) and \\(\\alpha \\in F\\) we have that </p> <ul> <li> <p>[1.] \\(T(\\bf{v}_1 + \\bf{v}_2) = T(\\bf{v}_1) + T(\\bf{v}_2)\\)</p> </li> <li> <p>[2.] $T(\\alpha\\bf{v}) = $ \\(\\alpha\\)\\(T(\\bf{v})\\). </p> </li> </ul> <p>As we will see, linear algebra is basically a special case of module theory. </p> <p> Let \\(R\\) be a ring and \\(M\\) and \\(N\\) a pair of \\(R\\)-modules. Then \\(\\hom_R(M,N)\\) is the set of all \\(R\\)-module homomorphisms from \\(M\\) to \\(N\\).  </p> <p>\\textcolor{MidnightBlue}{It turns out we can turn \\(\\hom_R(M,N)\\) into an abelian group, and under special circumstances it can actually be an \\(R\\)-module itself. It will be the case that \\(\\hom_R\\) will actually be an important functor, but that is for later.} \\ \\indent To turn this into an abelian group, we define addition of the elements to be </p> \\[ (f + g)(m) = f(m) + g(m) \\] <p>for all \\(f, g \\in \\hom_R(M, N)\\). We let the identity be the zero map, and realize associativity and closedness are a given to conclude that this is in fact an abelian group.  \\</p> <p>Suppose we want to make \\(R\\), our ring, act on \\(\\hom_R(M, N)\\) in order for it to be an \\(R\\)-module. Then we define scalar multiplication to be \\((af)(m) = a(f(m))\\); a pretty reasonable definition for scalar multiplication. </p> <p>\\textcolor{Red}{This issue with this is that \\(\\hom_R(M, N)\\) will not be closed under scalar multiplication of elements of \\(R\\) unless \\(R\\) is a commutative ring. }</p> <p>We'll demonstrate this as follows. Let \\(b \\in R\\) and \\(f \\in \\hom_R(M, N)\\). Then the second property of an \\(R\\)-module homomorphism tells us that \\(f(bm) = bf(m)\\) for all \\(m \\in M\\). Now suppose we try to use our definition of scalar multiplication, and consider \\(af\\) where \\(a \\in R\\). Then if we try to see if \\(af\\) will pass the second criterion for being an \\(R\\)-module homomorphism, we see that </p> \\[ (af)(bm) = a(f(bm)) = a(bf(m)) = abf(m). \\] <p>That is, we see that \\(af\\) isn't an \\(R\\)-module homomorphism because \\((af)(bm) \\ne b(af)(m)\\) (which is required for an \\(R\\)-module homomorphism); rather, \\((af)(bm) = abf(m).\\) Now if \\(R\\) is a commutative ring, then </p> \\[ abf(m) = baf(m) \\] <p>so we can then say that \\((af)(bm) = b(af)(m)\\), in which case \\(af\\) passes the test for being an \\(R\\)-module homomorphism. </p> <p>This proves the following propsition, which will be useful for reference for later.</p> <p> Let \\(M\\) and \\(N\\) be \\(R\\)-modules. Then \\(\\hom_R(M, N)\\) is an abelian group. Furthermore, it is an  \\(R\\)-module if and only if \\(R\\) is a commutative ring.  Next, we make the following definitions for completeness. </p> <p> Let \\(R\\) be a ring and \\(M\\) and \\(N\\) be \\(R\\)-modules. If \\(f: M \\to N\\) is an \\(R\\)-module homomorphism, then  <ul> <li> <p>[1.] The set \\(\\ker(f) = \\{m \\in M \\mid f(m) = 0\\}\\) is the kernal of \\(f\\) </p> </li> <li> <p>[2.] The set \\(\\im(f) = \\{f(m) \\mid m \\in M\\}\\) is the image of \\(f\\).</p> </li> </ul> <p></p>"},{"location":"algebra/Modules/Exact%20Sequences%20and%20the%20Hom%20Functor./","title":"3.5. Exact Sequences and the Hom Functor.","text":"<p>This section will be the first encounter with the extremely important algebraic concept of an exact sequence, which is something you may have already seen before without even knowing it. </p> <p> Let \\(R\\) be a ring. We define a sequence of \\(R\\)-modules to be a chain of homomorphisms between \\(R\\)-modules, generally denoted as <p> we say that that the above sequence is exact at \\(M_i\\) if \\(\\im(f_i) = \\ker(f_{i+1})\\). Hence, an exact sequence is a sequence which is exact at every \\(M_i\\).  </p> <p>\\noindent Short Exact Sequences.\\ Looking at \"short\" exact sequences aids out analysis of longer or infinite exact sequences. </p> <p> Let \\(M_1, M_2\\) and \\(M\\) be \\(R\\)-modules. Then <ul> <li> <p>[1.] The sequence \\begin{tikzcd}[column sep = \\smallish] 0 \\arrow[r] &amp; M_1 \\arrow[r, \"f\"] &amp; M    \\end{tikzcd} is exact if and only if \\(f\\) is injective.</p> </li> <li> <p>[2.] The sequence \\begin{tikzcd}[column sep = \\smallish] M \\arrow[r, \"g\"] &amp; M_2 \\arrow[r] &amp; 0   \\end{tikzcd} is exact if and only if \\(g\\) is surjective. </p> </li> <li> <p>[3.]  The sequence \\begin{tikzcd}[column sep = \\smallish] 0 \\arrow[r] &amp; M_1 \\arrow[r, \"f\"] &amp; M \\arrow[r, \"g\"] &amp; M_2 \\arrow[r] &amp; 0   \\end{tikzcd} is exact if and only if \\(f\\) is injective and \\(g\\) is injective.</p> </li> </ul> <p></p> <p> <ul> <li>[1.] (\\(\\implies\\)) Suppose the sequence  \\begin{tikzcd}[column sep = \\smallish] 0 \\arrow[r] &amp; M_1 \\arrow[r, \"f\"] &amp; M    \\end{tikzcd} is exact. Then we have that \\(\\im(0) = \\ker(f) \\implies \\ker(f) = \\{0\\}\\). Therefore we see that \\(f\\) is injective. </li> </ul> <p>(\\(\\impliedby\\))Now suppose \\(f\\) is injective. Then \\(\\ker(f) = 0\\). Since \\(\\im(0) = \\{0\\}\\) we see \\(\\im(0) = \\ker(f)\\), so that the sequence             \\begin{tikzcd}[column sep = \\smallish] 0 \\arrow[r] &amp; M_1 \\arrow[r, \"f\"] &amp; M    \\end{tikzcd} is exact.</p> <ul> <li>[2.] (\\(\\implies\\)) Suppose the sequence  \\begin{tikzcd}[column sep = \\smallish] M \\arrow[r, \"g\"] &amp; M_2 \\arrow[r] &amp; 0   \\end{tikzcd} is exact. Then we see that \\(\\im(g) = \\ker(0) = M_2\\), since the zero map simply takes all of \\(M_2\\) and sends it to \\(0\\). Hence we see that \\(g\\) is surjective. </li> </ul> <p>(\\(\\impliedby\\)) Now suppose \\(g\\) is surjective. Then \\(\\im(g) = M_2\\) and we also have that \\(\\ker(0) = M_2\\). Therefore \\(\\im(g) = \\ker(0)\\) so that we have an exact sequence. </p> <ul> <li>[3.] By applying (1.) and (2.), the result follows.</li> </ul> <p></p> <p>The above proposition offers the following definitions. </p> <p> Let \\(M_1, M_2\\) and \\(M\\) be \\(R\\)-modules. If the sequence \\  is exact then we say it forms an \\textbf{short exact sequence}. Furthermore, if there exists an \\(R\\)-module \\(N\\) such that \\(M = N \\oplus \\im(f) = N \\oplus \\ker(g)\\) (since \\(\\im(f) = \\ker(g)\\)) then we say the above sequence is \\textbf{split exact}. <p>In this case, we say \\(N\\) or \\(\\im(f)\\) is a \\textbf{direct  summand} of \\(M\\). </p> <p>We can offer a few short exact sequences with some familiar objects.  \\ \\ Examples.</p> <ul> <li>[1.] Let \\(M\\) be an \\(R\\)-module with a submodule \\(N\\). If \\(i:N \\to M\\) is the inclusion map and  \\(\\pi: M \\to M/N\\) is the projection map, then the sequence  \\  s exact. \\ \\(\\bm{\\im(i) \\subset \\ker(\\pi)}\\). Observe that if \\(n \\in N\\) then </li> </ul> \\[  \\pi(i(n)) = \\pi(n) = n + N = N \\] <p>so that \\(\\im(i) \\subset \\ker(g)\\). \\ \\(\\bm{\\ker(\\pi) \\subset \\im(i)}\\). Suppose \\(m \\in \\ker(\\pi)\\). Then we see that \\(\\pi(m) = m + N = N\\), so that \\(m \\in N\\). Since \\(m \\in N\\), we know that \\(i(m) = m\\). Therefore \\(m\\) is the image of some element in \\(M\\) mapped by \\(i\\) (namely, just \\(m\\) itself). Hence \\(\\ker(\\pi) \\subset \\im(i)\\).</p> <p>With both directions, we can conclude that \\(\\im(i) = \\ker(\\pi)\\) so so that the sequence is exact.</p> <ul> <li> <p>[2.] Let \\(N\\) and \\(P\\) be \\(R\\)-modules. If we define \\(i':N \\to N \\oplus P\\) where \\(i'(n) = (n, 0)\\) and \\(\\pi': N \\oplus P \\to P\\) where \\(\\pi'(n, p) = p\\), we see that the sequence \\  is exact. We can realize this by simply observing that \\(\\ker(\\pi')\\) is the set of all elements \\((n, 0) \\in N \\oplus P\\), which is exactly the image of \\(i'\\). Therefore \\(\\im(i') = \\ker(\\pi')\\), so that the sequence is exact.</p> </li> <li> <p>[3.] The sequence \\  where \\(f:\\ZZ_p \\to \\ZZ_{pq}\\) is given by \\(f(n) = qn\\) and \\(g: \\ZZ_{pq} \\to \\ZZ_q\\) is given by \\(g(n) = n \\mbox{ mod } q\\), then this sequence is exact. In fact, it is a split exact sequence. From group theory, we know that </p> </li> </ul> \\[ \\ZZ_{mn} \\cong \\ZZ_m \\oplus \\ZZ_n \\] <p>if and only if \\(m\\) and \\(n\\) are coprime. In our case, \\(p\\) and \\(q\\) are distint primes and hence are coprime so that \\(\\ZZ_{pq} \\cong \\ZZ_p \\oplus \\ZZ_q\\). We'll later show that this will be sufficient to conclude that this is a split sequence.</p> <ul> <li>[4.] If instead we have the sequence \\  where \\(f:\\ZZ_p \\to \\ZZ_{p^2}\\) is given by \\(f(n) = pn\\) and \\(g: \\ZZ_{p^2} \\to \\ZZ_p\\) is given by \\(g(n) = n \\mbox{ mod }p\\), then this becomes an exact sequence. However, this is not split exact as \\(p\\) is obviously not comprime with itself, and hence </li> </ul> \\[ \\ZZ_{p^2} \\not\\cong \\ZZ_p \\oplus \\ZZ_p \\] <p>which is why this is not a split exact sequence. </p> <p>The last two examples can be generalized into a theorem, which include other criterion for when a short exact sequence is split exact. </p> <p> Let \\(M_1, M_2\\) and \\(M\\) be \\(R\\)-modules such that  \\  is exact. Then the following are equivalent: <ul> <li> <p>[1.] There exists a homomorphism \\(\\alpha : M \\to M_1\\) such that \\(\\alpha \\circ f = 1_{M_1}\\) </p> </li> <li> <p>[2.] There exists a homomorphism \\(\\beta: M_2 \\to M\\) such that \\(g \\circ \\beta = 1_{M_2}\\) </p> </li> <li> <p>[3.] The above sequence is split exact. </p> </li> </ul> <p>Furthermore, we see that </p> \\[\\begin{align*} M &amp;\\cong \\im(f) \\oplus \\ker(\\alpha)\\\\ &amp;\\cong \\ker(g) \\oplus \\im(\\beta)\\\\ &amp;\\cong M_1 \\oplus M_2. \\end{align*}\\] <p>\\vspace{-.5cm}</p> <p></p> <p> \\begin{description} \\item[(\\(\\bm{1 \\implies 3}\\)).] Suppose there exists an \\(\\alpha : M \\to M_1\\) such that \\(\\alpha \\circ f = 1_{M_1}\\). Let \\(m \\in M_1\\). Then observe that  \\[\\begin{align*} \\alpha(m - f(\\alpha(m))) &amp;= \\alpha(m) - \\alpha(f(\\alpha(m)))\\\\ &amp;= \\alpha(m) - (\\alpha \\circ f)(\\alpha(m))\\\\ &amp;= \\alpha(m) - \\alpha(m)\\\\ &amp;= 0 \\end{align*}\\] <p>where in the third step we used the fact that \\(\\alpha \\circ f = 1_{M_1}\\), and hence \\(\\alpha(f(m)) = m\\) for all \\(m \\in M_1\\). Hence, \\(m - f(\\alpha(m)) \\in \\ker(\\alpha)\\). </p> <p>\\  Since \\(f: M_1 \\to M\\) is injective, we see that \\(\\alpha: M \\to M_1\\) is surjective. To see this, let \\(m' \\in M_1\\). Then there exists an \\(m'' \\in M\\) such that \\(\\alpha(m'') = m'\\); namely, \\(m'' = f(m')\\) works. </p> <p>Since \\(\\alpha\\) is surjective, we see that </p> \\[ \\{f(\\alpha(m)) \\mid m \\in M\\} = \\{f(m_1) \\mid m_1 \\in M_1 \\} = \\im(f). \\] <p>That is, \\(f(\\alpha(M)) = f(M_1) = \\im(f)\\). And because \\(\\textcolor{red}{m - f(\\alpha(m)) \\in \\ker(\\alpha)}\\) for all \\(m \\in M\\), we see that \\(m \\in \\im(f) + \\ker(\\alpha)\\) for all \\(m \\in M\\). Hence, \\(M \\subset \\im(f) + \\ker(\\alpha)\\). But both \\(\\im(f)\\) and \\(\\ker(\\alpha)\\) are subsets of \\(M\\). Therefore, \\(M = \\im(f) + \\ker(\\alpha)\\).     </p> <p>Now let \\(x \\in \\ker(\\alpha) \\cap \\im(f)\\). Then \\(f(y) = x\\) for some \\(y \\in M_1\\), and \\(\\alpha(x) = 0\\) as well. Hence, </p> \\[ \\alpha(f(y)) = \\alpha(x) = 0. \\] <p>But \\(\\alpha \\circ f = 0\\), which implies that \\(y = 0\\). Therefore \\(\\ker(\\alpha) \\cap \\im(f) = \\{0\\}\\).  \\  By Theorem 1.\\ref{fin_module_sums}, we see that this implies that </p> \\[ M \\cong \\im(f) \\oplus \\ker(\\alpha). \\] <p>Hence, \\(M\\) is split exact as \\(\\im(f)\\) is a direct summand of \\(M\\).</p> <p>\\item[(\\(\\bm{2 \\implies 3}\\)).] Suppose (2) holds. We'll show that \\(\\textcolor{blue}{m - \\beta(g(m)) \\in \\ker(g)}\\) for all \\(m \\in M\\). </p> <p>To show this, observe that </p> \\[\\begin{align*} g[m - \\beta(g(m))] &amp;=g(m) - g \\circ \\beta(g(m))\\\\ &amp;= g(m) - g(m)\\\\ &amp;= 0 \\end{align*}\\] <p>where in the second step we used the fact that \\(g \\circ \\beta = 1_{M_2}\\). Therefore, \\(\\textcolor{blue}{m - \\beta(g(m)) \\in \\ker(g)}.\\)</p> <p>\\ </p> <p>Now note that </p> \\[ \\{ \\beta(g(m)) \\mid m \\in M\\} = \\{\\beta(m_2) \\mid m_2 \\in M_2\\} = \\im(\\beta) \\] <p>where in the second step we used the fact that \\(g\\) is surjective. That is, \\(\\beta(g(M)) = \\beta(M_2) = \\im(\\beta)\\). Therefore we see that </p> \\[  m \\in \\im(\\beta) + \\ker(g) \\] <p>for all \\(m \\in M\\) which implies that  \\(M \\subset \\im(\\beta) + \\ker(g)\\). But since \\(\\im(\\beta)\\) and \\(\\ker(g)\\) are both subsets of \\(M\\), we see that \\(M = \\im(\\beta) + \\ker(g)\\). </p> <p>Now let \\(m' \\in \\im(\\beta) \\cap \\ker(g)\\). Then there exists an \\(m_2 \\in M_2\\) such that \\(\\beta(m_2) = m'\\). Furthermore, since \\(m' \\in \\ker(g)\\), </p> \\[ 0 = g(m') = g(\\beta(m_2)) = m_2 \\] <p>since \\(g \\circ \\beta = 1_{M_2}\\). Hence, \\(m_2 = 0\\), so that \\(\\beta(m_2) = 0 = m\\). Therefore \\(m = 0\\), so that \\(\\im(\\beta) \\cap \\ker(g) = \\{0\\}\\).  \\ </p> <p>By Theorem 1.\\ref{fin_module_sums}, we have that </p> \\[ M \\cong \\im(\\beta) \\oplus \\ker(g) \\] <p>so that \\(M\\) is split exact, as one of its direct summands is \\(\\ker(g)\\).</p> <p>\\item[(\\(\\bm{1 \\implies 2}\\)).] Suppose (1) holds. Construct a function \\(\\beta: M_2 \\to M\\) defined by </p> \\[ \\beta(u) = v - f(\\alpha(v)) \\] <p>where \\(g(v) = u\\). Since \\(G\\) is surjective, we know that such a \\(v\\) exists, although we don't know if it is the only \\(v \\in M\\) which maps to \\(u\\), and if that could cause us problems. Thus we'll show that this definition is well defined (i.e. independent of the choice of \\(v\\)). </p> <p>\\begin{description} \\item[Well-defined.]  Suppose \\(g(v') = u\\) for some other \\(v' \\in M\\). Then</p> \\[\\begin{align*} g(v') - g(v) &amp;= v - f(\\alpha(v)) - (v' - f(\\alpha(v')))\\\\ &amp;= (v - v') - f(\\alpha(v)) + f(\\alpha(v'))\\\\ &amp;= (v - v') - f(\\alpha(v) - \\alpha(v'))\\\\ &amp;= \\textcolor{red}{(v - v') - f(\\alpha(v - v'))}\\\\ &amp;= 0. \\end{align*}\\] <p>We will prove the conclusion made in red, i.e.,  \\(\\textcolor{red}{(v - v') - f(\\alpha(v - v'))} = 0.\\) \\</p> <p>To see this, first note that, as we proved earlier, \\(\\textcolor{red}{x - f(\\alpha(x))} \\in \\ker(\\alpha)\\) for all \\(x \\in M\\). Hence, \\(\\textcolor{red}{(v - v') - f(\\alpha(v - v'))} \\in \\ker(\\alpha)\\).  \\</p> <p>Furthermore, since \\(g(v) = g(v')\\), we see that \\(g(v - v') = 0 \\implies v - v' \\in \\ker(g)\\). But \\(\\ker(g) = \\im(f)\\), so that \\(\\textcolor{red}{v - v'} \\in \\im(f)\\). Obviously \\(\\textcolor{red}{f(\\alpha(v - v'))} \\in \\im(f)\\) for any \\(v \\in M\\), so that  \\(\\textcolor{red}{(v - v') - f(\\alpha(v - v'))} \\in \\im(f).\\) \\</p> <p>Thus we have that \\(\\textcolor{red}{(v - v') - f(\\alpha(v - v'))} \\in \\im(f) \\cap \\ker(\\alpha) = \\{0\\}\\), so that \\(g(v) - g(v') = 0\\). \\end{description} Next observe that for any \\(u \\in M_2\\) we have that </p> \\[\\begin{align*} g \\circ \\beta (u) &amp;= g(v - f(\\alpha(v)))\\\\ &amp;= g(v) - (g \\circ f)(\\alpha(v))\\\\ &amp;= g(v) \\end{align*}\\] <p>where in the second step we used the fact that \\((g \\circ f) = 0\\) as \\(\\ker(g) = \\im(f)\\). Thus we have that \\(g \\circ \\beta = 1_{M_2}\\), so that such a desired \\(\\beta: M_2 \\to M\\) exists. </p> <p>\\item[\\(\\bm{(2 \\implies 1)}\\).] Suppose (2) holds. Construct a function \\(\\alpha : M \\to M_1\\) defined by </p> \\[ \\alpha(m) = f^{-1}(m - \\beta (g(m))). \\] <p>Note that we must be careful since we're dealing with an inverse. To even make such a statement, we first recall that \\(f\\) is injective, so an inverse from \\(f^{-1}: \\im(f) \\to M\\) certainly exists. But it only exists if its domain is at most \\(\\im(f)\\). Thus we check that \\(\\textcolor{blue}{m - \\beta(g(m)) \\in \\im(f)}\\) for all \\(m \\in M\\).</p> <p>Earlier we proved that \\(\\textcolor{blue}{m - \\beta(g(m)) \\in \\ker(g)}\\), and we know that \\(\\ker(g) = \\im(f)\\) as the sequence is exact. Therefore, we already know that \\(\\textcolor{blue}{m - \\beta(g(m)) \\in \\im(f)}\\).</p> <p>Hence, \\(\\alpha\\) makes sense since \\(f^{-1}\\) exists and \\(m - \\beta(g(m)) \\in \\im(f)\\) for all \\(m \\in M\\).</p> <p>Now observe that for any \\(m_1 \\in M_1\\), </p> \\[\\begin{align*} \\alpha \\circ f(m_1) &amp;= f^{-1}(f(m_1) - \\beta(g(f(m_1))))\\\\ &amp;= f^{-1}f(m_1) - f^{-1}(0)\\\\ &amp;= m_1 \\end{align*}\\] <p>since \\(g(f(m_1)) = 0\\) for all \\(m_1 \\in M\\). Thus such a desired \\(\\alpha\\) exists.</p> <p>\\item[(\\(\\bm{3 \\implies 1}\\) \\&amp; 2).] Suppose that </p> \\[ M \\cong M'\\oplus M'' \\] <p>where \\(M' = \\im(f) = \\ker(g)\\), and \\(M''\\) is some other summand of \\(M\\). Define a projection map \\(\\pi: M \\to M'\\) as</p> \\[ \\pi(m) = \\begin{cases} m &amp; \\text{ if } m \\in M'\\\\ 0 &amp; \\text{ otherwise} \\end{cases} \\] <p>and similarly the injective map \\(i: M'' \\to M\\) as \\(i(m'') = m''\\) for all \\(m'' \\in M\\). </p> <p>Consider \\(\\pi \\circ f: M \\to M'\\). Since \\(M' = \\im(f)\\), this is clearly an isomorphism. Now define \\(\\alpha = (\\pi \\circ f)^{-1} \\circ \\pi_1\\) and observe that \\(\\alpha \\to M \\to M_1\\) and </p> \\[ \\alpha \\circ f = (\\pi \\circ f)^{-1} \\circ \\pi_1 \\circ f = 1_{M_1}. \\] <p>Hence, \\((3) \\implies (1)\\).</p> <p>Similarly, observe that \\(g \\circ i: M'' \\to M_2\\) is also an isomorphism. To see this, first observe that \\(M' = \\ker(g)\\), and since \\(M \\cong M' \\oplus M''\\) we know that \\(M' \\cap M'' = \\{0\\}\\). Therefore, if \\(m \\in M''\\) is nonzero, then \\(m \\not\\in \\ker(g)\\). Hence \\(g(i(m)) \\ne 0\\) if and only if \\(m = 0\\), so that \\(g \\circ i\\) is one to one. Now surjectivity is clear, as \\(g\\) itself is a surjective function. </p> <p>Now define \\(\\beta = i \\circ (g \\circ i)^{-1}\\), and observe that \\(\\beta : M_2 \\to M\\) and </p> \\[ g \\circ \\beta = g \\circ i \\circ (g \\circ i)^{-1} = 1_{M_2}. \\] <p>Therefore \\((3 \\implies 2)\\), which completes the entire proof. \\end{description} </p> <p>That was a long ass proof, but the theorem is very powerful and worthwhile. Next, we'll reintroduce the concept of \\(\\hom()\\). \\</p> <p>\\noindent Inducing Homomorphisms.</p> <p>\\begin{minipage}{0.6\\textwidth} Let \\(M, N\\) and \\(N'\\) be \\(R\\)-modules, and let \\(\\phi: M \\to N\\) and \\(f: N \\to N'\\) be \\(R\\)-modules homomorphisms. Then we see that the diagram to the right commutes.</p> <p>\\end{minipage} \\hfill \\begin{minipage}{0.4\\textwidth} \\  end{minipage} \\vspace{0.5cm}</p> <p>\\begin{minipage}{0.6\\textwidth} However, suppose we feed the above diagram with arbitrary \\(\\phi: M \\to N\\). That is, we keep \\(f: N \\to N'\\) fixed, but let \\(\\phi: M \\to  N\\) vary over all possible \\(\\phi\\). This is equivalent to grabbing elements from the abelian group \\(\\hom_R(M, N)\\). We can denote this with a red arrow, to remind the reader that this arrow \"picks\" \\(\\phi\\).  \\end{minipage} \\hfill \\begin{minipage}{0.4\\textwidth} \\  end{minipage}</p> <p>\\begin{minipage}{0.6\\textwidth} Note that we've described a well-defined system for assigning for each \\(\\phi \\in \\hom_R(M, N)\\) a function </p> \\[  f \\circ \\phi. \\] <p>Also, \\(f \\circ \\phi : M \\to N'\\), so that \\(f \\circ \\phi \\in \\hom_R(M, N')\\). We can denote this with a blue arrow, to communicate that \\(\\hom_R(M,N')\\) \"accepts\" \\(f \\circ \\phi\\) (after all, it is an element of the set). \\end{minipage} \\hfill \\begin{minipage}{0.4\\textwidth} \\  end{minipage} \\vspace{0.5cm}</p> <p>\\begin{minipage}{0.6\\textwidth} What we've just described is an induced function, which we denote as \\(f_*\\). That is, if we fix \\(f\\), then we can create a homomorphism \\(f_*\\)  between the abelian groups \\(\\hom_R(M, N)\\) and \\(\\hom_R(M, N')\\), where for each element \\(\\phi \\in \\hom_R(M, N)\\) we assign it the function \\(f \\circ \\phi \\in \\hom_R(M, N')\\). \\end{minipage} \\hfill \\begin{minipage}{0.4\\textwidth} \\  end{minipage}</p> <p>\\noindent \\(\\hom_R(- , M)\\).\\ The we restate our results. If \\(N, N'\\) are \\(R\\)-module homomorphisms and \\(f: N \\to N'\\) is an \\(R\\)-module homomorphism, then for any \\(R\\)-module \\(M\\) we can create an induced homomorphism </p> \\[  f_* : \\hom_R(M, N) \\to \\hom_R(M, N') \\] <p>defined as </p> \\[ f_*(\\phi) = f \\circ \\phi. \\] <p>\\noindent \\(\\hom_R(M, -)\\).\\ Similarly, if \\(N, N'\\) are again \\(R\\)-modules and \\(g: N' \\to N\\) is a an \\(R\\)-module homomorphism, then for any \\(R\\)-module \\(M\\), there is an induced homomorphism </p> \\[ g^*: \\hom_R(N, M) \\to \\hom_R(N', M) \\] <p>defined as </p> \\[ g^*(\\psi) = \\psi \\circ g. \\] <p>It turns out in category theory that the behavior of these functions fit the definition of a functor. \\(\\hom_R(- , M)\\) is known as a covariant functor, while \\(\\hom_R(M, -)\\) is known as a contravariant functor. We won't delve too much into this.</p> <p>\\textcolor{NavyBlue}{Since the \\(\\hom_R\\) groups are abelian, we see that \\(f_*\\) and \\(g_*\\) are in fact group homomorphisms. If \\(R\\) is commutative, then we know that \\(\\hom_R\\) forms an \\(R\\)-module in which case \\(f_*\\) and \\(g_*\\) become \\(R\\)-module homomorphisms.} \\</p> <p>Now suppose a family of \\(R\\)-modules \\(\\{M_i \\mid i \\in \\mathbb{N}\\}\\) associated with a set of homomorphisms \\(\\{f_i \\mid f_i :M_{i-1} \\to M_i, i \\in \\mathbb{N}\\}\\) for a long sequence, not necessarily exact. \\  hen if we apply the \\(\\hom_R(M, -)\\) functor, then we see that the above sequence implies a sequence between the \\(\\hom\\) groups: \\  nd applying the \\(\\hom_R(-, M)\\) functor we get  \\  Thus the long sequence of \\(R\\)-modules implies the existence of two other long sequences of abelian groups. The interesting thing is that the two sequences are similar but differ in the direction of the arrows (this is why we denote the functions separately with an asterik either in the subscript or superscript). Furthermore, the direction of the arrows in the first seuqence of \\(M_i\\) \\(R\\)-modules determines the direction of the arrows in the other two sequences. </p> <p> Let \\(M_1, M\\) and \\(M_2\\) be \\(R\\)-modules, and suppose \\(f:M_1 \\to M\\) and \\(g:M \\to M_2\\) are \\(R\\)-modules. Then the sequence \\begin{equation} \\begin{tikzcd} 0 \\arrow[r] &amp; M_{1} \\arrow[r, \"f\"] &amp; M \\arrow[r, \"g\"] &amp; M_{2} \\end{tikzcd} \\end{equation} is exact if and only if the sequence  \\begin{equation}  \\begin{tikzcd} 0 \\arrow[r] &amp; \\hom_R(N,M_{1}) \\arrow[r, \"f_\"] &amp; \\hom_R(N, M) \\arrow[r, \"g_\"] &amp; \\hom_R(N,M_{2}) \\end{tikzcd} \\end{equation} is an exact sequence of abelian groups. Furthermore, the sequence  \\begin{equation} \\begin{tikzcd} M_{1} \\arrow[r, \"f\"] &amp; M \\arrow[r, \"g\"] &amp; M_{2} \\arrow[r] &amp; 0 \\end{tikzcd} \\end{equation} is exact if and only if  \\begin{equation} \\begin{tikzcd} \\hom_R(M_{1}, N) &amp; \\hom_R(M, N) \\arrow[l, swap, \"f^\"] &amp; \\hom_R(M_{2}, N)\\arrow[l, swap, \"g^\"] &amp;  0 \\arrow[l] \\end{tikzcd} \\end{equation} is an exact sequence of abelain groups. </p> <p> \\textcolor{MidnightBlue}{To show that the sequence between the \\(\\hom\\) abelian groups is exact, we need to check that (1) \\(f_*\\) is injective and (2) \\(\\im(f_*) = \\ker(g_*)\\). } \\begin{description} \\item[\\(\\bm{f_*}\\) is injective.] Suppose that \\(f_*(\\psi) = 0\\) for some \\(\\phi \\in \\hom_R(N, M_1)\\). Then  \\[ f_*(\\psi) = 0 \\implies f(\\psi(n)) = 0 \\] <p>for all \\(n \\in M\\). However, \\(f\\) is injective, so that \\(\\ker(f) = \\{0\\}\\). Therefore \\(\\psi(n) \\in \\ker(f) = \\{0\\}\\) for all \\(n\\), which means that \\(\\psi\\) is the zero function. Therefore \\(\\ker(f_*) = \\{0\\}\\) (where the zero here stands for the zero function between \\(N\\) and \\(M_1\\)) so that \\(f_*\\) is injective. </p> <p>\\item[\\(\\bm{\\im(f_*) \\subset \\ker(g_*)}\\).] Let \\(\\phi \\in \\hom_R(N, M_1)\\). Then observe that </p> \\[ g_*(f_*(\\phi)) = g_*(f \\circ \\phi) = g \\circ f \\circ \\phi = 0 \\] <p>since \\(g \\circ f = 0\\) as \\(\\im(f) = \\ker(g)\\). Therefore we see that \\(\\im(f_*) \\subset \\ker(g_*)\\). </p> <p>\\item[\\(\\bm{\\ker(g_*) \\subset \\im(f_*)}\\).] Let \\(\\psi \\in \\hom_R(N, M)\\) and suppose that \\(g_*(\\psi) = 0\\). Note that </p> \\[ g_*(\\psi) = 0 \\implies g(\\psi(n)) = 0 \\] <p>for all \\(n \\in N\\). Since \\(\\ker(g) = \\im(f)\\), we know for all \\(n \\in N\\) that \\(\\psi(n) \\in \\im(f)\\). Therefore, there exist a set of \\(y \\in M_1\\) such that \\(f(y) = \\psi(n)\\), and since \\(f\\) is one to one this correspondence is uniquely determined. </p> <p>Thus construct a function \\(\\tau: N \\to M_1\\) such that </p> \\[ \\tau(n) = f^{-1}(\\psi(n)). \\] <p>As we discussed, this function is well defined since \\(f\\) is one-to-one, and therefore there is always a unique value of \\(f^{-1}(\\psi(n))\\) for each \\(n\\). Now note that this function is an \\(R\\)-module homomorphism since, for any \\(n_1, n_2 \\in N\\) and \\(a \\in R\\) </p> \\[\\begin{align*} \\tau(n_1 + n_2) &amp;=  f^{-1}(\\psi(n_1 + n_2))\\\\ &amp;= f^{-1}(\\psi(n_1) + \\psi(n_2))\\\\ &amp;= f^{-1}(\\psi(n_1)) + f^{-1}(\\psi(n_2))\\\\ &amp;= \\tau(n_1) + \\tau(n_2) \\end{align*}\\] <p>and </p> \\[\\begin{align*} \\tau(an_1) &amp;= f^{-1}(\\psi(an_1))\\\\ &amp;= f^{-1}(a\\psi(n_1))\\\\ &amp;= af^{-1}(\\psi(n))\\\\ &amp;= a\\tau(n_1). \\end{align*}\\] <p>Therefore we see that \\(\\tau \\in \\hom_R(N_1, M)\\) and that</p> \\[ f_*(\\tau) = f_*(f^{-1}(\\psi)) = f(f^{-1}(\\psi))= \\psi. \\] <p>Hence, \\(\\psi \\in \\im(f_*)\\). Hence \\(\\ker(g_*) \\subset \\im(f_*)\\), which proves that \\(\\ker(g_*) = \\im(f_*)\\). \\end{description} \\textcolor{MidnightBlue}{To prove the reverse direction, we will assume the exactness of the second sequence and show that (1) \\(f\\) is injective and (2) \\(\\im(f) = \\ker(g)\\).}</p> <p>\\begin{description} \\item[\\(\\bm{f}\\) is injective.] Suppose that sequence \\ref{0_homM1_homM_homM2_exact_sequence} is exact for all \\(R\\)-modules \\(N\\). Then let \\(N = \\ker(f)\\), and since \\(N \\subset M_1\\) consider the inclusion map \\(i: N \\to M_1.\\)  Note however that for any \\(n \\in N\\) we see that </p> \\[ f_*(i(n)) = f(i(n)) = 0 \\] <p>since \\(\\im(i) = \\ker(f)\\). Hence, \\(i \\in \\ker(f_*)\\).  However, since \\(f_*: \\hom_R(N, M_1) \\to \\hom_R(N, M)\\) is injective, we know that \\(\\ker(f_*) = 0\\). Therefore we have that \\(i = 0\\), (i.e. it is a zero map). But since we defined this to be the inclusion map, we have that \\(N = \\{0\\}\\). Hence, \\(\\ker(f) = N = \\{0\\}\\), so that \\(f\\) is one to one.</p> <p>\\item[\\(\\im(f) \\subset \\ker(g)\\).]  Let \\(N = M_1\\), and let \\(1_{M_1}:M_1 \\to M_1\\) be the identity. Then we see that </p> \\[ 0 = g_*(f_*(1_{M_1})) = g \\circ f \\] <p>by exactness of sequence \\ref{0_homM1_homM_homM2_exact_sequence}. Therefore we see that \\(\\im(f) \\subset \\ker(g)\\). </p> <p>\\item[\\(\\ker(g) \\subset \\im(f)\\).]</p> <p>\\end{description} </p> <p> Let \\(N\\) be an \\(R\\)-module. If  \\  is a split exact sequence of \\(R\\)-modules, then  \\  and  \\  are split exact sequences of abelian groups (\\(R\\)-modules if \\(R\\) is commutative). </p> <p> \\textcolor{MidnightBlue}{By the previous theorem, we only need to show that \\(g_*\\) and \\(f^*\\) are surjective and that the two sequences split. } Since the first sequence splits, let \\(\\beta: M_2 \\to M\\) be the function which splits the first sequence. Consider the function \\(\\beta_*: \\hom_R(N, M_2) \\to \\hom_R(N, M)\\).  Then observe that for any \\(\\psi \\in \\hom_R(N, M_2)\\) that  \\[ g_* \\circ \\beta_* (\\psi) = g_*(\\beta(\\psi)) = g \\circ \\beta(\\psi)  = \\psi. \\] <p>Therefore, we see that \\(g_* \\circ \\beta_* = 1_{\\hom_R(M_{2}, N)}.\\) Hence by Theorem 1.\\ref{split_exact_lemma}, we see that \\(\\beta_*\\) splits the second sequence. However, note also that \\(g_* \\circ \\beta_* = 1_{\\hom_R(M_{2}, N)}\\) implies that \\(g_*\\) is surjective. Therefore the second sequence is split exact. \\</p> <p>As for the third sequence, consider the function \\(\\alpha^*: \\hom_R(M_1, N) \\to \\hom_R(M, N)\\). Note that for any \\(\\phi \\in \\hom_R(M, N)\\), we have that </p> \\[ \\alpha^* \\circ f^*(\\phi) = \\alpha^*(f(\\phi)) = \\alpha \\circ f(\\phi) = \\phi. \\] <p>Hence we see that \\(\\alpha^* \\circ f^*\\) splits the third sequence. Furthermore, the fact that \\(\\alpha^* \\circ f^* = 1_{\\hom_R(M, N)}\\) implies that \\(f^*\\) is surjective. Thus in total we have that the third sequence is in fact a split exact sequence. </p> <p>The next theorem is a nice result that shows that \\(\\hom_R\\) is somewhat of a \"linear\" operator.</p> <p> Let \\(M_1, M_2\\) and \\(M\\) be \\(R\\)-modules. Then  \\[ \\hom_R(M, M_1\\oplus M_2) \\cong \\hom_R(M, M_1)\\oplus \\hom_R(M, M_2)    \\] <p>and </p> \\[ \\hom_R(M_1 \\oplus M_2, M) \\cong \\hom_R(M_1, M) \\oplus \\hom_R(M_2, M). \\] <p>\\vspace{-0.7cm}  These are in general isomorphisms of abelian groups, but can be isomorphisms of \\(R\\)-modules if \\(R\\) is commutative.</p> <p> Consider one of our earlier examples of a split exact sequences: \\  where \\(i\\) defined as \\(i(m_1) = (m_1, 0)\\) is the inclusion map and \\(\\pi\\) defined by \\(\\pi(m_1, m_2) = m_2\\) is the projection map. As this is split exact, we can apply the previous theorem to gaurantee the existence of sequences  \\  and  \\  which are both split exact. Then by applying Theorem 1.\\ref{split_exact_lemma} we have that  \\[ \\hom_R(M, M_1 \\oplus M_2) \\cong \\hom_R(M, M_1) \\oplus \\hom_R(M, M_2) \\] <p>and </p> \\[ \\hom_R(M_1 \\oplus M_2, M) \\cong \\hom_R(M_1, M) \\oplus \\hom_R(M_2, M). \\] <p></p>"},{"location":"algebra/Modules/Free%20%24R%24-modules./","title":"3.6. Free \\(R\\)-modules.","text":"<p>Free modules are the type of modules that you are probably already familiar with. Basically, they're modules who have some kind of generating set, which can create all other elements. As we can think of modules as vector spaces, we know that vectors spaces always have some kind of basis set, at least when they can be thought of as existing in \\(\\RR^n\\). It turns out that having a basis leads to many desirable properties. </p> <p>First, we make a definition on linear independence, a concept required for discussing bases, and then formally define a free module.</p> <p> Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. Then the set \\(S = \\{x_1, x_2, \\dots, x_n\\}\\) with \\(S \\subset M\\) is said to be linearly independent if and only if the only solution to the equation  \\[ a_1x_1 + a_2x_2 + \\cdots + a_nx_n = 0 \\] <p>is \\(a_1 = a_2 = \\cdots = a_n = 0\\) (where \\(a_1, a_2, \\dots, a_n \\in R\\)). </p> <p>If \\(S\\) is the smallest linear independent subset of \\(M\\), then we say that \\(S\\) is a basis for \\(M\\), in which case \\(M\\) is said to be a free \\(R\\)-module.  Hence, an \\(R\\)-module is a module with a basis.</p> <p>This is the exact same definition of linear independence we've seen in linear algebra.  Nothing is new here. It is a classic exercise in linear algebra to check the following statement, which we offer here.</p> <p> \\(S\\) is a basis for some \\(R\\)-module \\(M\\) if and only if every \\(x \\in M\\) can be written uniquely as  \\[ x = a_1x_1 + a_2x_2 + \\dots a_nx_n \\] <p>where \\(a_i \\in R\\) and \\(x_i \\in S\\) for \\(i = 1,2, \\dots, n\\). </p> <p>Examples</p> <ul> <li>[1.] Consider the \\(R\\)-module \\(M_{m,n}(R).\\) Observe that a basis for this module consists of </li> </ul> \\[ \\{E_{ij} \\mid 1 \\le i \\le m, 1 \\le j \\le n\\}. \\] <ul> <li>[2.] Consider an abelian group \\(G\\). Then as we showed before, \\(G\\) is technically a \\(\\mathbb{Z}\\)-module. However, if \\(G\\) is finite, then it is not a free \\(\\ZZ\\)-module. </li> </ul> <p>Suppose to the contrary that it is, and that \\(S = \\{x_1, x_2, \\dots, x_n\\}\\) is a linearly independent set which forms a basis of \\(G\\). Then if \\(\\{o_1, o_2, \\dots, o_n\\}\\) is a set such that \\(o_i = \\text{order}(x_1)\\) (which exists, by finiteness of \\(G\\)) then  </p> \\[ o_1x_1 + o_2x_2 + \\cdots + o_nx_n = 0. \\] <p>Hence, the set \\(\\{x_1, x_2, \\dots, x_n\\}\\) is not linearly independent, so \\(G\\) is not a free \\(\\ZZ\\)-module.</p> <ul> <li>[3.] Consider the set \\(R[X]\\). Observe that a suitable generating basis is </li> </ul> \\[ \\{x^n \\mid n \\in \\mathbb{N}\\} \\] <p>which is probably something you already knew. </p> <ul> <li>[4.] Suppose \\(M_1\\) and \\(M_2\\) are free modules with bases \\(S_1, S_2\\). Then the set \\(M_1 \\oplus M_2\\) is a free module, since it has a basis </li> </ul> \\[ \\{(x, 0) \\mid x \\in S_1\\} \\cup \\{(0, y) \\mid y \\in S_2\\}. \\] <p>More generally, if \\(\\{M_\\alpha\\}_{\\alpha \\in \\lambda}\\) is a of free modules where \\(S_\\alpha\\) is the basis of \\(M_\\alpha\\), then we see that </p> \\[ \\oplus_{\\alpha \\in \\lambda}M_\\alpha \\] <p>is also a free module with basis </p> \\[ \\bigcup_{\\alpha \\in \\lambda}\\{(\\delta_{jk}s_{j\\alpha}) \\mid s_{j\\alpha} \\in S_j\\}. \\] <p>where \\(\\delta_{jk}\\) is the Kronecker delta function.</p> <p> Let \\(M\\) be a free \\(R\\)-module. Suppose the basis of the set is \\(S\\). Let \\(N\\) be an \\(R\\)-module and \\(h: S \\to N\\) a function. Then there exists a function \\(f \\in \\hom_R(M, N)\\) such that \\(f\\mid_S = h\\).  </p> <p> Let \\(R\\) be commmutative and \\(M\\) and \\(N\\) free modules with bases. Then \\(\\hom_R(M, N)\\) is a finitely generated free module.  </p> <p> Suppose the basis for \\(M\\) is \\(S = \\{x_1, x_2, \\dots, x_n\\}\\), and the basis for \\(N\\) is \\(T = \\{y_1, y_2, \\dots, y_m\\}\\). Define a set of functions for \\(1 \\le i \\le m\\) and \\(1 \\le j \\le n\\) such that  \\[ f_{ij}(x_k) =  \\begin{cases} y_j &amp; \\text{ if } k = i\\\\ 0 &amp; \\text{ if } k \\ne j \\end{cases}. \\] <p>By the previous proposition, we know that each \\(f_{ij}\\) is a element in \\(\\hom_R(M, N)\\). Now let \\(f \\in \\hom_R(M,N)\\) be arbitrary. Since \\(T\\) is a basis for \\(N\\), we know that for each \\(v_k \\in S\\) there exists coefficients \\(a_{k1}, a_{k2}, \\dots, a_{kn}\\) such that  </p> \\[ f(v_k) = a_{k1}y_1 + \\cdots + a_{kn}y_n. \\] <p>However, observe that </p> \\[\\begin{align*} f(v_k) &amp;= a_{i1}y_1 + \\cdots + a_{in}y_n\\\\ &amp;= a_{k1}f_{k1}(x_k) + a_{k2}f_{k2}(x_k) + \\cdots + a_{kn}f_{kn}(x_k). \\end{align*}\\] <p>Therefore, we see that for any \\(b \\in M\\), </p> \\[\\begin{align*} f(b) &amp;= f(a_{b1}x_1 + \\cdots + a_{bm}x_m)\\\\ &amp;=  a_{b1}f(x_1) + \\cdots + a_{bm}f(x_m)\\\\ &amp;= a_{b1}[a_{11}f_{11}(x_1) + a_{12}f_{12}(x_1) + \\cdots + a_{1n}f_{1n}(x_1)]\\\\ &amp;\\hspace{.6cm} + a_{b2}[a_{21}f_{21}(x_2) + a_{22}f_{22}(x_2) + \\cdots + a_{2n}f_{2n}(x_2)]\\\\ &amp;\\hspace{.6cm} + \\cdots\\\\ &amp;\\hspace{.6cm} + a_{bm}[a_{m1}f_{m1}(x_m) + a_{m2}f_{m2}(x_2) + \\cdots + a_{mn}f_{mn}(x_m)]. \\end{align*}\\] <p>Therefore we see that \\(\\{f_{ij}\\}\\) generates \\(\\hom_R(M, N)\\), so that \\(\\hom_R(M, N)\\) is finitely generated.  The previous theorem doesn't hold if \\(M\\) and \\(N\\) are not finitely generated, since there are many counter examples to such a claim.  \\textcolor{purple}{ Let \\(R = \\mathbb{Z}\\) and \\(M = \\oplus_{i = 1}^{\\infty}\\mathbb{Z}\\). Then observe that </p> \\[ \\hom_R(M, \\ZZ) \\cong \\prod_{i = 1}^{\\infty}\\ZZ. \\] <p>by Theorem 1.13. However, we see that while \\(\\ZZ\\) is finitely  generated and \\(M\\) is finitely generated, but \\(\\displaystyle \\prod_{i = 1}^{\\infty}\\ZZ\\) is not. (The proof is nontrivial.)}</p> <p> Let \\(M\\) be a free \\(R\\)-module with basis \\(S = \\{x_j\\}_{j \\in J}\\) and suppose \\(I\\) is an ideal of \\(R\\). Let \\(\\pi: M \\to M/I\\). Then \\(M/IM\\) is a \\(R/I\\)-module and is free with basis \\(\\pi(S) =  \\{ \\pi(x_j)\\}_{j \\in J}\\). </p> <p> \\begin{description} \\item[\\(\\bm{M/IM}\\) is an \\(\\bm{R/I}\\)-module.] First recall that \\(IM\\) is a submodule of \\(M\\). Therefore it makes sense to consider the quotient \\(M/IM\\). Then we can define a mapping \\(\\cdot: R/I \\times M/IM \\to M/IM\\) as follows. Let \\(r + I \\in R/I\\) and \\(m + IM \\in M/IM\\). Then define the mapping as \\[\\begin{align*} (r + I)\\cdot(m + IM) &amp;= r(m + IM)\\\\ &amp;= rm + rIM\\\\ &amp;= rm + IM. \\end{align*}\\] <p>Since \\(M\\) is an \\(R\\)-module, \\(rm \\in M\\) so that \\(rm + IM\\) is in fact in \\(M/IM\\). The other module properties may be easily verified without difficulty by using this mapping. </p> <p>\\item[\\(\\bm{M/IM}\\) is free.]  Suppose \\(m+ IM\\) is an element of \\(M/IM\\). Since \\(\\pi: M \\to M/I\\) is a surjective mapping, we see that there exists at least one \\(m \\in M\\) such that \\(\\pi(m) = m + IM\\). Now since \\(m\\) is free, there exists a unique representation of \\(m\\) of its basis elements, i.e., there exists \\(\\{a_j\\}_{j \\in J}\\), a subset of \\(R\\), such that </p> \\[ m = \\sum_{j \\in J} a_jx_j \\implies  \\pi(m) = \\pi\\left(\\sum_{j \\in J} a_jx_j \\right) = \\sum_{j \\in J} a_j\\pi(x_j) + IM \\] <p>Hence \\(m + IM = \\sum_{j \\in J} a_j\\pi(x_j) + IM.\\) To finish showing that \\(\\{\\pi(x_j)\\}_{j \\in J}\\) is a basis for \\(M/IM\\), we only have to show that it is a linearly independent set. So consider the equation </p> \\[ \\sum_{j \\in J}a_j\\pi(x_j) = 0 + IM                 \\] <p>for some constants \\(\\{a_j\\}_{j \\in J}\\) in \\(\\mathbb{R}\\). Suppose additionally for contradiction that not all of the constants are nonzero. Then we that \\(\\sum_{j \\in J}a_j\\pi(x_j)\\) is an element of \\(IM\\). However, this is a contradiction since none of the elements of \\(\\{\\pi(x_j)\\}_{j \\in J}\\) is allowed to be in \\(IM\\). Hence this set generates \\(M/IM\\) and is linearly independent, so it is a basis. \\end{description} </p> <p>We can introduce an even more useful proposition regarding free modules, and more generally all modules. </p> <p> Let \\(M\\) be an \\(R\\)-module. Then  \\[ M \\cong F/K     \\] <p>for a free module \\(F\\) and some submodule \\(K\\) of \\(F\\). That is, \\(M\\) is the quotient of some free module \\(F\\). Furthermore, if \\(M\\) is finitely generated, then such an \\(F\\) is finitely generated and \\(\\mu(F) = \\mu(M)\\).  </p> <p> Suppose \\(S = \\{x_j\\}_{j \\in J}\\) is a set of elements which generate \\(M\\). Note that, even in the worst case scenario, such an \\(S\\) exists since we can at most take \\(S = M\\). Now suppose \\(F = \\oplus{j \\in J}R\\), which is a free module. Construct the module homomorphism \\(\\psi: F \\to M\\) as  \\[ \\psi((a_j)_{j \\in J}) = \\sum_{j \\in J} a_jx_j. \\] <p>Observe that since \\(S\\) generates \\(M\\), such a homomorphism is surjective onto \\(M\\). Hence, we see that \\(M\\) is the quotient of some free module \\(F\\).</p> <p>Now suppose that \\(F\\) is finitely generated. Then \\(S\\) is a finite set, so that \\(F\\) is also finitely generated (since in this case it is the direct sum of at most a finite number of copies of \\(R\\)). </p> <p>Now if \\(M\\) is finitely generated, and is a quotient of \\(F\\), then clearly \\(\\mu(M) \\le \\mu(F)\\). However, we also know that \\(\\mu(F) \\le |J| \\le \\mu(M)\\). Therefore, we see that \\(\\mu(M) = \\mu(F)\\).  </p> <p> Let \\(M\\) be an \\(R\\)-module and \\(F\\) a free \\(R\\)-module. Then the short exact sequence  <p> is called a free presentation of \\(M\\). Note by the previous proposition that every \\(R\\)-module has a free presentation.  </p> <p>Presentations are particularly useful since they make free modules convenient to work with. </p> <p> Suppose \\(F\\) is a free \\(R\\)-module. Then every short exact sequence  \\  is a split exact sequence.  </p> <p> Let \\(S = \\{x_j\\}_{j \\in J}\\) be a basis for \\(F\\). Now suppose \\(f: M \\to F\\) is the surjective function in the above exact sequence. Now construct a function \\(\\psi: F \\to M\\) as follows: \\(\\psi(x_j) = m_j\\) if and only if \\(f(m_j) = x_j\\). Since \\(f\\) is surjective, note that this will always be possible. Such a function may not be unique, but we don't care; we just want to know it exists.  <p>By proposition \\ref{prop: unique homomorphism}, we know that there exists a unique function \\(h: F \\to M\\) such that \\(h|_S = \\psi\\). Therefore we see that \\(f \\circ h = 1_F\\), so that by theorem \\ref{split_exact_lemma}, we see that the sequence is in fact split exact.  </p>"},{"location":"algebra/Modules/Generating%20Modules%2C%20Torsions%2C%20Annihilators./","title":"3.3. Generating Modules, Torsions, Annihilators.","text":"<p>The concepts we have introduced so far are not new. In fact, this is the third time you've probably seen all of these concepts. However, module theory is very deep, and here is where we will start seeing new concepts.  \\ \\ Generating Modules.\\ Let \\(M\\) be an \\(R\\)-module and suppose \\(S \\subset M\\) where \\(S\\) is nonempty. Denote the smallest submodule of \\(M\\) containing \\(S\\) as \\(\\left&lt; S \\right&gt;\\), and observe that </p> \\[ \\left&lt; S \\right&gt; = \\bigcap_{\\alpha \\in \\lambda} S_{\\alpha} \\] <p>where \\(\\{S_\\alpha\\}_{\\alpha \\in \\lambda}\\) is a family of submodules containing \\(S\\).</p> <p>Note that this is in fact a submodule, since arbitrary intersections of submodules yield a submodule. </p> <p>Now consider the set of all finite linear combinations of elements of \\(S\\) with coefficients in \\(R\\). That is, </p> \\[ S' = \\Big\\{\\sum_{i = 1}^{n} a_is_i \\mid a_i \\in R, s_i \\in S \\text{ for all } i \\in N\\Big\\}.  \\] <p>This is of course also an \\(R\\)-module. We claim that \\(\\left&lt; S \\right&gt; = S'\\). \\begin{description} \\item[\\phantom{meow}]\\(\\bf{\\left&lt; S \\right&gt; \\subset S'.}\\) To see this, consider any \\(s \\in \\left&lt;S\\right&gt;\\). Then \\(s \\in S_{\\alpha}\\) for all \\(\\alpha \\in \\lambda\\). Furthermore, since \\(S \\subset S'\\) we see that \\(S'\\) is one of the members of the families of all submodules containing \\(S\\). Therefore we must have that \\(s \\in S'\\), and thus \\(\\displaystyle s = \\sum_{i = 1}^{n}a_is_i\\) for some \\(a_i \\in R\\) and \\(s_i \\in S\\). Hence, \\(\\left&lt;S \\right&gt; \\subset S'\\).</p> <p>\\item[\\phantom{meow}]\\(\\bf{S' \\subset \\left&lt; S \\right&gt;}.\\) Simply observe that since \\(\\left&lt; S \\right&gt;\\) contains \\(S\\), and because \\(\\left&lt; S \\right&gt;\\) is a submodule, it must be that \\(\\left&lt; S \\right&gt;\\) contains all linear combinations of elements of \\(S\\) with coefficients in \\(R\\). That is, \\(\\displaystyle \\sum_{i = 1}^{n} a_is_i \\in \\left&lt; S \\right&gt;\\) for any \\(a_i \\in R, s_i \\in S\\).  \\end{description}</p> <p>Thus what we have shown is the following theorem. </p> <p> Let \\(M\\) be an \\(R\\)-module and suppose \\(S \\subset M\\). Then if \\(\\left&lt; S \\right&gt;\\) is the smallest submodule of \\(M\\) containing \\(S\\) then  \\[ \\left&lt; S \\right&gt; = \\bigcap_{\\alpha \\in \\lambda} S_\\alpha \\] <p>where \\(\\{S_\\alpha\\}_{\\alpha \\in \\lambda}\\) is the family of submodules containing \\(S\\). More explicilty, we have that </p> \\[ \\left&lt; S \\right&gt; = \\Big\\{\\sum_{i = 1}^{n} a_is_i \\mid a_i \\in R, s_i \\in S \\text{ for all } i \\in N\\Big\\}. \\] <p>\\vspace{-0.7cm} </p> <p>The above theorem then leads a very useful definition that we will work with frequently. </p> <p> Let \\(M\\) be an \\(R\\)-module and \\(S \\subset M\\). <ul> <li> <p>[1.] The \\(R\\)-submodule \\(\\left&lt; S \\right&gt;\\) of \\(M\\) is called the submodule of \\(M\\) generated by \\(S\\).</p> </li> <li> <p>[2.] If \\(M = \\left&lt; S \\right&gt;\\) for some \\(S \\subset M\\), then we say that \\(M\\) is generated by \\(S\\). Hence, the elements of \\(S\\) are referred to as the generators of \\(M\\).</p> </li> <li> <p>[3.] If \\(M\\) is generated by \\(S\\) and \\(S\\) is finite, then we say \\(M\\) is finitely generated by \\(S\\). In this case we refer to \\(|S|\\), denoted as \\(\\mu(M)\\), as the rank of \\(M\\). Furthermore, if \\(S = \\{x_1, x_2, \\dots, x_n\\}\\) it is convenient to write \\(M = \\left&lt; x_1, x_2, \\dots, x_n \\right&gt;\\).</p> </li> </ul> <p></p> <p>In the case of a cyclic group \\(G\\), we see that \\(G\\) has rank one and one generator. Thus we see that this concept is generalized in module theory if the generating set for some module is of cardinality one. </p> <p>Note we can also union modules together to get another module.</p> <p> Let \\(M\\) be an \\(R\\)-module and \\(\\{S_{\\alpha}\\}\\) a family of submodules of \\(R\\). Then the \\textbf{submodule generated by \\(\\{N_{\\alpha}\\}_\\{\\alpha \\in \\lambda\\}\\)} is  \\[ \\left&lt; \\bigcup_{\\alpha \\in \\lambda}S_\\alpha\\right&gt; \\] <p>which we often denote as \\(\\displaystyle \\sum_{\\alpha \\in \\lambda}S_{\\alpha}\\). </p> <p>Next we move onto the concept of an annihilator, which we define as follows. </p> <p> Let \\(M\\) be an \\(R\\)-module, and suppose \\(X \\subset M\\). Then we define the set  \\[ \\ann(X) = \\{a \\in R \\mid ax = 0 \\text{ for all }x \\in X\\} \\] <p>to be the annihilator of \\(X\\).  \\textcolor{NavyBlue}{Note that \\(\\ann(X) \\subset R\\) and that \\(0 \\in \\ann(X)\\) for any \\(X \\subset M\\). If \\(\\ann(X) = \\{0\\}\\), we of course say that it is trivial.}</p> <p>The annihilator captures all of the coefficients of \\(R\\) which annihilate every element in \\(X\\). Thus one can imagine that the size of \\(\\ann(X)\\) increases as the size of \\(X \\subset M\\) increases (of course, if \\(\\ann(X)\\) is not trivial for all \\(X\\).)</p> <p> Let \\(M\\) be an \\(R\\)-module and let \\(X \\subset M\\) be nonempty. Then  <ul> <li> <p>[1.] \\(\\ann(X)\\) is a left ideal of \\(R\\)</p> </li> <li> <p>[2.] If \\(N\\) is a submodule of \\(M\\), then \\(\\ann(N)\\) is an ideal of \\(R\\).</p> </li> <li> <p>[3.] If \\(R\\) is commutative and \\(N\\) is a cyclic submodule of \\(M\\) generated by \\(x \\in N\\) then \\(\\ann(N) = \\{a \\in R \\mid ax = 0 \\}\\).</p> </li> </ul> <p></p> <p> <ul> <li> <p>[1.]  Let \\(X \\subset M\\). In order for \\(\\ann(X)\\) to be a left ideal, it must be a subring of \\(R\\) which absorbs left multiplication.  \\begin{description}</p> </li> <li> <p>[It's a Subring.] We can apply the subring test to prove this. Recall earlier we said that \\(0 \\in \\ann(X)\\) for any \\(X \\subset M\\), so \\(\\ann\\) is nonempty. </p> </li> </ul> <p>Now let \\(a, b \\in \\ann(X)\\), so that \\(ax = bx = 0\\) for all \\(x \\in X\\). Then clearly \\(abx = 0\\) and \\((a - b)x = ax -bx = 0\\) so that \\(ab \\in \\ann(X)\\) and \\(a-b \\in \\ann(X)\\). Thus it is a subring of \\(R\\).</p> <ul> <li> <p>[It's an ideal.] For any \\(r \\in R\\) and \\(a \\in \\ann(X)\\) we have that \\(rax = r(ax) = 0\\). Therefore \\(ra\\in I\\) for all \\(r \\in R\\), which proves it is a left ideal.  \\ \\textcolor{Plum}{Why isn't it also a right ideal? Well, observe that we would need \\(arx = 0\\) whenever \\(a \\in \\ann(X)\\) and \\(r \\in R\\). This would require either that \\(ar = 0\\), which we can't always guarantee, or that \\(rx \\in X\\). But we don't know if \\(rx \\in X\\); we could only guarantee that if \\(X\\) was an \\(R\\)-module, which we'll see in the next proof. } \\end{description} </p> </li> <li> <p>[2.] Let \\(N\\) be a submodule of \\(M\\). Since we showed in (1.) that \\(\\ann(X)\\) is a left ideal for any \\(X \\subset M\\), we must simply show that \\(\\ann(N)\\) also absorbs right multiplication of \\(R\\) as well in order to show it is an ideal.</p> </li> </ul> <p>Thus let \\(r\\in R\\) and \\(a \\in \\ann(N)\\). Since \\(N\\) is a submodule of \\(N\\) we know  that \\(rx \\in N\\). Hence \\(a(rx) = 0\\) as \\(an = 0\\) for all \\(n \\in N\\). Therefore \\(ar \\in \\ann(N)\\) whenever \\(r \\in R\\) and \\(a \\in \\ann(N)\\), proving that \\(\\ann(X)\\) absorbs right multiplication and is therefore an ideal.</p> <ul> <li>[3.] Consider \\(\\ann(N)\\) where \\(N\\) is cyclic and generated by \\(x\\) and let \\(|N| = k\\). Suppose we have an \\(a \\in R\\) such that \\(ax = 0\\). Then we see that \\(ax^2 = (ax)x = 0\\), \\(ax^3 = (ax)x^2 = 0\\), and that in general \\(ax^j = (ax)x^{j-1} = 0\\) for any \\(j \\in \\{1, 2, \\dots, k\\}\\). Since every \\(n \\in N\\) is of the form \\(x^j\\) for some \\(j \\in \\{1, 2, \\dots, k\\}\\) we have \\(an = 0\\) for all \\(n \\in N\\). Hence, </li> </ul> \\[ \\ann(X) = \\{a \\in R \\mid an = 0 \\text{ for all } n \\in N\\} = \\{a \\in R \\mid ax = 0\\}. \\] <p>\\textcolor{red}{Where does commutativity come into play?}</p> <p> Note that in general we denoted \\(\\ann(x)\\) to be \\(\\ann(N)\\) where \\(N\\) is cyclic and generated by \\(x\\). This only really makes sense if \\(R\\) is commutative.</p> <p>For an \\(R\\)-module \\(M\\) and a family of submodules \\(\\{N_\\alpha\\}_{\\alpha \\in \\lambda}\\), we have been able to define the arbitrary intersection and addition of submodules. Next we define the product of \\(R\\)-modules. We now define a product of \\(R\\)-modules. </p> <p>If \\(M\\) is an \\(R\\)-module and \\(I\\) is an ideal, then we can define </p> \\[ IM = \\Big\\{ \\sum_{i = 1}^{n}a_im_i \\mid a_i \\in R, m_i \\in M \\text{ for } n \\in N \\Big\\}     \\] <p>which is a submodule of \\(R\\). Thus our main properties of submodules are intersection, addition, and products with ideals. </p> <p> Let \\(R\\) be an integral domain and \\(M\\) an \\(R\\)-module. Suppose \\(x \\in M\\). Then <ul> <li> <p>[1.] If \\(\\ann(x) \\ne \\{0\\}\\) then we define \\(x\\) to be torsion element. We define the set of torsion elements of \\(M\\) to be the torsion submodule and denote this as \\(M_{\\tau}\\).</p> </li> <li> <p>[2.] If \\(M_\\tau = \\{0\\}\\) then we say \\(M\\) is torsion free, while if \\(M_\\tau = M\\) we say that \\(M\\) is torsion module.</p> </li> </ul> <p></p> <p> Let \\(R\\) be an integral domain and \\(M\\) an \\(R\\)-module. Then  <ul> <li> <p>[1.] \\(M_\\tau\\) is a submodule of \\(M\\). </p> </li> <li> <p>[2.] \\(M/M_\\tau\\) is torsion free. </p> </li> </ul> <p></p> <p> <ul> <li>[1.] We can use the submodule test to show that this is a submodule of \\(M\\). First, recall that \\(M_\\tau\\) is nonempty as it always contains 0. Let \\(a, b \\in R\\) and suppose \\(m_1, m_2 \\in M_\\tau\\). Then observe that $(am_1 + bm_2)x = am_1x</li> <li> <p>bm_2x = 0$ since \\(m_1x = 0\\) and \\(m_2x = 0\\). Hence, this is a submodule.</p> </li> <li> <p>[2.] Suppose that \\((m + M_\\tau)x = M_\\tau\\) where \\(m \\in M\\) for some \\(x \\in M\\).  Then this implies that \\(mx \\in M_\\tau\\). Therefore there exists a \\(n \\in R\\) such that \\(n(mx) = (nm)x = 0\\). Since \\(R\\) is an integral domain, \\(nm \\ne 0\\) so that we must have \\(x = 0\\). Thus the torsion module is trivial.</p> </li> </ul> <p></p> <p> Let \\(R\\) be an integral domain and suppose \\(M = \\left&lt; x_1,x_2, \\dots, x_n \\right&gt;\\) (that is, \\(M\\) is finitely generated). Then  \\[ \\ann(M) = \\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n). \\] <p> Note that \\(0 \\in \\ann(x_i)\\) for all \\(i \\in \\{1, 2, \\dots, n\\}\\). Hence, the above intersection is never empty. </p> <p> \\begin{description} \\item[\\phantom{m}]\\(\\bf{\\ann(M) \\subset \\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n)}\\). Observe that for any \\(a \\in \\ann(M)\\), we see that \\(ax = 0\\) for all \\(x \\in M\\). In particular, \\(ax_i = 0\\) for \\(i \\in \\{1, 2, \\dots, n\\}\\). Hence we see that \\(a \\in \\ann(x_i)\\) for each \\(x_i\\), so that \\(a \\in \\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n)\\). <p>\\item[\\phantom{m}]\\(\\bf{\\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n) \\subset }\\bf{\\ann(M)}\\). Observe that for $ a \\in \\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n)$ we see that \\(ax_i = 0\\) for each \\(i \\in \\{1, 2, \\dots, n\\}\\).</p> <p>By 1.\\ref{submodule_intersection_theorem} we know that for any \\(m \\in M\\) we have that \\(\\displaystyle m = \\sum_{i = 1}^{n}a_ix_i\\) for some \\(a_i \\in R\\) and \\(x_i \\in M\\). But note that </p> \\[ am = a\\sum_{i =1}^{n}a_ix_i = \\sum_{i =1}^{n}aa_ix_i = \\sum_{i =1}^{n}a_i(ax_i) =0 \\] <p>where in the last step we used the commutativity of \\(R\\). Therefore \\(a \\in \\ann(X)\\), proving that \\(\\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n) \\subset \\ann(M)\\). \\end{description} With both directions of the proof complete, we can conclude that  \\(\\ann(M) = \\ann(x_1) \\cap \\ann(x_2) \\cap \\dots \\cap \\ann(x_n)\\) as desired. </p>"},{"location":"algebra/Modules/Submodules%2C%20Quotient%20Modules%20and%20Isomorphism%20Theorems./","title":"3.2. Submodules, Quotient Modules and Isomorphism Theorems.","text":"<p> Let \\(M\\) be a \\(R\\)-module. Then a set \\(N \\subset M\\) is said to be a submodule of \\(M\\) if \\(N\\) is also a \\(R\\)-module.  </p> <p>\\textcolor{MidnightBlue}{What do we need for \\(N \\subset M\\) to be a submodule?} For \\(N\\) to be a submodule,</p> <ul> <li> <p>\\(N\\) needs to be an nonnempty abelian group </p> </li> <li> <p>Axioms (a) - (d) in Definition 1.1 must be satisfied </p> </li> <li> <p>\\(N\\) needs to be closed under multiplication of \\(R\\). That is, \\(\\cdot : R \\times M|_{N} \\to N\\), where \\(M|_N\\) is \\(M\\) restricted to \\(N\\) (namely, just \\(N\\)).</p> </li> </ul> <p>However, since \\(N \\subset M\\), axioms (a) - (d) are already satisfied for \\(N\\). In addition, if \\(N\\) is a nonempty subgroup of \\(M\\) then it is automatically abelian. Thus \\(N\\) is a \\(R\\)-submodule of \\(M\\) if \\(N\\) is a subgroup of \\(M\\) and \\(N\\) is closed under multiplication of elements from \\(R\\). This leads us to the following submodule test. </p> <p>[(Submodule Test.)] Let \\(M\\) be an \\(R\\)-module and \\(N \\subset M\\) be nonempty. Then \\(N\\) is an \\(R\\)-submodule of \\(M\\) if and only if \\(an_1 + bn_2 \\in N\\) for all \\(n_1, n_2 \\in N\\) and \\(a, b \\in R\\).  </p> <p> (\\(\\implies\\)) If \\(N\\) is an \\(R\\)-submodule of \\(N\\) then obviously \\(an_1 + bn_2 \\in N\\) for all \\(n_1, n_2 \\in N\\) and \\(a, b \\in R\\).  <p>(\\(\\impliedby\\)) Suppose \\(an_1 + bn_2 \\in N\\) for all \\(n_1, n_2 \\in N\\) and \\(a, b \\in R\\).  First observe that \\(N\\) is nonempty. Now setting \\(a = 1\\) and \\(b = -1\\) we see that \\(n_1 - n_2 \\in N\\) for all \\(n_1, n_2 \\in N\\), and thus by the subgroup test we see that \\(N\\) is a subgroup of \\(N\\). </p> <p>Since \\(an_1 + bn_2 \\in N\\) for all \\(a, b \\in R\\) we see that \\(N\\) is closed under multiplication of elements of \\(R\\). </p> <p>Since \\(N\\) is an abelian subgroup of \\(M\\) and is closed under multiplication of elements of \\(R\\), we see that \\(N\\) is an \\(R\\)-submodule as desired.  </p> <p>\\noindent Example.\\ An immediate example we can create from our previous discussions the fact that if \\(f: M \\to N\\) is an \\(R\\)-module homomorphism then </p> <ul> <li> <p>[1.] \\(\\ker(f)\\) is an \\(R\\)-submodule of \\(M\\).</p> </li> <li> <p>[2.] \\(\\im(f)\\) is an \\(R\\)-submodule of \\(N\\). </p> </li> </ul> <p>As we saw in group and ring theory, arbitrary intersections of subgroups or subrings resulted in subgroups and subrings. Thus the following theorem should be of no surprise.</p> <p> Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. If \\(\\{N_\\alpha\\}_{\\alpha \\in \\lambda}\\) be a set of \\(R\\)-submodules of \\(M\\), then \\(N = \\bigcap_{\\alpha \\in \\lambda}N_{\\alpha}\\) is a submodule of \\(M\\).  </p> <p> First observe that \\(N = \\bigcap_{\\alpha \\in \\lambda}N_{\\alpha}\\) is nonempty, since \\(0 \\in N_\\alpha\\) (the identity) for all \\(\\alpha \\in \\lambda\\). Thus for any \\(n_1, n_2 \\in N\\) we know that \\(n_1, n_2 \\in N_\\alpha\\) for all \\(\\alpha \\in \\lambda\\). Since each such \\(N_\\alpha\\) is an \\(R\\)-submodule, we know that \\(an_1 + bn_2 \\in N_\\alpha\\) for all \\(\\alpha \\in  \\lambda\\) for any \\(a, b \\in R\\). Hence, \\(an_1 + bn_2 \\in N\\) for all \\(a, b \\in R\\), proving that \\(N\\) is an \\(R\\)-submodule as desired.  </p> <p>Note that what ring \\(R\\) is under discussion, we will just state a \\(R\\)-submodule as simply a submodule. \\ \\ Quotient Modules. \\ \\ As we discovered quotient groups in group theory and quotient rings in ring theory, it should again be no surprise that we can formalize the concept of quotient modules. </p> <p>In group theory, a quotient group \\(G/H\\) only made sense if the group \\(H\\) being quotiened out was normal to \\(G\\). This guaranteed that our desired group operation in the quotient group worked and made sense as desired. In ring theory, a quotient ring \\(R/I\\) only made sense if the ring \\(I\\) being quotiened out was an ideal of \\(R\\). Since we wanted \\(R/I\\) to be a ring, we needed not only addition but multiplication to be well-defined, but well-definedness only worked when \\(I\\) was an ideal. </p> <p>In both cases, we couldn't quotient out just any subgroup or a subring to get a quotient group or quotient ring. They had to be special subsets (e.g. normal groups, ideals).  However, in module theory, it does happen to be the case that we can just quotient out a submodule to get a quotient module. </p> <p>\\textcolor{purple}{ To define a quotient module, we first consider an \\(R\\)-module \\(M\\) and a submodule \\(N\\) of \\(M\\). To turn \\(R/N\\) into an \\(R\\)-module, we first turn this into an abelian group, which we can perfectly do since \\(N\\) is a subgroup of \\(M\\), an abelian group, so \\(M/N\\) makes sense. A result from group theory tells us that if \\(M\\) is abelian then \\(M/N\\) is abelain.  \\ \\indent Next, to turn this into an \\(R\\)-module we define scalar multiplication as </p> \\[ r(m + N) = rm + N \\] <p>where \\(r \\in R\\), and multiplication of elements as</p> \\[ (m + N)(m' + N) = mm' + N.    \\] <p>As always, when defining a quotient object we're worried about the ability of our multiplication to preserve equivalence of elements. This is usually where we run into trouble in group theory or ring theory, in which case we modify the set \\(N\\) which we're quotienting out. In group theory, we'd turn \\(N\\) into normal group, and in ring theory we'd turn \\(N\\) into an ideal. Here we'll leave \\(N\\) alone, since it works out in the end. \\ \\indent Thus  suppose that</p> \\[ m + N = m' + N \\] <p>that is, \\(m = m' + n\\) for some \\(n \\in N\\).  Then to check if our  multiplicaton is well-defined, we observe that for \\(a \\in R\\)</p> \\[ am + N = a(m' + n) + N = am' + an + N \\] <p>and since \\(N\\) is a submodule, it is closed under scalar multiplication of elements of \\(R\\). Hence, \\(an \\in N\\), so that </p> \\[ am' + an + N = am' + N. \\] <p>Thus we see that \\(am + N = am' + N\\), so that our scalar multiplication is well-defined. } This leads to the following definition.</p> <p> Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. If \\(N\\) is a submodule of \\(M\\), then we defined \\(M/N\\) to be the \\textbf{quotient \\(R\\)-module} of \\(M\\) with respect to \\(N\\). As we showed earlier, this is in fact an \\(R\\)-module. </p> <p>As before, it should be no surprise that the Noether Isomorphism Theorems apply to modules as well. In fact, the Noether Isomorphism Theorems were first introduced by Emmy Noether for modules; not through groups or for rings. The Isomorphism Theorems hold for groups and rings since abelian groups and rings are special cases of modules. </p> <p>First, we introduce two homomorphisms which seem as if they are so stupidly simple that they don't even deserve a definition; yet, they do. </p> <p> Let \\(R\\) be a ring and \\(M\\) and \\(N\\) be \\(R\\)-module homomorphisms. Then we define the following \\(R\\)-module homomorphisms. <ul> <li>[1.] The map \\(\\pi: M \\to M/N\\) given by </li> </ul> \\[ \\pi(m) = m + N \\] <p>is said to be the projection map. Note that \\(\\pi\\) is surjective, and that \\(\\ker(\\pi) = N\\) (since \\(m + N = N\\) if and only if \\(m \\in N\\).)</p> <ul> <li>[2.] The map \\(i: M/N \\to M\\) given by </li> </ul> \\[ i(m + N) = m                 \\] <p>is known as the inclusion map. More generally, if \\(M' \\subset M\\), the inclusion map can also be defined as \\(i: M' \\to M\\) where </p> \\[ i(m') = m' \\] <p>for all \\(m' \\in M'\\). Note that \\(i\\) is injective, and in the first case \\(\\im(i) = M/N\\cup \\{0\\}\\) and in the second case \\(\\im(i) = M'\\).</p> <p></p> <p>[(First Isomorphism Theorem)] Let \\(R\\) be a ring and \\(M\\) and \\(N\\) be \\(R\\)-modules. If \\(f: M \\to N\\) is an \\(R\\)-module homomorphism, then  \\[ M/\\ker(f) \\cong \\im(f).   \\] <p>\\vspace{-0.8cm} </p> <p> The proof is the same as before. Define the map \\(\\phi: M/\\ker(f) \\to N\\) as  \\[  \\phi(m + \\ker(f)) = f(m). \\] <p>\\textcolor{NavyBlue}{We quickly show that this is well-defined.} If \\(m + \\ker(f) = m' + \\ker(f)\\) for some \\(m, m' \\in M\\), then \\(m = m' + k\\) for some \\(k \\in K\\). Therefore, </p> \\[ \\phi(m + \\ker(f)) = f(m) = f(m' + k) = f(m') = \\phi(m' + \\ker(f)). \\] <p>\\textcolor{NavyBlue}{Next, we show this is in fact an \\(R\\)-module homomorphism.} Linearity is obvious, so we check the second criterion. Now for any \\(a \\in R\\) we see that </p> \\[ \\phi(a(m + \\ker(f))) = \\phi(am + \\ker(f)) = f(am) = af(m) = a(\\phi(m + \\ker(f))) \\] <p>where we pulled the \\(a\\) outside from \\(f(am)\\) to make \\(af(m)\\) from the fact that \\(f\\) is an \\(R\\)-module homomorphism. </p> <p>\\textcolor{NavyBlue}{Now we make two observations.} First, we see that there is a one-to-one correspondence between \\(M/\\ker(f) \\to \\im(f)\\). Second, this implies that \\(\\phi\\) is an isomorphism between the two modules, so that </p> \\[ M/\\ker(f) \\cong \\im(f) \\] <p>as desired. </p> <p>[(Second Isomorphism Theorem.)] Let \\(R\\) be a ring and \\(M\\) and \\(N\\) and \\(P\\) be submodules of \\(M\\). Then  \\[ (N + P)/P \\cong N/(N \\cap P). \\] <p>\\vspace{-0.8cm} </p> \\[\\begin{minipage}{0.35 \\textwidth} \\begin{figure}[H] \\begin{tikzcd}[column sep=small]  &amp;   N + P \\\\ N \\arrow[ur, dash] &amp;&amp; P \\arrow[ul,swap,\"\\text{(submodule)}\"] \\\\ &amp; N\\cap P  \\arrow{ul}{\\text{(submodule)}} \\arrow[ur, dash] \\end{tikzcd} \\end{figure} \\end{minipage} \\hfill \\begin{minipage}{0.6\\textwidth} The diagram on the left is the same one we used in group theory and ring theory. That is, the second isomorphism theorem can still be described using the diamond diagram.  \\end{minipage}\\] <p> Construct the projection map \\(\\pi : M \\to M/P\\) and let \\(\\pi'\\) be the restriction of \\(\\pi\\) to \\(N\\). Then we see that \\(\\ker(\\pi') = N \\cap P\\), while  \\[ \\im(\\pi') = \\{\\pi'(n) \\mid n \\in N\\} = \\{n + P \\mid n \\in N\\} = (N + P)/P. \\] <p>Thus by the First Isomorphism Theorem we have that </p> \\[ N/\\ker(\\pi') \\cong \\im(\\pi') \\implies (N + P)/P \\cong N/(N \\cap P) \\] <p>as desired.  </p> <p>[(Third Isomorphism Theorem)] Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. Suppose \\(N\\) and \\(P\\) submodules such that \\(P \\subset N\\). Then  \\[ M/N \\cong (M/P)/(N/P). \\] <p>\\vspace{-0.8cm} </p> <p> Construct the map \\(f: M/P \\to M/N\\) by defining \\(f(m + P) = m + N\\) where \\(m+P \\in M/P\\) and \\(m + N \\in M/N\\). First observe that this is a surjective mapping since \\(P \\subset M\\), so the correspondence \\(m + P \\to m + N\\) will cover all of \\(M/N\\).  <p>Now observe that </p> \\[ \\ker(f) = \\{m + p \\mid m \\in N\\} = N/P. \\] <p>Therefore, by the First Isomorphism Theorem</p> \\[ (M/P)/\\ker(f) \\cong M/N \\implies (M/P)/(N/P) \\cong M/N \\] <p>as desired. </p> <p>[(Fourth Isomorphism Theorem)] Let \\(R\\) be a ring and \\(M\\) an \\(R\\)-module. Suppose \\(N\\) is a submodule of \\(M\\). Then every submodule of \\(M/N\\) is of the form \\(P/N\\) where \\(N \\subset P \\subset M\\).   Another way to understand this statement is to realize there is a one to one correspondence between the submodules of \\(M\\) containing \\(N\\) and the submodules of \\(M/N\\).</p> <p> <p></p>"},{"location":"algebra/Rings/Definitions./","title":"2.1. Definitions.","text":"<p>While many mathematical objects come in the form of groups, we also know that there are objects and spaces which require more than one operation. Can we generalize them?</p> <p>For example, we know that the integers \\(\\mathbb{Z}\\) form a group under addition. But don't we also know that multiplication of elements of \\(\\mathbb{Z}\\) also yield elements of \\(\\mathbb{Z}\\)? Isn't this another type of group-like structure we would like to generalize?</p> <p>We could do this on \\(\\mathbb{R}\\) too. It's a group under addition, but we know it's closed under multiplication and has some identity element.</p> <p>This is where rings come into play, which we define as follows. </p> <p> Let \\(R\\) be a set. We define \\((R, +, \\cdot)\\) to be a ring if there exist binary operations \\(+: R\\times R \\to R\\) and \\(\\cdot: R \\times R \\to R\\) (referred to as addition and multiplication) such that  <ul> <li> <p>[(R1)] Group addition. \\((R, +)\\) is an abelian group, with \\(0\\) denoted as the identity. (In this group, the additive inverse of an element \\(a\\) is always denoted \\(-a\\).)</p> </li> <li> <p>[(R2)] Closure. For all \\(a\\), \\(b \\in R\\), we have that \\(a \\cdot b \\in R\\).</p> </li> <li> <p>[(R3)] Associativity. For all \\(a\\), \\(b\\), \\(c \\in R\\), we have that \\(a \\cdot (b \\cdot c) = (a \\cdot b) \\cdot c\\)</p> </li> <li> <p>[(R4)] Distributivity. Similarly, we have that \\(a \\cdot (b + c) = a\\cdot b + a \\cdot c\\) and $(b</p> </li> <li> <p>c) \\cdot a = b \\cdot a + c \\cdot a$.</p> </li> <li> <p>[(R4)] There exists an element \\(1 \\ne 0\\) in \\(R\\) such that  \\(1 \\cdot a = a \\cdot 1 = a\\) for all \\(a \\in R\\). This is the unit of the ring. </p> </li> </ul> <p></p> <p> <ul> <li> <p>As usual, if the multiplication operation \\(\\cdot\\) is specified and well-understood, then we will drop \\(\\cdot\\) and write multiplication of ring elements as \\(gh\\) instead of \\(g \\cdot h\\).</p> </li> <li> <p>Axioms (R5) is technically optional. However, we don't really  care about rings without unity, so we just add it to ou defintion.</p> </li> </ul> <p></p> <p> Suppose \\(R\\) is a ring with identity \\(1 \\ne 0\\). Then  <ul> <li> <p>[1.] \\(0 \\cdot a = a \\cdot 0\\) for all \\(a \\in R\\) </p> </li> <li> <p>[2.] \\(-(a \\cdot b) = (-a) \\cdot b = a \\cdot (-b)\\) for all \\(a, b \\in R\\) </p> </li> <li> <p>[3.] \\(-a = a \\cdot (-1) = (-1) \\cdot a\\)</p> </li> <li> <p>[4.] \\((-a) \\cdot (-b) =  a \\cdot b\\) </p> </li> <li> <p>[5.] The multiplicative identity is unique.   </p> </li> </ul> <p> This is just the stuff you would expect from a ring \\(R\\) based on the fact that many domains you've seen are in fact rings, and some of these facts are obvious in those domains.</p> <p> <ul> <li>[1.] Observe that </li> </ul> \\[\\begin{align*} (0 \\cdot a) + (0 \\cdot a) &amp;= (0 + 0) \\cdot a \\text{ (by R4 )}\\\\ &amp; = (0 \\cdot a) + 0 \\text{  (since 0 + 0 = 0)} \\end{align*}\\] <p>where we added \\(0\\) to the righthand side (which of course  does not change the value of the equation.) Subtracting \\((0 \\cdot a)\\) from both sides, we get that </p> \\[ 0 \\cdot a  = 0. \\] <p>Similarly, observe that </p> \\[\\begin{align*} (a \\cdot 0) + (a \\cdot 0) &amp; = a \\cdot (0 + 0) \\text{ (by R4)}\\\\ &amp; = (a \\cdot 0)+ 0 \\text{ (since 0 + 0 = 0)} \\end{align*}\\] <p>where again, we added \\(0\\) to both sides. Subtracting \\(-(a \\cdot 0)\\) from both sides, we get </p> \\[ a \\cdot 0 = 0 \\] <p>as desired. </p> <ul> <li>[2.] First we'll show that \\(-(a \\cdot b) = (-a) \\cdot b\\). To prove this, observe that </li> </ul> \\[\\begin{align*} (a \\cdot b) - [(a \\cdot b)] &amp; = 0 \\\\ &amp; = a \\cdot 0 \\text{ (which we just proved)}\\\\ &amp; = a \\cdot [b + (-b)]\\\\ &amp; = a \\cdot b + a \\cdot (-b) \\text{ (by R4)} \\end{align*}\\] <p>and adding \\(-(a \\cdot b)\\) to both sides yields </p> \\[\\begin{align*} -(a \\cdot b) = a \\cdot (-b) \\end{align*}\\] <p>as desired. Now we'll show that \\(-(a \\cdot b) = (-a) \\cdot b\\). Observe that </p> \\[\\begin{align*} (a \\cdot b) - [(a \\cdot b)] &amp; = 0 \\\\ &amp; = 0 \\cdot b \\text{ (which we just proved)}\\\\ &amp; = [a + (-a)] \\cdot b\\\\ &amp; = a \\cdot b + (-a) \\cdot b \\text{ (by R4)} \\end{align*}\\] <p>and adding \\(-(a \\cdot b)\\) gives that </p> \\[\\begin{align*} -(a \\cdot b) = (-a) \\cdot b \\end{align*}\\] <p>which proves the asserition.</p> <ul> <li> <p>[3.] Simply let \\(b = 1\\) in the previous statements. </p> </li> <li> <p>[4.] To prove that \\((-a) \\cdot (-b) = a \\cdot b\\), first observe that for any \\(c \\in  R\\) we already proved that </p> </li> </ul> \\[ (-a) \\cdot c = a \\cdot (-c). \\] <p>Thus let \\(c = -b\\). Then observe that </p> \\[ (-a) \\cdot (-b) = a \\cdot [-(-b)] \\] <p>and from group theory, we know that \\(-(-b) = b\\). Therefore, we see that </p> \\[ (-a) \\cdot (-b) = a \\cdot b \\] <p>as desired. </p> <ul> <li>[5.] To prove that uniqueness of the multiplicative identity, first suppose that it is not unique. That is, there exists elements \\(1_1\\) and \\(1_2\\) such that </li> </ul> \\[ 1_1 \\cdot a = a \\cdot 1_1 = a \\hspace{1cm}   1_2 \\cdot a = a \\cdot 1_2 = a. \\] <p>for all \\(a \\in R\\). Then observe that </p> \\[ 1_1 = 1_1 \\cdot 1_2 = 1_2 \\] <p>so that the uniqueness must hold.</p> <p> \\textcolor{NavyBlue}{An example of a ring is of course \\(\\mathbb{Z}\\), but that's boring.  Is \\((\\mathbb{Z}/n\\mathbb{Z}, + , \\cdot)\\), where \\(n\\) is a positive integer, a ring? Let's check if it is. } \\begin{description} \\item[Abelian.] Since addition is commutative, we already know that \\(\\mathbb{Z}/n\\mathbb{Z}\\) is abelian (in fact, it is cyclic.)</p> <p>\\item[Associativity.] Let \\(a, b\\) and \\(c \\in \\ZZ/n\\ZZ\\). Now obviously, \\(a(bc) = (ab)c\\) under standard or \"normal\" multiplication of integers. Therefore we see that   </p> \\[\\begin{align*} a\\cdot(b \\cdot c) &amp;= a(bc) \\mbox{ mod } n \\\\ &amp; = (ab)c \\mbox{ mod }n \\\\ &amp; =(a \\cdot b ) \\cdot c. \\end{align*}\\] <p>\\item[Distributivity.] Let \\(a, b\\) and \\(c\\) be defined as before. Again, we know that \\(a(b + c) = ab + ab\\) in \\(\\mathbb{Z}\\). Therefore </p> \\[\\begin{align*} a\\cdot(b + c) &amp;= a(bc) \\mbox{ mod } n \\\\ &amp; = (ab + ac) \\mbox{ mod }n \\\\ &amp; = ab \\mbox{ mod }n + ac \\mbox{ mod }n\\\\ &amp; = a \\cdot b + a \\cdot c. \\end{align*}\\] <p>The argument is exactly the same to prove left distributivity. Altogether, we see that \\(\\ZZ/n\\ZZ\\) satisfies the axioms of a ring when endowed with modulo addition for \\(+\\) and modulo multiplication for \\(\\cdot\\).  \\end{description}</p> <p>\\noindentMultiplication yielding zeros.\\ For our ring \\(\\mathbb{Z}\\), we know that the only way to ever obtain \\(0\\) by multiplication is to just take \\(0\\) itself and multiply it by an integer. Thus in this ring, if \\(n, m\\) are nonzero then we always know that \\(n \\cdot m\\) is nonzero.</p> <p>However, note that in \\(\\ZZ/n\\ZZ\\), we have that \\(a \\cdot b = 0\\) if and only if \\(a \\cdot b\\) is a multiple of \\(n\\).</p> <p>\\textcolor{Plum}{If \\(n\\) is prime, then there are no elements in \\(\\ZZ/n\\ZZ = \\{0, 1, 2, \\dots, n-1\\}\\) whose product will be a multiple of \\(n\\). This is just because nothing divides \\(n\\). \\ \\ But if \\(n\\) is composite, then there exist integers \\(pq\\) such that \\(n = pq\\), and since \\(p &lt; n\\) and \\(q &lt; n\\), you can be certain that \\(p, q \\in \\ZZ/n\\ZZ\\). Then we'd see that \\(pq = 0\\) in \\(\\ZZ/n\\ZZ\\). If \\(p\\) or \\(q\\) are also composite, then there are even more combinations of integers in \\(\\ZZ/n\\ZZ\\) whose product yields 0 in \\(\\ZZ/n\\ZZ\\).  }</p> <p>So in the ring \\(\\ZZ\\), multiplication of nonzero elements will be nonzero. But in the ring \\(\\ZZ/n\\ZZ\\) there are many ways one one can multiply elements to get zero (if \\(n\\) is not prime). Obviously these are both rings, but they're behaving differently! Hence we introduce the following definitions. </p> <p> Let \\((R, +, \\cdot)\\) be a ring and suppose \\(a \\ne 0\\) and \\(b \\ne 0\\) are elements of \\(R\\), while  \\[ a \\cdot b = 0. \\] <p>Then \\(a\\) and \\(b\\) are both called zero divisors of the ring \\({R}\\). Note that \\(0\\) is not a zero divisor. Meanwhile, if \\(R\\) has an identity, and for some \\(a \\in R\\) there exists a \\(b \\in R\\) such that </p> \\[ ab = 1 = ba \\] <p>then we call both \\(a\\) and \\(b\\) units in \\(R\\). It turns out the set of units of a ring \\(R\\) form an abelian group, which we denote as \\(R^*\\). </p> <p>Note that \\(\\ZZ\\) has no zero divisors, and its unit group \\(R^*\\) is just \\(\\{1, -1\\}\\). We can see that since if \\(ab = 1\\) for \\(a, b \\in \\ZZ\\), then we know that \\(a = b = 1\\) or \\(-1\\).</p> <p>On the other hand, \\(\\ZZ/n\\ZZ\\) can have a more interesting unit group. Observe that if there exists integers \\(p, q \\in \\ZZ/n\\ZZ\\) such that </p> \\[ pq = n +1 \\] <p>then we see that $p \\cdot q = pq \\mbox{ mod } n = n + 1 \\mbox{ mod } n = 1 $ in \\(\\ZZ/n\\ZZ\\). If either \\(p\\) or \\(q\\) are composite, then \\(R^*\\) becomes even more interesting.</p> <p>As a more specific example, observe that the ring \\(\\ZZ/10\\ZZ\\) has units \\(\\{1, 3, 7, 9\\}\\) and zero divisors \\(\\{2, 4, 6, 8\\}\\).</p> <p> A zero divisor can never be a unit. </p> <p> Let \\(R\\) be a ring and suppose \\(a \\in R\\) is a zero divisor. Then there exists an element \\(b \\in R\\) where \\(b \\ne 0\\) and \\(ab = 0.\\) Now suppose that \\(a\\) is also a unit, so that there exists a \\(c \\in R\\) such tha \\(ac = ca = 1\\). Then observe that  \\[\\begin{align*} 1 = ca \\implies b &amp;= (ca)(b)\\\\ &amp;= c(ab)\\\\ &amp;= c(0)\\\\ &amp; = 0 \\end{align*}\\] <p>which is a contradiction since we said \\(b \\ne 0\\). Hence \\(a\\) cannot be a unit. </p> <p>We'll next prove another useful lemma which is commonly known as the cancellation law. </p> <p> Let \\(R\\) be a ring, and \\(a \\in R\\) such that \\(a \\ne 0\\). If \\(a\\) is not a zero divisor, then for any \\(b, c \\in R\\) such that \\(ab = ac\\) we have that \\(b = c\\). In addition, if \\(ba = ca\\) then \\(b = c\\). </p> <p> Suppose \\(ac = ab\\) for some elements \\(a, b, c \\in R\\) where \\(a\\) is not a zero divisor. Then observe that  \\[ ab = ac \\implies ac - ab = 0 \\implies a(b - c) = 0. \\] <p>Since \\(a\\) is not a zero divisor, the only way for the above equation to hold is if \\(b - c = 0 \\implies b = c\\). Proving the analagous statement is identical to this proof.  </p> <p>\\textcolor{NavyBlue}{Now that we have identified terms and can describe the specific elements of a ring \\(R\\) based on their properties, we again return to our observation that \\(\\ZZ\\) and \\(\\ZZ/n\\ZZ\\) behaved differnetly. This is not uncommon in ring theory, so we can divide rings into specific classes as follows.}</p> <p> Let \\(R\\) be a ring.  <ul> <li> <p>[1.] If \\(R\\) is commutative ring with identity and has no zero divisors, then \\(R\\) is said to be an integral domain.</p> </li> <li> <p>[2.] The ring \\(R\\) is a said to be a division ring if every element of \\(R\\) has a multiplicative inverse. An equivalent condition is if \\(R^* = R\\setminus \\{0\\}\\).</p> </li> <li> <p>[3.] If \\(R\\) is a commutative division ring, then \\(R\\) is said to be a field.</p> </li> </ul> <p></p> <p>You've probably read textbooks that called \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\) fields. This is what they're talking about. </p> <p> <ul> <li> <p>[1.] If \\((R, +, \\cdot)\\) is an integral domain, then the cancellation law holds for all elements of \\(R\\). </p> </li> <li> <p>[2.] \\((R, +, \\cdot)\\) is an integral domain if and only if for \\(a, b \\in R\\), the equation \\(a\\cdot b = 0\\) implies either \\(a = 0\\) or \\(b = 0\\). </p> </li> <li> <p>[3.] \\((R, +, \\cdot)\\) is a division ring if and only if \\(ax = b\\) and \\(ya = b\\) are solvable in \\(R\\) for every \\(a, b \\in R\\) where $a \\ne 0 $.</p> </li> </ul> <p></p> <p>\\textcolor{MidnightBlue}{Consider again the ring \\(\\ZZ/p\\ZZ\\) where \\(p\\) is a positive integer. We noted that if \\(p\\) is prime then there are no zero divisors. Thus we could state that this an integral domain. However, we can strengthen this even further and state that this is a field, as follows. \\ \\indent Let \\(a \\in \\ZZ/p\\ZZ\\) be nonzero. We know that there exists an inverse \\(a^{-1}\\) such that </p> \\[  aa^{-1} = 1 \\mbox{ mod } p  \\] <p>if \\(a\\) is coprime with \\(p\\), which is of course true. Since every element has a multiplicative invesrse we see that \\(\\ZZ/p\\ZZ\\) is a division ring. Since this is a commutative division ring, we have that \\(\\zz/p\\zz\\) is a field. } Observe that we could have more easily proved thiss tatemen with the following theorem.</p> <p> Any finite integral domain is a field. </p> <p> Let \\(R\\) be a finite integral domain and let \\(a \\in R\\) be nonzero. Construct a function \\(\\phi_a: R \\to R\\) by \\(\\phi_a(b) = ab\\) for \\(b \\in R\\).  <p>Suppose \\(\\phi_a(b) = \\phi_a(c)\\) for \\(b, c \\in R\\). Then \\(ab = ac \\implies b = c\\) since \\(R\\) is an integral domain. Therefore \\(\\phi_a\\) is injective, and it is clearly surjective so that \\(\\phi_a(R) = R\\).</p> <p>Since \\(\\phi_a\\) is bijective for each \\(a \\in R\\), we know  there always exists a \\(b \\in R\\) such that \\(\\phi_a(b) = 1 \\implies ab = 1\\). In other words, each \\(a \\in R\\) has an inverse, proving \\(R\\) is a division ring. Since \\(R\\) is an integral domain and thus a commutative we have that it is a commutative divison ring, and hence a field. </p> <p>\\textcolor{MidnightBlue}{As we said before \\(\\ZZ/p\\ZZ\\) is an integral domain. Since it's also finite, this allows us to conclude it is a field, which we proved before we proved the above theorem.}</p> <p>The following is apparently too difficult for any introductory algebra book to prove in terms of elementary language.</p> <p> Any finite division ring is a field. </p> <p>With the integral domain, division ring and field introduced, we have a solid footing in the fundamentals of ring theory. We move forward by introducing the concept of a subring.</p> <p>\\subsection*{Subrings.}</p> <p> Let \\(R\\) be a ring and \\(S\\) be a nonempty subset of \\(R\\). Then \\(S\\) is a subring of \\(R\\) if \\(S\\) is a ring under the addition and multiplication equipped on \\(R\\).  <p>Specifically, \\(S\\) is a subring if \\(S\\) is an abelain group under addition and is closed under multiplication.  \\noindent Examples.\\ We already have an example from our previous work. We know that \\(\\ZZ\\) and \\(\\ZZ/n\\ZZ\\), where \\(n\\) is a positive integer, are both rings. Since \\(\\ZZ/n\\ZZ \\subset \\ZZ\\), we see that \\(\\ZZ/n\\ZZ\\) is a subring of \\(\\ZZ\\).  \\</p> <p>\\textcolor{Blue!80!White}{Define the set \\(\\ZZ[i] = \\{m + ni: m, n \\in \\ZZ\\}\\).  Then this is a ring.} This is clearly an abelian group under addition (0 is the identity, associativity is obvious, closedness is clear, inverse of any given element is the same element with coefficients of opposite sign). Multiplicative associativity and left and right distributions are clear. However, since \\(\\ZZ[i] \\subset \\mathbb{C}\\), we see that \\(\\ZZ[i]\\) is a subring of \\(\\mathbb{C}\\). \\</p> <p>\\textcolor{Green!80!White}{Let \\(R\\) be a ring. Then the set of \\(n \\times n\\) matrices with entries \\(R\\), denoted \\(M_{n}(R)\\), forms a ring.} Addition on this set forms an abelian group. And we know from linear algebra that matrix multiplication is associative and left and right distributive. It turns out this ring has many interesting subrings, which we'll list here  \\begin{description} \\item[Diagonals.]</p> \\[ D_n(R) = \\{A \\in \\mathbb{R}: a_{ij} = 0 \\text{ if } i \\ne j\\} \\] <p>\\item[Upper Triangulars.]</p> \\[ T^n(R) = \\{A \\in M_n(R): a_{ij} = 0 \\text{ if } i &gt; j\\} \\] <p>\\item[Lower Triangulars.]  </p> \\[ T_n(R) = \\{A \\in M_n(R): a_{ij} = 0 \\text{ if } i &lt; j\\}. \\] <p>\\end{description} These are all subrings of \\(M_n(R)\\). \\</p> <p>\\textcolor{Purple!80!White}{Let \\(G\\) be abelian. Then \\(\\mbox{End}(G)\\), the set of endomorphism (homomorphisms from \\(G\\) to itself), forms a ring under addition as function addition and multiplication as function composition.}  \\begin{description} \\item[Abelian Group.] First observe that this is a commutative structure since \\(G\\) is abelian. We just have to show that this is a group. \\begin{description} \\item[Identity.] Let \\(0_G\\) be the identity element of \\(G\\). Construct the identity element for \\(\\mbox{End}(G)\\) to be the zero map \\(0\\) defined as \\(0: G \\to G\\) such that \\(0(g) = 0_G\\) for all \\(g \\in G\\). </p> <p>\\item[Associativity.] Since \\(G\\) is associative, and the images of elements in \\(\\mbox{End}(G)\\) are in \\(G\\), associativity is inherited. </p> <p>\\item[Closedness.] Let \\(f, g \\in \\mbox{End}(G)\\) and define \\(h = f + g\\). Then \\(h: G \\to G\\), and is obviously a homomorphism, so that \\(h \\in \\mbox{End}(G)\\). </p> <p>\\item[Inverses.] Let \\(f \\in \\mbox{End}(G)\\). Then construct the function \\(f^{-1} : G  \\to G\\) such that \\(f^{-1}(g) = -h\\) whenever \\(f(g) = h\\). (Note that \\(-h\\) is the inverse of \\(h\\).) Then we see that \\(f^{-1}(g) + f(g) = 0\\) for all \\(g \\in G\\), and that \\(f^{-1}(g) \\in \\mbox{End}(G)\\), so that \\(f^{-1}\\) is an inverse of \\(f\\).  \\end{description}</p> <p>\\item[Multiplicatively Closed.] Observe that if \\(h: f \\circ g\\), then \\(h: G \\to G\\), and it is a homomorphism. Hence \\(h \\in \\mbox{End}(G)\\). Therefore our multiplcative operator is closed.</p> <p>\\item[Multiplicative Associativity.] This holds in our case since function composition is in general associative for homomorphisms.</p> <p>\\item[Distributivity.] Let \\(f, g, h \\in \\mbox{End}(G)\\). Then observe that </p> \\[ f(g + h) = f \\circ (g + h) = f \\circ g + f \\circ h = fg + fh \\] <p>and </p> \\[ (g + h)f = (g + h) \\circ f = g \\circ f + h \\circ f = gf + hf \\] <p>by linearity of \\(f, g\\) and \\(h\\) (since homomorphisms in general are linear functions). \\end{description} Therefore, we have that \\(\\mbox{End}(G)\\) forms a ring under function additon and composition. \\</p> <p>\\textcolor{Red!70!Blue}{ Polynomial Rings. Polynomials are an interesting example of a ring, which we construct as follows.}</p> <p>\\indent Let \\(R[x]\\) be the set of all functions \\(f: \\ZZ^+ \\to R\\) such that \\(f(n) = 0\\) for all but finitely many \\(n\\). These functions will be the coefficients to our polynomials, and we want them to be finite, so we request that only finitely many of our cofficients are nonzero. That is, \\(f(n)\\) represents the \\(n\\)-th coefficient. \\</p> <p>\\noindent Define addition and multiplication for two \\(f, g \\in R[x]\\) as </p> \\[ (f + g)(n) = f(n) + g(n) \\hspace{0.2cm}\\text{ and }\\hspace{0.2cm}  (f \\cdot g)(n) = \\sum_{i = 0}^{n}f(i)g(n - i). \\] <p>This last formula is the formula for the \\(n\\)-th coefficient from the product of two polynomials. We'll show this is a ring. \\begin{description} \\item[Abelian.] First we'll show this is an abelian group under addition. \\begin{description} \\item[Identity.]Let \\(0_R \\in R\\) be the 0 element of \\(R\\). If we define 0 to be the map \\(0(n) : \\mathbb{Z} \\to R\\) such that \\(0(n) = 0_R\\) for all \\(n \\in \\mathbb{Z}\\), then clearly \\(0 \\in R[x]\\) and \\(0 + f = f + 0 = f\\) for any \\(f \\in R[x]\\). It is our additivite identity. </p> <p>\\item[Associativity.] Associativity is derived from the fact that \\(R\\) is associative under addition.</p> <p>\\item[Closedness.]  To show this is closed we, show that \\(f + g\\) is nonzero for at most finitely many elements for any \\(f, g \\in R[x]\\). Simply observe if \\(f\\) is nonzero for \\(k\\)-many elements and \\(g\\) is nonzero  for \\(l\\)-many elements then \\((f + g)\\) is nonzero for at most \\((l + k)\\)-many elements. Therefore \\((f + g) \\in R[x]\\).</p> <p>\\item[Inverses.]  For any \\(f \\in R[x]\\), define \\(f^{-1}\\) to be \\(f^{-1}(n) = -f(n)\\) for all \\(n \\in \\ZZ^{+}\\). Obviously \\(f^{-1}\\) is nonzero for at most finitely many elements if \\(f\\) is, so \\(f^{-1} \\in R[x]\\), and \\(f^{-1}(n) + f(n) = 0\\) for any \\(n \\in \\ZZ^{+}\\). Therefore \\(R[x]\\) contains inverses. \\end{description} </p> <p>\\item[Multiplicatively Closed.] Observe now that this is closed under multiplication. For any \\(f, g \\in R[x]\\), we can simply observe that since \\(f, g\\) are nonzero for at most finitely many values of \\(n \\in ZZ^{+}\\), we note that </p> \\[ fg(n) = \\sum_{i = 1}^{n}f(i)g(n - i) \\] <p>is a function which is nonzero for at most finitely many values, since it is always a finite sum of \\(f\\) and \\(g\\).</p> <p>\\item[Multiplicative Associativity.] Let \\(f, g, h \\in R[x]\\). Then observe that </p> \\[\\begin{align*} (fg)h(n) = \\sum_{i = 0}^{n}(fg)(i)h(n - i) &amp;=  \\sum_{i = 0}^{n}\\left( \\sum_{j = 0}^{i}f(j)g(i - j) \\right)h(n - i)\\\\ &amp;= f(0)g(0)h(n) + \\Big(f(0)g(1) + f(1)g(0)\\Big)h(n-1)\\\\  &amp;+ \\Big(f(0)g(2) + f(1)g(1) + f(2)g(0)\\Big)h(n-2) + \\cdots \\\\ &amp;= \\sum_{i = 0}^{n}f(n)\\sum_{j = 0}^{n-i}g(j)h(n-j-i)\\\\ &amp;= \\sum_{i = 0}^{n}f(n)(gh)(n - i)\\\\ &amp;= f(gh)(n). \\end{align*}\\] <p>Therefore multiplicative associativity is satisfied.</p> <p>\\item[Distributivity.]  Since the image of our functions are elements in \\(R\\), distributivity is inherited from the ring \\(R\\), which must be left and right distributed.  \\end{description}</p> <p>Therefore we see that \\(R[x]\\) forms a rings. We'll now realize that this is the set of polynomials by describing the function a stupidly simple function: </p> \\[ x^n(m) =  \\begin{cases} 1 &amp; \\text{ if } n = m\\\\ 0 &amp; \\text{ otherwise } \\end{cases}. \\] <p>Then observe that for any \\(f \\in R[x]\\), we may uniquely associate with it the following object: </p> \\[ f = \\sum_{n = 0}^{\\infty}f(n)x^n. \\] <p>The \\(\\infty\\) in the upper limit is there to allow us to define any polynomial of an arbitrary degree. We know it will always a finite polynomial since we said that \\(f(n) \\ne 0\\) for at most finitely many \\(n\\). </p> <p>Thus what we've shown is that the space \\(R[x]\\), constructed by focusing on the coefficients, defining their rules for polynomial multiplication, and realizing the polynomial structure we wanted, is in fact a ring! \\</p> <p>Note that if we don't assume that \\(f(n)\\) is nonzero for finitely many \\(n\\), then we'll end up constructing a different ring, know as the formal power series ring denoted \\(R[[x]]\\). This has the same rules of addition and multiplication, so the ring structure doesn't change. The only thing that changes is that \\(R[[x]]\\) includes infinitely long polynomials. </p> <p>Thus, we see that \\(R[x] \\subset R[[x]]\\) and therefore \\(R[x]\\) is a subring of \\(R[[x]]\\).</p> <p>In group theory there was a Subgroup Test which simplified the task of determine whether or not a subspace form a group or not. Fortunately, such a tool is available in ring theory.</p> <p>[Subring Test.] Let \\(R\\) be a ring and \\(S \\subset R\\). Then \\(S\\) is a subring of \\(R\\) if and only if, for all \\(x, y \\in S\\) we have that \\(x - y \\in S\\) and \\(xy \\in S\\). </p> <p> (\\(\\implies\\)) Suppose \\(S \\subset R\\) is a subring. Then certain \\(x - y \\in S\\) and \\(xy \\in S\\).  <p>(\\(\\impliedby\\)) Suppose now that \\(x - y \\in S\\) and \\(xy \\in S\\) for all \\(x, y \\in S\\). The first condition immediately \\((S, +)\\) is a subgroup of \\((R, +)\\), since \\(x - y \\in S\\) for all \\(x, y \\in S\\) is just the subgroup test. Now observe that \\(xy \\in S\\) for all \\(x, y \\in S\\). Since \\(R\\) is an abliean group under addition and is closed under multiplication of its elements, we have that \\(S\\) is a subring of \\(R\\) as desired. </p> <p>It turns out that arbitrary intersections of subrings produce a subring, an important result we include here. </p> <p> Let \\(R\\) be a ring and \\(\\{S_\\alpha\\}_{\\alpha \\in \\lambda}\\) be a family of subrings of \\(R\\). Then \\(S = \\bigcap_{\\alpha \\in \\lambda} S_\\alpha\\) is a subring of \\(R\\).  </p> <p> From group theory, we know that the arbitrary intersection of subgroups is again a group. So \\(S = \\bigcap_{\\alpha \\in \\lambda} S_\\alpha\\) is an abelian subgroup of \\(R\\).  Therefore, we just need to check that \\(S\\) is closed under multiplication.  <p>From group theory, we know that the arbitrary intersection of a family of subgroups is a group. Thus \\(S\\) is an abelian group, and we just need to check that it is closed under multiplication. </p> <p>For any \\(s, s' \\in S\\) we know that \\(s, s' \\in S_\\alpha\\) for all \\(\\alpha \\in \\lambda\\). Since each subring is obviously closed under multiplication we see that \\(ss' \\in S_\\alpha\\) for all \\(\\alpha \\in \\lambda\\). Hence, \\(ss' \\in S\\) as desired. </p>"},{"location":"algebra/Rings/Ideals%20and%20Quotient%20Rings./","title":"2.3. Ideals and Quotient Rings.","text":"<p>Consider a ring homomorphism \\(f: R \\to S\\). Let \\(a \\in R\\) and suppose \\(b \\in \\ker(f)\\). Then </p> \\[ f(ab) = f(a)f(b) = 0f(b) = 0. \\] <p>Therefore, if \\(a \\in \\ker(R)\\), then \\(ab \\in \\ker(R)\\) for all \\(b \\in R\\). Many subrings behave this way and are particularly interesting, so we give them a special name!  \\ First, we'll introduce the concept of a coset.</p> <p> Let \\((R, +, \\cdot)\\) be a ring with identity \\(1 \\ne 0\\). Suppose \\(I\\) is a subring. Then we define the set  \\[ \\overline{a} = a + I = \\{a + i \\in R \\mid i \\in I\\}    \\] <p>to be a coset \\(I\\) in \\(R\\). Since \\(R\\) is an abelian group under addition, we see that </p> \\[ a + I = I + a \\] <p>for all \\(a \\in R\\). Hence, left and right cosets are the concept here. Finally, we define the \\textbf{collection of cosets} by </p> \\[ R/I = \\{\\overline{a} \\mid a \\in R\\}.                \\] <p> We are now ready to introduce the concept of an ideal. </p> <p> Let \\(R\\) be a ring and suppose \\(I \\subset R\\). Then we define \\(I\\) to be an ideal of \\(R\\) if and only if  <ul> <li> <p>[1.] \\(I\\) is an additive subgroup of \\(R\\) </p> </li> <li> <p>[2.] \\(rI \\subset I\\) for all \\(r \\in R\\) </p> </li> <li> <p>[3.] \\(Ir \\subset I\\) for all \\(r \\in R\\)</p> </li> </ul> <p>\\textcolor{Purple}{An ideal is simply an interesting subring \\(R'\\) of a ring \\(R\\) which sort of \"sucks in\" elements of \\(R\\) and sends them into \\(R'\\). That is, \\(rr' \\in R'\\) for every \\(r \\in R\\) and \\(r' \\in R'\\). \\ \\ We've already seend many examples of this, although we don't usually think of them that way. For instance, it's a well known fact that for any integer times an even number is again an even number. Algebraically, for \\(n \\in \\ZZ\\) and \\(k \\in 2\\ZZ\\) we have that \\(nk \\in 2\\ZZ\\) and \\(kn \\in 2\\ZZ\\).  \\ \\ Thus \\(2\\ZZ\\) is an ideal of \\(\\ZZ\\). In fact, if \\(k\\) is any even integer then \\(k\\ZZ\\) is an ideal of \\(\\ZZ\\). \\ \\ The set of odd integers is not an ideal of \\(\\ZZ\\), since we could always take an even number \\(n \\in \\ZZ\\) and any odd \\(k\\), and multiply them to obtain an even number \\(nk\\) which is obviously not in the set of odd integers. }</p> <p>If \\(I \\subset R\\) satisfies (2) then \\(I\\) is said to be a left ideal. On the other hand if \\(I \\subset R\\) satisfies (3) then it is said to be a right ideal.  </p> <p>Thus any ideal \\(I\\) is both a left and right ideal. In addition, the concept of a left ideal is identical to a right ideal in a commutative ring.  </p> <p> Suppose \\(I \\subset R\\) is a proper subring. Then the following are equivalent: <ul> <li> <p>[1.] \\(I = \\ker(f)\\) for some \\(f: R \\to S\\)</p> </li> <li> <p>[2.] \\(r\\cdot x = x \\cdot r \\in I\\) for any \\(r \\in R\\), \\(x \\in I\\) </p> </li> <li> <p>[3.] \\(R/I\\) is a ring with \\(\\overline{1} \\ne \\overline{0}\\). </p> </li> <li> <p>[4.] \\(I\\) is an ideal.  </p> </li> </ul> <p></p> <p> <ul> <li>[1.] We'll show \\((i) \\implies (ii)\\). Assume \\(I = \\ker(f)\\). Given \\(r \\in R\\) and \\(i \\in I\\), </li> </ul> \\[\\begin{align*} \\phi(r \\cdot i) = \\phi(r)\\cdot\\phi(i) = \\phi(r)\\cdot 0 = 0\\\\ \\phi(i \\cdot r) = \\phi(i)\\cdot\\phi(r) = 0 \\cdot\\phi(r) = 0 \\end{align*}\\] <p>This shows that \\(r \\cdot i, i \\cdot r \\in \\ker(f)\\). </p> <ul> <li>[ii.] We'll show that \\((ii) \\implies (iii)\\). Assume \\(ri, ir \\in I\\) for all \\(r \\in R, i \\in I\\). We'll show that this is a ring with \\(\\overline{1} \\ne \\overline{0}\\). </li> </ul> <p>First, we define that </p> \\[\\begin{align*} \\overline{a} + \\overline{b} = \\overline{a + b}\\\\ \\overline{a}\\overline{b} =\\overline{ab}. \\end{align*}\\] <p>We first need to show that these definitions are well-defined. Suppose \\(\\overline{a_1} = \\overline{a_2}\\) and \\(\\overline{b_1} = \\overline{b_2}\\). Then \\(a_1 = a_2 + x\\) and \\(b_1 = b_2 + y\\) for some \\(x,y\\in I\\). Then </p> \\[ a_1 + b_1 = (a_2 + b_2) + (x + y). \\] <p>Since \\(I \\subset R\\) is a subring, \\(x+y \\in I\\). So, </p> \\[ \\overline{a_1 + b_1} = \\overline{a_1}\\overline{a_2}.    \\] <p>Simiarly, \\(\\cdot\\) is well defined on \\(R/I\\). Again, suppose \\(\\overline{a_1} = \\overline{a_2}\\) and \\(\\overline{b_1} = \\overline{b_2}\\). Then \\(a_1 = a_2 + x\\) and \\(b_1 = b_2 + y\\) for some \\(x,y\\in I\\). Then </p> \\[\\begin{align*}   a_1 \\cdot b_1 &amp; = (a_2 + x) \\cdot (b_2 + y)\\\\ &amp; = (a_2\\cdot b_2) + [(a_2 \\cdot y) +(x \\cdot b_2) + (x \\cdot y)]. \\end{align*}\\] <p>\\(I\\) is a subring, so \\(x \\cdot y \\in I\\). Now \\((ii)\\) is true, so \\(a_2 \\cdot y \\in I\\) and \\(x \\cdot b_2 \\in I\\). Therefore, \\(\\overline{a_1\\cdot b_1} = \\overline{a_2 \\cdot b_2}\\). </p> <p>Finally, we'll show that \\((R/I, +, \\cdot)\\) is a ring.  \\begin{description}</p> <ul> <li> <p>[(R1: Addition)] Observe that \\(\\overline{0} \\in R/I\\) is the identity and \\(\\overline{-a}\\) are inverses of \\(\\overline{a} \\in R/I\\). </p> </li> <li> <p>[(R2: Closure)] The set is closed by construction on \\(\\cdot\\). </p> </li> <li> <p>[(R3: Assoc), (R5: Distributivity)] hold for \\(R/I\\) because they hold for \\(R\\). </p> </li> <li> <p>[(R4: Identity)] The identitty holds for \\(\\overline{1} \\in R/I\\). One can check that \\(\\overline{1} \\ne \\overline{0}\\).  \\end{description}</p> </li> <li> <p>[iii] Now we can show that \\((iii) \\implies (i)\\). Assume \\(S = R/I\\) is a ring. Define </p> </li> </ul> \\[ \\phi: R \\to S \\quad a \\mapsto \\overline{a} = a + I \\] <p>One checks that \\(\\ker(\\phi) = I\\). </p> <ul> <li>[iv.] Our work in the previous section has allowed us to prove \\((i) \\implies (iv)\\). Now observe that we can prove \\((iv) \\implies (i)\\) by simply considering the map in \\((iii)\\).</li> </ul> <p></p> <p>[ (Properties of Ideals)] Let \\(R\\) be a ring and \\(I, J\\) ideals of \\(R\\). Then  <ul> <li> <p>[1.] \\(I +J\\) is an ideal of \\(R.\\) (Note we may extend this to larger, finit sums)</p> </li> <li> <p>[2.] \\(IJ = \\left\\{\\displaystyle \\sum_{k=1}^ni_kj_k \\mid \\text{for all } n \\in \\mathbb{N}, i_k \\in I, j_k \\in J\\right\\}\\) is an ideal of \\(R\\). (Note we can extend this to larger, finite products.) </p> </li> <li> <p>[3.] \\(I \\cap J\\) is an ideal of \\(R\\). Morever, if \\(\\{I_\\alpha\\}_{\\alpha \\in \\lambda}\\) is a family of ideals of \\(R\\), then \\(\\bigcap\\limits_{\\alpha \\in \\lambda} I_\\alpha\\) is an ideal of \\(R\\). </p> </li> </ul> <p></p> <p> <ul> <li>[1.]  By the Second Isomorphism Theorem we know that \\(I + J\\) is a subring of \\(R\\). Thus, we just need it to be closed under multiplication for it to be an ideal. </li> </ul> <p>Let \\(i + j \\in I + J\\) and let \\(r \\in R\\). then \\(r(i + j) = ri +rj \\in I + J\\), since \\(ri \\in I\\) and \\(rj \\in J\\). Similarly, \\((i + j)r \\in I + J\\), so that \\(I + J\\) is an ideal of \\(R\\).</p> <ul> <li>[2.] In words, \\(IJ\\) is the set of all finite sums of elements of the form \\(ij\\) where \\(i \\in I\\) and \\(j \\in J\\). Thus is clearly an abelian group. To show it is closed under multiplication, let \\(r \\in R\\). Then observe that \\(r(\\sum_{k=1}^{n}i_kj_k) = \\sum_{k=1}ri_kj_k\\).  Now \\(ri_k \\in I\\) for all \\(k\\) since \\(I\\) is an ideal. Therefore \\(r(\\sum_{k=1}^{n}i_kj_k) \\in IJ\\). </li> </ul> <p>For similar reasons \\((\\sum_{k=1}^{n}i_kj_k)r \\in I\\), so that \\(IJ\\) is an ideal. </p> <ul> <li>[3.] By our knowledge of group theory we know that intersections of subgroups form a group, so that this is an abelian subgroup. To see it is an ideal we just need to check it is closed under scalar multiplication. </li> </ul> <p>Let \\(i \\in I \\cap J\\). Then \\(i \\in I\\) and \\(i \\in J\\). Hence, \\(ir \\in I\\) and \\(ri \\in J\\), and \\(ri \\in I\\) and \\(rj \\in J\\) as \\(I\\) and \\(J\\) are ideals. Hence \\(ir \\in I \\cap J\\) and \\(ri \\in I \\cap J\\), so that \\(I \\cap J\\) is an ideal. </p> <p>The more general statement has the same proof structure.</p> <p></p> <p> If \\(S\\) is a nonempty partially ordered set in which every chain \\(I_1 \\subset I_2 \\subset \\cdots\\) has an upper bound \\(I\\), then \\(S\\) has a maximal element \\(M\\). </p> <p>[ (Properties of Ideals)] Let \\((R, +, \\cdot)\\) be a ring with identity \\(1 \\ne 0\\). Consider a chain \\(I_1 \\subseteq I_2 \\subseteq \\cdots \\subseteq I_n \\subseteq \\cdots \\subseteq R\\) of proper ideals of \\(R\\). <ul> <li> <p>[1.] \\(\\displaystyle I = \\bigcup_{n \\ge 1}I_n\\) is a proper ideal of \\(R\\). </p> </li> <li> <p>[2.] Each proper ideal \\(I\\) of \\(R\\) is contained in a maximal ideal \\(M\\) of \\(R\\).</p> </li> </ul> <p></p> <p> <ul> <li>[1.] \\underline{\\(\\bm{I}\\) is nonempty.}\\ [1.2ex] Observe that \\(I\\) is nonempty if at least one \\(I_k\\) is nonempty. </li> </ul> <p>\\noindent\\underline{\\(\\bm{a, b \\in I \\implies a -b \\in I}\\).}\\[1.2ex] Pick \\(a, b \\in I\\). Then \\(a \\in I_n\\) and \\(b \\in I_m\\) for some \\(n, m\\). Without loss of generality assume \\(n \\le m\\). Then \\(I_n \\subseteq I_m\\). Thus \\(a \\in I_m\\) as well, and since \\(I_m\\) is an ideal, we see that \\(a - b \\in I_m\\). Hence \\(a - b \\in I\\). </p> <p>\\noindent\\underline{\\(\\bm{ra \\in I}\\) if \\(\\bm{r \\in R, a \\in I}\\)}.\\[1.2ex] If \\(a \\in I\\) then \\(a \\in I_k\\) for some \\(k\\). Since \\(I_k\\) is an ideal, we have that \\(ra \\in I_k\\). Hence \\(ra \\in I\\). </p> <p>\\noindent\\underline{\\(\\bm{I \\ne R}\\).}\\[1.2ex] Suppose on the contrary that \\(I = R\\). Then for every \\(r \\in R\\) there exists an integer \\(k\\) such that \\(r \\in I_k\\). In particular, for some \\(u \\in R^{\\times}\\) (the unit group), there is a \\(k\\) such that \\(u \\in I_k\\). Since \\(I_1 \\subseteq I_2 \\subseteq \\cdots \\subseteq I_k\\), we see that all ideal \\(I_1, I_2, \\dots, I_k\\) are not proper (as they contain a unit.) \\ \\ However, this is a contradition, since each \\(I_n\\) must be proper. Thus \\(I\\) cannot be all of \\(R\\). </p> <ul> <li>[2.] Consider any proper ideal \\(I_1\\) of \\(R\\). If \\(I_1\\) is not maximal, then there exists an ideal \\(I_2\\) such that \\(I_1 \\subset I_2\\). If \\(I_2\\) is not maximal, then there exists an ideal \\(I_3\\) such that \\(I_2 \\subset I_3\\). Now construct the set  [ S = {I_n \\text{ is proper } \\mid I_{n} \\subset I_{n+1}}. ]</li> </ul> <p>where \\(I_n \\in S_j\\) whenever there exists a proper ideal \\(I_{n+1}\\) where \\(I_n \\subset I_{n+1}\\).</p> <p>If this set is finite, then we take the maximal element (relative to partial ordering on subset inclusion) \\(M\\) as the maximal ideal. </p> <p>Suppose on the other hand that this set is infinite.  By part \\((a)\\), we see that every \\(I_n \\in S\\) is a subset of the proper ideal \\(\\bigcup_{n \\ge 1} I_n\\), so that this is an upper bound on the set of elements \\(S_J\\) (in terms of set inclusion). Hence by Zorn's lemma, we see that there must exist a maximal element \\(M \\in S\\). As all members of \\(S\\) are proper ideals, we see that \\(M\\) is by definition a maximal ideal where \\(M \\ne R\\). As \\(I_1\\) was arbitrary, we see that all ideals are contained in some maximal ideal \\(M\\), as we set out to show.</p> <p></p> <p>The following is a useful example of an ideal known as the nilradical:</p> <p> Let \\((R, +, \\cdot)\\) be a commutative ring with \\(1 \\ne 0\\), and let \\(I \\subset R\\) be a proper ideal. The  radical of \\(I\\) is the set  \\[ \\sqrt{I} = \\{r \\in R \\mid r^n \\in I \\text{ for some } n \\in \\zz_{&gt; 0}  \\}. \\] <p>\\begin{enumerate} \\item \\(\\sqrt{I}\\) is an ideal containing \\(I\\). </p> <p>\\item \\(\\sqrt{I}\\) is the intersection of all prime ideals \\(P\\) which contain \\(I\\). \\end{enumerate} </p> <p> \\begin{enumerate} \\item First observe that \\(I \\subset \\sqrt{I}\\). Since for any \\(r \\in I\\), we see that \\(r^1 = r \\in I\\). Hence \\(r \\in \\sqrt{I}\\).  <p>Now we'll show that \\(\\sqrt{I}\\) is an ideal.\\ [1.2ex] \\noindent\\underline{\\(\\bm{\\sqrt{I} \\ne \\varnothing}\\).}\\[1.2ex] Since \\(I \\subset \\sqrt{I}\\), we see that \\(\\sqrt{I}\\) is nonempty.  \\[1.2ex] \\noindent\\underline{\\(\\bm{a, b \\in \\sqrt{I} \\implies a - b \\in \\sqrt{I}}\\).}\\[1.2ex] Let \\(a, b \\in \\sqrt{I}\\). Then there exist positive integers \\(m, n\\) such that \\(a^m \\in I\\) and \\(b^n \\in I\\). Now observe that  [ (a -  b)^{n + m} = \\sum_{k = 0}^{m + n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k}. ]</p> <p>by the binomial theorem. Observe that when \\(k \\le n\\),</p> \\[\\begin{align*} k \\le n &amp; \\implies n - k \\ge 0\\\\ &amp; \\implies n + m - k \\ge m. \\end{align*}\\] <p>Hence we see that \\(a^{n + m - k} = a^{n - k}a^m \\in I\\) because \\(a^m \\in I\\). Since \\(I\\) is an ideal, we see that </p> \\[ \\sum_{k = 0}^{n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k}   \\] <p>is a sum of terms in \\(I\\), so therefore it is in \\(I\\). </p> <p>Now suppose \\(k &gt; n\\). Then we get that </p> \\[\\begin{align*} n &lt; k &amp;\\implies k =  n + j \\text{ for some } j \\in \\mathbb{Z}^{+}. \\end{align*}\\] <p>Therefore we see that \\(b^{k} = b^{j}b^{n} \\in I\\). Since \\(I\\) is an ideal, the sum</p> \\[\\begin{align*} \\sum_{k = n+1}^{n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k}   \\end{align*}\\] <p>is a sum of terms in \\(I\\). Hence the total sum is in \\(I\\). Now we see that </p> \\[ \\sum_{k = 0}^{m + n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k} = \\sum_{k = 0}^{n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k}   +  \\sum_{k = n+1}^{n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k} \\] <p>so that \\(\\displaystyle (a - b)^{m+n} = \\sum_{k = 0}^{m + n}\\binom{m+n}{k}a^{n + m - k}(-b)^{k}\\) is a sum of two terms in \\(I\\), and hence is in \\(I\\). Thus we have that \\(a, b \\in \\sqrt{I} \\implies a - b \\in \\sqrt{I}\\). \\ [1.2ex] \\noindent\\underline{\\(\\bm{ra \\in I}\\) if \\(\\bm{r \\in R, a \\in I}\\).}\\[1.2ex] Suppose that \\(a \\in \\sqrt{I}\\). Then \\(a^n \\in I\\) for some positive integer \\(n\\). Since \\(R\\) is a commutative ring, we see that \\((ra)^n = r^na^n \\in I\\) since \\(a^n \\in I\\) and \\(I\\) is an ideal. Thus \\(ra \\in I\\) for any \\(r \\in R\\), \\(a \\in I\\).  \\[1.2ex] \\noindent\\underline{\\(\\bm{\\sqrt{I} \\ne R}\\).}\\[1.2ex] Suppose that \\(\\sqrt{I} = R\\). Then for every \\(r \\in R\\), there exists a positive integer \\(n\\) such that \\(r^n  \\in I\\). </p> <p>Then in particular for some unit \\(u \\in R^{\\times}\\) we have that \\(u^m \\in I\\) for some integer \\(m\\). However, since \\(R^\\times\\) is a group under multiplication, we know that \\(u^m \\in R^{\\times}\\). Hence \\(u^m\\) is a unit. Since \\(u^m \\in I\\), this implies that \\(I\\) contains a unit, which ultimately implies that \\(I = R\\).  \\ \\ (\\underline{Note}: It is a fact from class that if an ideal \\(I\\) of \\(R\\) contains a unit, it is all of \\(R\\). I am utilizing this fact. Please don't dock off points for this literal fact from class.) \\ \\ However, this is a contradition since we assumed that \\(I\\) was proper. Hence \\(\\sqrt{I} \\ne R\\), which proves that it is a proper ideal.</p> <p>\\item  First we prove the hint.  \\ \\ Following the hint, suppose \\(x \\not\\in \\sqrt{I}\\). If we let \\(D = \\{1, x, x^2, \\dots\\}\\), pick a maximal ideal \\(M\\) in the ring \\(S = D^{-1}R/D^{-1}I\\). </p> <p>Let \\(\\phi: R \\to S\\) where \\(\\phi(r) = \\dfrac{r}{1} + D^{-1}I\\). Let \\(P\\) be the pull-back of \\(M\\) under \\(\\phi\\).  We'll now prove the hint by showing \\(P\\) is prime, \\(x \\not\\in \\sqrt{I} \\implies x \\not\\in P\\) and that \\(I \\subset P\\). \\[1.2ex] \\underline{\\(\\bm{P}\\) is prime}.\\[1.2ex] First observe we need to make sure that the pullback is well defined, in the sense that  if \\(M\\) is maximal then \\(P\\) is prime. First observe that since \\(M\\) is maximal, it is prime by our previous lemma. Thus we know from Hw 2  that we need to show two things.</p> <ul> <li>[1.] \\(\\bm{\\phi(1) = 1}.\\) Observe that  [ \\phi(1) = \\dfrac{1}{1} + D^{-1}I  ]</li> </ul> <p>which is the identity element in \\(D^{-1}R/D^{-1}I\\). Hence, \\(\\phi(1) = 1\\).  \\(\\phi^{-1}(P)\\) is a prime ideal. From problem 2, we know that this allows us to conclude the pull-back is well defined.</p> <ul> <li> <p>[2.] \\(\\bm{P = \\phi^{-1}(M)}\\) is prime. (It may help the reader for me to refer to \\(P\\) explicitly as \\(\\phi^{-1}(M)\\), in terms of clarity of the solution, so I'll follow that convention.) \\begin{description}</p> </li> <li> <p>[\\(\\bm{\\phi^{-1}(M)}\\) is nonempty.] Observe that \\(\\phi(0) = \\dfrac{0}{1} + D^{-1}I \\in M\\), as \\(M\\) is an ideal of \\(D^{-1}R/D^{-1}I\\) and hence contains the zero element. Therefore \\(0 \\in \\phi^{-1}(M) = P\\) and so \\(P\\) is nonempty. </p> </li> <li> <p>[\\(\\bm{a, b \\in \\phi^{-1}(M)\\implies a-b \\in \\phi^{-1}(M)}\\).]  Let \\(a, b \\in \\phi^{-1}(M)\\). Then \\(\\phi(a), \\phi(b) \\in M\\). Hence, we see that </p> </li> </ul> \\[\\begin{align*} \\phi(a), \\phi(b) \\in M &amp; \\implies \\phi(a) - \\phi(b) \\in M \\text{ (since } M \\text{ is a prime ideal)}\\\\ &amp; \\implies \\phi(a - b) \\in M \\text{ (by homomorphism properties)}\\\\ &amp; \\implies a - b \\in \\phi^{-1}(M). \\end{align*}\\] <p>Therefore, we see that \\(a - b \\in \\phi^{-1}(M)\\) if \\(a, b \\in \\phi^{-1}(M)\\).  </p> <ul> <li>[\\(\\bm{ra \\in \\phi^{-1}(M)}\\) if \\(\\bm{r \\in R, p \\in \\phi^{-1}(M)}\\).]  We'll show that \\(r \\cdot a \\in \\phi^{-1}(M)\\) for all \\(r \\in R\\). Observe that </li> </ul> \\[\\begin{align*} \\phi(r\\cdot a) = \\phi(r)\\phi(a). \\end{align*}\\] <p>Since \\(\\phi(a) \\in M\\), and \\(M\\) is a prime ideal, \\(s\\phi(a) \\in M\\) for all \\(s \\in D^{-1}R/D^{-1}I\\). In particular, since \\(\\phi(r) \\in D^{-1}R/D^{-1}I\\), we see that \\(\\phi(r)\\phi(a) \\in M\\). Therefore, \\(\\phi(r \\cdot a) \\in M\\) so that \\(r\\cdot a \\in \\phi^{-1}(M)\\).</p> <ul> <li>[\\(\\bm{ab \\in \\phi^{-1}(M) \\implies a \\in \\phi^{-1}(M)}\\) or \\(\\bm{b \\in \\phi^{-1}(M)}\\)] Suppose \\(ab \\in \\phi^{-1}(M)\\). Then we see that </li> </ul> \\[  \\phi(ab) \\in M \\implies \\phi(a)\\phi(b) \\in M. \\] <p>Since \\(M\\) is a maximal, and hence a prime ideal (as proven earlier), we see that either \\(\\phi(a) \\in M\\) or \\(\\phi(b) \\in M\\). In either case, we see that either \\(a \\in \\phi^{-1}(M)\\) or \\(b \\in \\phi^{-1}(M)\\), which is what we set out to show.</p> <ul> <li>[\\(\\bm{\\phi^{-1}(M)}\\) is proper.] Finally, we show that \\(\\phi^{-1}(M)\\) is proper. Suppose that \\(\\phi^{-1}(M) = R\\). Then </li> </ul> \\[ \\phi^{-1}(M) = R \\implies \\phi(R) = M.    \\] <p>Thus we see that \\(\\phi(r) \\in M\\) for all \\(r \\in R\\). Let \\(r = 1\\). </p> \\[ \\phi(r) \\in M \\implies \\phi(1) \\in M \\implies 1 \\in M \\] <p>since we have that \\(\\phi(1) = 1\\). However, \\(1 \\not\\in M\\) since \\(M\\) is maximal and hence proper. As we've reached a contradiction, we see that the pullback \\(P\\) must always be proper.  \\end{description} </p> <p>Thus we see that the pullback is well-defined (i.e., if \\(M\\) is prime, so is its pullback \\(P\\)) in this case and that \\(P\\) is prime.  \\ \\ (Note: it was technically unnecessary to do all of this work. Even in terms of clarity, I could have just referenced Hw 2, problem 3, and argued that the work carries over via the \\(\\phi(1) = 1\\) argument, since that was the only reason we  need \\(R\\) and \\(S\\) to be integral domains there, and then used the fact that maximal ideals are prime. However, I included the full work to be explicitly clear.) \\ \\ Next, we continue and prove the hint.\\ \\underline{\\(\\bm{x \\not\\in \\sqrt{I} \\implies x \\not\\in P}\\)}.\\ [1.2ex] Recall we supposed \\(x \\not\\in \\sqrt{I}\\). Now if \\(M\\) is an ideal of \\(D^{-1}R/D^{-1}I\\), then by the Fourth Isomorphism theorem we have that \\(M\\) corresponds to some ideal \\(M'\\) of \\(D^{-1}R\\) where \\(D^{-1}I \\subset M'\\). Hence we can write [ M = M' +  D^{-1}I. ]</p> <p>(\\underline{Note}: before you dock off points, the above choice of notation was introduced by Professor Goins himself. I think it's a bit unorthodox, which you may also think as well, but again, Goins used this notation so I will as well.) \\ \\ Now suppose for a contradiction that \\(x \\in P\\). Then we have that \\(\\dfrac{x}{1} + D^{-1}I \\in M\\). For this to be the case, we need that \\(\\dfrac{x}{1} \\in M'\\). Since \\(M'\\) is an ideal of \\(D^{-1}R\\), we know that \\(r\\dfrac{x}{1} \\in M\\) for all \\(r \\in D^{-1}R\\). In particular, we see that </p> \\[ \\dfrac{1}{x} \\cdot \\dfrac{x}{1} \\in M \\implies \\dfrac{1}{1} \\in M'. \\] <p>As \\(\\dfrac{1}{1}\\) is a unit, this implies that \\(M' = D^{-1}R\\) (\\underline{Note}: It is a fact from class that if an ideal \\(I\\) of \\(R\\) contains a unit, it is all of \\(R\\). I am utilizing this fact. Please don't dock off points for this literal fact from class.) \\ However, by the Fourth Isomorphism Theorem, this implies that \\(M = D^{-1}R/D^{-1}I\\); a contradiction to the assumption that \\(M\\) is a maximal ideal. Thus we see that \\(x \\not\\in P\\).  \\ [1.2ex] \\underline{\\(\\bm{I \\subset P}.\\)}\\[1.2ex] Now since \\(M\\) is an ideal, we see that it contains the zero element \\(D^{-1}I\\). Now observe that for any \\(i \\in I\\), [ \\phi(i) = \\dfrac{i}{1} + D^{-1}I = D^{-1}I \\in M. ]</p> <p>Therefore we see that \\(I \\subset \\phi^{-1}(M) = P\\). \\ \\ As this point we have shown that if \\(P\\) is the pullback of \\(M\\) under the given homomorphism, then (1) the pull back is well-defined (2) \\(P\\) is prime (3) if \\(x \\not\\in \\sqrt{I}\\) then \\(x \\not\\in P\\) and (4) \\(I \\subset P\\).</p> <p>Now consider the fact that \\(x \\not\\in \\sqrt{I} \\implies x \\not\\in P\\). Let \\(\\displaystyle \\bigcap_{I \\subset P' \\text{, prime}}P'\\) denote the intersection of all prime ideals containing \\(I\\). Since </p> \\[ \\bigcap_{I \\subset P' \\text{, prime}}P' \\subset P     \\] <p>because \\(P\\) is a prime ideal contaning \\(I\\), we see that if \\(x \\not\\in P\\) then \\(\\displaystyle x \\not\\in \\bigcap_{I \\subset P' \\text{, prime}}P'\\). As we proved that if \\(x \\not\\in \\sqrt{I}\\), then \\(x \\not\\in P\\), we see that </p> \\[ x \\not\\in \\sqrt{I} \\implies x \\not\\in \\bigcap_{I \\subset P' \\text{, prime}}P'. \\] <p>Taking the contrapositive of the statement, we can then conclude that </p> \\[ x \\in \\bigcap_{I \\subset P' \\text{, prime}}P' \\implies x \\in \\sqrt{I} \\] <p>which ulimately implies that \\(\\displaystyle \\bigcap_{I \\subset P', \\text{prime}}P' \\subset \\sqrt{I}\\).  \\ [1.2ex] \\underline{\\(\\bm{x \\in \\sqrt{I} \\implies x \\in P}\\)}\\[1.2ex] To show the reverse inclusion, suppose  \\(x \\in \\sqrt{I}\\), and let \\(P\\) be a prime ideal such that \\(I \\subset P\\). Then \\(x^n \\in I\\) for some positive integer \\(n\\). </p> <p>Suppose for the sake of contradiction that \\(x \\not\\in P\\) Let \\(N\\) be the smallest positive integer such that \\(x^N \\in I\\).  Since \\(x^N \\in I \\subset P\\), we see that \\(x^N \\in P\\). Note that  [ x^N = x \\cdot x^{N-1} \\in P.           ]</p> <p>Since \\(P\\) is a prime ideal, either \\(x \\in P\\) or \\(x^{N-1} \\in P\\). However, by assumption \\(x \\not\\in P\\). Thus we must have that \\(x^{N-1} \\in P\\). But since \\(I \\subset P\\), this implies that \\(x^{N-1} \\in I\\). This contradicts our choice of \\(N\\) as the smallest positive integer as \\(x^N \\in I\\). We have our contradiction, so we must have that \\(x \\in P\\). </p> <p>Since \\(x \\in \\sqrt{I} \\implies x \\in P\\)  for every prime ideal \\(P\\) such that \\(I \\subset P\\), we see that </p> \\[ \\sqrt{I} \\subset \\bigcap_{I\\subset P \\text{, prime}} P.     \\] <p>Since we already showed that \\(\\displaystyle \\bigcap_{I\\subset P, \\text{ prime}} P \\subset \\sqrt{I}\\), both set inclusions imply  that </p> \\[ \\sqrt{I} = \\bigcap_{I\\subset P \\text{, prime}} P  \\] <p>as desired. \\end{enumerate} </p> <p> Let \\(R\\) be a ring and \\(I, J\\) be ideals of \\(R\\) such that \\(I \\subset J \\subset R\\). Then \\(I\\) is an ideal of \\(J\\). </p> <p> To prove this, simply observe that for any \\(j \\in J\\) and \\(i \\in I\\) we have that \\(ij \\in I\\) and \\(ji \\in I\\).  </p> <p>A primary example of an ideal is any kernal of a homomorphism.</p> <p> Let \\(\\phi:R \\to S\\) be ring homomorphism. Then \\(\\ker(\\phi)\\) is an ideal of \\(R\\). </p> <p>We already partially showed this earlier, and the full proof is not difficult. </p> <p> If \\(R\\) is a division ring then the only ideals of \\(R\\) are \\(\\{0\\}\\) and \\(R\\) itself. </p> <p> Of course, \\(\\{0\\}\\) is an ideal for any ring. Therefore let \\(I\\) be a nonzero ideal. Then  \\[  ir \\in I \\] <p>for any \\(i \\in I\\) and \\(r \\in R\\). Since \\(R\\) is a division ring, every element has a multiplicative inverse (except 0). Hence for any nonzero \\(i\\) we can choose \\(r = i^{-1}\\) to conclude that \\(ii^{-1} = 1_R \\implies 1_R \\in I\\). </p> <p>Since \\(1_R \\in I\\), we can set \\(r \\in R\\) to be any element to conclude that \\(1_Rr = r \\implies r \\in I\\). Therefore \\(I = R\\). So every ideal is either \\(R\\) or \\(\\{0\\}\\). </p> <p> Let \\(R\\) be an integral domain. Any ring homomorphism \\(\\phi\\) from \\(R\\) to an arbitrary ring \\(S\\) is injective or the zero map. </p> <p> Since \\(\\ker(\\phi)\\) is an ideal of \\(R\\), it is either \\(\\{0\\}\\), in which case \\(\\phi\\) in injective, or \\(R\\), in which case \\(\\phi\\) is the zero map. </p> <p>Next we can introduce the concept of a quotient ring, which involves quotienting out an ideal. Note that for a ring \\(R\\) and an ideal \\(I\\), the concept of \\(R/I\\) makes sense since \\(R\\) is an abelian group, while \\(I\\) is a subgroup and is therefore a normal group to \\(R\\). Thus we make the following definition.</p> <p> Let \\(R\\) be a ring and \\(I\\) an ideal of \\(R\\). Then \\(R/I\\), the set of all elements \\(r + I\\) where \\(r \\in R\\), is defined to be a quotient ring whose operations are specified as follows.  \\begin{description} \\item[Addition.] For any \\(r + I, s + I \\in R/I\\) we have that  \\[ (r + I) + (s + I) = (r + s) + I. \\] <p>\\item[Multiplication.] For \\(r + I, s + I \\in R/I\\) we have that </p> \\[ (r + I)\\cdot(s + I) = rs + I. \\] <p>\\end{description}  First, let's check that this is even sensical. Again, we know from our group theory intuition that \\(R/I\\) definitely makes sense when looked at as an additive group. The identity is \\(I\\), inverses exist, it is closed and of course associative. Nothing has changed from our group theory perspective. </p> <p>We want \\(R/I\\) to not only be an abelian group, but also a ring, we defined multiplication of elements as \\((r + I)\\cdot(s + I) = rs + I\\). Thus we'll check the validity such multiplication.  \\</p> <p>\\textcolor{MidnightBlue}{The issue at hand is that, for any \\(r + I \\in R/I\\), there are many ways we can represent the element. For instance, for any \\(r' \\in R\\) such that \\(r = r' + i\\) for some \\(i \\in I\\), we have that \\(r + I = r' + I\\). That is, the way we decide to represent our elements is not unique. Thus we just need to check that the way we defined multiplication doesn't depend on the chosen representative of an element \\(r + I \\in R/I\\).}</p> <p>To do this suppose that \\(r + I = r' + I\\) and \\(s + I = s' + I\\) are elements of \\(R/I\\). Then \\(r = r' + i\\) and \\(s = s' + j\\) for some \\(i, j \\in I\\).  Therefore, \\((r' + I)(s' + I) = r's' + I\\). On the other hand</p> \\[\\begin{align*} (r + I)\\cdot(s + I) &amp;= rs + I\\\\ &amp;= (r' + i)(s' + j) + I\\\\ &amp;= r's' + \\underbrace{r'j + is' + ij}_{\\text{all are in } I} + I\\\\ &amp;= r's' + I. \\end{align*}\\] <p>where in the last step we used the fact that since \\(I\\) is an ideal, \\(r'j \\in I\\) and \\(is' \\in I.\\) Obviously \\(ij \\in I\\) as well. Therefore \\((r + I)(s + I) = (r' + I)(s' + I)\\), so our definition for multiplication is clear and well-defined.            \\</p> <p>\\textcolor{Plum}{ You may be wondering the following: In a quotient ring \\(R/I\\), why does \\(I\\) have to be an ideal of \\(R\\)? To answer this,    note in the second to last step above, we used the fact that \\(I\\) was  an ideal of \\(R\\) to conclude that \\(r'j, is' \\in I\\). If \\(I\\) hadn't been an ideal, we wouldn't have been able to absorb these elements into \\(I\\). Hence, we wouldn't have been able to make sure that our desired multiplication is well-defined. So this is why a quotient ring must always quotient out an ideal, and why we can't just quotient out any subring of \\(R\\). }</p> <p> Consider the following map \\(\\pi: R \\to R/I\\), known as the projection map, defined as  \\[ \\pi(r) = r + I. \\] <p>Note that this is a stupidly simple map. It's so stupid it almost doesn't even deserve a name. But it will be convenient to be able to refer back to the concept of associating an element \\(r \\in R\\) with a coset \\(r + I \\in R/I\\) as a projection. It's so convenient that if you go on in algebra you won't stop this \"coset\" mapping, yet everytime you see it you'll probably think it's dumb.</p> <p>Also notice that in this case \\(\\ker(\\pi) = I\\), and that \\(\\im(\\pi) = R/I\\).  </p>"},{"location":"algebra/Rings/Isomorphism%20Theorems./","title":"2.4. Isomorphism Theorems.","text":"<p>With the concept of a quotient ring defined, we can formulate analagous Isomorphism Theorems as we had in group theory. As we move forward, recall that the main ingredients of the isomorphism theorems in group theory were \\textbf{normal subgroups} and quotient groups. For our ring isomorphism theorems, the \"normal groups\" will be ideals while the \"quotient groups\" will be the quotient rings. </p> <p>The reasons for having such analogous theorems available to us for ring theory is that \\textcolor{Red}{groups are a special case of rings. The only thing that makes a group different from a ring is that we've just added a few extra axioms.} But it turns out that, even after adding these extra axioms, the Isomorphism Theorems still hold. </p> <p>If you go on in algebra you'll see the Isomorphism Theorems again, proved for algebraic objects called modules. In fact, the Isomorphism Theorems were first proved by Emmy Noether in terms of modules; not groups, or rings, but the theorems hold for groups and rings since groups and rings are special cases of modules.</p> <p>[(First Isomorphism Theorem.)]If \\(R\\) and \\(S\\) are rings, and \\(\\phi: R \\to S\\) is a homomorphism, then  \\[ R/\\ker(f) \\cong \\im(f). \\] <p></p> <p> The proof of this is analogous to the proof in group theory. We construct a homomorphism \\(\\phi:G/\\ker(f) \\to \\im(f)\\) by defining  \\[ \\phi(r + \\ker(f)) = f(r). \\] <p>Observe that for any nonzero \\(s \\in \\im(f)\\), there exists a \\(r \\in R\\) such that \\(f(r) = k\\). Since \\(s\\) is nonzero, \\(r \\not\\in \\ker(f)\\). However, observe that \\(f(r + \\ker(f)) = k\\). Therefore \\(\\phi\\) is surjective. </p> <p>Now observe that \\(\\phi\\) is one to one. Suppose that </p> \\[  \\phi(r + \\ker(f)) = \\phi(r' + \\ker(f)) \\] <p>for some elements \\(r + \\ker(f), r' + \\ker(f) \\in R/\\ker(f)\\). Then \\(f(r) = f(r')\\). But this implies that  \\(f(r) - f(r') = 0\\) or that \\(f(r - r') = 0 \\implies r-r' \\in \\ker(f)\\). Therefore \\(r - r' = s\\) for some \\(s \\in \\ker(f)\\) so that </p> \\[ r + \\ker(f) = r' + s + \\ker(f) = r' + \\ker(f). \\] <p>Thus we have that \\(r + \\ker(f) = r' + \\ker(f)\\), proving that \\(\\phi\\) is injective. Altogether we have constructed an isomorphism from \\(R./\\ker(f)\\) to \\(\\im(f)\\), which proves the theorem. </p> <p>As an application of this, we can revisit one of the examples we computed. Earlier we found that for a homomorphism \\(\\phi: \\RR[x] \\to \\mathbb{C}\\) defined as </p> \\[ \\phi(p(x)) = p(i) \\] <p>that \\(\\im(f) = \\mathbb{C}\\) and \\(\\ker(f) = \\{p(x) \\in \\RR[x] \\mid (x^2 + 1)\\Big|p(x) \\}\\). Now that we can equivalently dtescribe the kernal as \\(K = \\{p(x) \\in \\RR[x] \\mid p(x) = q(x)(x^2 + 1) \\text{ for some } q(x) \\in \\RR[x]\\}\\). Therefore, by the First Isomorphism Theorem,</p> \\[ \\RR[x]/K \\cong \\mathbb{C}. \\] <p>That is, the set of complex numbers is isomorphic to \\(\\RR[x]/K\\). Well, what is this set? This set is all the elements of the form </p> \\[ q(x) + K  \\] <p>where \\(q(x) \\in \\RR[x]\\) is an element which does not have \\(x^2 + 1\\) as a factor. Thus, the complex numbers are isomorphic to the equivalence class of polynomials which are not divisble by \\(x^2 + 1\\).</p> <p>[(Second Isomorphism Theorem.)] Let \\(R\\) be a ring, \\(I\\) an ideal of \\(R\\), and \\(S\\) a subring of \\(R\\). Then  <ul> <li> <p>[1.] \\(S + I\\) is a subring of \\(R\\) </p> </li> <li> <p>[2.] \\(I\\) is an ideal of \\(S + I\\)</p> </li> <li> <p>[3.] \\(S \\cap I\\) is an ideal of \\(S\\)</p> </li> <li> <p>[4.] \\((S + I)/I \\cong S/(S \\cap I)\\).</p> </li> </ul> <p></p> <p>\\begin{minipage}{0.25 \\textwidth} \\begin{figure}[H] \\begin{tikzcd}[column sep=small]  &amp; S + I \\ S  \\arrow[ur, dash] &amp;&amp; I \\arrow[ul,swap,\"\\text{(ideal)}\"] \\ &amp; S\\cap I  \\arrow{ul}{\\text{(ideal)}} \\arrow[ur, dash] \\end{tikzcd} \\end{figure} \\end{minipage} \\hfill \\begin{minipage}{0.7\\textwidth} The diagram on the left is analogous to the one used in the second isomorphism theorem for groups. Hence, this is again known as the diamond theorem. </p> <p>Although it is important to have this diagram in mind, it is also important to remmeber that \\((S + I)/I \\cong S/(S\\cap I)\\) (given the appropriate hypotheses). \\end{minipage}  </p> <p> <ul> <li>[1.] To prove the first statement we first make the following connection. From the Second Isomorphism Theorem for groups, we know that \\(S + I\\) is an abelian group. We just need to show it is closed under multiplication. Thus let \\((s + i), (s' + i') \\in S + I\\). Then </li> </ul> \\[ (s + i)(s' + i') = \\underbrace{ss'}_{\\text{in }S} + \\overbrace{si' + is' + ii'}^{\\text{in } I}. \\] <p>Therefore, we see that \\((s + i)(s' + i') \\in I\\), so that \\(S + I\\) is closed under multiplication. Therefore it is a subring of \\(R\\). </p> <ul> <li>[2.] Let \\(s + i \\in S + I\\), and let \\(j \\in I\\). Then observe that </li> </ul> \\[ (s + i)j = sj + ij \\hspace{0.2cm}\\text{ and }\\hspace{0.2cm} j(s + i) = js + ji. \\] <p>However, since \\(I\\) is an ideal, \\(sj, js \\in I\\), and clearly \\(ij, ji \\in I\\). Therefore, \\((s + i)I \\subset I\\) and \\(I(s + j) \\subset I\\) for any \\((s + j) \\in S + I\\), which shows that \\(I\\) is an ideal of this set.</p> <ul> <li>[3.] From our study of groups, we know that \\(S \\cap I\\) is an abelain group. We just need to check that it is closed under multiplication. Thus for any \\(i \\in S \\cap I\\) and \\(s \\in S\\), we see that \\(is \\in I\\) since \\(I\\) is an ideal. </li> </ul> <p>But \\(i \\in S \\cap I \\implies i \\in S\\). Therfore \\(is\\) is also a product of two elements in \\(S\\).</p> <p>Since \\(is \\in I\\) and \\(is \\in S\\), we see that \\(is \\in S \\cap I\\), proving that it is an ideal of \\(S\\).</p> <ul> <li>[4.] Consider the projection map \\(\\pi: R \\to R/I\\) restircted to \\(S\\), which we'll define as \\(\\pi|_S : S \\to R/I\\). (What we mean by \"restricted\" is that, we let \\(\\pi\\) do its job, but we only let it act on elements in \\(S \\subset R\\).)</li> </ul> <p>Note that \\(\\ker(\\pi|_S) = S \\cap I\\), while \\(\\im(\\pi|_S) = (S + I)/I\\) (namely, all the elements of the form \\(s + I\\) where \\(s \\not\\in I\\).) Thus by the First Isomorphism Theorem, we have that </p> \\[ S/\\ker(\\pi|_S) \\cong \\im(\\pi|_S)   \\implies S/(S \\cap I) \\cong (S + I)/I  \\] <p>as desired.</p> <p></p> <p>[(Third Isomorphism Theorem)] Let \\(R\\) be a ring and \\(I\\) and \\(J\\) ideals of \\(R\\) such that \\(I \\subset J\\). Then  <ul> <li> <p>[1.] \\(J/I\\) is an ideal of \\(R/J\\) </p> </li> <li> <p>[2.] \\(R/J \\cong (R/I)/(J/I)\\). </p> </li> </ul> <p></p> <p> For this theorem, we offer a two-in-one proof.  Construct the ring homomorphism \\(\\phi:R/I \\to R/J\\) as follows:  \\[ f(r + I) = r + J. \\] <p>We first demonstrate that this is well-defined. Suppose \\(r + I = r' + I\\); that is, there exists a \\(i \\in I\\) such that \\(r - r' = i\\). Then observe that </p> \\[ f(r + I) = r + J = r' + i + I = r' + I = f(r' + I). \\] <p>Thus this homomorphism is well defined. Now observe that </p> \\[\\begin{align*} \\ker(f) &amp;= \\{r + I \\in R/I \\mid r + J = J\\}\\\\  &amp;= \\{r + I \\in R/J \\mid r \\in J\\} = J/I. \\end{align*}\\] <p>Now the first result comes by recalling that the kernal is an ideal of the domain ring; that is, \\(J/I\\) is an ideal of \\(R/J\\). The second result comes from realizing that \\(\\im(f) = R/J\\), and by applying the First Isomorphism Theorem to that </p> \\[ (R/I)/(J/I) = R/J. \\] <p></p> <p>[(Fourth Isomorphism Theorem.)] Let \\(R\\) be a ring, \\(S\\) a subring of \\(R\\) and \\(I\\) an ideal of \\(R\\). Then every subring of \\(R/I\\) is of the form \\(S/I\\) where \\(I \\subset S \\subset R\\). Moreover, ideals \\(J\\) of \\(R\\) containing \\(I\\) correspond to ideals of \\(R/I\\). </p>"},{"location":"algebra/Rings/PIDs%20and%20Euclidean%20Domains./","title":"2.7. PIDs and Euclidean Domains.","text":"<p>Rings were invented in order to generalize mathematical domains which are parralel to the properties of integers and polynomials, since there exist many mathematical structures which share such properties. Two major properites of interest include the \\textbf{fundamental theorem of arithmetic} and factorization via Euclid's algorithm. These concepts generalize to rings, specifically to Principal Ideal Domains, which we will demonstrate in this section. </p> <p>First we begin with defintions. </p> <p> Let \\(R\\) be a commutative ring, and suppose \\(a, b \\in R\\) are nonzero. Then  <ul> <li> <p>[1.] We say \\(a\\) divides \\(b\\) if \\(b = ac\\) for some \\(c \\in R\\). This is denoted as \\(a\\mid b\\).</p> </li> <li> <p>[2.] Let \\(a\\) not be a unit. Then \\(a\\) is irreducible if \\(a = bc\\) implies \\(b\\) or \\(c\\) is a unit. </p> </li> <li> <p>[3.] If \\(a\\) is not a unit, then \\(a\\) is prime if \\(a | bc\\) implies \\(a|b\\) or \\(a | c\\). </p> </li> </ul> <p></p> <p>As a consquence of these definitions, we have the following proposition. </p> <p> Let \\(R\\) be an integral domain and suppose \\(a, b \\in R\\) are nonzero. Then  <ul> <li> <p>[1.] If \\(\\left&lt; a \\right&gt;\\) is the principal ideal generated by \\(a\\), then \\(a \\mid b\\) if and only if \\(\\left&lt; b \\right&gt; \\subset \\left&lt; a \\right&gt;\\). </p> </li> <li> <p>[2.] The element \\(a\\) is a prime element of \\(R\\) if and only if \\(\\left&lt; a \\right&gt;\\) is a prime ideal.</p> </li> <li> <p>[3.] If \\(a \\mid b\\) then \\(au \\mid bv\\) for any units \\(u,v \\in R\\) </p> </li> <li> <p>[4.] If \\(a \\mid b\\) and \\(a\\) is not a unit then \\(b\\) is not a unit. </p> </li> <li> <p>[5.] If \\(p\\) is prime and \\(p \\mid a_1a_2\\cdots a_n\\) then \\(p \\mid a_i\\) for some \\(i \\in \\{1, 2, \\dots, n\\}\\).</p> </li> </ul> <p></p> <p> <ul> <li>[1.] (\\(\\implies\\)) Suppose \\(a \\mid b\\). Then \\(b = ac\\) for some \\(c \\in R\\). Now observe that </li> </ul> \\[ \\left&lt; b \\right&gt; = \\{rb \\mid r \\in R\\} = \\{r(ac) \\mid r \\in R\\} = \\left&lt;a\\right&gt;c. \\] <p>Since \\(\\left&lt; a \\right&gt;c \\subset \\left&lt; a \\right&gt;\\), we have that \\(\\left&lt; b \\right&gt; \\subset \\left&lt; a \\right&gt;\\), as desired.</p> <p>(\\(\\impliedby\\)) Now suppose that \\(\\left&lt; b \\right&gt; \\subset \\left&lt; a \\right&gt;\\). Then \\(b \\in \\left&lt; a \\right&gt;\\), so that \\(b = ac\\) for some \\(c \\in R\\). Hence, \\(a \\mid b\\), which proves the result. </p> <ul> <li> <p>[2.] This is just the definition of an element being prime in \\(R\\). </p> </li> <li> <p>[3.] Suppose \\(a \\mid b\\) and let \\(u, v\\) be units. Since \\(b = ac\\) for some \\(c\\in R\\), we see that \\(bu = acv = avc\\). Therefore \\(bu \\mid av\\). (Since \\(u, v\\) were units, they are not zero divisors, so they did not change the value of the equation.)</p> </li> <li> <p>[4.] Suppose \\(a\\) is not a unit and \\(a \\mid b\\) for some \\(b \\in R\\). Suppose for the sake of contradiction that \\(b\\) is a unit. Then \\(b = ac\\) for some \\(c \\in R\\), and furthermore there exists a \\(d \\in R\\) such that \\(br = 1\\). </p> </li> </ul> <p>Therefore, \\(bd = acd \\implies 1 = acd\\). Hence \\(a\\) is a unit since \\(a(bd) = (bd)a= 1\\). But this is a contradiction since we said \\(a\\) was not a unit, which completes the proof. </p> <ul> <li>[5.] Let \\(p\\) be a prime element and suppose \\(p \\mid a_1a_2 \\cdots a_n\\) for elements \\(a_1, a_2, \\dots, a_n \\in R\\).  Then \\(a_1a_2\\cdots a_n = pb\\) for some \\(b \\in R\\). Hence we see that \\(a_1a_2\\cdots a_n \\in Rp\\). Since \\(p\\) is prime, \\(Rp\\) is a prime ideal and hence one element \\(a_i\\) where \\(i \\in \\{1, 2, \\dots, a_n\\}\\) must be in \\(P\\). In other words, \\(p \\mid a_i\\) for some \\(i \\in \\{1, 2, \\dots, n\\}\\), as desired. </li> </ul> <p></p> <p>The above propsition generalizes rules that we hold to be familiar in \\(\\ZZ\\). For example, we know the units of \\(\\ZZ\\) are \\(\\{1, -1\\}\\), and it is obvious to us that if \\(a \\mid b\\) for some integers \\(a, b\\) then \\(-1\\cdot a \\mid -1\\cdot b\\) and \\(1 \\cdot a \\mid 1 \\cdot b\\). We also know that if \\(p\\) is prime integer and \\(p \\mid a_1a_2\\cdots a_n\\) for some integers \\(a_1, a_2 \\dots, a_n\\) then \\(p \\mid a_i\\) for some \\(i \\in \\{1, 2, \\dots, n\\}\\). The proposition just tells us that our intuition on \\(\\ZZ\\) does in fact generalize to integral domains, and what we've seen in \\(\\ZZ\\) is just a tiny snap shot of algebra at work.</p> <p> Let \\(R\\) be an integral domain. If \\(p \\in R\\) is prime then \\(p\\) is also irreducible. </p> <p> Let \\(p \\in R\\) be prime and suppose \\(p = qm\\) for some \\(q, m \\in R\\). Then by definition we know that \\(p \\mid q\\) or \\(p \\mid m\\). Without loss of generality suppose \\(p \\mid q\\). Then \\(q = pc\\) for some \\(c \\in R\\). Then we have that  \\[ p = qm \\implies p = pcm \\implies 1 = cm \\] <p>where we used the cancellation law, valid on integral domains. Then \\(m\\) is a unit, and hence by definition this implies that \\(p\\) is irreducible.  </p> <p>\\textcolor{Red}{Keep in mind that the converse of the above statement is not true. It will, however, turn out to be true for PIDs.}</p> <p>We now see that greatest common divisors can be generalized to integral domains. </p> <p> Let \\(R\\) be an integral domain and let \\(A \\subset R\\) be nontrivial. Then we define  </p> <p>Say \\((R, +, \\cdot)\\) is an integral domain. That is, a commutative rwith \\(1 \\ne 0\\) having no zero divisors. We say it is a Euclidean Domain if </p> <ul> <li> <p>[1.] We have a map \\(N: R \\to mathbb{Z}_{\\ge 0}\\) with \\(N(0)= 0\\).</p> </li> <li> <p>[2.] Given \\(a, b \\in R\\), we can find \\(q, r \\in R\\) with \\(a = bq + r\\) and either  \\(r = 0\\)  or \\(N(r) &lt; N(b)\\). </p> </li> </ul> <p> Every ideal in a Euclidean Domain is principal. That is,  \\[ \\text{(Euclidean Domain)} \\subseteq \\text{(Principal Ideal Domains)} \\subseteq \\text{(Integral Domain)}    \\] <p> Question: Are there PIDs that are not Euclidean Domains?</p> <p>\\subsection*{Unique Factorization Domain}</p> <p> Say \\((R, +, \\cdot)\\) is an integral domain. Pick \\(r \\in R\\) that is neither zero nor a unit, i.e., \\(r \\not\\in R^{\\times}\\cup\\{0\\}\\).  <p>We say that \\(R\\) is irreducible if when \\(r=ab\\) either \\(a \\in R^{\\times}\\) or \\(b \\in R^{\\times}\\). Otherwise, we say that r is reducible. </p> <p>Example. Say \\(R = \\ZZ\\). Then </p> \\[\\begin{align*} r &amp;= 5 \\text{ is irreducible.}\\\\ r &amp;= 6 = 2 \\cdot 3 \\text{ is irreducible.}\\\\ \\end{align*}\\] <p> We say \\(R\\) is a Unique Factorization Domain (UFD) if the following holds for all \\(r \\in  R^{\\times} \\cup \\{0\\}\\): <ul> <li> <p>[1.] \\(r = p_1p_2\\cdots p_m\\) is  a finite product of irreducibles \\(p_i \\in R\\).</p> </li> <li> <p>[2.] If \\(r = q_1q_2\\cdots q_m\\) is another factorization into irreducibles, then \\(n = m\\) and \\(q_i=q_i \\cdot p_i\\) for some \\(u_i \\in R^{\\times}\\). </p> </li> </ul> <p> Again, let \\(R = \\ZZ\\) and observe that  \\(r = 6 = 2 \\cdot 3 = (-2)\\cdot(-3)\\). Really, we see that \\(2 = (-1)\\cdot 2\\) and \\(3 = (-1)\\cdot 3\\). </p> <p> Say \\(R\\) is a UFD. Given \\(r \\not\\in R^{\\times} \\cup\\{0\\}\\), the following are equivalent: <ul> <li> <p>[1.] If \\(I = (r)\\) is a nonzero prime ideal of \\(R\\).</p> </li> <li> <p>[2.] \\(r\\) is irreducible.  </p> </li> </ul> <p> Remark: We avoid saying \"\\(r\\) is prime\", we say \"\\(I = (r)\\) is prime\".</p> <p> \\begin{description} \\item[\\(\\bm{i \\implies ii}\\)] Assume \\(I = (r)\\) is a prime Write \\(r = a \\cdot b\\). We want to show either \\(a \\in R^{\\times}\\) and \\(b \\in R^{\\times}\\).  <p>We have \\(a \\cdot b = r \\in I\\), so by definition either \\(a \\in I\\) or \\(b \\in I\\). Either  \\(a = v \\cdot r\\) where \\(b = u \\cdot r\\) for some \\(u, v \\in R\\). </p> <p>Hence either \\(r = a \\cdot b = (v \\cdot r)\\cdot b\\) or  \\(r = (v \\cdot a) \\cdot r\\). Since \\(R\\) is an integral domain, the cancellation law holds. Hence either</p> \\[\\begin{align*} r = 0 \\quad \\text{or} \\quad v \\cdot b = 1 \\quad \\text{if} \\quad a \\in I\\\\  r = 0 \\quad \\text{or} \\quad v \\cdot a = 1 \\quad \\text{if} \\quad b \\in I   \\end{align*}\\] <p>Since \\(r \\not\\in R^{\\times}\\cup\\{0\\}\\), either \\(v \\cdot b = 1\\) or \\(u \\cdot a = 1\\). Hence either \\(b \\in R^{\\times}\\) or \\(a \\in R^{\\times}\\) is a unit. </p> <p>\\item[\\(\\bm{ii  \\implies i}\\).] Assume \\(r \\in R\\). We'll show that \\(I\\) is a proper ideal. Since \\(r \\not\\in R^{\\times}\\), we know that \\(I  \\ne R\\). </p> <p>Now we show that \\(I\\) is prime. Say \\(a, b \\in R\\) satisfying \\(a \\cdot b \\in I\\). We must show that either \\(a \\in I\\) or \\(b \\in I\\). Since \\(a \\cdot b \\in I\\), we can write  \\(a \\cdot b = r \\cdot c\\) for some \\(c \\in  R\\).  \\ \\ Case #1. Either \\(a,  b = 0\\). Then either \\(a, b \\in I\\). \\ \\ Case #2.  Either  \\(a, b \\in R^{\\times}\\). Without loss of generality, say  \\(a \\cdot u = 1\\). Then \\(b = u \\cdot r \\cdot  c \\in I\\).  \\ \\ Case #3. \\(a, b \\not\\in R^{\\times}\\cup\\{0\\}\\). Since \\(R\\) is a UFD, factor \\(a\\) and \\(b\\). </p> \\[\\begin{align*} a = p_1p_2\\cdots p_n\\\\ b = q_1q_2\\cdots q_m. \\end{align*}\\] <p>Hence \\(a \\cdot b = p_1\\cdots p_nq_1 \\cdots q_m\\). But \\(r\\) is an irreducible that divides \\(a \\cdot b = c \\cdot r\\). Thus either \\(q_i = u \\cdot r\\) or \\(p_j = v \\cdot r\\) for some \\(i, j\\). But then either </p> \\[\\begin{align*} a = (v\\cdot r) p_2 \\cdots p_n \\in I = (r)\\\\ b = (v \\cdot r) q_2 \\cdots q_m \\in I = (r) \\end{align*}\\] <p>if for example \\(i, j= 1\\). </p> <p>\\end{description} </p> <p> Say \\((R, +, \\cdot)\\) is an integral domain. We say \\(R\\) is a principal ideal domain (PID) if every ideal \\(I = (r)\\) is principal. </p> <p> <ul> <li> <p>[1.] If \\(R\\) is a Euclidean Domain, then \\(R\\) is a PID.</p> </li> <li> <p>[2.] If \\(R\\) is a PID, then \\(R\\) is a UFD. </p> </li> </ul> <p></p> <p>Here's a diagram of what we have so far. </p> \\[ \\text{(Euclidean Domains)} \\subseteq \\text{(Principal Ideal Domains)} \\subseteq \\text{(UFDs)} \\subseteq \\text{(Integral Domains)} \\] <p> We showed (1) before. Now we show (2). Assume \\(R\\) is a PID. We'll show that every \\(r \\in R\\) that is nonzero has unique factorization.  \\[ r = u \\cdot p_1p_2 \\cdots p_n \\] <p>where \\(u \\in R^{\\times}\\) and \\(p_i\\) are irreducibles. We will in two parts: (i) existence (of at least one factorzation) and (ii) uniqueness (of at most one factorization).</p> <p>\\begin{description} \\item[Existence.] Consider</p> \\[ S = \\{I = (r) \\mid r \\in R-\\{0\\} \\text{ does not have a factorization}\\}.    \\] <p>We want to show that \\(S = \\varnothing\\). Thus assume otherwise. Pick some \\(I_1 = (r_1)\\) in \\(S\\).  Then \\(r_1 \\not\\in R^{\\times}\\cup\\{0\\}\\) and \\(r_1\\) is not irreducible. This means that \\(r_1\\) is reducible, so write </p> \\[ r_1 = r_2 \\cdot r_2' \\quad r_2,r_2' \\in R^{\\times}\\cup\\{0\\}. \\] <p>Consider \\(I_2 = (r_2)\\) and \\(I_2' = (r_2')\\). If both \\(r_2, r_2'\\) have factorizations into irreducibles, then so would \\(r_1 = r_2\\cdot r_2'\\). Thus without loss of generality, \\(r_2\\) does not have a factorization into irreducibles. Thus the ideal \\(I_2 \\in S\\). Observe \\(I_1 \\subsetneqq I_2\\). We then have a chain of proper ideals:</p> \\[ I_1 \\subsetneqq I_2 \\subsetneqq \\cdots \\subsetneqq I_n \\subsetneqq \\cdots \\subsetneqq R. \\] <p>Let \\(\\displaystyle I = \\bigcup_{n \\ge 1} I_n = (r_0)\\) as the maximal ideal. ask: is \\(I \\in S\\)? If  \\(I \\in S\\), then we see that \\(I\\) is not maximal. If \\(I \\not\\in S\\), then \\(r_0\\) can be written as a product of irreducibles. </p> <p>\\item[Uniqueness.] We show uniqueness. Consider a proof by induction via the following statement:</p> \\[\\begin{align*} P(n) = \\text{\"If } r \\in R - \\{0\\} \\text{ has some factorization }\\\\  r=u\\cdot p_1\\cdots p_n \\text{ into }  n \\text{ irreducibles, then such a factorization is unique.\"} \\end{align*}\\] <p>We will show \\(P(n)\\) is true for \\(n=0,1,2\\). </p> <ul> <li>[Base Case.] If \\(r \\ne 0\\) has a factorization into \\(n = 0\\) irreducibles, then \\(r = u\\) is a unit. Say it has a second factorization </li> </ul> \\[ r = u \\cdot q_1 \\cdots q_n \\text{  for } m&gt;0. \\] <p>Then </p> \\[ 1 = (u^{-1}\\cdot v \\cdot q_1 \\cdots q_m)\\cdot q_m \\] <p>So \\(q_m \\in R^{\\times}\\) is a unit. This is a contradiction</p> <ul> <li>[Induction.]  Suppose that \\(P(n_0)\\) is true for some \\(n_0 \\ge 0\\). Then we'll show that \\(P(n_0 + 1)\\) is true. Consider \\(r \\ne 0\\) that is the product of \\(n\\) irreducibles: </li> </ul> \\[\\begin{align*} r &amp; = u \\cdot p_1 \\cdots p_n\\\\ &amp; = (u \\cdot p_1 \\cdots p_{n_0}) \\cdot p_n \\end{align*}\\] <p>Say we have a second factorization:</p> \\[ r = v \\cdot q_1 \\cdots q_m. \\] <p>We then have the following facts:\\ \\(P = (p_n)\\) is a prime ideal of \\(R\\). (Recall \\(I = (r)\\) is prime if and only if \\(r\\) is irreducible.)\\ \\(\\overline{Q_i} = q_i \\cdot P\\) is a coset in \\(\\overline{R} = R/P\\).  Not all \\(\\overline{q_i} \\ne \\overline{0}\\). Why? Because </p> \\[ \\overline{u} \\cdot \\overline{q_1} \\cdots \\overline{q_m}\\overline{p_m} = \\overline{u\\cdot  q_1 \\cdots q_m} \\cdot \\overline{p_m} = 0. \\] <p>Since \\(\\overline{R}\\) is an integral domain, without of generality,  suppose \\(\\overline{q_m} = \\overline{0}\\). Hence \\(q_m = q_m \\cdot p_n\\). Now consider </p> \\[ (u \\cdot p_1 \\cdots p_{n_0}) \\cdot p_n =  r =  (u_m\\cdot v \\cdot  q_1 \\cdots q_{m-1})\\cdot p_n. \\] <p>Since we're in an integral domain, we can use the cancellation law. Hence we have that </p> \\[ u\\cdot p_1 \\cdots p_{n_0} = (u_m \\cdot v)\\cdot q_1 \\cdots q_{m-1}.   \\] <p>Now we can invoke the inductive hypothesis. We see that \\(m - 1 = n_0\\)  and \\(q_i = u_i \\cdot p_i\\) for some unit \\(u_i \\in R^{\\times}\\). However, this is equivalent to saying that \\(m = n\\). and as we showed \\(q_m = u_m \\cdot p_n\\), we have that the factorizations are the same. Hence \\(P(n)\\) is true for all positive integers. </p> <p>\\end{description} </p> <p>Primes, irreducibles and Maximal Ideals.\\</p> <p> Say \\((R, +, \\cdot)\\) is a PID. Then the following are equivalent for  \\(r \\in R\\) with \\(r \\not\\in R^{\\times}\\cup\\{0\\}\\): <ul> <li> <p>[i.] \\(r\\) is irreducible.</p> </li> <li> <p>[ii] \\(I = (r)\\) is a prime ideal.</p> </li> <li> <p>[iii.] \\(I=(r)\\) is a maximal ideal.   </p> </li> </ul> <p></p> <p> First observe that \\(i \\iff ii\\) since they are true in a UFD, and we know that a PID is a UFD. Hence we just need to show that \\(ii \\iff iii\\).  <p>We show that \\((ii \\iff iii)\\). By way of contradiction, sy \\(I = (r)\\) is a prime that is not maximal. Then we can find an ideal \\(M = (a)\\) such that \\(I \\subsetneqq M \\subsetneqq R\\). This means that \\(a | r\\). That is, one can find \\(a, b \\in R\\) such that \\(r = a\\cdot b\\). </p> <p>Since \\(I\\) is prime and \\(a \\cdot b \\in  I\\), either \\(a \\in I\\) or \\(b \\in I\\). If \\(a \\in I\\), then \\(M = (a) \\subset I\\), which would imply that \\(M = I\\). Thus we have a contradiction. </p> <p>Thus we need that \\(b \\in I\\). Since \\(b \\in I = (r)\\), write \\(b = u \\cdot r\\). Then \\(r = a \\cdot b = (u \\cdot a) \\cdot r\\). Since \\(r \\ne 0\\), we find \\(u \\cdot a = 1\\). This means that \\(a \\in R^\\times\\) so \\(M = (a) = R\\). But \\(M \\subsetneqq R\\), so again we find a contradiction.</p> <p>Thus we see that \\(I\\) must be a maximal ideal. The other direction, that every maximal ideal is a prime ideal, is trivial. </p> <p>\\section{Dedekind-Hasse Norms.} Motivating questions.*</p> <ul> <li> <p>[1.] Are there examples of PIDs that are not Euclidean Domains?</p> </li> <li> <p>[2.] Are there examples of PIDs that are not \\(\\ZZ\\)?  </p> </li> </ul> <p>Say \\((R, +, \\cdot)\\) is an integral domain. A norm is a map \\(\\mathbb{N} : R \\to \\mathbb{Z}_{\\ge 0}\\). such that \\(\\mathbb{N}(0) = 0\\).</p> <p>On the other hand, a Dedekind-Hasse Norm is a norm \\(\\mathbb{N}\\) satisfying  \\begin{description} \\item[DH1.] \\({N}(0) = 0\\) \\item[DH2.] \\(N(a) &gt; 0\\) for all \\(a &gt; 0\\) \\item[DH3.] For all nonzero \\(a, b \\in R\\), either </p> <ul> <li> <p>[1.] \\(b\\) divides \\(a\\), i.e., \\(a = q \\cdot b\\) or </p> </li> <li> <p>[2.] there exits \\(s, t \\in R\\) such that </p> </li> </ul> \\[ x = s \\cdot a -  t \\cdot b \\in  (a , b)  \\] <p>satisfies \\(0 &lt; N(x) &lt; N(b)\\).</p> <p>\\end{description} Remark: If in (DH3) we can always choose \\(s = 1\\), then we say that \\(R\\) is a Euclidean Domain.</p> <p>Examples.\\ Let \\(R = \\mathbb{Z}\\). This a Euclidean Domain. The statement (DH3) is the Euclidean algorithm for \\(N(a) = |a|\\). \\ \\ Let \\(R\\) be any integral domain. Then \\(S = R[x]\\) is also an integral domain. We define a norm on \\(S\\) be saying \\(N(g(x)) = \\text{deg}(g)\\) if \\(g \\ne 0\\). If \\(g(x) \\equiv 0\\) is the zero polynomial, then \\(\\deg(g) = -\\infty\\). So define \\(N(0) = 0\\). This is not Dedekind-Hasse. As an example, consider \\(R = \\ZZ\\). Then let \\(I =(2, x)\\) in \\(S\\). This is not a principal ideal.</p> <p>Now consider \\(F\\) a field. Let \\(S' = F[x]\\) which is a PID. Define \\(N(g(x= 2^{\\deg(g)}\\). And \\(N(0) = 2^{-\\infty} = 0\\).</p>"},{"location":"algebra/Rings/Polynomial%20Rings%20%28for%20Galois%20Theory%29./","title":"2.8. Polynomial Rings (for Galois Theory).","text":"<p> Say \\((R, +, \\cdot)\\) is a ring with identity \\(1 \\ne 0\\). We will always assume \\(R\\) is of this form.  <ul> <li>[1.] If \\(x\\)  is indeterminate (i.e. a variable) a polynomial in \\(x\\) is a formal sum </li> </ul> \\[ a_dx^d + \\cdots + a_1x + a_0. \\] <p>with \\(a_k \\in R\\).</p> <ul> <li> <p>[2.] We say the degree \\(\\deg(p) = d\\) if \\(a_d\\). Otherwise, set  \\(\\deg(p) = -\\infty\\) if \\(a_d = \\cdots = a_0 = 0\\).</p> </li> <li> <p>[3.] Let \\(R[x]\\) be the collectionn of all such \\(p(x)\\). This is the polynomial ring over \\(R\\).</p> </li> <li> <p>[4.] More generally, say \\(\\{x_1, x_2, \\dots, x_n\\}\\) is a collection of \\(n\\) indeterminates. Inductively define \\(R[x_1, x_2, \\dots, x_n] = S[x_n]\\) where \\(S = R[x_1, x_2, \\dots, x_{n-1}]\\). A typical element is in the form </p> </li> </ul> \\[ p(x_1, \\cdots, x_n) = \\sum_{i_1 =  1}^{d_1}\\cdots\\sum_{i_n = 1}^{d_n}a(i_1, \\cdots, i_n)x_1^{i_1}\\cdots x_n^{i_n}. \\] <p>We then say that this \\(p(x_1, \\dots, x_n)\\) is a polynomial in \\(n\\) variables. </p> <ul> <li>[5.] Assuming that the highest coefficient \\    \\(a(d_1, \\dots, d_n) \\ne 0\\), we then say the degree is</li> </ul> \\[ \\deg(p) = \\max\\{i_1  + \\cdots + i_n\\} \\text{ (fix here)}.                 \\] <p>Otherwise, if all terms are zero, i.e., if all \\(a(i_1, \\dots, i_n) = 0\\), then we set \\(\\deg(p) = -\\infty\\).</p> <p></p> <p>Example. Consider the polynomial ring \\(R[x, y]\\) as the polynomial ring in \\(n = 2\\) variables. Let \\(p(x, y) = 1 + x^3 + y^3\\). Then \\(\\deg(p) = 3\\). </p> <p> \\((R[x_1, \\cdots, x_n], + , \\cdot)\\) is a ring with \\(1 \\ne 0\\). </p> <p> We'll showt this by induction: \\[\\begin{align*} P(n) = \"(R[x_1, \\dots, x_n]) \\text{ is a ring with } 1 \\ne 0\". \\end{align*}\\] <p>We have already shown the base case \\(P(1)\\) is true. Assume that \\(P(n)\\) is true for some \\(n_0\\). We show \\(P(n)\\) is true for \\(n = n_0 + 1\\).</p> <p>By definition, \\(R[x_1, \\dots, x_n] = S[x_n]\\) where \\(S = R[x_1, \\dots, x_{n_0}]\\). By our inductive hypothesis, we have that \\ \\((S, +, \\cdot )\\) is a ring with \\(1 \\ne 0\\). Hence by \\(P(1)\\) we have that \\((S[x_n], \\cdot, +)\\) is also a ring with \\(1 \\ne 0\\). Hence \\(P(n_0 +  1)\\) is true. </p> <p> Assume \\(R\\) is an integral domain.  <ul> <li>[1.] Suppose \\(p, q \\in R[x_1, \\cdots x_n]\\). Then </li> </ul> \\[ \\deg(p \\cdot q) = \\deg(p) + \\deg(q).    \\] <ul> <li> <p>[2.] \\(R[x_1, \\dots, x_n]\\) is also an integral domain. </p> </li> <li> <p>[3.] The units of \\(R[x_1, \\dots, x_n]\\) is \\(R^{\\times}\\).  </p> </li> </ul> <p></p> <p> <ul> <li>[1.] If either \\(p = 0\\) or \\(q = 0\\), then \\(p \\cdot q = 0\\). Then observe that </li> </ul> \\[ \\deg(p \\cdot q) = \\deg(p) + \\deg(q) \\] <p>doesn't make sense unless  we choose  to set \\(\\deg(0) = -\\infty\\). Hence we see that</p> \\[ \\deg(p \\cdot q) = -\\infty.    \\] <p>We could make it positive infinity, but we assume that it is negative for subtle reasons later on. </p> <p>Now assume that \\(p, q \\ne 0\\). We write our polynomials </p> \\[ p = \\sum_{i_1, \\dots, i_n}^{d_1, d_2, \\cdots, d_n}a(i_1, \\dots, i_n)x_1^{i_1}\\cdots x_n^{i_n} \\qquad  q = \\sum_{j_1, \\dots, j_n}^{e_1, e_2, \\cdots, e_n}a(j_1, \\dots, j_n)x_1^{j_1}\\cdots x_n^{j_n} \\] <p>where \\(\\deg(p) = d_1 + d_2 + \\cdots + d_n\\) and \\(\\deg(q) = e_1 + e_2 + \\cdots + e_n\\). Then we see that </p> \\[ p \\cdot q   =  \\sum_{k_1, \\dots, k_n}^{d_1 + e_1, d_2 + e_2, \\cdots, d_n + e_n}c(i_1, \\dots, i_n)x_1^{k_1}\\cdots x_n^{k_n}   \\] <p>where </p> \\[ c(k_1, \\dots, k_n)  =  \\sum_{i_1+j_1 = k_1, \\dots, i_n+j_n = k_n.} a(i_1,  \\dots i_n)b(j_1, \\dots, j_n). \\] <p>The leading term is  \\(c(d_1 + e_1, \\dots, d_n + e_n) = a(d_1, \\dots, d_n )b(e_1, \\dots, e_n)\\). Hence, the leading term is also nonzero, so the degree must be </p> \\[ d_1 + e_1 +  \\dots + d_n + e_n = \\deg(p) + \\deg(q).   \\] <p>There are actually many ways to define the degree of a polynomial in \\(R[x_1, \\dots, x_n]\\) when\\(n \\ge 2\\). </p> <ul> <li>[2., 3.] We can show both (2)  and (3) at the same time via induction. Let </li> </ul> \\[ P(n) = \"R[x_1, \\dots, x_n] \\text{ is an integral domain and } R[x_1, \\dots, x_n]^{\\times} = R^{\\times}.\" \\] <p>We've done this in the one-variable case, so that \\(P(1)\\) is true. We can invoke our inductive hypothesis to suppose that \\(P(n_0)\\) is true for some \\(n_0\\). We next show that \\(n = n_0 + 1\\) is true. </p> <p>By definition, our ring \\(R[x_1, \\dots, x_{n_0}] = S[x_n]\\) for </p> \\[ S = R[x_1, \\dots, x_{n_0}]. \\] <p>By our inductive hypothesis, we know that (1)  \\(S\\) is an integral domain. Since \\(P(1)\\) is true, we know that \\(S[x_{n_0}]\\) is an integral domain. By (2), we know that \\(S^\\times = R^\\times\\). By \\(P(1)\\), we see that \\(S[x_n] = R^\\times\\). This show that \\(P(n)\\) is true for all \\(n\\).</p> <p></p> <p> Say \\((R, +, \\cdots)\\) is a ring with \\(1 \\ne 0\\) and \\(I \\subsetneqq R\\) is a ideal. Denote \\(S = R[x_1, \\dots, x_n]\\). <ul> <li> <p>[1.] \\(J = I[x_1, \\dots, x_n]\\) is a proper ideal of \\(S\\)</p> </li> <li> <p>[2.] \\(S/J = (R/J)[x_1, \\dots, x_n]\\) </p> </li> <li> <p>[3.] Moreover, say \\(R\\) is commutative. If \\(I \\subset R\\) is a prime ideal of \\(R\\), then \\(J\\) is a prime ideal of \\(S\\). </p> </li> </ul> <p></p> <p> <ul> <li>[1., 2.] Denote \\(\\overline{R} = R/I\\) as a ring  with identity \\(1 \\ne 0\\), which holds since  we are working with a proper ideal. We know both \\(S\\) and \\(\\overline{R}[x_1, \\dots, x_n]\\) is also a ring with \\(1 \\ne 0\\). </li> </ul> <p>Consider the following \"reduction mod \\(I\\)\" map:</p> \\[ \\phi: R[x_1,\\dots, x_n] \\to \\overline{R}[x_1,\\dots, x_n] \\] <p>where if \\(\\displaystyle p = \\sum_{i_1, \\dots, i_n}^{d_1, d_2, \\cdots, d_n}a(i_1, \\dots, i_n)x_1^{i_1}\\cdots x_n^{i_n}\\) then </p> \\[ \\overline{p} = \\sum_{i_1, \\dots, i_n}^{d_1, d_2, \\cdots, d_n}\\overline{a(i_1, \\dots, i_n)}x_1^{i_1}\\cdots x_n^{i_n} \\] <p>where \\(\\overline{a} = a + I\\) in \\(R/I\\). \\ Claim: This is a ring homomorphism. In fact, this map is surjective.  \\ Neither of these are difficult to show. </p> <p>Observe that </p> \\[ \\ker(\\phi) = \\Big\\{p = \\sum_{i_1, \\dots, i_n}^{d_1, d_2, \\cdots, d_n}a(i_1, \\dots, i_n)x_1^{i_1}\\cdots x_n^{i_n} mid a(i_1, \\dots, i_n)\\in I \\Big\\} = I[x_1, \\dots, x_n]. \\] <p>The First Isomorphism Theorem for Rings states that </p> \\[\\begin{align*} S/J &amp;\\cong R[x_1, \\dots, x_n]/\\ker(\\phi)\\\\ &amp;\\cong \\im(\\phi)\\\\ &amp;= (R/I)[x_1, \\dots, x_n]. \\end{align*}\\] <p>At this point, we've shown (2). Now observe that \\(J \\subsetneqq R\\) since \\((R/I)[x_1, \\dots, x_n]\\) is a ring with identity \\(1 \\ne 0\\). This proves (1). To show (3), say \\(R\\) is a commutative ring and \\(I \\subset R\\) is a prime ideal. Since \\(I\\) is prime, we see that \\((R/I)\\) is an integral domain. Hence we  see that \\((R/I)[x_1, \\dots, x_n]\\) is an integral domain. Since \\(S/J \\cong (R/I)[x_1, \\dots, x_n]\\), and  we know that \\(R[x_1, \\dots, x_n]\\) is also a commutative ring, we see  that \\(J \\subset S\\) must be a prime ideal as well. </p> <p> </p>"},{"location":"algebra/Rings/Principal%2C%20Maximal%20and%20Prime%20Ideals./","title":"2.5. Principal, Maximal and Prime Ideals.","text":"<p>We'll now move more deeper into ring theory. The results prior were already things we've been familiar with, since they were true for groups. It is here that we'll move onto new, deeper concepts regarding the ideal of a ring. </p> <p>Let \\(X \\subset R\\). Then we can talk about the \\textbf{subring generated by} \\(X\\) as the smallest subring containing \\(X\\), or equivalently, the intersection of all the subrings containing \\(X\\). More explicitly, we can define it to be the set of all finite sums of elements of \\(X\\). </p> <p>Similarly, we can define the ideal generated by \\(X\\), which again is the smallest ideal containing \\(X\\) or equivthalently the intersection of all ideals contianing \\(X\\).  More explicitly, if \\(R\\) is a ring with identity, then the ideal generated by \\(X\\) is </p> \\[ \\left&lt; X \\right&gt; = \\Big\\{\\sum_{i=1}^{n}r_ix_is_i \\mid r_i, s_i \\in R, x_i \\in X \\text{ and } n \\in \\mathbb{N}\\Big\\} \\] <p>while if \\(R\\) is commutative (and again, has an identity) this becomes </p> \\[ \\left&lt; X \\right&gt;= \\Big\\{\\sum_{i=1}^{n}r_ix_i \\mid r_i \\in R, x_i \\in X \\text{ and } n \\in \\mathbb{N}\\Big\\}. \\] <p>Note: this is not valid for rings without identity. </p> <p>\\textcolor{MidnightBlue}{Why are these formulas correct? We'll show this for the more general case for when \\(R\\) may not be commutative. Specifically, we'll show that these formulas are not only ideals, but that they are the smallest ideals containing \\(X\\) as we have claimed.  \\ \\indent If \\(r \\in R\\) then </p> \\[ r\\left( \\sum_{i=1}^{n}r_ix_is_i \\right) = \\sum_{i=1}^{n}(rr_i)x_is_i \\in \\left&lt; X \\right&gt;  \\] <p>since \\(rr_i \\in R\\) for each \\(i \\in \\{1, 2, \\dots, n\\}\\). Similarly, </p> \\[ \\left( \\sum_{i=1}^{n}r_ix_is_i \\right)r = \\sum_{i=1}^{n}r_ix_i(s_ir) \\in \\left&lt; X \\right&gt;  \\] <p>since again, \\(s_ir \\in R\\) for each \\(i \\in \\{1, 2, \\dots, n\\}\\). Thus this is an ideal. Now let \\(X'\\) be an ideal which contains \\(X\\). Pick an arbitrary element \\(\\displaystyle \\sum_{i=1}^{n}r_ix_is_i \\in \\left&lt; X \\right&gt;\\). Observe that since \\(X'\\) is an ideal containing \\(X\\), we know that \\(r_ix_is_i \\in X'\\) for each \\(i \\in \\{1, 2, \\dots, n\\}\\), and hence the sum itself, \\(\\sum_{i=1}^{n}r_ix_is_i\\), must be in \\(X'\\).  \\ \\indent Thus for any ideal \\(X'\\) containing \\(X\\), we see that \\(\\left&lt; X \\right&gt; \\subset X'\\). Hence, \\(\\left&lt; X \\right&gt;\\) is the smallest ideal containing \\(X\\). The proof is similar, and easier, for the case of \\(\\left&lt; X \\right&gt;\\) when \\(R\\) is commutative. }</p> <p> Let \\(R\\) be commutative and \\(X = \\{a\\}\\), where \\(a \\in R\\). Then the ideal generated by \\(X\\) given by  \\[ \\left&lt; X \\right&gt; = \\{ra \\mid r \\in R\\} \\] <p>is said to be a principal ideal generated by \\(a\\). Note that since \\(R\\) is commutative, we could have also written \\(\\left&lt; X \\right&gt; = \\{ar \\mid r \\in R\\}\\).</p> <p>One may also view a principal ideal generated by \\(a\\) as the set \\(Ra\\) (or again, equivalently as \\(aR\\)). </p> <p>What's an example of this? Consider the ring \\(\\ZZ\\). Then the subring \\(2\\ZZ\\) is a principle ideal generated by the element 2. That is </p> \\[ 2\\ZZ = \\{2n \\mid n \\in \\ZZ\\} \\] <p>which is pretty basic fact that we already know. But note that every ideal \\(I\\) of \\(\\ZZ\\) is of this form. </p> <p>\\textcolor{NavyBlue}{To see this, consider an ideal \\(I\\) of \\(\\ZZ\\) and suppose \\(i\\) is the smallest positive element of \\(I\\). First observe that there is no \\(j \\in I\\) such that \\(i &lt; j &lt; 2i\\). Suppose there was. Then \\(j = i + k\\) for some \\(0 &lt; k &lt; i\\). Since \\(I\\) is closed, we know that </p> \\[ j - i = i + k - i = k    \\] <p>is a member of \\(I\\). But this contradicts our assumption that \\(i\\) was the smallest positive element of \\(I\\). Therefore, there is no \\(j \\in I\\) such that \\(i &lt; j &lt; 2i\\), so that \\(I\\) is of the form </p> \\[ I = \\{\\dots, -2i, -i, 0, i, 2i, \\dots\\}. \\] <p>Hence, \\(I\\) is generated by \\(i\\), a single element, so that \\(I\\) is principal. } Thus every ideal of \\(\\ZZ\\) is principal. Rings who exhibit this type of behavior get a special name.  </p> <p> Let \\(R\\) be an integral domain. Then \\(R\\) is a principal ideal domain (PID) if every ideal of \\(R\\) is principal.  As we just showed, \\(\\ZZ\\) is a principal ideal domain.</p> <p> Let \\(R\\) be a ring. Then an ideal \\(M \\ne R\\) is called maximal if, for any other ideal \\(I\\) such that \\(M \\subset I \\subset R\\) we have that \\(M = I\\) or \\(I = R\\).  </p> <p>An example of this is the ring \\(\\ZZ\\) with the ideal \\(p\\ZZ\\) where \\(p\\) is prime. To show this, suppose \\(I\\) is an ideal such</p> \\[ p\\ZZ \\subset I \\subset \\ZZ \\] <p>and further that there exists an \\(i \\in I\\) such that \\(i \\not\\in p\\ZZ\\). </p> <p>Since \\(i\\) is not divisible by \\(p\\), we know by Fermat's Little Theorem that </p> \\[  i^{p-1} = 1 \\mbox{ mod }p. \\] <p>Thus for some \\(n \\in \\ZZ\\), \\(pn - i^{p-1} = 1.\\) Since \\(p\\ZZ \\subset I\\) and \\(i \\in I \\implies i^{p-1} \\in I\\), we see that \\(pn - i^{p-1} = 1 \\in I\\). Since \\(1 \\in I\\), we can repeatedly add and subtract \\(1\\) to generated all of \\(\\ZZ\\), and since this must all be contained in \\(I\\), we have that \\(I = \\ZZ\\). Thus \\(p\\ZZ\\) is maximal.</p> <p> Let \\(R\\) be a ring and \\(I\\) a proper ideal (i.e., \\(I \\ne R\\).) Then there is a maximal ideal of \\(R\\) containing \\(I\\). Furthermore, if \\(R\\) is a ring with identity, then there are always maximal ideals. </p> <p>How do we know when a given ideal is maximal or not? That is, how do we know there aren't \"bigger\" proper ideals which contain the one we are interested in?</p> <p> Let \\(R\\) be a commutative ring with identity. Then an ideal \\(M\\) of \\(R\\) is maximal if and only if \\(R/M\\) is a field. In other words,  \\[ M \\text{ is maximal } \\iff R/M \\text{ is a field.} \\] <p>\\vspace{-0.8cm}</p> <p></p> <p> (\\(\\implies\\)) Suppose \\(M\\) is a maximal ideal. To show that \\(R/M\\) is a field, we first realize that it is commutative since \\(R\\) is commutative. Thus we just need to show it is a divison ring.  <p>Let \\(a + M \\in R/M\\) where \\(a \\not\\in M\\). Since \\(aR\\) and \\(M\\) are both ideals, we have that \\(aR + M\\) is an ideal. Note that \\(M \\subset aR + M\\), which implies that \\(aR +  M = R\\). Therefore, there exists an element \\(r \\in R\\) and \\(m \\in M\\) such that \\(ar + m = 1\\). </p> <p>Since \\(ar = 1 - m\\), consider \\(r + R \\in R/M\\), and observe that </p> \\[ (a + M)(r + M) = ar + M = 1 - m + M = 1 + M \\] <p>so that \\(r + M\\) is the desired inverse of \\(a + M\\). Since \\(a \\not\\in M\\), this shows that \\(R/M\\) is a division ring. Since it is commutative, it is a field. </p> <p>(\\(\\impliedby\\)) Now suppose that \\(R/M\\) is a field. It is a commutative division ring, so that by Lemma \\ref{divison_ring_thm} we know that its only ideals are either \\(0\\) or \\(R/M\\). But by the Fouth Isomorphism Theorem, we know that these ideals of \\(R/M\\) correspond to \\(M\\) and \\(R\\). Thus there are no other ideals of \\(R\\) containing \\(M\\) other than \\(M\\) and \\(R\\), proving that \\(M\\) is maximal.  </p> <p> Let \\(R\\) be a commutative ring. Then an ideal \\(P\\) is said to be prime if \\(P \\ne R\\) and if \\(ab \\in P\\), then either \\(a \\in P\\) or \\(b \\in P\\). Furthermore, if there exists a \\(p \\in R\\) such that \\(Rp = \\left&lt; p \\right&gt;\\) is a prime ideal, then \\(p\\) is said to be prime. </p> <p>\\textcolor{NavyBlue}{You may wonder why we would make this definition, since ideals tend to suck in elements of \\(R\\) (i.e., \\(ri \\in R\\) for all \\(i \\in I\\), \\(r \\in R\\)). However, just because \\(ab \\in I\\) does not mean \\(a \\in I\\) or \\(b \\in I\\).  Consider for instance the ring \\(4\\ZZ\\). Obviously, \\(4 \\in 4\\ZZ\\), but \\(2\\cdot2 = 4\\) and yet \\(2 \\not\\in 4\\ZZ\\).}</p> <p>Prime ideals have a similar theorem that maximal ideals have. </p> <p> Let \\(R\\) be a commutative ring with identity. Then an ideal \\(P\\) of \\(R\\) is prime if and only if \\(R/P\\) is an integeral domain. That is,  \\[ P \\text{ is prime } \\iff R/P \\text{ is an integral domain.} \\] <p>\\vspace{-0.8cm} </p> <p> (\\(\\implies\\)) Suppose \\(P\\) is a prime ideal. To show that \\(R/P\\) is an integral domain, we must show that it has no zero divisor (as we already know it is commutative). Thus  let \\(r + P \\in R/P\\) where \\(r \\not\\in P\\). Suppose for the sake of contradiction that \\[ (r + P)(r' + P) = P    \\] <p>for some \\(r' + P \\in R/P\\) where \\(r' \\not\\in P\\) (i.e., that there are zero divisors). Then this implies that \\(rr' \\in P\\). Since \\(P\\) is prime, we have that either \\(r \\in P\\) for \\(r' \\in P\\), which is a contradiction since our hypothesis was that \\(r, r' \\not\\in p\\). Therefore \\(R/P\\) is commutative and has no zero divisors, so it is an integral domain. </p> <p>(\\(\\impliedby\\)) Now suppose that \\(R/P\\) is an integral domain. Then </p> \\[ (r + P)(r' + P) \\ne P \\] <p>for any \\(r\\) and \\(r' \\not\\in P\\). In other words, if \\(r, r' \\not\\in P\\) then \\(rr' \\not\\in P\\). Taking the contrapositive of this statement, we have equivalently that if \\(rr' \\in P\\) then \\(r\\) or \\(r' \\in P\\) (notice how that \"and\" changed to an \"or\" upon negation) which proves that \\(P\\) is prime. </p> <p>Finally, we can combine all of these theorems into one useful criterion for primeness. </p> <p> Let \\(R\\) be a commutative ring with identity. Let \\(I\\) be an ideal. If \\(I\\) is maximal then \\(I\\) is prime. In other words,  \\[ I \\text{ is maximal } \\implies I \\text{ is prime.} \\] <p>\\vspace{-0.8cm} </p> <p> By Theorem 1.\\ref{maximal_theorem}, if \\(I\\) is maximal then \\(R/I\\) is a field. But since \\(R/M\\) is a field, it is an integral domain. Hence by Theorem 1.\\ref{prime_theorem}, we have that \\(I\\) is a prime ideal, which proves the theorem. </p> <p>The corollaries of these theorems are immediate. </p> <p> Let \\(R\\) and \\(S\\) be commutative rings with identity, and suppose  \\(\\phi: R \\to S\\) be a surjective ring homomorphism. Then  <ul> <li> <p>[1.] If \\(S\\) is a field, then \\(\\ker(\\phi)\\) is a maximal ideal of \\(R\\). </p> </li> <li> <p>[2.] If \\(S\\) is an integral domain then \\(\\ker(\\phi)\\) is a prime ideal of \\(R\\). </p> </li> </ul> <p></p> <p> <ul> <li> <p>[1.] If \\(\\phi\\) is surjective, then by the First Isomorphism Theorem \\(R/\\ker(\\phi) = \\im(\\phi) = S\\). Therefore \\(R/\\ker(\\phi)\\) is a field, so by Theorem 1.\\ref{maximal_theorem} we have that \\(\\ker(\\phi)\\) is maximal. </p> </li> <li> <p>[2.] In this case, we again have that \\(R/\\ker(\\phi) = S\\), so that \\(R/\\ker(\\phi)\\) is an integral domain. Therefore \\(\\ker(\\phi)\\) is a prime ideal of \\(R\\) by Theorem 1.\\ref{prime_theorem}.</p> </li> </ul> <p></p>"},{"location":"algebra/Rings/Ring%20homomorphisms./","title":"2.2. Ring homomorphisms.","text":"<p>After one understand the fundamentals of group theory, they go on to construct maps between different groups. This is the same strategy we'll follow here, since we can definitely define ring homomorphisms between rings.</p> <p>The ring homomorphisms are also useful since they can help us deduce when two rings \\(R\\) and \\(S\\) are the \"same,\" a concept which evolves into the concept of isomorphisms.</p> <p> Let \\(R\\) and \\(S\\) be rings, and \\(f:R \\to S\\). We define \\(f\\) to be a ring homomorphism if it preserves addition and multiplication; that is, if \\[ f(a + b) = f(a) + f(b) \\hspace{0.2cm}\\text{ and }\\hspace{0.2cm}  f(ab) = f(a)f(b)  \\] <p>for all \\(a, b \\in R\\). If \\(f\\) is a bijection, then we say that \\(f\\) is a ring isomorphism. </p> <p>\\textcolor{NavyBlue}{Note that a ring homomorphism is simply a group homomorphism, with the extra condition of which preserves multiplication of the ring elements.} Therefore the following proposition, which hold for group homomorphisms, holds for ring homomorphisms too. </p> <p> Let \\(R\\) and \\(S\\) be rings and \\(f: R \\to S\\) a ring homomorphism. Then  <ul> <li> <p>[1.] if \\(0_R \\in R\\) and \\(0_S \\in S\\) are zero elements, then \\(f(0_R) = 0_S\\). </p> </li> <li> <p>[2.] if \\(f(-a) = -f(a)\\) for all \\(a \\in R\\)</p> </li> <li> <p>[3.] \\(f(a_1a_2\\cdots a_n) = f(a_1)f(a_2)\\cdots f(a_n)\\) for all \\(a_1, a_2, \\dots a_n \\in R\\)</p> </li> <li> <p>[4.] \\(f(a_1 + a_2 + \\cdots + a_n) = f(a_1) + f(a_2) + \\cdots + f(a_n)\\) for all \\(a_1, a_2, \\dots a_n \\in R\\).</p> </li> </ul> <p></p> <p> Observe that  \\[\\begin{align*} \\phi(r) + \\phi(-r) = \\phi(r + (-r))\\\\ = \\phi(r)\\\\ = 0\\\\ = \\phi(r) +[-\\phi(r)]. \\end{align*}\\] <p>Since \\((R, +)\\) is a group, subtract \\(\\phi(r)\\). </p> <p>\\textcolor{red}{Note that it is not necessarily true that \\(f(1_R) = 1_S\\).} In group theory, it was always guaranteed that we could map the identity element from one group to another. In our case, that's still true: \\(f(0_R) = 0_S\\). Group identity of \\((R,+)\\) is still mapped to the identity of \\((S, +)\\). But this is mapping the additive identity of \\(R\\) to the additive identity of \\(S\\). </p> <p>\\textbf{What we're saying is that multiplicative identities may not always be mapped to each other. }</p> <p>Now since we can't always guarantee that \\(f(1_R) = 1_S\\), \\textcolor{red}{we also can't guarantee that \\(f(a^{-1}) = f(a)^{-1}\\) for some invertible \\(a \\in R\\).} However, there is a clear cut case for when these things do happen.    \\</p> <p> Let \\(R\\) and \\(S\\) be rings and \\(\\phi:R \\to S\\) a nonzero ring homomorphism. Denote \\(1_R \\in R\\) and \\(1_S \\in S\\) to be the respective multiplicative identities. Then  <ul> <li> <p>[1.] If \\(\\phi(1_R) \\ne 1_S\\) then \\(\\phi(1_R)\\) is a zero divisors of \\(S\\). </p> </li> <li> <p>[2.] If \\(S\\) is an integral domain then \\(\\phi(1_R) = 1_S\\). </p> </li> <li> <p>[3.] If \\(\\phi(1_R) = 1_S\\) and \\(u \\in R\\) is a unit then \\(\\phi(u)\\) is a unit in \\(S\\). In other words, \\(\\phi(R^*) \\subset S^*\\)</p> </li> <li> <p>[4.] If \\(\\phi(1_R) = 1_S\\) and if \\(u \\in R\\) has an inverse \\(u^{-1} \\in R\\) then \\(\\phi(u^{-1}) = \\phi(u)^{-1}\\).</p> </li> </ul> <p></p> <p>\\textcolor{MidnightBlue}{An immediately corollary is this: \\(\\phi: R \\to S\\) is a not nonzero ring homomorphism if and only if \\(\\phi(1_R) \\ne 0_S\\). Furthermore,  If \\(S\\) is an integral domain then \\(\\phi(R^*) \\subset S^*\\) for any homomorphism \\(\\phi: R \\to S\\). }</p> <p> <ul> <li>[1.] Suppose \\(\\phi(1_R) \\ne 1_S\\). Since \\(1_R1_R = 1_R\\), we know that</li> </ul> \\[\\begin{align*} \\phi(1_R1_R) - \\phi(1_R) = 0_S \\implies &amp; \\phi(1_R)\\phi(1_R) - \\phi(1_R) = 0_S\\\\ \\implies &amp; \\big(\\phi(1_R) - 1_S\\big)\\phi(1_R) = 0_S. \\end{align*}\\] <p>Since \\(\\phi(1_R) \\ne 1_S\\), either \\(\\phi(1_S) = 0\\) or it is a zero divisor of \\(S\\). </p> <p>Suppose \\(\\phi(1_R) = 0_S\\) and let \\(a \\in R\\). Then</p> \\[\\begin{align*} \\phi(a) = \\phi(1_Ra)  &amp;= \\phi(1_R)\\phi(a)\\\\ &amp;= 0_S\\phi(a)\\\\ &amp;= 0_S. \\end{align*}\\] <p>Thus we see that \\(\\phi\\) send every element of \\(R\\) to \\(0_S\\). However, this cannot be the case since we supposed that \\(\\phi\\) is a nonzero homomorphism.  Therefore \\(\\phi(1_R)\\ne 0\\), leaving us with no choice but to conclude that \\(\\phi(1_R)\\) is a zero divisor in \\(S\\) as desired.   </p> <ul> <li>[2.] Suppose \\(S\\) is an integral domain, and that \\(\\phi(1_R) \\ne 1_S\\) for the sake of contradiction. Then observe for any \\(a \\in R\\)</li> </ul> \\[\\begin{align*} \\phi(1_R a) - \\phi(a) = 0_S \\implies \\phi(1_R)\\phi(a) - \\phi(a) = 0_S \\implies (\\phi(1_R) - 1_S)\\phi(a) = 0_S. \\end{align*}\\] <p>Since \\(\\phi(1_R) \\ne 1_S\\), and \\(\\phi\\) is a nonzero homomorphism, this implies that \\(\\phi(a)\\) and \\(\\phi(1_R) - 1_S\\) are zero divisors in \\(S\\) for at least one \\(a \\in R\\). However, this is a contradiction since \\(S\\) is an integral domain and hence has no zero divisors. Thus by contradiction \\(\\phi(1_R) = 1_S\\).</p> <ul> <li>[3.] Suppose \\(\\phi(1_R) = 1_S\\) and  let \\(u\\) be a unit in \\(R\\). Then \\(uv = 1_R\\) for some \\(v \\in R\\). So </li> </ul> \\[ \\phi(uv) = \\phi(1_R) = 1_S \\implies \\phi(u)\\phi(v) = 1_S.         \\] <p>Therefore, \\(\\phi(u)\\) is a unit in \\(S\\). Next, since \\(uu^{-1} = 1_R\\),</p> \\[ \\phi(1_R) = 1_S \\implies \\phi(uu^{-1}) = 1_S \\implies  \\phi(u)\\phi(u^{-1}) = 1_S \\implies \\phi(u)^{-1} = \\phi(u^{-1}) \\] <p>as desired.</p> <ul> <li>[4.] Suppose \\(\\phi(1_R) = 1_S\\) and that \\(u \\in R\\) has some inverse \\(u^{-1} \\in R\\). Since \\(uu^{-1} = 1_R\\),</li> </ul> \\[ \\phi(1_R) = 1_S \\implies \\phi(uu^{-1}) = 1_S \\implies  \\phi(u)\\phi(u^{-1}) = 1_S \\implies \\phi(u)^{-1} = \\phi(u^{-1}) \\] <p>as desired.</p> <p></p> <p>\\noindentExamples.\\ Let \\(n \\in \\ZZ\\), and define the function \\(f: \\ZZ \\to \\ZZ\\) as </p> \\[ f(m) = nm. \\] <p>Then this is a homomorphism if and only if \\(n = 0\\) or 1. Suppose otherwise. Then observe that the second condition of the definition of a ring homomorphism specifies that </p> \\[\\begin{align*} f(ab) = f(a)f(b) \\implies nab &amp;= nanb \\\\ &amp; = n^2ab. \\end{align*}\\] <p>This is only true if \\(n = 0\\) or 1, which is our contradiction.</p> <p>Instead, we can construct the following function to form a homomorphism between \\(\\ZZ\\) and \\(\\ZZ/n\\ZZ\\), where \\(n\\) is a positive integer. Let \\(f: \\ZZ \\to \\ZZ/n\\ZZ\\) such that </p> \\[ f(m) = [m] \\] <p>where \\([m] = \\{k \\in \\ZZ \\mid k = m \\mbox{ mod } n\\}\\). \\</p> <p>Suppose we construct a homomorphism \\(\\phi: \\mathbb{R}[x] \\to S\\). (Recall that \\(\\RR[x]\\) is the set of finite polynomials with coefficients in \\(\\RR\\)). Define \\(\\phi\\) as  </p> \\[ \\phi(p(x)) = p(i). \\] <p>First, observe that this is surjective, since for any \\(a + bi \\in \\mathbb{C}\\) we can send \\(a + bx \\in \\RR\\) to this element via \\(\\phi\\). Therefore \\(\\im(\\phi) = \\mathbb{C}\\). </p> <p>Let us now describe \\(\\ker(\\phi)\\). First suppose that \\(p(i) = 0\\) for some \\(p(x) \\in \\RR[x]\\). At this point, we know that \\(p(x)\\) must be at least a second degree or greater polynomial. Therefore we can express \\(p(x)\\)  as </p> \\[ p(x) = q(x)(x^2 + 1) + bx + a \\] <p>for some \\(q(x) \\in \\RR[x]\\). Then </p> \\[\\begin{align*} p(i) &amp;= q(i)(i^2 + 1) + bi + a \\\\ &amp;= a + bi \\end{align*}\\] <p>but this implies that \\(a + bi = 0 \\implies a = b = 0\\). Therefore, \\(p(i) = 0\\) if and only if \\(p(x) = q(x)(x^2 + 1)\\) some \\(q(x) \\in \\RR[x]\\). In other words, </p> \\[ \\ker(\\phi) = \\{p(x) \\in \\RR[x] \\mid (x^2 + 1)\\big|p(x)\\}. \\] <p>\\ \\ As in group theory, we have the following theorem regarding isomorphisms. We won't prove this again. </p> <p> Let \\(R\\) and \\(S\\) be rings. A ring homomorphism \\(f: R \\to S\\) is an isomorphism if and only if there exists a h             omomorphism \\(g: S \\to R\\) such that \\(f \\circ g\\) is the identity map on \\(R\\) and \\(g \\circ f\\) is the identity map on \\(S\\). </p> <p>With the ring homomorphism defined, we again have \\(\\ker(f)\\) and \\(\\im(f)\\) as valid and important concepts.</p> <p> Let \\(R\\) and \\(S\\) be rings and \\(f: R \\to S\\) a ring homomorphism. Then we define  \\[ \\ker(f) = \\{a \\in R \\mid f(a) = 0\\}   \\] <p>and </p> \\[ \\im(f) = \\{f(a) \\mid a \\in R\\}. \\] <p></p> <p> Suppose \\(f: R \\to S\\) is a ring homomorphism. Then  <ul> <li> <p>[1.] The kernal \\(\\ker(f)\\) is a subring of \\(R\\).</p> </li> <li> <p>[2.] The image \\(\\im(f)\\) is a subring of \\(S\\).  </p> </li> </ul> <p> Caveat: Recall that \"subrings\" are rings that might not possibly contain \\(1\\), the multiplcative identity.</p> <p> <ul> <li>[1.] We can show this using the Subring Criterion. As we stated before, \\(f(0) = 0\\). Hence \\(0 \\in \\ker(f)\\) so that \\(\\ker(f)\\) is nonempty. </li> </ul> <p>To prove this, observe that </p> \\[\\begin{align*} f(0) + f(0) &amp; = f(0 + 0)\\\\ &amp; = (0)\\\\ &amp; = f(0) + 0. \\end{align*}\\] <p>Since \\((R, +)\\) is a group, we can subtract \\(f(0)\\) from both sides to get \\(f(0) = 0\\).</p> <p>Next, we want to show that \\(r_1, r_2 \\in \\ker(f) \\implies r_1 - r_2 \\in \\ker(f)\\). Since we showed that \\(f(-r) = -f(r)\\) for all \\(r \\in R\\), we know that </p> \\[\\begin{align*} f(r_1 - r_2) &amp; = f(r_1) + f(-r_2)\\\\ &amp; = f(r_1) - f(r_2)\\\\ &amp; = 0 - 0\\\\ &amp; = 0. \\end{align*}\\] <p>Hence, we see that \\(r_1 - r_2 \\in \\ker(f)\\). </p> <p>Now again suppose \\(r_1r_2 \\in \\ker(f)\\). Then </p> \\[\\begin{align*} f(r_1r_2) &amp; = f(r_1)f(r_2)\\\\ &amp; = 0 \\end{align*}\\] <p>so that \\(r_1r_2 \\in \\ker(f)\\). By the subring test, we see that \\(\\ker(f)\\) is a subring of \\(R\\). </p> <ul> <li>[2.] We can similarly prove this via the Subring Test. First, observe that \\(f(0) = 0\\), so that \\(0 \\in \\im(f)\\). Hence, \\(\\im(f)\\) is nonempty. </li> </ul> <p>Next, suppose \\(s_1, s_2 \\in \\im(f)\\). Then we want to show that \\(s_1 - s_2 \\in \\im(f)\\). Now </p> \\[\\begin{align*} s_1 - s_2 &amp; = f(r_1) - f(r_2)\\\\ &amp; =f(r_1 - r_2).\\\\ \\end{align*}\\] <p>This shows that \\(s_1 - s_2 \\in \\im(f)\\). Finally, we'll show that \\(s_1s_2 \\in \\im(f)\\). Observe that </p> \\[\\begin{align*} s_1 \\times s_2 = f(r_1)f(r_2) = f(r_1r_2). \\end{align*}\\] <p>Hence we see that \\(s_1\\times s_2 \\in \\im(f)\\). Thus \\(\\im(f)\\) is a subring of \\(R\\). </p> <p> Finally, we end this section by noting that two important and useful mathematical identites continue to hold in the context of rings. We won't offer their proofs though since they are a bit tedious. </p> <p> Let \\(R\\) be a ring and let \\(a_1, a_2, \\dots, a_m\\) and \\(b_1, b_2, \\dots, b_n\\) be elements of \\(R\\). Then  \\[ (a_1 + a_2 + \\cdots + a_m)(b_1 + b_2 + \\cdots + b_n)  = \\sum_{i = 1}^{m}\\sum_{j = 1}^{n}a_ib_j \\] <p></p> <p>[Binomial Theorem] Let \\(R\\) be a ring (with identity) and let \\(a, b \\in R\\) with \\(ab = ba\\). Then for any \\(n \\in \\mathbb{N}\\) \\[ (a + b)^n = \\sum_{k = 0}^{n} {n\\choose k} a^kb_{n-k}. \\] <p></p>"},{"location":"algebra/Rings/Ring%20of%20Fractions./","title":"2.6. Ring of Fractions.","text":"<p>As rings were modeled based on the behavior of the integers, we can hypothesize that it is possible to generalize the construction of \\(\\mathbb{Q}\\) from  \\(\\mathbb{Z}\\). Such a construction is possible, and studying it will utilize all of the concepts introduced up to this point.  However, there is a very subtle issue that when we try to do this. The main issue is in defining \\(\\dfrac{a}{b}\\) when $b \\ne 0 $ doesn't have an inverse.</p> <p> Say \\((R, +, \\cdot)\\) is a commutative ring with \\(1 \\ne 0\\). We say that \\(D \\subset R\\) is a multiplicative set if  <ul> <li> <p>[1.] \\(1 \\in D\\)</p> </li> <li> <p>[2.] \\(x \\cdot y \\in D\\) for all \\(x, y \\in D\\).  </p> </li> </ul> <p>Condition 1. is not really crucial. We just need to make sure that \\(D\\) is nonempty.   An example of a multiplicative set includes \\(D = \\{1\\}\\). A less boring example is \\(D = \\{2n+1 \\mid n \\in \\mathbb{Z}\\}\\), since the product of two odd integers is odd. And the maximal example we can come up with for any ring is to set \\(D = R\\). </p> <p>Now we introduce a proposition regarding a relation on elements. </p> <p> Define a relation \\(\\sim\\) on \\(R \\times D\\) by saying \\((a, b) \\sim (c,d)\\) whenever we can find \\(x \\in D\\) such that  \\[ x\\cdot (a\\cdot d - b \\cdot c) = 0. \\] <p>Then \\(\\sim\\) is an equivalence relation on \\(R \\times D\\).  </p> <p> We must show the three properties of an equivalence relation.  \\begin{description} \\item[Reflexivity.] \\((a, b)\\sim(a, b)\\). Choosing \\(x = 1\\), then we see that  \\[ x\\cdot(a\\cdot b - a\\cdot b) = 0.   \\] <p>\\item[Symmetry.] \\((a, b) \\sim (c, d)\\) if and only if \\((c, d) \\sim (a, b)\\). </p> <p>Say \\((a, b) \\sim (c, d)\\). We can find an \\(x \\in D\\) such that </p> \\[ x\\cdot(a\\cdot d - b \\cdot c) = 0. \\] <p>Now consider </p> \\[ x\\cdot (c\\cdot b - d \\cdot a) = (-1)\\cdot[x\\cdot (a\\cdot d - b \\cdot c)] = 0 \\] <p>This shows that \\((c, d) \\sim (a, b)\\) </p> <p>\\item[Transitivity.] If \\((a, b) \\sim (c, d)\\) and \\((c, d) \\sim (e, f)\\), then \\((a, b) \\sim (e, f)\\). </p> <p>We can find \\(x, y \\in D\\) such that </p> \\[ x \\cdot (a \\cdot d - b \\cdot c) = y \\cdot (c \\cdot f - d \\cdot e) = 0. \\] <p>Denote \\(z = d \\cdot x \\cdot y\\). This is an element of \\(D\\) since it is a multiplicative set. Now </p> \\[\\begin{align*} z \\cdot (a \\cdot f - b \\cdot e) &amp; = yfxad - yfxbc - xbycf - xbyde\\\\ &amp; =  y \\cdot f [x \\cdot (a \\cdot d  - b \\cdot c)] + x\\cdot b[y \\cdot(c \\cdot f - d \\cdot e)]\\\\ &amp; = 0. \\end{align*}\\] <p>\\end{description}  \\noindent We'll define the collection of all equivalence classes by the set </p> \\[ D^{-1}R = \\left\\{\\frac{a}{b} \\mid a \\in R, b \\in D  \\right\\} \\] <p>where </p> \\[  \\dfrac{a}{b} = \\big\\{ (c, d) \\in R \\times D \\mid (a , b) \\sim (c, d) \\big\\}. \\] <p>\\textcolor{MidnightBlue}{Why are we doing this? Let's say that \\(R = \\mathbb{Z}\\), and \\(D = \\mathbb{Z}\\setminus\\{0\\}\\). Then \\(D\\) is a multiplicative set and \\((a, b) \\sim (c, d) \\text{ if and only if } ab - bc = 0\\). As an example, \\((1, 2) \\sim (2, 4)\\) implies that in our set, \\(\\dfrac{1}{2} = \\dfrac{2}{4}\\). In other words, we're basically saying we don't care whether or not the fraction is in reduced terms.}</p> <p> Let \\(R\\) be a ring with identity \\(1 \\ne 0\\).  <ul> <li> <p>[1.] \\(D^{-1}R\\) is a commutative ring </p> </li> <li> <p>[2.] \\(D^{-1}R = \\left\\{\\dfrac{0}{1}\\right\\}\\) is the trivial ring if and only if \\(0 \\in D\\).</p> </li> <li> <p>[3.] The units \\((D^{-1}R)^\\times\\) contains </p> </li> </ul> \\[ D^{-1}D = \\left\\{ \\frac{a}{b} \\mid a, b \\in D \\right\\}. \\] <p></p> <p> <ul> <li>[1.] Define addition and multiplication. </li> </ul> <p> The following is well-defined.  \\[\\begin{align*} +: D^{-1}R \\times D^{-1}R &amp;\\longrightarrow D^{-1}R\\\\ \\frac{a}{b}, \\frac{c}{d} &amp;\\longmapsto \\frac{ad - bc}{bd} \\end{align*}\\] <p> Suppose that \\((a_1, b_1) \\sim (a_2, b_2)\\) and \\((c_1, d_1) \\sim (c_2, d_2)\\). We can find \\(x, y \\in D\\) such that </p> \\[ x \\cdot (a_1b_2 - a_2b_1) = y \\cdot[c_1d_2 - c_2d_1] = 0. \\] <p>Denote \\(z = xy \\in D\\). Then </p> \\[\\begin{align*} z[(a_1d_1 + c_1b_1)(b_2d_2) - (a_2d_2 + c_2b_2)(b_1d_1)] \\\\= yd_1d_2[x(a_1b_2 - a_2b_1)] + xb_2b_1[y(c_1d_2 - c_2d_1)] = 0. \\end{align*}\\] <p>Hence \\((a_1d_1 + c_1b_1, b_1d_1) \\sim (a_2d_2 + c_2d_2, b_2d_2)\\). </p> <p> The following is well-defined: \\[\\begin{align*} \\cdot: D^{-1}R \\times D^{-1}R &amp;\\longrightarrow D^{-1}R\\\\ \\frac{a}{b}, \\frac{c}{d} &amp;\\longmapsto \\frac{ac}{bd} \\end{align*}\\] <p> Again, say that \\((a_1, b_1) \\sim (a_2, b_2)\\) and \\((c_1, d_1) \\sim (c_2, d_2)\\). Denote \\(z = xy \\in D\\). Then </p> \\[\\begin{align*} z[(a_1c_1) \\cdot(b_2d_2) - (a_2c_2)(b_1d_1)] = yc_1d_2[x(a_1b_2 - a_2b_1)] +  xa_2b_1[y(c_1d_2 - c_2d_1)] = 0 \\end{align*}\\] <p>Hence \\((a_1c_1, b_1d_1) \\sim (a_2c_2, b_2d_2)\\). Showing that \\(D^{-1}R\\) is a commutative ring with these properties is straightforward. </p> <ul> <li>[2.] We show that \\(D^{-1}R\\) is trivial if and only if \\(0 \\in D\\). </li> </ul> <p>Say \\(D^{-1}R = \\left\\{\\dfrac{0}{1}\\right\\}\\). Since \\(1 \\in R\\), we see that \\(\\dfrac{1}{1} \\in D^{-1}R\\) so \\(\\dfrac{1}{1} = \\dfrac{0}{1}\\). By definition, we can find an \\(x \\in D\\) such that </p> \\[ x \\cdot (1 \\cdot 1 - 0 \\cdot 1) = 0 \\] <p>so \\(x = 0\\). Hence \\(0 \\in D\\). </p> <p>Now suppose \\(0 \\in D\\). Pick any \\(\\dfrac{a}{b} \\in D^{-1}R\\). For \\(x = 0\\), we have that \\(x \\cdot (a \\cdot 1 - b \\cdot 0) = 0\\). Hence we see that \\(\\dfrac{a}{b} = \\dfrac{0}{1}\\), so that \\(D^{-1}R = \\left\\{\\dfrac{0}{1}\\right\\}\\).</p> <ul> <li>[3.] If \\(a, b \\in D\\), then \\(\\dfrac{a}{b} \\in D^{-1}R\\) so that \\(\\dfrac{a}{b} \\cdot \\dfrac{b}{a} = \\dfrac{1}{1}\\). Hence \\(\\dfrac{a}{b} \\in (D^{-1}R)^{*}\\). That is, \\(DD^{-1} \\subset (D^{-1}R)^*\\). </li> </ul> <p> \\(D^{-1}R\\) is the ring of fractions of \\(R\\) by \\(D\\). We also remark that if \\(D \\subset R \\setminus \\{0\\}\\), then \\(D^{-1}R\\) is a commutative ring with \\(\\dfrac{1}{1} \\ne \\dfrac{0}{1}\\). Finally, we also note that the elements of \\(D\\) are invertible in \\(D^{-1}R\\). We want to construct \\(D^{-1}R\\) as the \"smallest\" ring such that \\(D\\) is invetible.</p> <p>Next we introduce a theorem. </p> <p> Let \\((R, +, \\cdot)\\) be a commutative ring with \\(1 \\ne 0\\), and \\(D \\subset R\\) be a multiplicative set. Denote \\(S = D^{-1}R = \\{a/b \\mid a \\in R, b \\in D\\}\\) as the \\textit{ring of fractions of \\(R\\) by \\(D\\)}. Then  \\begin{description} \\item[1.] Let \\(I\\) be an ideal of \\(R\\). Then \\(D^{-1}I =\\{x/b \\mid x \\in I, b \\in D\\}\\) is an ideal of \\(S\\). (This is called the extension of \\(I\\) to \\(S\\).) <p>\\item[2.] Let \\(J\\) be an ideal of \\(S\\). Define the ring homomorphism \\(\\pi: R \\to S\\) which sends \\(r \\mapsto r/1\\). Then the preimage \\(\\pi^{-1}(J)\\) is an ideal of \\(R\\). (This is called the contraction of \\(J\\) to \\(R\\).)</p> <p>\\item[3.] For any ideal \\(J\\) of \\(S\\) we have that \\(D^{-1}[\\pi^{-1}(J)] = J\\). (That is, \"extension is the left inverse of contraction.\") \\end{description} </p> <p> \\begin{description} \\item[1.] To show this we proceed as follows. \\ \\underline{$\\bm{D^{-1}I} $ is nonempty.}\\ [1.2ex] Observe that since \\(D\\) is a multiplicative set we have that \\(1 \\in D\\). Hence if \\(I\\) is nonempty, then \\(\\dfrac{i}{1} \\in D^{-1}I\\) for all \\(i \\in I\\). Hence it is nonempty. \\ <p>\\noindent\\underline{\\(\\bm{a - b \\in D^{-1}I}\\) if \\(\\bm{a, b \\in D^{-1}I}\\).}\\[1.2ex] Let \\(a, b \\in D^{-1}I\\). Then \\(a = \\dfrac{i_1}{d_1}\\) and \\(b = \\dfrac{i_2}{d_2}\\). Then observe that  [ a - b = \\dfrac{i_1d_2 - i_2d_1}{d_1d_2}. ]</p> <p>Since \\(I\\) is an ideal, \\(i_1d_2. i_2d_1 \\in I\\). Therefore their difference \\(i_1d_2 - i_2d_1 \\in I\\). Since \\(D\\) is a multiplicative set we have that \\(d_1d_2 \\in D\\). Hence we see that \\(\\dfrac{i_1d_2 - i_2d_1}{d_1d_2} \\in D^{-1}I\\), so that \\(a - b \\in D^{-1}I\\) whenever \\(a, b \\in D^{-1}I\\). </p> <p>\\noindent\\underline{\\(\\bm{s\\cdot a\\in D^{-1}I}\\) if \\(\\bm{s \\in S, a \\in D^{-1}I}\\)}\\ [1.2ex] Let \\(s \\in S\\) and \\(a \\in D^{-1}I\\). Then \\(s = \\dfrac{r}{d}\\) and \\(a = \\dfrac{i}{d'}\\) for \\(r \\in R, i \\in I, d,d' \\in D\\).  Hence observe that </p> \\[\\begin{align*} s\\cdot a = \\dfrac{r}{d} \\cdot \\dfrac{i}{d'} = \\dfrac{ri}{dd'}. \\end{align*}\\] <p>Since \\(I\\) is an ideal, we see that \\(ri \\in I\\). Since \\(D\\) is a multiplicative set, we also see that \\(dd' \\in D\\). Hence, we see that \\(\\dfrac{ri}{dd'} \\in D^{-1}I\\), so that \\(s\\cdot a \\in D^{-1}I\\) if \\(s \\in S\\) and \\(a \\in D^{-1}I\\).\\ Thus in total we have that \\(D^{-1}I\\) is an ideal of \\(S\\).</p> <p>\\item[2.] To show this is an ideal, we proceed as follows.\\ \\underline{$\\bm{\\pi^{-1}(J)} $ is nonempty.}\\[1.2ex] Observe that if \\(J\\) is nonempty, then \\(\\dfrac{0}{1} \\in J\\). This is because if \\(J\\) is an ideal, then \\(sj \\in J\\) for all \\(s \\in S\\) and \\(j \\in J\\). Hence let \\(s = \\dfrac{0}{1}\\), and \\(j = \\dfrac{r}{d} \\in J\\). Then  [ sj \\in J \\implies \\dfrac{0}{1}\\cdot \\dfrac{r}{d} \\in J \\implies \\frac{0}{d} \\in J. ]</p> <p>Now observe that \\((0, 1) \\sim (0, d)\\) for any \\(d \\in D\\). This is because </p> \\[ x \\cdot(0 \\cdot d - 0 \\cdot 1) = x \\cdot (0) = 0 \\] <p>is automatically satisfied by any \\(x \\in D\\). Hence, \\((0, 1) \\sim (0, d)\\). Now we argue that</p> \\[ \\pi^{-1}(J) = \\{r \\in R \\mid \\pi(r) \\in J\\}. \\] <p>is nonempty. Observe that \\(\\pi\\left(\\dfrac{0}{1}\\right) = \\dfrac{0}{1}\\), which we know is true because \\(\\pi\\), as a ring homomorphism, is also a group homomorphism between the abelian groups \\(R\\) and \\(S\\) (i.e., the abelian groups we get when we remove the multiplicative ring stucture on them). We know from group theory that group homomorphisms map additive zero elements from one group to the additive zero element of the other group, so that \\(\\pi\\left(\\dfrac{0}{1}\\right) = \\dfrac{0}{1}\\).</p> <p>As we just showed, \\(\\dfrac{0}{1} \\in J\\). Since \\(\\pi\\left(\\dfrac{0}{1}\\right) = \\dfrac{0}{1}\\), we see that \\(\\dfrac{0}{1} \\in \\pi^{-1}(J)\\), so it is nonempty. </p> <p>\\noindent\\underline{\\(\\bm{a - b \\in \\pi^{-1}(J)}\\) if \\(\\bm{a, b \\in \\pi^{-1}(J)}\\).}\\ [1.2ex] Suppose \\(a, b \\in \\pi^{-1}(J)\\). Then \\(\\pi(a) = \\dfrac{a}{1}\\) and \\(\\pi(b) = \\dfrac{b}{1}\\) are both members of \\(J\\). Since \\(J\\) is an ideal, we know that on one hand,</p> \\[\\begin{align*} \\frac{a}{1}, \\frac{b}{1} \\in J \\implies \\frac{a}{1} - \\frac{b}{1} \\in J \\implies \\frac{a - b}{1} \\in J. \\end{align*}\\] <p>On the other hand, observe that \\(\\pi(a - b) = \\dfrac{a - b}{1}\\), which we just showed is inside \\(J\\). Therefore \\(a - b \\in \\pi^{-1}(J)\\) when \\(a, b \\in \\pi^{-1}(J)\\), which is what we set out to show.</p> <p>\\noindent\\underline{\\(\\bm{r\\cdot a \\in \\pi^{-1}(J)}\\) if \\(\\bm{r \\in R, a \\in \\pi^{-1}(J)}\\).}\\[1.2ex] Now suppose that \\(r \\in R\\) and \\(a \\in \\pi^{-1}(J)\\). Then \\(\\pi(a) = \\dfrac{a}{1} \\in J\\) by definition. Hence, observe that  [ \\pi(r \\cdot a) = \\dfrac{ra}{1} = \\frac{r}{1} \\cdot \\frac{a}{1}. ]</p> <p>Observe that since \\(J\\) is an ideal of \\(S\\), \\(\\frac{r}{1} \\cdot \\frac{a}{1} \\in R\\). Hence we see that \\(\\pi(r \\cdot a) \\in J\\), so that \\(r \\cdot a \\in \\pi^{-1}(J)\\) whenever \\(r \\in R\\), \\(a \\in \\pi^{-1}(J)\\), as desired. </p> <p>\\item[3.] We can prove equality between the sets by demonstrating mutual subset properties. \\ \\noindent\\underline{\\(\\bm{D^{-1}[\\pi^{-1}(J)] \\subset J}\\)}\\ [1.2ex] Consider any \\(\\frac{a}{b} \\in D^{-1}[\\pi^{-1}(J)]\\) where by definition \\(a \\in \\pi^{-1}(J)\\) and \\(b \\in D\\). </p> <p>Since \\(a \\in \\pi^{-1}(J)\\) we know that \\(\\pi(a) = \\dfrac{a}{1} \\in J\\). Consider the element \\(\\dfrac{1}{b} \\in S\\). Since \\(J\\) is an ideal of \\(S\\), we know that \\(sj \\in J\\) for all \\(s \\in S, j \\in J\\). Set \\(j = \\dfrac{a}{1}\\) and \\(s = \\dfrac{1}{b}\\), and observe that  [ sj \\in J \\implies \\frac{1}{b} \\cdot \\frac{a}{1} = \\frac{a}{b} \\in J. ]</p> <p>Hence we have that \\(D^{-1}[\\pi^{-1}(J)] \\subset J\\).  \\ \\noindent\\underline{\\(\\bm{J \\subset D^{-1}[\\pi^{-1}(J)]}\\)}\\ [1.2ex] Now consider any \\(j = \\dfrac{a}{b} \\in J\\). To prove this direction, we just need to show that \\(a \\in \\pi^{-1}(J)\\) since \\(b\\) is already a member of \\(D\\). And to prove that, we just need to show that \\(\\dfrac{a}{1} \\in J\\). Hence we formalize our claim:</p> <p>Claim: \\(\\dfrac{a}{b} \\in J \\implies \\dfrac{a}{1} \\in J\\).\\ To show this, observe that \\(\\dfrac{b}{1} \\in S\\) and since \\(J\\) is an ideal,  [ \\frac{b}{1} \\cdot \\frac{a}{b} \\in J \\implies \\frac{ab}{b} \\in J. ]</p> <p>Now observe that \\((ab, b) \\sim (a, 1)\\), since </p> \\[ x \\cdot (ab\\cdot 1 -b\\cdot a) = x \\cdot (ab - ab) = 0 \\] <p>is satisfied by any choice of \\(x \\in D\\). Therefore, \\(\\dfrac{ab}{b} = \\dfrac{a}{1}\\), and since \\(\\dfrac{ab}{b} \\in J\\) we have that \\(\\dfrac{a}{1} \\in J\\). </p> <p>Finally, since \\(\\dfrac{a}{1} \\in J\\), we see that \\(a \\in \\pi^{-1}(J)\\). Therefore, \\(\\dfrac{a}{b} \\in D^{-1}[\\pi^{-1}(J)]\\), so that \\(J \\subset D^{-1}[\\pi^{-1}(J)]\\) as desired. </p> <p>With both directions, we can then conclude that \\(D^{-1}[\\pi^{-1}(J)] = J\\), which is what we set out to show. \\end{description} </p> <p> Let \\((R, +, \\cdot)\\) be a commutative ring with \\(1 \\ne 0\\), and \\(D \\subset R\\) be a multiplicative set. Let \\(P\\) be an ideal of \\(R\\) with the property that if \\(d \\cdot x \\in P\\) for \\(d \\in D\\) then \\(x \\in P\\). Then the following are equivalent. <ul> <li> <p>[i.] \\(P\\) is a prime ideal of \\(R\\) with \\(P \\cap D = \\varnothing\\)</p> </li> <li> <p>[ii.] \\(D^{-1}P\\) is a prime ideal of the ring of fractions \\(D^{-1}R\\).  </p> </li> </ul> <p></p> <p> \\begin{description} \\item[\\(\\bm{i \\implies ii}\\).] Suppose \\(P\\) is disjoint with \\(D\\). Then we show that \\(D^{-1}P\\) is a prime ideal of \\(D^{-1}R\\).\\ \\noindent\\underline{\\(\\bm{D^{-1}P}\\) is nonempty.}\\ [1.2ex] If \\(P\\) is nonempty, then since \\(1 \\in D\\), we know that \\(\\dfrac{p}{1} \\in D^{-1}P\\) for each \\(p \\in P\\). Hence, it is nonempty. <p>\\noindent\\underline{\\(\\bm{a, b \\in D^{-1}P \\implies a - b \\in D^{-1}P}\\).}\\[1.2ex] Suppose \\(a, b \\in D^{-1}P\\). Write \\(a = \\dfrac{p_1}{d_1}\\) and \\(b = \\dfrac{p_2}{d_2}\\). Then we see that  [ a - b = \\frac{p_1d_2 - p_2d_1}{d_1d_2}. ]</p> <p>Observe that \\(p_1d_2, p_1d_2 \\in P\\) since \\(P\\) is an ideal of \\(R\\). Hence, \\(p_1d_2 - p_2d_1 \\in P\\), and since \\(d_1d_2 \\in D\\), we have that \\(a - b \\in D^{-1}P\\) whenever \\(a, b \\in D^{-1}P\\).</p> <p>\\noindent\\underline{\\(\\bm{r\\cdot a \\in D^{-1}P}\\) ** for ** \\(\\bm{r \\in  D^{-1}R, a \\in D^{-1}P}\\).}\\ [1.2ex] Consider an element \\(a = \\dfrac{p}{d} \\in D^{-1}{P}\\) and \\(r = \\dfrac{r'}{d'} \\in D^{-1}R\\). Then  [ r \\cdot a = \\frac{r'}{d'} \\cdot \\frac{p}{d} = \\frac{r'p}{d'd}. ]</p> <p>Since \\(P\\) is a prime ideal, we see that \\(r'p \\in P\\) and \\(d'd \\in D\\) as it is a multiplicative set. Therefore it is an ideal. </p> <p>\\noindent\\underline{\\(\\bm{a\\cdot b \\in D^{-1}P \\implies a \\in D^{-1}P}\\) or \\(\\bm{b \\in D^{-1}P}\\)} \\ [1.2ex] Let \\(a = \\dfrac{r_1}{d_1}\\) and \\(b = \\dfrac{r_2}{d_2}\\) where \\(r_1,r_2 \\in R\\) and \\(d_1,d_2 \\in D\\). Now  [ a \\cdot b \\in D^{-1}P \\implies \\frac{r_1r_2}{d_1d_2} \\in D^{-1}P. ]</p> <p>Then we see that \\(r_1r_2 \\in P.\\) Since \\(P\\) is a prime ideal disjoint with \\(D\\), we see that \\(r_1 \\in P\\) or \\(r_2 \\in P\\) and \\(r_1,r_2 \\not\\in D\\) by assumption. Therefore, we see that \\(a \\in D^{-1}P\\) or \\(b \\in D^{-1}P\\), so that \\(D^{-1}P\\) is a prime ideal. </p> <p>\\item[\\(\\bm{ii \\implies i}\\).]  Suppose \\(D^{-1}P\\) is a prime ideal of \\(D^{-1}R\\).  \\ [1.2ex] \\underline{\\(\\bm{P}\\) is nonempty.}\\[1.2ex] If \\(D^{-1}P\\) is nonempty, then since \\(D\\) at least contains \\(1\\), there exists at least one \\(\\dfrac{a}{1} \\in D^{-1}P\\) where \\(a \\in P\\). Hence we see that \\(P\\) is nonempty. \\[1.2ex] \\underline{\\(\\bm{a,b \\in P \\implies a - b \\in P}\\).}\\[1.2ex] Suppose \\(a, b \\in P\\). Then we see that  \\(\\dfrac{a}{1}, \\dfrac{b}{1} \\in D^{-1}P\\). Hence, [ \\frac{a}{1} - \\frac{b}{1} \\in D^{-1}P \\implies  \\frac{a - b}{1} \\in P. ]</p> <p>Since \\(\\dfrac{a - b}{1} \\in D^{-1}P\\), we see that \\(a - b \\in P\\). Hence \\(a, b \\in P \\implies a - b \\in P\\). \\ [1.2ex] \\underline{\\(\\bm{rp \\in P}\\) if \\(\\bm{r \\in R, p \\in P}\\).}\\[1.2ex] Consider \\(\\dfrac{p}{1} \\in D^{-1}P\\) and \\(\\dfrac{r}{1} \\in D^{-1}R\\) for any \\(r \\in R\\). Then since \\(D^{-1}P\\) is an ideal, [ \\frac{r}{1} \\cdot \\frac{p}{1} \\in D^{-1}P \\implies \\frac{rp}{1} \\in D^{-1}P. ]</p> <p>Thus we see that \\(rp \\in P\\). Therefore \\(r \\in R, p \\in P \\implies rp \\in P\\).  \\ [1.2ex] \\underline{\\(\\bm{D \\cap P = \\varnothing}.\\)}\\[1.2ex] Suppose that \\(D \\cap P \\ne \\varnothing\\). Then there exists a \\(p \\in P\\) where \\(p \\in D\\). Hence, observe that \\(\\dfrac{p}{p} = \\dfrac{1}{1} \\in D^{-1}P\\). Then since \\(D^{-1}P\\) is an ideal of \\(D^{-1}R\\), we see that \\(pr \\in D^{-1}P\\) for any \\(p \\in D^{-1}P\\) and \\(r \\in D^{-1}R\\). Thus see that for any \\(\\dfrac{a}{b} \\in D^{-1}R\\),  [ \\frac{1}{1} \\cdot \\frac{a}{b} \\in D^{-1}P \\implies \\frac{a}{b} \\in D^{-1}P ]</p> <p>which shows that \\(D^{-1}P = D^{-1}R\\). Hence, if we want \\(D^{-1}P\\) to be a proper ideal, we need that \\(D \\cap P = \\varnothing.\\) \\ [1.2ex] \\underline{\\(\\bm{ab \\in P \\implies a \\in P}\\) or \\(\\bm{b\\in P}\\)}\\[1.2ex] Suppose that \\(a = \\dfrac{p_1}{d_1}\\) and \\(b = \\dfrac{p_2}{d_2}\\) such that  [ \\frac{p_1}{d_1}\\cdot \\frac{p_2}{d_2} \\in D^{-1}P \\implies \\frac{p_1}{d_1} \\text{ or } \\frac{p_2}{d_2} \\in D^{-1}P. ]</p> <p>Since \\(\\dfrac{p_1}{d_1}\\cdot \\dfrac{p_2}{d_2} \\in D^{-1}P\\), we have that \\(\\dfrac{p_1p_2}{d_1d_2} \\in D^{-1}P\\), which implies that \\(p_1p_2 \\in P\\). </p> <p>The fact that \\(\\dfrac{p_1}{d_1} \\in D^{-1}P\\) or \\(\\dfrac{p_2}{d_2} \\in D^{-1}P\\) implies that \\(p_1 \\in P\\) or \\(p_2 \\in P\\). Hence we see that \\(p_1p_2 \\in P \\implies p_1 \\in P\\) or \\(p_2 \\in P\\), which proves that \\(P\\) is a prime ideal. \\end{description} With both directions proven, we can conclude that the two given statments are in fact equivalent. </p> <p>Localization. The construction we have been implementing relates to a concept as localization, which we define as follows. </p> <p> Let \\((R, +, \\cdot)\\) be a commutative ring with identity \\(1 \\ne 0\\). Let \\(S \\subset R\\) and define \\(D = R - S\\). We define the localization of  \\(R\\) at \\(S\\) as the ring of fractions  \\[ R_P = D^{-1}R = \\left\\{ \\frac{r}{d} \\mid r \\in R, d \\in D \\right\\}. \\] <p> It turns out that if we localize at a prime ideal, nice things happen. Specifically, the localization contains a unique maximal ideally. </p> <p>Generally, rings do not have unique maximal ideals, although the definition of a maximal ideal can often confuse people. For example, consider the ring \\(\\mathbb{Z}\\). Then for any prime \\(p\\), we see that \\(p\\mathbb{Z}\\) is a maximal ideal; given the infinitude of the primes, we have infinitely many maximal primes. </p> <p>For rings that do have a unique, maximal ideal, we give them a special name. </p> <p> Let \\((R, +, \\cdot)\\) be a ring. If \\(R\\) has a unique, maximal ideal \\(M\\), then we say that \\(R\\) is a local ring. </p> <p> Let \\((R, +, \\cdot)\\) be an integral domain, and \\(P\\) be a prime ideal of \\(R\\).  \\begin{description} \\item[1.] The set \\(D = R - P\\) is a multiplicative set. <p>\\item[2.] The localization of \\(R\\) at \\(P\\), \\(R_P = D^{-1}R\\), is an integral domain. </p> <p>\\item[3.] The ring \\(R_P\\) is a local ring i.e., \\(R_P\\) has a unique maximal ideal \\(M_P\\). \\end{description} </p> <p> \\begin{description} \\item[1.] First we show that \\(1 \\in D\\).  Suppose \\(P\\) is a prime ideal such that \\(R \\ne P\\). Then observe that \\(1 \\not\\in P\\). For if \\(1 \\in P\\), then for any \\(r \\in R\\), we'd see that  \\[ 1 \\cdot r = r \\in P. \\] <p>Since \\(r\\) is arbitrary, we'd have that \\(R = P\\), a contradiction. Therefore, we see that \\(1 \\in R - P = D\\), which proves the first property.</p> <p>Now we show \\(x, y \\in D \\implies xy \\in D\\). Since \\(P\\) is a prime ideal, we know that \\(p_1p_2 \\in P \\implies p_1 \\in P\\) or \\(p_2 \\in P\\). Hence the reverse negative of the statement is true: if \\(p_1 \\not\\in P\\) and \\(p_2 \\not\\in P\\) then \\(p_1p_2 \\not\\in P\\). </p> <p>Therefore for any \\(x, y \\in D =R - P\\), we see that \\(xy \\not\\in P\\). Hence \\(xy \\in D\\), which proves the second property.</p> <p>\\item[2.] Since we already know that \\(R_P\\) is a commutative ring, it suffices to show that there are no zero divisors. Suppose on the contrary that \\(\\dfrac{a}{b}, \\dfrac{c}{d} \\in R_P\\) are zero divisors of each other. Hence, \\(a \\ne 0\\) and \\(c \\ne 0\\). Then we see that </p> \\[ \\frac{a}{b} \\cdot \\frac{c}{d} = \\frac{0}{1}. \\] <p>In this case, we see that there exists a \\(x \\in D\\) such that </p> \\[ x \\cdot (ac \\cdot 1 - bd \\cdot 0) \\implies x \\cdot(ac).             \\] <p>Since \\(a, c \\ne 0\\), and \\(R\\) is an integral domain, we see that \\(ac \\ne 0\\). But \\(x\\) is also nonzero, while \\(x \\cdot (ac) = 0\\). This cannot happen since \\(R\\) is an integral domain, so we have a contradiction. Therefore there are no zero divisors, and since \\(R_P\\) is a commutative, this makes it an integral domain. </p> <p>\\item[3.] Let \\(M_P = D^{-1}P\\). We'll show that this is our unique, maximal ideal. </p> <p>First observe that in the previous theorem, if \\(P\\) is a prime ideal and \\(P \\cap D = \\varnothing\\), then \\(D^{-1}P\\) is a prime ideal of \\(D^{-1}R\\). </p> <p>In our case, \\(D = R - P\\). Hence \\(P \\cap D = \\varnothing\\), so we may conclude that \\(M_P\\) is a prime ideal.</p> <p>Now we show \\(M_p\\) is maximal. Let \\(I\\) be a proper ideal, i.e. \\(I \\ne D^{-1}R\\), and suppose \\(I \\not\\subset M_P\\). That is, there exist an element \\(\\dfrac{a}{b} \\in I\\) such that \\(\\dfrac{a}{b} \\not\\in M_P\\).</p> <p>Since \\(\\dfrac{a}{b} \\not\\in M_P\\), we see that \\(a \\not\\in P\\). Hence, \\(a \\in R - P = D\\), and of course \\(b \\in D\\) as well. Now consider the element \\(\\dfrac{b}{a}\\). Observe that \\(\\dfrac{b}{a} \\not\\in M_P\\), since \\(b, a \\in D\\), as shown earlier. Since \\(\\dfrac{a}{b} \\in I\\), we have that </p> \\[ \\dfrac{a}{b}\\cdot \\dfrac{b}{a} \\in I \\implies = \\dfrac{1}{1} \\in I. \\] <p>Since \\(I\\) contains \\(\\dfrac{1}{1}\\), we have that \\(I = D^{-1}R\\). This is because we see that for any \\(\\dfrac{c}{d} \\in D^{-1}R\\), </p> \\[ \\dfrac{c}{d} =\\dfrac{c}{d}\\cdot\\dfrac{1}{1} \\in I \\implies \\dfrac{c}{d} \\in I. \\] <p>Hence, \\(I = D^{-1}R\\). But we assumed \\(I\\) was a proper ideal; thus we have a contradiction, so we see that \\(M_P = D^{-1}P\\) is in fact maxmial. Since we assumed \\(I\\) was any ideal of \\(D^{-1}R\\), this also proves that \\(M_P\\) is a unique maximal ideal, since what we showed is that any other ideal is automatically contained in \\(M_P\\).  \\end{description} </p> <p>As an example, consider the prime ideal \\(\\{0\\}\\) of the ring \\(\\mathbb{Z}\\). Then the localization of \\(\\mathbb{Z}\\) at \\(\\{0\\}\\) is given by </p> \\[ \\left\\{ \\dfrac{a}{b} \\mid a \\in \\ZZ, b \\in \\ZZ - \\{0\\}  \\right\\} \\] <p>which is just the rational numbers. </p> <p> Let \\((R, +, \\cdot)\\) be an integral domain, and \\(P\\) be a prime ideal of \\(R\\). Let \\(R_P\\) be the localization of \\(R\\) at \\(P\\), and \\(M_P\\) be its unique maximal ideal. Consider the map \\(\\phi: R/P \\to R_P/M_P\\) which sends \\(r + P \\mapsto r/1 + M_P\\).  \\begin{description} \\item[1.] \\(\\phi\\) is a well-defined, injective ring homomorphism.  \\item[2.] \\(\\phi\\) is an isomorphism if \\(P\\) is a maximal ideal.  \\end{description} </p> <p> \\begin{description} \\item[1.]     \\underline{Well-defined.}\\ First observe that this function is well-defined. Suppose that \\(r + P = r' + P\\); that is, \\(r = r' + p\\) for some \\(p \\in P\\). Observe that  \\[\\begin{align*} \\phi(r + P) = \\frac{r}{1} + M_P &amp;= \\frac{r' + p}{1} + M_P\\\\ &amp;= \\frac{r'}{1} + \\frac{p}{1} + M_P\\\\ &amp;= \\frac{r'}{1} + M_P\\\\ &amp;= \\phi(r' + P) \\end{align*}\\] <p>where in the fourth step we used that fact that \\(\\dfrac{p}{1} \\in M_P\\) since \\(p \\in P\\), \\(1 \\in D\\). Since \\(\\phi(r + P) = \\phi(r' + P)\\), we see that this function is well-defined.  \\ \\ \\underline{Ring homomorphism.}\\ We demonstrate that \\(\\phi: R/P \\to R_P/M_P\\) is a ring homomorphism.  \\begin{description} \\item[\\(\\bm{\\phi(a + b) = \\phi(a) + \\phi(b)}\\).] Let \\(a = r + P\\) and \\(b = r' + P\\) be elements or \\(R/P\\). Then</p> \\[\\begin{align*} \\phi(a + b) = \\phi\\big((r + r') + P \\big) &amp; = \\frac{r + r'}{1} + M_P\\\\ &amp; = \\frac{r}{1} + \\frac{r'}{1} + M_P\\\\ &amp; = \\left( \\frac{r}{1} + M_P\\right) + \\left(\\frac{r'}{1} + M_P\\right)\\\\ &amp; = \\phi(a) + \\phi(b) \\end{align*}\\] <p>which is what we set out to show. \\item[\\(\\bm{\\phi(ab) = \\phi(a)\\phi(b)}\\).] Again, suppose \\(a = r + P\\) and \\(b = r' + P\\). Then observe that </p> \\[\\begin{align*} \\phi(ab) = \\phi\\big( (r + P)(r' + P) \\big) &amp; = \\phi(rr' + P)\\\\ &amp; = \\frac{rr'}{1} + M_P\\\\ &amp; = \\left(\\frac{r}{1} + M_P\\right)\\left(\\frac{r'}{1} + M_P\\right)\\\\ &amp; = \\phi(a)\\phi(b) \\end{align*}\\] <p>which is what we set out to show.  \\end{description} With these two properties, we have that \\(\\phi: R/P \\to R\\) is a ring homomorphism. \\ \\ \\underline{Injectivity.}\\ Next we show that this is an injective function. Suppose \\(r + P,r' + P \\in D^{-1}R\\) such that </p> \\[ \\phi(r + P)= \\phi(r' + P). \\] <p>Then we have that \\(\\dfrac{r}{1} + M_P = \\dfrac{r'}{1} + M_P\\). In other words, we see that </p> \\[ \\frac{r}{1} = \\frac{r'}{1} + \\frac{a}{b} \\] <p>for some \\(\\dfrac{a}{b} \\in M_P\\). This further implies that \\(\\dfrac{r}{1} = \\dfrac{r'b + a}{b}\\). Hence, \\((r, 1) \\sim (r'b + a, b)\\). For this equivalence to occur, there must exist an element \\(x \\in D\\) such that </p> \\[ x\\cdot(rb - (r'b + a)) = 0. \\] <p>Since \\(R\\) is an integral domain, and \\(x \\ne 0\\), we require that \\(rb = r'b + a\\). Rearranging, we see that this implies that </p> \\[ rb - r'b = a \\implies (r - r')b = a. \\] <p>The above equality implies that \\((r - r')b \\in P\\); recall that \\(\\dfrac{a}{b} \\in M_P = D^{-1}P\\), so that \\(a \\in P\\) and \\(b \\in D = R - P\\). </p> <p>Since \\(P\\) is a prime ideal, we thus see that either \\(r - r' \\in P\\) or \\(b \\in P\\). Since we just said that \\(b \\not\\in P\\), we must have that \\(r - r' \\in P.\\) In other words, </p> \\[ r - r' = p \\implies r = r' + p \\] <p>for some \\(p \\in P\\). Hence, </p> \\[ r + P = r' + p + P = r' + P.    \\] <p>Therefore we see that \\(\\phi(r + P) = \\phi(r' + P) \\implies r + P = r' + P\\), so that \\(\\phi\\) is injective.</p> <p>\\item[2.] Suppose \\(P\\) is a maximal ideal (in addition to being prime). To demonstrate that \\(\\phi: R/P \\to R_P/M_P\\) is an isomorphism, it suffices to show that \\(\\phi\\) is surjective, since in the previous step we already showed that it was injective. </p> <p>Since \\(P\\) is a maximal ideal, we see that \\(R/P\\) is a field. Therefore inverse elements exist, so for the element \\(b + P\\), there exists an element \\(b' + P\\) where \\(b' \\not\\in P\\) such that </p> \\[ (b + P)(b' + P) =  1 + P. \\] <p>That is, \\(bb' = 1 + p\\) for some \\(p \\in P\\). </p> <p>Let \\(r = ab'\\), and consider the elements \\(\\dfrac{a}{b} + M_P\\) and \\(\\dfrac{r}{1} + M_P\\). Since we want \\(\\dfrac{a}{b} + M_P\\) to be nontrivial, we let \\(a, b \\in D\\) (that is, \\(a \\not\\in P\\) or else in that case \\(\\dfrac{a}{b} \\in M_P\\)).</p> <p>Observe that \\(\\dfrac{r}{1} \\not\\in M_P\\) since \\(a \\in D, b' \\in D\\), and because \\(D\\) is a multiplicative set, \\(r = ab \\in D\\). Therefore \\(r \\not\\in P\\), so that \\(\\dfrac{r}{1} \\not\\in M_P\\). Hence, \\(\\dfrac{r}{1} + M_P \\ne \\dfrac{0}{1} + M_P\\). </p> <p>Now observe that </p> \\[\\begin{align*} \\left(\\frac{a}{b} + M_p\\right) - \\left(\\frac{r}{1} + M_P\\right) &amp; = \\left( \\frac{a}{b} - \\frac{r}{1} \\right) + M_P\\\\ &amp; = \\frac{a - br}{b} + M_P\\\\ &amp; = \\frac{a - b(ab')}{b} + M_P\\\\ &amp; = \\frac{a - abb'}{b} + M_P \\text{ (by commutativity)}\\\\ &amp; = \\frac{a - a(1 + p)}{b} + M_P\\\\ &amp; = \\frac{-ap}{b} + M_P. \\end{align*}\\] <p>Since \\(P\\) is an ideal, we see that \\(-ap \\in P\\). Therefore, \\(\\dfrac{-ap}{b} \\in M_P\\), so that \\(\\dfrac{-ap}{b} + M_P = \\dfrac{0}{1} + M_P\\). Thus what we've shown is that </p> \\[ \\left(\\frac{a}{b} + M_P\\right) - \\left( \\frac{r}{1} + M_P \\right) = \\frac{0}{1} + M_P. \\] <p>which implies that </p> \\[ \\frac{a}{b} + M_P = \\frac{r}{1} + M_P. \\] <p>Thus we see that \\(\\phi(r + P) = \\dfrac{r}{1} + M_P = \\dfrac{a}{b} + M_P\\). However, \\(\\dfrac{a}{b} + M_P\\) was an arbitrary element of \\(R_P/M_P\\). Hence, we've shown that for any \\(\\dfrac{a}{b} + M_P\\), there exists an \\(r + P\\in R/P\\) such that  \\(\\phi(r + P) = \\dfrac{a}{b} + M_P\\). In particular, \\(\\phi(0 + P) = \\dfrac{0}{1} + M\\). Therefore \\(\\phi\\) is surjective, and as we already showed it is injective, this makes it an isomorphism.  \\end{description} </p>"},{"location":"associahedron/","title":"Associahedron","text":"<p>The Associahedra are a very mathematically deep family of polytopes (one in each dimension) that occur frequently in combinatorics, under many different interpretations. A fascinating fact about them is that the coefficients of the multiplicate inverse of a power series encode the polyhedra face data of the Associahedra (see Marcelo Aguiar and Frederico Ardilla's paper).</p> <p>Here's an interactive 3D web app of the first 7. They're all projected to R^3, and K_5 is the family member that naturally lives in R^3. I made this with the help of this 3D-Force-Graph library.</p>"},{"location":"associahedron/#comments-on-the-app","title":"Comments on the app","text":"<ul> <li> <p>Click and drag to rotate the figure. You can zoom in and out and also hover over the nodes to see what binary word it corresponds to. </p> </li> <li> <p>You can use the slider to change which Associahedron you look at. I have the data for K_8, K_9, and K_10, but displaying them has made my browser crash.</p> </li> <li> <p>You can toggle the Binary Words option to literally see what binary word corresponds to each node.</p> </li> <li> <p>You can toggle the Arrows to further understand the combinatorial interpretation. I interpret an arrow as shifting the parenthesis of a binary word once to the left. Thus, you'll notice the binary word with its parentheses all to the left have arrows point to it, and the binary word with its parentheses all to the right have arrows pointing from it. </p> </li> </ul>"},{"location":"associahedron/#motivation","title":"Motivation","text":"<p>I came across the associahedra during my undergraduate thesis, which involved obtaining a true understanding of Mac Lane's Coherence theorem and Mac Lane's proof, so that I could spell the whole thing out in a organized manner once and for all. This was actually due to a lack of finding satisfactory resources that explained Mac Lane's proof. Resources that attempted to explain it were either wrong, hand-waved, or simply copying Mac Lane's language without putting it in their own words. (I came to understand why this is, and it's because it's a very, very long, subtle, time consuming, but not terribly hard proof). It dawned on me that nobody who had taken the time to truly understand it wrote about their understanding. I thought that was unfortunate because it's a frequently cited result and it states a very important, powerful idea in category theory. </p> <p>Interestingly, in Mac Lane's proof, he does not mention at all the connection that his proof had with the associahedra. He most definitely knew there was a connection (I would bet that he had spoken with or knew of Jim Stasheff, who wrote about the associahedra, since they worked in related areas), but I do wonder why he didn't mention it.</p> <p>Here are some further things you can look at.</p> <ul> <li>My thesis A Coherent Proof of Mac Lane's Coherence Theorem. It is for anyone who ever needs to really understand each exact detail of Mac Lane's proof, or anyone who is just curious about the details. Here's a nice picture of K_5 that I produced in my thesis.</li> </ul> <p> </p> <ul> <li>The raw polyhedra face data of the associahedra in JSON format, as well as the python code that generated the data here. It was a quite a pain to generate and verify to be correct, so I hope if someone needs this data they can find it here before going through the trouble of generating it again.        </li> </ul>"},{"location":"category_theory/","title":"Category Theory","text":"<p>These are a set of notes on category theory I worked on for the latter two years as an undegraduate. </p> <p>This is the html version rendered via MathJax.  If you'd like the original LaTeX PDF version, see here.</p> <p>Category Theory is a very beautiful area of mathematics  that unites different areas of mathematics, in particular algebra and topology.  The field attempts to discover patterns and relationships between  seemingly different mathematical constructions, and it came about as a field when  algebraic topologists wanted to associate topological spaces with  algebraic groups, since this was useful, but they had no formal  way of describing this process.</p> <p>My goal with these notes was to read most of the classic texts in category theory and  then find the most intuitive way to explain the concepts that I learned for the  benefit of others. Many category theory texts are often quite cryptic and terse,  and I figured I could save people some time in understanding these concepts with these notes. </p> <p>I relied heavily on Categories for the Working Mathematician, the de facto category theory text  which is great but is extremely terse and challenging.</p>"},{"location":"category_theory/Abelian%20Categories/Abelian%20Categories/","title":"8.5. Abelian Categories","text":"<p>Let \\(\\cc\\) be a preabelian category, and consider an arbitrary morphism  \\(\\phi: A \\to B\\). Then, since we are in an abelian category, we can calculate  the kernel and cokernel of this morphism, which both have their familiar  universal properties. </p> <p> One thing we can do is examine both the kernel and the cokernel of these two morphisms. Specifically, we can calculate the kernel \\(\\ker(c)\\) of \\(c\\) and the cokernel  \\(\\coker(e)\\) of \\(e\\). However, since we have a map \\(\\phi: A \\to B\\) such that  \\(c \\circ \\phi = 0\\), we see that there exists a unique map \\(u: A \\to \\ker(\\coker(f))\\) such that \\(\\phi = e' \\circ u\\).  Dually, since \\(\\phi \\circ e = 0\\), there exists a unique map \\(v: \\coker(\\ker(f)) \\to B\\).  such that \\(\\phi = v \\circ c'\\).  \\ </p>"},{"location":"category_theory/Abelian%20Categories/Additive%20Categories/","title":"8.2. Additive Categories","text":"<p>Let \\(G\\) and \\(H\\) be abelian groups in Ab. A natural question to ask in any  given category is if a binary product such at \\(G \\times H\\) exists in the category. In our case,  the answer is yes; it is the direct sum \\(G \\oplus H\\). The direct sum satisfies the  universal property </p> <p> Here, \\(K\\) is a third group, \\(\\phi\\) and \\(\\psi\\) are arbitrary group homomorphisms, and \\(\\pi_G, \\pi_H\\) are the natural projection morphisms.  Interestingly, this object also satisfies the universal property  \\  ere \\(i_G\\) and \\(i_H\\) are the natural injections, e.g. \\(i_G(g) = g \\otimes e_H\\).  However, this implies that \\(G \\oplus H\\) is a coproduct! What this implies is  that \\textcolor{NavyBlue}{product and coproducts coincide in Ab}. This is actually a pretty remarkable property because this isn't the case even  in nice categories. For example, in Set, products and coproducts are  definitely distinct.</p> <p>\\begin{center} Why is this the case?  \\end{center}  Let \\(\\cc\\) be a preadditive category with a zero object \\(z\\).  Then for any objects \\(A, B \\in \\cc\\), the following are equivalent <ul> <li> <p>[\\((i)\\)] \\(A \\times B\\) exists </p> </li> <li> <p>[\\((ii)\\)] \\(A \\amalg B\\) exists </p> </li> </ul> <p>Moreover, there exists an isomorphism </p> \\[ \\prod_{i \\in \\lambda} A_i \\isomarrow \\coprod_{i \\in \\lambda}A_i \\] <p>for any objects \\(A_i \\in \\cc\\).  </p> <p> We only demonstrate one direction because the proof is self-dual.  <p>Suppose \\(A \\times B\\) exists. Then then if \\(C\\) is an object equipped  with morphisms \\(f: C \\to A\\) and \\(g: C \\to B\\), the following diagram  must hold.  \\  Equip \\(A\\) with the morphisms \\(1_A: A \\to A\\) and the unique zero morphism \\(\\emptyset_A^B: A \\to B\\). Then there exists a unique  \\(i_A: A \\to A \\times B\\) such that the diagram commutes.  \\  Symmetrically, equip \\(B\\) with the unique zero morphism \\(\\emptyset_B^A: B \\to A\\) and \\(1_B: B \\to B\\). Then there exists a unique \\(i_B: B \\to A\\times B\\) such that the  diagram commutes. \\  Now we'll demonstrate that we have a coproduct structure on our hands.  To do this, suppose we have an object \\(C\\) equipped with morphisms  \\(f: A \\to C\\) and \\(g: B \\to C\\). Then we can construct a morphism \\(h\\) such that the following  diagram commutes.  \\  Observe that \\(h = f\\circ \\pi_A + g \\circ \\pi_B\\) suffices, where  \\(+\\) is the group operation on the abelian group \\(\\hom(A \\times B, C)\\). Observe that </p> \\[\\begin{align*} h \\circ i_A &amp;= (f\\circ \\pi_A + g \\circ \\pi_B) \\circ i_A\\\\ &amp;= f \\circ (\\pi_A \\circ i_A) + g \\circ (\\pi_B \\circ i_A)\\\\ &amp;= f. \\end{align*}\\] <p>Similarly, </p> \\[\\begin{align*} h \\circ i_B &amp;= (f\\circ \\pi_A + g \\circ \\pi_B) \\circ 1_B\\\\ &amp;= f \\circ (\\pi_A \\circ 1_B) + g \\circ (\\pi_B \\circ 1_B)\\\\ &amp;= g. \\end{align*}\\] <p>Hence the commutativity of the above diagram holds; therefore, we see  that \\(A \\times B\\) is also a coproduct. Finally, recall that if two  distinct objects satisfy the same universal property, they are necessarily isomorphic;  therefore the existence of an isomorphism between the product and coproduct is immediate.   \\textcolor{NavyBlue}{The above proof is not hard, but it's also not trivial.  Moreover, there are three extremely important ingredients we utilized that demonstrate that  the assumptions we've made so far are actually necessary and useful.}</p> <ul> <li> <p>This proof does not hold for a category without a zero object because  there is not, in general, an obviously conceivable morphism to go from any two objects  \\(A\\) and \\(B\\). </p> </li> <li> <p>Notice that calculating \\(h\\) was only possible because we had an abelian  group operation. </p> </li> <li> <p>Finally, notice that we utilized bilinearity of the composition operator in order to  calculate \\(h \\circ i_A\\) and \\(h \\circ i_B\\) and thereby verify the universal property.</p> </li> </ul> <p>Therefore, all of our assumptions so far have been necessary and useful. And all of  this now motivates the following definition. </p> <p> Let \\(\\cc\\) be an abelian category. A biproduct of two objects \\(A, B\\) of \\(\\cc\\) is an object  \\(A \\otimes B\\) which is both a product and coproduct. \\ \\textcolor{Purple}{Equivalently}, A biproduct is an object \\(A \\oplus B\\)  equipped with morphisms  \\[\\begin{align*} &amp;\\pi_A: A \\oplus B \\to A &amp;&amp; i_A: A \\to A \\oplus B\\\\ &amp;\\pi_B: A \\oplus B \\to B &amp;&amp; i_B: B \\to A \\oplus B \\end{align*}\\] <p>such that </p> <ul> <li> <p>[1.] \\(\\pi_A \\circ i_A = 1_A\\)</p> </li> <li> <p>[2.] \\(\\pi_B \\circ i_B = 1_B\\)</p> </li> <li> <p>[3.] \\(i_A \\circ \\pi_A + i_B \\circ \\pi_B = 1_{A\\oplus B}\\) </p> </li> </ul> <p></p> <p> An Additive Category is a preadditive category \\(\\cc\\)  such that finite biproducts exist.  </p> <p> Consider the category \\(**Grp**\\).  </p>"},{"location":"category_theory/Abelian%20Categories/Kernels%20and%20Cokernels/","title":"8.4. Kernels and Cokernels","text":"<p>At this point we've discussed preadditive, additive, and preabelian categories, where  preabelian categories are just additive categories with the additional hypothesis  that kernels and cokernels exist. This additional hypothesis is extremely useful,  so we will demonstrate what this implies for us. </p> <p>Let \\(\\cc\\) be a preabelian category. Consider an arbitrary morphism \\(f: A \\to B\\).  One way to think about kernels and Cokernels is that they give rise to objects  in the comma categories \\((\\cc \\downarrow A)\\) and \\((B \\downarrow \\cc)\\). </p> <p> Now in the comma category \\((\\cc \\downarrow A)\\), a morphism between two objects  \\((C, f: C \\to A)\\) and \\((D, g: D \\to A)\\) is a morphism \\(h: D \\to C\\) in \\(\\cc\\) such that  \\(f = g \\circ h\\). Similarly, a morphism in the comma category \\((A \\downarrow \\cc)\\)  between two objects \\((P, m: A \\to P)\\) and \\((Q, n: A \\to Q)\\) is a morphism \\(k: P \\to Q\\) such that  \\(n = h \\circ m\\). These relations give rise to the bow-tie diagram: \\  ith that said, we can actually turn these categories into partial orders.  In \\((\\cc \\downarrow A)\\), we say \\(g \\le f\\) if there exists an \\(h\\) such that \\(f \\circ h = g\\),  and in \\((A \\downarrow \\cc)\\), we say \\(m \\le n\\) if there exists a \\(k\\) such that  \\(n = k \\circ m\\). </p> <p>It turns out that this perspective is actually quite useful. </p> <p> Let \\(\\cc\\) be a category with a zero object, equalizers and coequalizers.  Then for each object \\(A\\) of \\(\\cc\\), we have the functors  \\[\\begin{align*} \\ker&amp;: (A \\downarrow \\cc) \\to (\\cc \\downarrow A)\\\\ \\coker&amp;:(\\cc \\downarrow A) \\to (A \\downarrow \\cc). \\end{align*}\\] <p>that assign kernels and cokernels. Moreover, these functors establish a antitone Galois correspondence; hence we have that </p> \\[\\begin{align*} \\ker(\\coker(\\ker(f))) = \\ker(f)  \\quad  \\coker(\\ker(\\coker(f))) = \\coker(f). \\end{align*}\\] <p>Therefore, any \\(\\phi\\) is a kernel if and only if \\(\\phi = \\ker(\\coker(\\phi))\\), while  any \\(\\psi\\) is a cokernels if and only if \\(\\psi = \\coker(\\psi(\\psi))\\).  </p> <p> We demonstrate functoriality. First we want our functor  to act on objects as  \\[ (C, f: A \\to C) \\mapsto (\\ker(f), e_1: \\ker(f) \\to A).  \\] <p>Now we explain how the functor works on morphisms.  Suppose we have two objects of our  comma category \\((C, f: A \\to C)\\) and \\((D, g: A \\to D)\\), and that \\(h: D \\to C\\) is a morphism in \\((A \\downarrow \\cc)\\)  from \\((D, g: A \\to D)\\) to \\((C, f: A \\to C)\\). Then we have the diagram below. \\  Now note that </p> \\[ f \\circ e_2 = (h \\circ g) \\circ e_2 = h \\circ (g \\circ e_2) = 0. \\] <p>Thus, by the universal property of \\(e_1: \\ker(f) \\to A\\), we know there  exists a unique morphism \\(h': \\ker(g) \\to \\ker(f)\\) such that the diagram  below commutes. \\ </p> <p>However, this is exactly what it means to have a morphism between the objects  \\((\\ker(g), e_2: \\ker(g) \\to A)\\) and \\((\\ker(f), e_1: \\ker(f) \\to A)\\).  Thus we see that our functor maps on morphisms in \\((A \\downarrow \\cc)\\) in a nice  way:  </p> \\[ h  \\mapsto  h': (\\ker(g), e_2: \\ker(g) \\to A)  \\to  (\\ker(f), e_1: \\ker(f) \\to A). \\] <p>where \\(h'\\) is the unique map obtained from \\(h\\) as explained above.  With the remaining properties easily verified, this defines a functor between the categories.  In addition, we can dualize our work above to also get the functor  \\(\\coker: (\\cc \\downarrow A) \\to (A \\downarrow \\cc)\\). </p> <p>Now this creates a Galois correspondence by regarding the comma categories as  partially ordered sets. Suppose that \\(g \\le \\ker(f)\\). That is, there exists a \\(h\\) such that  \\(\\ker(f) \\circ h = g\\). Then we can compare \\(\\coker(g)\\) and \\(f\\) by considering the diagram below.  \\  Now observe that </p> \\[ f \\circ g = f \\circ (e \\circ h) = 0 \\circ h = 0. \\] <p>Therefore, by the universal property of the cokernel, we know there  exists a unique morphism \\(h': \\coker(g) \\to f\\) such that the diagram  below commutes. This then implies that \\(f \\le \\coker(g)\\).  \\  By a similar argument, we have that if \\(f \\le \\coker(g)\\),  then \\(g \\le \\ker(f)\\). Hence we have that </p> \\[ g \\le \\ker(f) \\iff f \\le \\coker(g) \\] <p>so that, as preorder, the kernel and cokernels functors are adjoint pairs  that form an antitone Galois correspondence. Moreover, this implies that for each \\(f: B \\to A\\) and \\(g: A \\to C\\),</p> \\[ f \\le \\coker(\\ker(f)) \\qquad g \\le \\ker(\\coker(g)). \\] <p>In particular, if \\(f\\) is the cokernel of some morphism \\(\\phi\\), and if \\(g\\) is the kernel of some morphism \\(\\psi\\), then we have that </p> \\[ \\coker(\\phi) \\le \\coker(\\ker(\\coker(\\phi))) \\quad \\ker(\\psi) \\le \\ker(\\coker(\\ker(\\psi))). \\] <p>However, applying the order reversing functors \\(\\coker\\) and \\(\\ker\\) on the relations  \\(\\phi \\le \\ker(\\coker(\\phi))\\) and \\(\\psi \\le \\coker(\\ker(\\psi))\\) yields </p> \\[ \\coker(\\ker(\\coker(\\phi))) \\le \\coker(\\phi) \\quad  \\ker(\\coker(\\ker(\\psi))) \\le \\ker(\\psi). \\] <p>Hence we have that \\(\\coker(\\ker(\\coker(\\phi))) \\cong \\coker(\\phi)\\) and \\(\\ker(\\coker(\\ker(\\psi))) \\cong \\ker(\\psi)\\) as desired.  </p>"},{"location":"category_theory/Abelian%20Categories/Preabelian%20Categories/","title":"8.3. Preabelian Categories","text":"<p>In Ab, kernels and cokernels exists for every group homomorphism. \\</p> <p>First, recall their definitions. </p> <p> Let \\(\\phi: G \\to H\\) be a group homomorphism. Then a kernel is  an equalizer of \\(\\phi: G \\to H\\) and \\(0: G \\to H\\), where \\(0\\) maps everything to \\(e_H\\),  while a cokernel is a coequalizer of \\(\\phi: G \\to H\\) and \\(0: G \\to H\\).  <p> </p> <p>In Ab, we set \\(\\coker(\\phi) \\cong H/\\im(\\phi)\\) while \\(\\ker(\\phi)\\) is the natural  normal subgroup of \\(G\\). </p> <p>Note that the necessary conditions for creating kernels and cokernels is  (1) the existence of a zero object and (2) the existence of equalizers. If we have these ingredients, can we extend the concept of kernels and cokernels to  additive categories? We can.</p> <p> Let \\(\\cc\\) be a category with a zero object as well as equalizers  and coequalizers. Let \\(f: A \\to B\\) be a morphism between two objects in \\(\\cc\\). We define <ul> <li> <p>kernel  to be the equalizer of \\(f\\) and \\(\\emptyset_A^B: A \\to B\\), the zero morphism,</p> </li> <li> <p>cokernel of \\(f\\) to be the coequalizer of \\(f\\) and \\(\\emptyset_A^B: A \\to B\\). </p> </li> </ul> <p>In diagrams, we have that  \\  end{definition}</p> <p> In the category Grp, we certainly have a zero object \\(z = \\{e\\}\\). Observe that for a given morphism \\(\\phi: G \\to H\\),  we can also form the equalizer of \\(\\phi\\) by considering the pair  \\((\\ker(\\phi), e: \\ker(\\phi) \\to G)\\) where \\(\\ker(\\phi) \\subset G\\) and \\(e\\) being inclusion. For the same morphism, we can form the coequalizer be considering the pair  \\((\\overline{N}, c: H \\to H/\\overline{N})\\) where  \\[ \\overline{N} = \\bigcap_{N \\in \\lambda} N \\] <p>where \\(\\lambda = \\{H' \\subset H \\mid \\im(\\phi) \\subset H' \\text{ and } H' \\normal H\\}\\).  It's a simple exercise to show that these satisfy the necessary universal properties. </p> <p>\\textcolor{NavyBlue}{However, it's important to observe the subtle difference between the behaviors  of Grp and Ab}. Because every subgroup of an abelian group  is normal, we know that in the case of Ab, \\(\\overline{N} = \\im(\\phi)\\) So the coequalizer becomes </p> \\[ (\\im(\\phi), c: H \\to H/\\im(\\phi)). \\] <p></p> <p>It turns out that kernels and cokernels are extremely flexible in additive categories.</p> <p> Suppose \\(\\cc\\) is an additive category. Then the following are equivalent.  <ul> <li> <p>[\\((i)\\)] \\(\\cc\\) has equalizers and coequalizers. </p> </li> <li> <p>[\\((ii)\\)] \\(\\cc\\) has kernels and cokernels.</p> </li> </ul> <p></p> <p> We only prove the statement for equalizers as the proof will be self-dual.  <p>First note that \\((i) \\implies (ii)\\) is immediate because a kernel is  an equalizer with a morphism \\(\\phi\\) and a zero morphism. To show \\((ii) \\implies (i)\\),  suppose that we have kernels for every morphism in \\(\\cc\\). Then consider two  morphisms \\(\\phi, \\psi: G \\to H\\). We can combine these two morphisms by our  group operation on \\(\\hom(G, H)\\) and consider \\(\\phi - \\psi\\). Since we can take kernels, we  take the kernel of this morphism. \\  We now argue that this is the equalizer of \\(\\phi, \\psi\\). First observe that </p> \\[ (\\phi - \\psi)\\circ e = 0 \\implies \\phi \\circ e - \\psi \\circ e = 0 \\implies \\phi \\circ e = \\psi \\circ e \\] <p>using bilinearity of \\(\\circ\\). Hence we see that \\(e\\) equalizes \\(\\phi\\) and \\(\\psi\\), although  we now need to demonstrate its universal property. </p> <p>Now suppose that there exists an object \\(K\\) equipped with a morphism \\(\\sigma: K \\to G\\) such  that \\(\\psi \\circ \\sigma = \\psi \\circ \\phi\\).  \\  However, note that </p> \\[ \\phi \\circ \\sigma = \\psi \\circ \\sigma  \\implies  (\\phi - \\psi)\\circ \\sigma = 0. \\] <p>Since \\(e: \\ker(\\phi) \\to G\\) is kernel, we note that its  universal property implies that because \\((\\phi - \\psi)\\circ \\sigma = 0\\)  that there must exists a unique morphism \\(u: K \\to \\ker(\\phi)\\) such that  \\(e \\circ u = \\sigma\\). Thus we have shown the diagram below  \\  must hold so that \\((\\ker(\\phi), e: \\ker(\\phi) \\to G)\\), is actually an equalizer! </p> <p>Note that we've once more utilized the bilinearity of \\(\\circ\\) to construct the above proof,  which again reminds us that the assumptions we've made so far are necessary and useful. The above proof now motivates  the following definition.</p> <p>\\begin{definition} Let \\(\\cc\\) be an additive category. Then we say \\(\\cc\\) is preabelian if it has kernels and cokernels; or, equivalently, if it has all equalizers and coequalizers.  </p> <p>What we have on our hands now is a very nice category where (1) finite biproducts  exist and (2) all equalizers and coequalizers exist. If we recall from our experience  with limits, this automatically grants us the following proposition. </p> <p> A preabelian category has all finite limits and finite colimits. </p> <p> If a category has finite products and equalizers, it has finite limits. If  it has finite coproducts and coequalizers, it has finite colimits. This is  Theorem \\ref{products_equalizers_all_limits}. </p> <p>The fact that there exist finite limits and colimits is extremely convenient  in preabelian categories. </p> <p> Let \\(\\cc\\) be a preabelian category. Let \\(J\\) be a connected category and suppose \\(F: J \\to \\cc\\) is a functor.  Then  \\[ \\Lim F \\cong \\Colim F.    \\] <p></p> <p> Recall the limit satisfies universal property  \\  for every object \\(C\\) equipped with a family of morphisms \\(f^i: C \\to F(i)\\).  Construct the family of morphisms  \\[ f_i^j =  \\begin{cases} \\emptyset_i^j: F(i) \\to F(j) &amp; \\text{if } i \\ne j\\\\ 1_{F(i)} &amp; \\text{if } i = j \\end{cases} \\] <p>where \\(\\emptyset_i^j: F(i) \\to F(j)\\) is the unique zero morphism from \\(F(i)\\) to \\(F(j)\\). Then by the universal property of the limit, for each \\(i \\in J\\), there  exists a unique morphism \\(h_i: F(i) \\to \\Lim F\\) such that the diagram below commutes.  \\  That is, we have \\(u^j \\circ h_i = f_i^j\\).  We now argue that we have a colimit on our hands. Specifically, suppose \\(D\\) is an  object of \\(\\cc\\) equipped with a family of morphisms \\(g_j: F(j) \\to D\\).  Then observe that we can supply a morphism </p> \\[ \\sum_{k \\in J}g_ku^k: \\Lim F \\to D \\] <p>where the addition operation is from the group structure of \\(\\hom(\\Lim F, D)\\),  such that the diagram below commutes.  \\  This diagram commutes since </p> \\[\\begin{align*} \\left( \\sum_{k \\in J}g_ku^k \\right) \\circ h_j = \\sum_{k \\in J}g_k(u^k \\circ h_j) = g_j(u^j \\circ h_j) = g_j  \\end{align*}\\] <p>where we utilized the bilinearity of the composition operator. Thus we see that \\(\\Lim F\\) is behaving just like a colimit! </p> <p>The only thing we  must verify at this point is that this morphism is unique. Towards that goal,  suppose that \\(\\ell: \\Lim F \\to D\\) is another morphism such that  \\(\\ell \\circ h_j = g_j\\). Recall that \\(u^i\\circ h_i= 1_{F(i)}\\),  so that \\(h_i\\) is a monomorphism. Then observe that we can take  the image of the map </p> \\[ h_i: F(i) \\to \\Lim F   \\] <p>under the contravariant hom functor to get an epic group homomorphism  \\  between abelian groups, as \\(\\circ\\) obeys bilinearity  properties. By the first isomorphism theorem we then have that </p> \\[ \\hom(F(i), D) \\cong \\hom(\\Lim F, C)/\\ker(\\circ h_i). \\] <p>Now we want to show that this map is also injective, because then  we could observe that since</p> \\[ \\left(\\ell - \\sum_{k \\in J}g_k \\circ u^k\\right)\\circ h_i = 0 \\] <p>that </p> \\[ \\ell - \\sum_{k \\in J}g_k \\circ u^k = 0. \\] <p>But it seems like we don't have enough to show that at the moment...</p> <p></p>"},{"location":"category_theory/Abelian%20Categories/Preadditive%20Categories/","title":"8.1. Preadditive Categories","text":"<p>Consider two abelian groups \\((G, +)\\) and \\((H, \\cdot)\\) of Ab.  Recall from group theory that we can turn the set \\(\\hom(G, H)\\) into an abelian group \\((\\hom(G, H), *)\\) as follows. Given \\(\\phi, \\psi: G \\to H\\), we can create another  group homomorphism \\(\\phi * \\psi: G \\to H\\) where </p> \\[\\begin{align*} (\\phi * \\psi)(g) = \\phi(g) \\cdot \\psi(g). \\end{align*}\\] <p>Observe that this is in fact a group homomorphism: if \\(g, g' \\in G\\), then </p> \\[\\begin{align*} (\\phi * \\psi)(g + g') &amp;= \\phi(g + g') \\cdot \\psi(g + g')\\\\ &amp;=\\phi(g) \\cdot \\phi(g') \\cdot \\psi(g) \\cdot \\psi(g')\\\\ &amp;\\textcolor{Red}{=} \\phi(g) \\cdot \\psi(g) \\cdot \\phi(g') \\cdot \\psi(g')\\\\ &amp;= (\\phi * \\psi)(g) \\cdot (\\phi *\\psi)(g'). \\end{align*}\\] <p>In the third step we utilized the fact that \\((H, \\cdot)\\) is abelian.  Thus \\((\\hom(G, H), *)\\) is not necessarily a group unless \\(H\\) is an  abelian group. Therefore, this construction doesn't extend to Grp.</p> <p>At this point, your category-theory-voice in your head is probably asking: \\begin{center} \\begin{minipage}{0.8\\textwidth} \\textcolor{NavyBlue}{If \\(H\\) is an abelian group, can we create a functor \\(F_H: **Ab** \\to **Ab**\\)  where \\(G \\mapsto \\hom(G, H)\\)?} \\end{minipage} \\end{center}The answer is yes; the functor is actually contravariant, for suppose we have a group homomorphism</p> \\[ \\phi: G \\to G'. \\] <p>Then define the function</p> \\[ F_H(\\phi): \\hom(G', H) \\to \\hom(G, H)  \\] <p>where \\begin{statement}{NavyBlue!10}</p> \\[ F_H(\\phi)(\\psi: G' \\to H) = \\psi \\circ \\phi: G \\to H. \\] <p>\\end{statement} To verify functoriality, we have to check that this function is actually a group  homomorphism. Towards that goal, consider \\(\\psi, \\sigma: G \\to H\\). Then  observe that for any \\(g \\in G\\),</p> \\[\\begin{align*} F_H(\\phi)(\\psi + \\sigma)(g) &amp;= \\phi(\\psi(g) + \\sigma(g))\\\\ &amp;= \\phi(\\psi(g)) + \\phi(\\sigma(g))\\\\ &amp;= F_H(\\phi)(\\psi)(g) + F_H(\\phi)(\\psi)(g) \\end{align*}\\] <p>which verifies that \\(F_H(\\phi)\\) is a group homomorphism. Therefore, we see that  \\(F_H: **Ab** \\to **Ab**\\) is in fact a functor.</p> <p>Now your category-theory-voice should be asking:  \\begin{center} \\begin{minipage}{0.8\\textwidth} \\textcolor{NavyBlue}{If \\(G\\) is an abelian group, can we also create  a functor \\(F^G: **Ab** \\to **Ab**\\) where \\(H \\mapsto \\hom(G, H)\\)?} \\end{minipage} \\end{center}ne can easily show that the answer is yes. In this direction, the functor is covariant. That  is, for \\(\\psi: H \\to H'\\), we have that </p> \\[ F^G(\\psi): \\hom(G, H) \\to \\hom(G, H') \\] <p>where  \\begin{statement}{NavyBlue!10}</p> \\[ F^G(\\psi)(\\phi: G \\to H) = \\psi \\circ \\phi: G \\to H'. \\] <p>\\end{statement} Note that for our functors, we have that</p> \\[ F_H(G) = F^G(H). \\] <p>This is bifunctor-ish. Therefore, our category theory voice is now  asking:  \\begin{center} \\begin{minipage}{0.8\\textwidth} \\textcolor{NavyBlue}{Do we have a bifunctor  \\(F: **Ab**\\times **Ab** \\to **Ab**\\) on our hands, where  \\(F(G, H) = \\hom(G, H)\\)? } \\end{minipage} \\end{center}o see if this answer is true, we ought to be able to show that, given  \\(\\phi: G' \\to G\\) and \\(\\psi: H \\to H'\\), the diagram  \\  is commutative. The above diagram is in fact commutative since function composition  is associative! That is, given \\(\\sigma: G \\to H\\), observe that going right and then down  gives</p> \\[\\begin{align*} \\psi \\circ (\\sigma \\circ \\phi)  \\end{align*}\\] <p>while going down and then right gives </p> \\[\\begin{align*} (\\psi \\circ \\sigma) \\circ \\phi. \\end{align*}\\] <p>Hence we have commutativity of the above diagram, and we therefore have a  true bifunctor \\(F: **Ab**\\times**Ab** \\to **Ab**\\) where </p> \\[ F(G,H) = \\hom(G, H). \\] <p>\\textcolor{NavyBlue}{What this really shows is that \\(\\hom(-, -)\\) is a functor; specifically, a bifunctor.  So while we typically think of \\(\\hom(G, H)\\) as a set, it had hidden functorial properties.  Thus what makes Ab special is that plugging in abelian groups outputs an  abelian group, and this is not the case with other constructions (e.g. Grp).}</p> <p>Let us now consider a new observation of \\(**Ab**\\). For any triple of abelian groups</p> \\[ (G, \\star), (H, +), (K, \\cdot) \\] <p>we can create abelian groups </p> \\[\\begin{align*} &amp;\\big(\\hom(G, H), +'\\big) &amp;&amp;(\\phi_1 +' \\phi_2)(g) = \\phi_1(g) + \\phi_2(g) \\\\ &amp;\\big(\\hom(H, K), \\cdot'\\big) &amp;&amp;(\\psi_1 \\cdot' \\psi_2)(h) = \\psi_1(h)\\cdot \\psi_2(h)  \\\\ &amp;\\big(\\hom(G, K), *\\big) &amp;&amp;(\\sigma_1 * \\sigma_2)(g)= \\sigma_1(g) \\cdot \\sigma_2(g) \\\\ \\end{align*}\\] <p>where \\(\\phi_i \\in \\hom(G, H), \\psi_i \\in \\hom(H, K)\\) and \\(\\sigma_i \\in \\hom(G, K)\\)  for \\(i = 1, 2\\). Now since these are abelian groups in Ab, there is a composition operator </p> \\[ \\circ: \\hom(G, H)\\times \\hom(H,K) \\to \\hom(G, K) \\] <p>where \\(\\circ(\\phi: G \\to H, \\psi: H \\to K ) \\mapsto \\psi \\circ \\phi: G \\to K\\).  However, we now run into a problem where our operators might not play nicely with each other. Specifically, is  it true that </p> \\[ \\psi \\circ (\\phi_1 +' \\phi_2) = (\\psi \\circ \\phi_1) * (\\psi \\circ \\phi_2) \\] <p>or </p> \\[ (\\psi_1 \\cdot' \\psi_2) \\circ \\phi = (\\psi_1 \\circ \\phi) * (\\psi_2 \\circ \\phi)? \\] <p>For the first case, the answer is yes. Observe that</p> \\[\\begin{align*} \\psi \\circ (\\phi_1 +' \\phi_2)(g) &amp;= \\psi(\\phi_1(g) + \\phi_2(g))\\\\ &amp;= \\psi(\\phi_1(g) + \\phi_2(g))\\\\ &amp;= \\psi(\\phi_1)(g) \\cdot \\psi(\\phi_2)(g)\\\\ &amp;= \\big((\\psi \\circ \\phi_1) * (\\psi \\circ \\phi_2)\\big)(g). \\end{align*}\\] <p>\\textcolor{NavyBlue}{The reason we have linearity here is because of the way we defined  the group operations on the homsets. The definition of these operations  is intuitively correct, but we get accidentally get an extra bonus of obtaining linearity  so that we don't have to worry about the above equations not holding.}</p> <p>In order to mimic this behavior, we abstract this into a category to define  a Ab-category. </p> <p> An Ab-category or Preadditive Category is a  category \\(\\mathcal{C}\\) such that, for each pair of objects \\(A, B\\),  there exists an abelian group operation \\(+\\) on the set \\(\\hom(A, B)\\) such  that  \\[\\begin{align*} &amp;\\circ: \\hom(A, B)\\times \\hom(B, C) \\to \\hom(A, C)\\\\ &amp;(f, g) \\mapsto g \\circ f \\end{align*}\\] <p>is bilinear. What we mean by bilinear is that, given morphisms \\(f, g: A \\to B\\) and \\(h, k: B \\to C\\),  we have that  \\begin{statement}{Red!10}</p> \\[\\begin{align} (h + k) \\circ f = h \\circ f + k \\circ f\\\\ h \\circ (g + f) = h \\circ g + h \\circ f. \\end{align}\\] <p>\\end{statement} </p> <p>\\textcolor{NavyBlue}{Note that since we demand that \\(\\hom_{\\cc}(A, B)\\) always  be a group, we see that any category such that \\(\\hom_{\\cc}(A, B) = \\varnothing\\)  can never be an abelian group. A group always requires the existence of an identity;  a demand that an empty set can never meet}. Therefore, as an example, any discrete  category cannot be a preadditive category because all of the nontrivial homsets  are empty. </p> <p>As we demonstrated building up to this definition, Ab is a trivial example  of a preadditive category. A less trivial example is \\(**Vect**_K\\) where \\(K\\) is a field, but this is nearly automatic since this takes advantage  of the fact that vector spaces have their own hidden abelian group structure. </p> <p> Suppose \\(\\cc\\) is a one object category \\(R\\) which is also preadditive. Then  this means that we have two binary operations \\(+\\) and \\(\\circ\\) on the  abelian group \\(\\hom_{\\cc}(R, R)\\)  such that  \\[\\begin{align*} (h + k) \\circ f = h \\circ f + k \\circ f\\\\ h \\circ (g + f) = h \\circ g + h \\circ f. \\end{align*}\\] <p>However, this is simply a ring! The addition is the ring addition, while the  ring multiplication is given by composition. Conversely, a ring regarded as the homset of a  one object category can be defined to be an abelian category. This is because  when regarding a group as a one object category, the group operation becomes the  composition operation. Thus adding the extra axiom of an addition bilinear operation  grants us that the category is preadditive. </p> <p> Let \\(\\cc\\) be a preadditive category. Then  \\(\\cc\\op\\) is also a preadditive category. To demonstrate this, we know that  every pair of objects \\(A, B \\in \\cc\\) gives rise to a group \\((\\hom_{\\cc}(A, B), +)\\) for some operation \\(+\\). This allows us to place a group structure \\(+'\\) on  \\(\\hom_{\\cc\\op}(B, A)\\) where for two \\(f\\op, g\\op: B \\to A\\) in \\(\\cc\\op\\), \\[ f\\op +' g\\op = (f + g)\\op. \\] <p>That is, we rely on the preexisting group operation \\(+\\) from  \\(\\hom_{\\cc}(A, B)\\).  Given that the composition operator of \\(\\cc\\op\\) is \\(\\circ\\op\\), we can check that  this satisfies the bilinearity conditions of \\(\\circ\\op\\). Suppose \\(h\\op, k\\op : B \\to A\\) are two morphisms in \\(\\hom(B, A)\\) which  are composable with some \\(f\\op\\). Then </p> \\[\\begin{align*} (h\\op +' k\\op)\\circ\\op f\\op  =  (h + k)\\op \\circ \\op f\\op &amp;=  f \\circ (h + k)\\\\ &amp;= f \\circ h + f \\circ k \\\\ &amp;= h\\op \\circ\\op f\\op +' k\\op \\circ\\op f\\op.  \\end{align*}\\] <p>The other direction can be verified dually, so that the the group operation  \\(+'\\) distributes bilinearly over \\(\\circ\\op\\). Therefore, \\(\\cc\\op\\) is a preadditive  category. </p> <p> If \\(\\cc\\) is preadditive, then the functor category \\(\\cc^J\\) is preadditive. To demonstrate this, consider the hom-set \\(\\hom_{\\cc^J}(F, G)\\) between two  functors \\(F, G: J \\to \\cc\\). Now consider two natural transformations \\(\\eta, \\epsilon \\in \\hom_{\\cc^J}(F, G)\\). Then  for each \\(f \\in \\hom_{\\cc}(A, B)\\), the familiar diagram commutes.  \\  This diagram tells us that \\(G(f) \\circ \\eta_A  = \\eta_B \\circ F(f)\\)  and that \\(G(f) \\circ \\epsilon_A  = \\epsilon_B \\circ F(f)\\). However, since  \\(\\cc\\) is abelian, we can combine these morphisms and add both equations  to get  \\[ G(f) \\circ \\eta_A + G(f) \\circ \\epsilon_A = \\eta_B \\circ F(f) + \\epsilon_B \\circ  F(f)  \\implies G(f) \\circ (\\eta_A + \\epsilon_A) = (\\eta_B + \\epsilon_B) \\circ F(f). \\] <p>Hence the diagram below  \\  commutes. Therefore, using the group product of \\((\\hom_{\\cc}(F(A), F(B)), +)\\), we've derived a new natural transformation from \\(F\\) to \\(G\\) using \\(\\eta\\) and \\(\\epsilon\\)  in \\(\\hom_{\\cc^J}(F, G)\\). This allows us to endow the homset \\(\\hom_{\\cc^J}(F, G)\\)  with the operation \\(+'\\)  defined so that for two \\(\\eta, \\epsilon \\in \\hom_{\\cc^J}(F, G)\\), \\(\\eta + ' \\epsilon\\) is the natural transformation where  for each object \\(A\\)</p> \\[ (\\eta +' \\epsilon)_A = \\eta_A + \\epsilon_A \\] <p>where \\(+\\) is the group operation on \\((\\hom_{\\cc}(F(A), G(A)), +)\\). The fact that  this distributes bilinearly over the composition operator is inherited from  \\(\\cc\\), and can easily be verified, so that \\(\\cc^J\\) is preadditive. </p> <p> Let \\(\\cc\\) be a category such that for every pair of objects  \\(A, B\\), the hom set \\(\\hom_{\\cc}(A, B)\\) is nonempty. Then we can create the category  \\(\\text{PreAdd}(\\cc)\\) where the objects are the same as \\(\\cc\\), except each \\(\\hom_{\\text{PreAdd}(\\cc)}(A, B)\\) is now regarded as the free  abelian group generated by the elements of \\(\\hom_{\\cc}(A, B)\\). This results  in a preadditive category if we force the composition operator \\(\\circ'\\) in \\(\\text{PreAdd}(\\cc)\\) to be bilinear. This forcing makes sense in our case since,  if \\(\\sum_{f \\in \\hom_{\\cc}(A,B)}n_f f, \\sum_{f \\in \\hom_{\\cc}(A,B)}n'_f f\\)  are two arbitrary elements in \\(\\hom_{\\text{PreAdd}(\\cc)}(A, B)\\),  then if \\(\\sum_{k \\in \\hom_{\\cc}(B,C)}m_k k \\in \\hom_{\\text{PreAdd}(\\cc)}(B, C)\\) for some object \\(C\\),  where  \\(n_f, n'_f, m_k\\) are all nonzero for finitely many integers, then  \\[\\begin{align*} &amp;\\sum_{k \\in \\hom_{\\cc}(B,C)}m_k k \\circ' \\left( \\sum_{f \\in \\hom_{\\cc}(A,B)}n_f f + \\sum_{f \\in \\hom_{\\cc}(A,B)}n'_f f \\right)\\\\ &amp;=  \\sum_{f \\in \\hom_{\\cc}(A,B)}\\sum_{k \\in \\hom_{\\cc}(B, C)}n_f \\cdot m_k(k \\circ f)  + \\sum_{f \\in \\hom_{\\cc}(A,B)}\\sum_{k \\in \\hom_{\\cc}(B, C)}n'_f \\cdot m_k(k\\circ f) \\end{align*}\\] <p>and the above last expression is in fact an element of \\(\\hom_{\\text{PreAdd}(\\cc)}(A, C)\\).  </p>"},{"location":"category_theory/Adjunctions./Adjoints%20on%20Preorders./","title":"4.4. Adjoints on Preorders.","text":"<p>Interesting things happen when one applies adjoint concepts to functors between preorders; ones which preserve order in a special way. It's actually often the case where we have two mathematical structures involving chains of arrows which reverse when transferring between one and the other. We give such a concept a definition first, before introducing a theorem about such structures. </p> <p> Let \\(\\pp\\) and \\(\\qqq\\) be two preorders. If there exists functors  \\(F: \\pp \\to \\qqq\\) and \\(G:\\qqq \\to \\pp\\) such that  \\[ F(P) \\le Q \\iff P \\le G(Q),   \\] <p>That is, there exists \\(f:F(P) \\to Q\\) if and only if there exists \\(g: P \\to G(Q)\\), then \\(F\\) and \\(G\\) are called a monotone Galois connection. On the other hand,  if we have that </p> \\[ F(P) \\le Q \\iff P \\ge G(Q) \\] <p>then \\(F\\) and \\(G\\) are called a \\textbf{antitone Galois connection}.  </p> <p> Let \\(\\mathcal{P}, \\mathcal{Q}\\) be two preorders, and suppose \\(F: \\mathcal{P} \\to \\mathcal{Q}\\op\\) and \\(G:\\mathcal{Q}\\op \\to \\mathcal{P}\\) are two order preserving  functors. Then \\(F\\) is left adjoint to \\(G\\) if and only if for all \\(P \\in \\mathcal{P}\\) and \\(Q \\in \\mathcal{Q}\\) \\[ F(P) \\ge Q \\iff P \\le G(Q). \\] <p>Given such an adjunction, we then have that our unit establishes \\(P \\le G(F(P))\\) and the counit establishes \\(F(G(Q)) \\le Q\\).  </p> <p> Observe that if \\(F\\) is left adjoint to \\(G\\), then we have the  bijection  \\[ \\hom_{\\mathcal{Q}\\op}(F(P), Q) \\cong \\hom_{\\mathcal{P}}(P, G(Q) ) \\] <p>which gives rise to the desired correspondence; on the other hand, such a bijection gives rise to an adjunction.  With such an adjunction, we know that for each \\(P, Q\\), there exist morphisms \\(\\eta_P: P \\to G(F(P))\\) and \\(\\epsilon_Q: F(G(Q)) \\to Q\\). Hence \\(P \\le G(F(P))\\)  and \\(F(G(Q)) \\ge Q\\).  </p> <p>The above theorem came out of the observation that there is a connection between fields, their subfields, and their groups of automorphisms, an observation which arises in Galois Theory. The goal of Galois Theory is to understand polynomials and their roots; when they can be factorized, when and where we can find their roots. The study of Galois groups is now used widely in number theory. For example, part of Andrew Wiles' work in proving Fermat's Last Theorem involved Galois representations.</p> <p>It was this theorem, rooted in Galois Theory, that motivated the Theorem 4.\\ref{galois_connections} at the beginning of this section.  The Fundamental Theorem of Galois Theory is simply a stronger, special case, since in this case, the functors are literally inverses of each other. The theorem we introduced, however, simply requires the functors to be adjoints of one another. </p> <p> Let \\(U, V\\) be sets, and observe that their power sets \\(\\mathcal{P} (U)\\) and \\(\\mathcal{P}(V)\\) form categories; specifically, preorders, ordered by set inclusion.  <p>Suppose \\(f: U \\to V\\) is a function in Set. Then \\(f\\) induces a functor \\(f_*: \\mathcal{P}(U) \\to \\mathcal{P}(V)\\), where </p> \\[ f_*(X) = \\{f(x) \\mid x \\in X\\}. \\] <p>Note that if \\(X\\subset X'\\), then \\(f_*(X) \\subset f_*(X')\\). Hence this is an order-preserving functor. Now observe that \\(f\\) also induces a functor \\(f^*: \\mathcal{P}(V) \\to \\mathcal{P}(U)\\) where </p> \\[ f^*(Y) = \\{x \\mid f(x) \\in Y\\}. \\] <p>Note that this also preserves order. In addition, we have that if  \\(f_*(X) \\le Y\\), then this holds if  and only if \\(f(X) \\subset Y\\). We then have that this holds if and only if \\(X \\subset f_*(Y)\\), Hence we have a Galois connection, so that we may apply  Theorem 4.\\ref{galois_connections} to conclude that \\(f_*\\) is left adjoint to \\(f^*\\). </p>"},{"location":"category_theory/Adjunctions./Equivalence%20of%20Categories/","title":"4.3. Equivalence of Categories","text":"<p>In an ideal world, if we have a category of which we are interested in, our goal would be to find an isomorphism between it and a category of which we understand very well. We then know that certain mathematical structures are invariant between transitioning between the two, so that we could better understand our desired category. </p> <p>However, this is generally too much to ask for. Many categories which are constructed are constructed in such a way that they're not isomorphic to anything we're familiar with; if they were, then they probably wouldn't be interesting. Hence we have a more useful notion of equivalence between categories. </p> <p> Let \\(F : \\cc \\to \\dd\\) be a functor. We say that \\(\\cc\\) is equivalent to \\(\\dd\\) if there exists a functor \\(G:\\dd \\to \\cc\\) and natural isomorphisms $\\eta: I_\\cc \\to G \\circ F $ and \\(\\epsilon: F \\circ G \\to I_\\dd\\). <p>In this case, we say both \\(F\\) and \\(G\\) are an equivalence of categories. </p> <p> Let \\(X\\) and \\(Y\\) be sets, and regard them as discrete categories. Then a functor \\(F: X \\to Y\\) is just a function between sets. In this case, to say  that \\(X\\) and \\(Y\\) are equivalent is if there exists a functor (function!)  \\(G: Y \\to X\\) such that we have natural isomorphisms  \\(\\eta_x: x \\to G(F(x))\\) and \\(\\epsilon_x: F(G(x)) \\to x\\). However,  each category has nontrivial morphisms; hence we see that each of  these must be identity morphisms so that  \\[ G(F(x)) = x \\qquad F(G(x)) = x. \\] <p>What this then means is that an equivalence of categories  for sets is just a pair of invertible functions. That is,  it gives rise to an isomorphism.  </p> <p>Since \\(\\eta, \\epsilon\\) are already natural transformations, this simply makes them natural isomorphisms. It turns out that the notion of equivalence is more useful than of an isomorphism. An isomorphism is just too much to ask, but equivalence does give us nice invariants too. </p> <p> A adjoint equivalence between categories \\(C\\) and \\(D\\) is an adjunction \\((F, G, \\eta, \\epsilon)\\) where the unit and counit \\(\\eta\\) and \\(\\epsilon\\) are natural isomorphisms. </p> <p>It turns our an adjoint equivalence is the same thing as an equivalence between categories. But before we move on, we prove a lemma and a proposition.</p> <p> Let \\(\\cc\\) be a category, and \\(f: A \\to B\\) a morphism. Then  \\(f\\) induces a natural transformation  \\[ f^*: \\hom_{\\cc}(C, -) \\to \\hom_{\\cc}(C', -) \\] <p>Then \\(f^{*}\\) is a monomorphism if and only if \\(f\\) is an epimorphism,  and \\(f^{*}\\) is an epimorphism if and only if \\(f\\) is a split monomorphism  (that is, if and only if \\(f\\) has a left-inverse.) </p> <p> \\begin{description} \\item[\\(\\bm{\\implies}\\)] Observe that \\(\\hom_{\\cc}(C, -) \\to \\cc \\to **Set**\\) is a functor. Then \\(f^*: \\hom_{\\cc}(C, -) \\to \\hom_{\\cc}(C', -)\\) is a natural transformation where \\(f: C' \\to C\\). Now suppose \\(\\eta, \\eta': F \\to \\hom_{\\cc}(C, -)\\), where \\(F: \\cc \\to **Set**\\) is a functor, are natural transformations. Then if \\(f^*\\) is monic,  \\[   f^* \\circ \\eta = f^* \\circ \\eta' \\implies \\eta = \\eta'. \\] <p>Now let \\(h: A \\to A'\\) be a morphism in \\(\\cc\\). Then we have the commutative diagram </p> <p> where we denote \\(\\eta_A, \\eta'_{A}\\) on the arrow to signify the fact that both \\(\\eta_A, \\eta_A'\\) are morphisms from \\(F(A)\\) to \\(\\hom_{\\cc}(C, A)\\). Now  let \\(x \\in F(A)\\). Then</p> \\[ f^* \\circ \\eta_A(x) = f^* \\circ \\eta'_A(x) \\iff \\eta_A(x) \\circ f = \\eta'_A(x) \\circ f. \\] <p>But if \\(f\\) is monic, then \\(f^* \\circ \\eta_A(x) = f^* \\circ'_A(x)\\) implies that \\(\\eta_A = \\eta'_A\\). Hence we see that \\(\\eta_A(x) \\circ f = \\eta'_A(x) \\circ f \\implies \\eta_A(x) = \\eta'_A(x).\\)</p> <p>\\item[\\(\\bm{\\impliedby}\\)] Now suppose \\(f\\) is epic. Then using the same notation as earlier, note that </p> \\[ f^* \\circ \\eta_A(x) = f^* \\circ \\eta'_A(x) \\iff \\eta_A(x) \\circ f = \\eta'_A(x) \\circ f \\implies \\eta_{A} = \\eta_{A}. \\] <p>Hence we see that \\(f^*\\) is a monomorphism.  \\end{description}  Taking the dual of what we proved, we prove the second part of the lemma. Now we'll use this lemma in the theorem below, one which will be very useful.</p> <p> Let \\((F, G, \\eta, \\epsilon)\\) be an adjunction between categories \\(\\cc\\) and \\(\\dd\\). Then  \\begin{description} \\item[\\(\\bm{(i)}\\)] \\(G\\) is faithful if and only if for each \\(D \\in \\dd\\), \\(\\epsilon_{D}\\) is epic  \\item[\\(\\bm{(ii)}\\)] \\(G\\) is full if and only if every \\(\\epsilon_{D}\\) is split monic.  \\end{description} Therefore, \\(G\\) is full and faithful if and only if \\(\\epsilon_D\\) is an isomorphism between \\(F(G(D))\\) and \\(D\\).  </p> <p> If \\(G: \\dd \\to \\cc\\) is a functor, then we see that \\(G\\) itself becomes a natural transformation between the two functors:  \\[ G_{D,-}: \\hom_{\\dd}(D, -) \\to \\hom_{\\dd}(G(D), G(-)).     \\] <p>Recall that we have an adjunction given by \\(F, G\\). Then there exists a bijection \\(\\phi\\) where </p> \\[ \\phi_{C, D'}: \\hom_{\\cc}(F(C), D') \\to \\hom_{\\dd}(C, G(D)). \\] <p>Thus \\(\\phi^{-1}: \\hom_{\\dd}(C, G(D)) \\to \\hom_{\\dd}(F(C), D')\\). Moreover, if \\(D\\) is an arbitrary object, this becomes a natural transformation between the two functors: </p> \\[ \\phi^{-1}_{C, -}: \\hom_{\\dd}(C, G(-)) \\to \\hom_{\\cc}(F(C), -). \\] <p>Let \\(C = G(D)\\). Then we have the following sequence of natural transformations: \\  Composing the natural transformations, we finally obtain a natural transformation  \\(\\phi^{-1}_{G(D), G(-)} \\circ G_{D, -} : \\hom_{\\dd}(D, -) \\to \\hom_{\\dd}(F(G(D)), -)\\). How is this natural transformation given? We can assign \\(-\\) as \\(D\\) itself, and see what happens when we consider the identity morphism \\(1_D: D \\to D\\). In this case</p> \\[ \\phi^{-1}_{G(D), G(D)} \\circ G_{D, D}(1_D)  =  \\phi^{-1}_{G(D), G(D)}(1_{G(D)}) =  \\epsilon_{D} \\] <p>by definition of the counit \\(\\epsilon_D\\). Now we understand how this poorly-notated natural transformation works! In general, for and \\(f: D \\to D'\\), we see that </p> \\[\\begin{align} \\phi^{-1}_{G(D), G(D')} \\circ G_{D, D'}(f) = f \\circ \\epsilon_{D}. \\end{align}\\] <p>Thus, we see that this natural transformation is in disguise; it's actually just \\(\\epsilon_D^*: \\hom_{\\dd}(D, -) \\to \\hom_{\\dd}(F(G(D), -)\\)! \\begin{description} \\item[\\(\\bm{(i)}\\)] \\begin{description} \\item[\\(\\bm{\\iff}\\)] If \\(G\\) is faithful, then the natural transformation in equation (7) is one to one. This makes \\(\\epsilon_D^*\\) a monomorphism. By the previous lemma, this holds if and only if \\(\\epsilon_D\\) is epic for every \\(D\\) in \\(\\dd\\).</p> <p>\\end{description}  \\item[\\(\\bm{(ii)}\\)] \\begin{description} \\item[\\(\\bm{\\iff}\\)] On the other hand, if \\(G\\) is full, then this natural transformation in equation (7) surjective. This makes \\(\\epsilon_{D}^*\\) an epimorphism, and by the previous lemma, that holds if and only if \\(\\epsilon_D\\) is a split monomorphism.  \\end{description}  \\end{description} </p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor. Then the following are equivalent.  \\begin{description} \\item[\\(\\bm{(i)}\\)] \\(G\\) is an equivalence of categories  \\item[\\(\\bm{(ii)}\\)] \\(G\\) is part of an adjunction \\((F, G, \\eta, \\epsilon)\\) where \\(\\eta, \\epsilon\\) are natural isomorphisms  \\item[\\(\\bm{(iii)}\\)] \\(F\\) is full and faithful, and each object \\(C\\) is isomorphic to \\(G(D)\\) for  some object \\(D\\).  \\end{description}   Note that this theorem is symmetric; one could interchange \\(G\\) with \\(F\\), and then obtain the same exact results. \\textcolor{MidnightBlue}{Thus, one  way of stating this theorem is that \\(\\cc\\) and \\(\\dd\\) are equivalent as categories if and only if there exits full and faithful functors \\(F: \\cc \\to \\dd\\) and \\(G: \\dd \\to \\cc\\); or if and only if \\(F, G\\) form an adjoint equivalence.}</p> <p> \\begin{description} \\item[\\(\\bm{(i) \\implies (iii)}\\)]  Suppose we have an equivalence of categories given by \\(F: \\cc \\to \\dd\\) and \\(G: \\dd \\to \\cc\\), with natural isomorphisms  \\[ \\phi: F \\circ G \\cong I_\\dd \\qquad \\psi: G \\circ F \\cong I_\\cc. \\] <p>Let \\(f: C \\to C'\\) be a morphism in \\(\\cc\\). Then observe that the following diagram \\  is commutative. In an equations, we have that \\(f = \\psi_C' \\circ G(F(f)) \\circ \\psi_{C'}^{-1}\\). Thus suppose that \\(f_1, f_2: C \\to C'\\) are two morphisms such that \\(F(f_1) =F(f_2)\\). Then we get a pair of commutative diagrams, similar to the ones above, which translate into the equations </p> \\[ f_1 = \\psi_{C}'\\circ G(F(f_1)) \\circ \\psi_{C'}^{-1}   \\qquad  f_2 = \\psi_{C'}\\circ G(F(f_2)) \\circ \\psi_{C'}^{-1}. \\] <p>Then if \\(F(f_1) = F(f_2)\\), the above equations guarantee that \\(f_1 = f_2\\). Hence we see that \\(F\\) is a faithful functor. Since the statement is symmetric in both \\(F\\) and \\(G\\), we have also that \\(G\\) is faithful.</p> <p>To show that \\(F\\) is full, suppose there exists a morphism \\(h: F(C) \\to F(C')\\) for a pair of objects \\(C, C'\\). Let \\(f = \\psi_{C'} \\circ G(h) \\circ \\psi_C\\). Then we have the commutative squares \\  and hence we have that \\(G(h) = G(F(f))\\). But since \\(G\\) is faithful,  this implies that \\(h = F(f)\\). Hence we have that there exists a \\(f': C \\to C'\\) such that \\(h = F(f)\\), so that \\(F\\) is full. Again, by symmetry, we have that \\(G\\) is full, as desired. </p> <p>Now since \\(\\phi: G \\circ F \\cong I_\\cc\\), we see that every object \\(C\\) is assigned an isomorphism \\(\\phi_C: G(F(C)) \\to C\\). Hence every object \\(C\\) is isomorphic to some \\(G(D)\\) where \\(D = F(C)\\). </p> <p>Similarly, since \\(\\psi: F \\circ G \\cong I_\\dd\\), we know that each object \\(D\\) is assigned an isomorphism \\(\\psi_D: F(G(D)) \\to D\\). Hence every object \\(D\\) is isomorphic to some object \\(F(C)\\) for \\(C = G(D)\\). </p> <p>\\item[\\(\\bm{(iii) \\implies (ii)}\\)] Suppose \\((iii)\\) holds. For any  arbitrary object \\(C \\in \\cc\\), there exists an isomorphism \\(\\eta_C: C \\to G(D)\\) for some object \\(D \\in \\dd\\). Denote such an object as \\(F_0(C)\\).  Now consider any other morphism \\(g: C \\to G(D')\\). Then we have that  \\  is commutative. Now since \\(g \\circ \\eta_C^{-1}: G(F_0(C)) \\to G(D')\\), and because \\(G\\) is full, we know that there exists a \\(h: F_0(C) \\to D'\\) such that \\(g \\circ \\eta_C^{-1} = G(h)\\). To show that this is unique, suppose there existed another \\(k: G(F_0(C)) \\to G(D')\\) such that \\(g = k \\circ \\eta_C\\). Then by the same argument, there exists a \\(h': F_0(C) \\to D'\\) such that \\(G(h') = k\\). Furthermore, we'll have that </p> \\[ k = G(h') = g \\circ \\eta_C^{-1} \\qquad G(h) = g \\circ \\eta_C^{-1} \\] <p>so that \\(G(h') = G(h)\\). However, since \\(G\\) is faithful, we have that \\(h' = h\\). Hence, \\(h\\) is unique! </p> <p>Since \\(h\\) is unique, this implies that \\(\\eta_C : C \\to G(F_0(C))\\) is universal from \\(C\\) to \\(G\\). Since such a universal isomorphism exists for each object of \\(C\\), we have by Proposition 4.1 that there exists a functor \\(F: \\cc \\to \\dd\\) with object function \\(F_0(C)\\) which is left adjoint to \\(G\\). Hence we have an adjunction \\((F, G, \\eta', \\epsilon)\\). However, since universal morphisms are unique, we see that \\(\\eta' = \\eta\\), so that \\(\\eta\\), our unit, is a natural isomorphism. </p> <p>Finally, observe that for any object \\(D\\), we have that </p> \\[ G(\\epsilon_D) \\circ \\eta_{G(D)} = 1_{G(D)} \\] <p>for our adjunction. Since \\(\\eta_{G(D)}\\) is an isomorphism, we have that \\(G(\\epsilon_D) = \\eta_{G(D)}^{-1}\\). Sine \\(G\\) is full and faithful, we see that \\(\\epsilon_D\\) must be an isomorphism as well. </p> <p>Thus, in total, we have an adjoint equivalence \\((F, G, \\eta, \\epsilon)\\), as desired. </p> <p>\\item[\\(\\bm{(ii) \\implies (i)}\\)] This direction is clear, since an adjoint equivalence automatically establishes an equivalence of categories.  \\end{description} With \\((i) \\implies (iii) \\implies (ii) \\implies (i)\\), we see that all of the conditions are equivalent.  </p> <p> Let \\(R\\) and \\(S\\) be rings and consider the categories  \\(R\\)-Mod and \\(S\\)-Mod. Then there are two different  \"product\" categories we can form: The categories \\((R \\times S)\\)-Mod  and \\(R**-Mod**\\times S**-Mod**\\) </p> <p>Next, we introduce some properties of equivalences. </p> <p> Let \\(F: \\cc \\to \\dd\\) be an equivalence of categories with the corresponding inverse functor \\(G: \\dd \\to \\cc\\). Let \\(f: C \\to C'\\) be a morphism in \\(\\cc\\). Then \\begin{description} \\item[\\((i)\\)] \\(f\\) is a monomorphism (epimorphism) if and only if  \\(F(f)\\) is a monomorphism (epimorphism) \\item[\\((ii)\\)] \\(C\\) is initial (terminal) if and only if \\(F(C)\\) is initial (terminal). \\end{description} </p> <p>Consequently, we have that \\(f\\) is an isomorphism (a monomorphism and epimorphism) if and only if \\(F(F)\\) is an isomorphism.  \\textcolor{Red!90}{Note this is not generally true!} Additionally, we also have that \\(C\\) is a zero object (terminal and initial) if and only if \\(F(C)\\) is a zero object. Finally, observe that this proposition is symmetric, so that the same conclusions hold for morphisms and objects in \\(\\dd\\) governed by \\(G: \\dd \\to \\cc\\). </p> <p> \\begin{description} \\item[\\(\\bm{(i)}\\)]  \\begin{description} \\item[\\(\\bm{\\implies}\\)] Suppose \\(f: C \\to C'\\) is a monomorphism. Consider two morphisms \\(g,h: D \\to F(C)\\) such that \\(F(f) \\circ g = F(f) \\circ h\\). By the previous theorem, we know however that there exists an object \\(A\\) of \\(\\cc\\) such that \\(D \\cong F(A)\\). Hence there exists an isomorphism \\(\\theta: F(A) \\to D\\). We then have the diagram: \\  Note that \\(h \\circ \\theta, g \\circ \\theta: F(A) \\to F(C)\\). Since \\(F\\) is full, we know that there exists  morphism \\(k, k': A \\to C\\) such that \\(g \\circ \\theta = F(k)\\) and \\(h \\circ \\theta = F(k')\\).  Now observe that  \\[\\begin{align*} &amp;F(f \\circ k) = F(f) \\circ F(k) = F(f) \\circ h \\circ \\theta\\\\ &amp;F(f \\circ k')= F(f) \\circ F(k') = F(f) \\circ g \\circ \\theta. \\end{align*}\\] <p>However, since \\(F(f) \\circ h = F(f) \\circ g\\), we see that \\(F(f \\circ k) = F(f \\circ k')\\). However, since \\(F\\) is faithful, we have that \\(f \\circ k = f \\circ k'\\). But since \\(f\\) is a monomorphism, we have that \\(k = k'\\). Hence \\(F(k) = F(k') \\implies g \\circ \\theta = k \\circ \\theta\\), and since \\(\\theta\\) is an isomorphism, we have that \\(h = g\\). Therefore, \\(F(f)\\) is also monic.</p> <p>\\item[\\(\\bm{\\impliedby}\\)]  Suppose \\(f: C \\to C'\\) and \\(F(f)\\) is monic. Consider two morphism \\(g, h: A \\to C'\\) in \\(\\cc\\), and suppose that \\(f \\circ g = f \\circ k\\). Then \\(F(f) \\circ F(g) = F(f) \\circ F(k) \\implies F(g) = F(k)\\), since \\(F(f)\\) is monic. However, \\(F\\) is faithful, so that \\(g = k\\). Hence \\(f\\) is monic as well. \\end{description}  </p> <p>\\item[\\(\\bm{(ii)}\\)]  \\begin{description} \\item[\\(\\bm{\\implies}\\)] Suppose \\(C\\) is initial in \\(\\cc\\). Let \\(D\\) be an object in \\(\\dd\\). Then observe that, since \\(\\cc\\) and \\(\\dd\\) are equivalent, there exists an isomorphism \\(\\theta: F(A) \\to D\\) for some object \\(A\\) of \\(\\cc\\). Since \\(C\\) is initial, we know that there exists a unique morphism \\(f_C: C \\to A\\). Hence \\(F(f_C): F(C)\\to F(A)\\). We then have that \\(F(f_c) \\circ \\theta : F(C) \\to D\\). Hence there exists a morphism from \\(F(C)\\) to \\(D\\). </p> <p>Now suppose \\(f_1, f_2; F(C) \\to D\\). Then \\(\\theta^{-1} \\circ f_1, \\theta^{-1}\\circ f_2: F(C) \\to F(A)\\). Since \\(F\\) is full, we know that there exist morphism \\(k_1, k_1: C \\to A\\) such that \\(F(k_1) = \\theta^{-1}\\circ f_1\\) and \\(F(k_2) = \\theta^{-1}\\circ f_2\\). However, since \\(C\\) is initial, we see that \\(k_1 = k_2 = f_C\\). Hence \\(f_1 = f_2\\), so that there is exactly one morphism \\(f_1=f_2:F(C) \\to D\\). </p> <p>Since \\(D\\) was an arbitrary object of \\(\\dd\\), we have that \\(F(C)\\) is initial. </p> <p>\\item[\\(\\bm{\\impliedby}\\)]  Suppose \\(F(C)\\) is an initial object. Consider any object \\(C'\\) of \\(\\cc\\). Then since \\(F(C)\\) is initial, there exists a unique morphism \\(f: F(C) \\to F(C')\\). Since \\(F\\) is full, we know that this corresponds with a morphism \\(k: C \\to C'\\) such that \\(F(k) = f\\). Hence we have a unique morphism \\(k: C \\to C'\\). And since \\(C'\\) was an  arbitrary object of \\(\\cc\\), we have that \\(C\\) is initial, as desired.  \\end{description} \\end{description}  The proofs in which we proved \\(f\\) to be an epimorphism, and for \\(C\\)  to be a terminal object, are very similar. This proposition will soon be generalized, but this gives us insight into how useful the concept of equivalent categories truly is. </p>"},{"location":"category_theory/Adjunctions./Exponential%20Objects%20and%20Cartesian%20Closed%20Categories./","title":"4.5. Exponential Objects and Cartesian Closed Categories.","text":"<p>Before we introduce the notion of cartesian closed category, we  begin with a preliminary proposition. </p> <p> Suppose \\(\\cc\\) is a category, and consider the functors  \\[ U: C \\to **1** \\qquad \\Delta: \\cc \\to \\cc \\times \\cc. \\] <p>where \\(**1**\\) is the one object category. \\begin{description} \\item[\\(\\bm{(i)}\\)] If \\(U\\) has a left adjoint, then  \\(\\cc\\) has an initial object.   </p> <p>\\item[\\(\\bm{(ii)}\\)] If \\(\\Delta\\) has a left adjoint, then \\(\\cc\\) has finite coproducts.</p> <p>\\item[\\(\\bm{(iii)}\\)] If \\(U\\) has a right adjoint, then \\(\\cc\\) has a terminal object. </p> <p>\\item[\\(\\bm{(iv)}\\)] If \\(\\Delta\\) has a right adjoint, then  \\(\\cc\\) has finite products. \\end{description}  The proof is a straightforward, although tedious, so we sketch it out as follows. </p> <p> <p>\\textcolor{RedViolet}{**Adjoints of \\(\\bm{U**.\\)}} First, let \\(F: **1** \\to \\cc\\) be a left adjoint of \\(U\\). Suppose \\(F(1) = I\\) in \\(\\cc\\). Then for any \\(C \\in \\cc\\), we have the bijection \\(\\hom_{\\cc}(F(1), C) \\cong \\hom_{**1**}(1, U(C))\\) which implies that</p> \\[  \\hom_{\\cc}(I, C)  \\cong \\hom_{**1**}(1, 1). \\] <p>In other words, for each object \\(C\\), there is exactly one and only one morphism \\(i_C: I \\to C\\), which makes \\(I\\) an initial object. </p> <p>On the other hand, suppose \\(G: {1} \\to \\cc\\) is a right adjoint of \\(U\\). Then if \\(G(1) = T\\), we have the bijection  \\(\\hom_{**1**}(U(C), 1) \\cong \\hom_{\\cc}(C, G(1))\\) which implies that </p> \\[ \\hom_{**1**}(1, 1) \\cong \\hom_{\\cc}(C, T) \\] <p>so that for each object \\(C\\) there exists a unique morphism \\(t_C: C \\to T\\), which makes \\(T\\) a terminal object. Hence left and right adjoints guarantee the existence of initial and terminal objects.\\ \\ \\textcolor{RedViolet}{**Adjoints of \\(\\bm{\\Delta**\\).}} Let \\(F: \\cc \\times \\cc \\to \\cc\\) be a left adjoint of \\(\\Delta\\), so that we have the relation</p> <p> Then for each object \\((A, B) \\in \\cc \\times \\cc\\), we have the morphism \\(\\eta_{(A,B)}: (A, B) \\to \\Delta(F(A,B))\\), which we can  rewrite as \\(\\eta_{(A,B)}: (A, B) \\to (F(A,B), F(A,B))\\). We can put this into a universal diagram  \\  where the diagram on the right is the coproduct diagram of \\(A \\times B\\). Since both of the pairs \\(\\Big((F(A,B), F(A,B)), \\eta_{(A,B)}\\Big)\\)  and \\(\\Big((A\\times B, A \\times B), (\\pi_A, \\pi_B)\\Big)\\) are universal from \\((A, B)\\) to \\(\\Delta\\), they must be isomorphic. As two universal objects are isomorphic, we therefore have, </p> \\[ F(A, B) \\cong A \\amalg B \\] <p>so that a left adjoint gives rise to products. </p> <p>Let \\(G: \\cc \\times \\cc \\to \\cc\\) be a right adjoint of \\(\\Delta\\), so that we have  \\  The adjunction gives rise to a universal morphism \\(\\epsilon_{(A,B)}: \\Delta(G(A,B)) \\to (A,B)\\), which we can rewrite as \\(\\epsilon_{(A,B)}: (G(A,B), G(A,B)) \\to (A,B)\\). We then have the diagram  \\  where the diagram on the right is the product diagram of \\(A \\times B\\).  Thus we see that\\ \\(\\Big((G(A,B), G(A, B)), \\epsilon_{(A,B)}\\Big)\\)  and \\(\\Big((A\\times B, A \\times B), (\\pi_A, \\pi_B)\\Big)\\) are both universal from \\(\\Delta\\) to \\((A, B)\\). As universal objects from the same construction are isomorphic, we have that </p> \\[ G(A, B) \\cong A \\times B \\] <p>so that this adjunction gives rise to coproducts.  </p> <p>\\textcolor{MidnightBlue}{Thus if we have left and right adjoints of the functors \\(U\\) and \\(\\Delta\\), we get initial and terminal objects as  well as finite products and coproducts. Note, however, that finite products require (and give rise to) initial objects, and similarly that finite coproducts require (and give rise to) terminal objects.} </p> <p>Next, we make the following definition. </p> <p> Let \\(\\cc\\) be a category with finite products. Suppose \\(Y, Z\\) are  objects in \\(\\cc\\). We say \\(Z^Y\\) is an exponent object in \\(\\cc\\) if there exists a morphism \\(**eval**: (Z^Y \\times Y) \\to Z\\) which is universal from \\(-\\times Y: \\cc \\to \\cc\\) to the object \\(Z\\).  <p>Visually, this translates into requiring that the following diagram commutes.  \\   \\textcolor{Purple}{Hence, every morphism, with the domain being any product with \\(\\bm{Y}\\), and codomain being \\(\\bm{Z}\\), uniquely factors through \\(\\bm{Z^Y \\times Y}\\).}</p> <p>Here, we'll stop and look at a pretty cool real world example.</p> <p> Consider the category Set. Then we know that, for any two given objects \\(Y\\) and \\(Z\\), we can form a set of functions  between the objects: \\[ \\hom_{**Set**}(Y, Z). \\] <p>Thus, the collection of morphisms from sets \\(Y\\) to \\(Z\\) is itself a set, and hence a member of Set. Now let \\(A\\) be any object in \\(**Set**\\), and let </p> \\[ X = \\{f \\in **Set** \\mid f: A \\times Y \\to Z\\}.             \\] <p>Define \\(**eval**: \\hom_{**Set**}(Y, Z)\\times Y \\to Z\\) as, who would've guessed, the evaluation:</p> \\[ **eval**(f(y), y') = f(y'). \\] <p>Now for each \\(a \\in A\\), we can define a function \\(g_a: X \\times Y \\to Z\\) where for each \\(f: A \\times Y \\to Z\\)</p> \\[ g_a(f, y') = f(a, y') \\in Z \\] <p>so this is sort of a \"double\" evaluation function. Then for every such \\(g_a\\), there exists a unique \\(h_a: X \\to \\hom_{**Set**}(Y, Z)\\) where for each \\(f: A \\times Y \\to Z\\)</p> \\[  h_a(f) = f(a, y): Y \\to Z. \\] <p>Thus we get the following commutative diagram: \\  What is this? What's really going on and why do we care?\\ \\textcolor{MidnightBlue}{This construction relates to a concept in computer science called currying. Applied category theory in computer science generally works in Set, so that's why this idea transfers over. \\ \\ The idea is: given a multivariable function, do we evaluate all arguments at once, or evaluate just one argument, thereby sending a function to another function? Both methods can offer  advantages. But universality tells us that, in the end, they're the same thing.}</p> <p>We can think of \\(X \\times Y\\) as being elements \\((f(a, y), y')\\) where \\(f: A \\times Y \\to Z\\). Then \\(h\\) evaluates \\(f(a',y)\\) for some \\(a'\\), thus sending the function \\(f: A \\times Y \\to Z\\) to the  function \\(f:Y \\to Z\\). That is, </p> \\[ (h \\times \\id_y) \\circ \\big( (f(a, y), y') \\big) = (f(a', y),  y'). \\] <p>Finally, \\(**eval**\\) evaluates \\(f(a', y)\\) at \\(y'\\), returning an object in \\(Z\\).</p> <p>Alternatively, we can start with the object \\((f(a, y), y')\\), and simply act on \\(g\\), which evaluates it at both \\(a'\\) and \\(y'\\), returning the same object \\(f(a', y')\\).  Thus in the realm of computer science, we may think of the morphisms \\((h, \\id_y)\\), \\(g\\) and eval as commands, as this is how currying is often done. </p> <p>The universality of this constructions states that both methods are the same; that is, </p> \\[ g = **eval**\\circ (h \\times \\id_Y). \\] <p>Since we started with arbitrary objects in Set, the consequence for computer science is that we can always curry these functions. Typically what is curried are types, such as  Bool or Int.  </p> <p>In an arbitrary category of finite products, the exponential object is just a generalization of currying. But in Set, we see that an exponential object exists for any two pairs of sets. Thus, can we turn this exponential assignment into a functor? Yes,we can. </p> <p> Let \\(\\cc\\) have finite products and exponential objects for every pair of objects. Then for each \\(Y\\) in \\(\\cc\\) we can create an  exponential functor \\(E^Y: \\cc \\to \\cc\\) as follows.  \\begin{description} \\item[Objects.] For each \\(Z \\in \\cc\\), we define \\(E^Y(Z) = Z^Y\\).  <p>\\item[Morphisms.] Let \\(f: A \\to B\\) be in \\(\\cc\\). Then we  note that we have the following diagrams.  \\  Now observe that we can form the morphism \\(f \\circ \\textcolor{Red}{**eval**_A} : A^Y \\times Y \\to B\\).  Hence by universality of \\(B^Y\\), there exists a unique morphism \\(h': A^Y \\to B^Y\\). Diagrammatically, we take the above diagram on the right, and replace \\(X\\) with \\(A^Y\\) and \\(g\\) with \\(f \\circ **eval**_A\\). \\  Since \\(h\\) exists if \\(f: A \\to B\\) exists, we therefore define </p> \\[ E^Y(f: A \\to B) = h': A^Y \\to B^Y \\] <p>where \\(h'\\) is the unique morphism such that </p> \\[ f \\circ \\textcolor{Red}{**eval**_A} = \\textcolor{RoyalBlue}{**eval**_A} \\circ (h', \\id_Y). \\] <p>\\end{description}    Note that there's one more cool connection here. If we have a category  with finite products, and one in which exponential objects exist, then we have a morphism \\(**eval**_A: A^Y \\times Y \\to A\\) which is universal from the functor \\(-\\times Y: \\cc \\to \\cc\\)  to \\(A\\). Therefore, this is a counit! There's an adjunction hiding here. </p> <p> Let \\(\\cc\\) be a category with finite products and exponential objects. Let \\(Y\\) be an object, and define the functors  \\[\\begin{align*} P_Y &amp;= (-) \\times Y: \\cc \\to \\cc\\\\ E^Y &amp;= (-)^Y: \\cc \\to \\cc. \\end{align*}\\] <p>Then \\(E^Y\\) is right adjoint to \\(P_Y\\) for every \\(Y \\in \\cc\\). Therefore, </p> \\[ \\hom_{\\cc}(X \\times Y, Z) \\cong \\hom_{\\cc}(X, Z^Y) \\] <p>which is natural for all objects \\(X, Y, Z \\in \\cc\\).  </p> <p> For each object \\(A \\in \\cc\\), the exponential object gives rise to a universal morphism \\(**eval**_A: A^Y \\times Y \\to A\\). So on one hand, we get the diagram on the left \\  but on the other hand, the diagram on the right is exactly equivalent. Hence we see that \\(**eval**\\) is actually a counit \\(\\epsilon_A: P_Y(E^Y(A)) \\to A\\). Since such a counit exists for each \\(A\\), this gives rise to an adjunction, so that \\(E^Y\\) is right adjoint to \\(P_Y\\) for every object \\(Y\\) in \\(\\cc\\). </p> <p>Finally, we have everything we need to move onto to the main point of this section. </p> <p> Let \\(\\cc\\) be a category. We say \\(\\cc\\) is a \\textbf{cartesian closed  category} if the functors  \\[ U: **C** \\to **1**  \\qquad \\Delta: \\cc \\to \\cc  \\times \\cc  \\qquad  P_Y = (-)\\times Y: \\cc \\to \\cc \\] <p>have right adjoints. In other words, \\(\\cc\\) is  cartesian closed if </p> <ul> <li> <p>[1.] There exists a terminal object \\(T\\) </p> </li> <li> <p>[2.] \\(\\cc\\) has finite products </p> </li> <li> <p>[3.] An exponential object \\(A^Y\\) for every \\(A \\in \\cc\\) for all \\(Y\\).   </p> </li> </ul> <p></p> <p>Thus the work we just did was used in showing that our three-bullet point list is another definition of a cartesian closed category. Often, only one definition or the other is offered, and it's not trivial how they're equivalent, so it can be confusing. Thus our work shows that either definition is equivalent. </p> <p>Some examples include Set, which we already dealt with. Set has a terminal object (empty set), has finite products, and has an exponential object. More interesting is Cat, which is cartesian closed. In this case, 1 is the terminal object, Cat is closed under finite products, and the exponential object exists. In this case, \\(\\cc^{\\bb}\\) is simply the functor category! </p> <p>\\textcolor{MidnightBlue}{At first, it seemed silly to define \\(\\cc^\\bb\\) as the category of functors from \\(\\bb\\) to \\(\\cc\\), since it seemed that it ought to be denoted \\(\\bb^\\cc\\). However, we see that this was really just because of the concept of exponentials, which isn't known when being introduced functor categories.}</p> <p>\\chapterimage{chapter5_pic/chapt5head.pdf} </p>"},{"location":"category_theory/Adjunctions./Introduction%20to%20Adjunctions./","title":"4.1. Introduction to Adjunctions.","text":"<p>As promised, we now build upon the work we did with universal morphisms to  define the concept of an adjunction. Adjunctions are special cases of universal morphisms  that occur between two functors \\(F\\) and \\(G\\) which assemble between two categories \\(\\cc\\) and \\(\\dd\\)  as below.  \\begin{center} \\adjunction{\\cc}{F}{\\dd}{G} \\end{center}Studying adjunctions allows us to give an answer to many questions that appear  in categories. For example, adjunctions can explain why, for instance, given  two sets \\(X\\), \\(Y\\), we have the isomorphism </p> \\[ F(X \\times Y) \\cong F(X) * F(Y)     \\] <p>where \\(F: **Set** \\to **Grp**\\) is the free group functor and \\(*\\) denotes the  \\hyperref[example:free_product]{\\textcolor{blue}{free product}}. They can also  explain why this property, and other similar properties, hold for similar free functors. </p> <p>We begin with an example of an adjunction. </p> <p> Recall that for a fixed unital ring \\(R\\) in \\(**Ring**\\), we may  form the functor  \\[ R[-]: **Grp** \\to R-**Alg**  \\] <p>which sends a group \\(G\\) to its \\hyperref[example:group_ring_functor]{\\textcolor{blue}{group ring}} \\(R[G]\\). Recall that </p> \\[ R[G] = \\left\\{ \\sum_{g \\in G}a_g g \\;\\middle|\\; g \\in G, \\, a_g \\in R, \\text{ and } a_g = 0 \\text{ for all but finitely many } a_g \\right\\}. \\] <p>Recall also that we can form the functor </p> \\[ (-)^{\\times}: R**-Alg** \\to **Grp** \\] <p>which sends an \\(R\\)-algebra \\(A\\) to its group of units \\(A^{\\times}\\). These two functors are related in the following way. Consider a group \\(G\\) and  its group ring \\(R[G]\\).  In general, the units of \\(R[G]\\) are nontrivial. One thing we do know  is that  elements of the form \\(1_R g\\), with \\(g \\in G\\), are units of \\(R[G]\\). (The multiplicative  inverse of such an element is \\(1_R g^{-1}\\).)  This allows us to construct a group homomorphism</p> \\[ i: G \\to (R[G])^{\\times} \\qquad g \\mapsto 1_R g. \\] <p>What is interesting about this is the following fact: \\((G, i: G \\to (R[G])^{\\times})\\) is universal from \\hyperref[definition:universal_morphism_from_D_to_F]{\\textcolor{blue}{\\(G\\) to \\((-)^{\\times}\\)}}. That is, if \\(K\\) is a  ring, and we have a mapping \\(\\phi: G \\to K^{\\times}\\), then there exists a unique ring homomorphism \\(h: R[G] \\to K\\) such that the diagram below commutes.  \\  The reason why this works is as follows: \\(\\phi\\) tells us to where to send  elements of \\(G\\). Since a map on \\(R[G]\\) can be defined by (1) defining where  elements of \\(G\\) go and (2) extending linearly, \\(\\phi\\) induces the existence of \\(h\\). </p> <p>By Proposition \\ref{proposition:universality_bijection}, we then have the following result:  If \\(K\\) is an \\(R\\)-algebra, then for each group \\(G\\) there is a natural bijection</p> \\[ \\hom_{**Ring**}(R[G], K) \\cong \\hom_{**Grp**}(G, (K)^{\\times}) \\] <p>Specifically, the bijection is natural in \\(G\\).</p> <p>But wait---There's more! For every ring \\(K\\), there is a natural  ring homomorphism </p> \\[ \\epsilon: R[(K)^{\\times}] \\to K \\qquad \\sum_{k \\in K^{\\times}}a_k k \\mapsto z(a_k) k \\] <p>where \\(z(a_k) = 1_k\\), the identity of \\(K\\), if \\(a_k \\ne 0\\), and \\(z(a_k) = 0\\) if \\(a_k = 0\\).  The reason why we care about this is because \\((K, R[(K)^{\\times}] \\to K)\\) is universal from \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\(R[-]\\) to \\((K)^{\\times}\\)}}. That is, if \\(G\\) is a group and we have a mapping \\(\\psi: R[G] \\to K\\),  then there exists a unique \\(j: G \\to (K)^{\\times}\\) such that the following diagram  commutes.  \\  We obtain \\(j\\) as follows: Note that \\(\\phi(1_R g) \\in K^{\\times}\\), since  ring homomorphisms send units to units. Hence, the composite  \\  where \\(i\\) is defined earlier, yields \\(j\\). Moreover, the diagram commutes in this way.  By Exercise \\ref{exercise:universality_bijection}, if \\(K\\) is a ring, then for every group \\(G\\)  we have the following natural bijection </p> \\[ \\hom_{**Grp**}(G, (K)^{\\times})\\cong \\hom_{**Ring**}(R[G], K). \\] <p>Specifically, the bijection is natural in \\(K\\). However, we just saw this isomorphism before! This demonstrates our first example of an adjunction.  </p> <p> Let \\(\\cc, \\dd\\) be categories. Consider a pair of functors  \\  We say that \\(F, G\\) form an adjunction and that  \\(F\\) is left adjoint to \\(G\\) (and so \\(G\\) is right adjoint to \\(F\\)) if, for all \\(C \\in \\cc\\), \\(D \\in \\dd\\), there is a natural bijection  \\[ \\hom_{\\dd}\\bigg(F(C), D\\bigg) \\cong \\hom_{\\cc}\\bigg(C, G(D)\\bigg)  \\] <p></p> <p>This definition is somewhat strange, so we comment a few remarks. </p> <p> <ul> <li> <p>To define an adjunction between two functors, it suffices to specify which  functor is the left adjoint, or which functor is the right adjoint (since one specification determines the other).  Thus, the sentence \"\\(F\\) and \\(G\\) form an adjunction\" alone  does not make sense; namely, it is missing information of which  functor is the left or the right adjoint. </p> </li> <li> <p>In an adjunction, we are always going to have some  kind of bijection as above. But there are two different ways we could decide to write it:</p> </li> </ul> \\[ \\hom_{\\dd}(F(C), D) \\cong \\hom_{\\cc}(C, G(D))  \\quad \\text{ or } \\quad \\hom_{\\cc}(C, G(D)) \\cong \\hom_{\\dd}(F(C), D) \\] <p>This can potentially confuse us on which functor is the left adjoint, and which one is the right.  However, one thing that does not change in the above expressions is the position of \\(F(C)\\) and \\(G(D)\\) in their hom-sets. In their hom-sets,  the symbol \\(F(C)\\) is always in the left position, while \\(G(D)\\) is in the right. Hence we can determine if \\(F\\) or \\(G\\) is left or right based on glancing at the  bijection. Conversely, knowing the left and rightedness of our functors  tells us how to write down the bijection. </p> <p></p> <p>We now observe that this definition is equivalent to the existence of universal morphisms;  this is something we already saw in our introductory example. </p> <p> Let \\(\\cc, \\dd\\) be categories and consider a pair of functors        \\begin{tikzcd} \\cc \\arrow[r, shift right = -0.5ex, \"F\"] &amp; \\dd \\arrow[l, shift right = -0.5ex, \"G\"] \\end{tikzcd}. The following are equivalent.  <ul> <li> <p>[(i.)] The functors \\(F\\), \\(G\\) form an adjunction where \\(F\\) is left adjoint to \\(G\\) (and so \\(G\\) is right adjoint to \\(G\\)). </p> </li> <li> <p>[(ii.)] There exist natural transformations </p> </li> </ul> \\[ \\eta: I_{\\cc} \\to G \\circ F \\qquad \\epsilon: F \\circ G \\to I_{\\dd} \\] <p>such that \\begin{itemize}</p> <ul> <li> <p>For each \\(C \\in \\cc\\), the morphism  \\(\\eta_C: C \\to G(F(C))\\) is universal from  \\hyperref[definition:universal_morphism_from_D_to_F]{\\textcolor{blue}{\\(C\\) to \\(G\\)}}</p> </li> <li> <p>For each \\(D \\in \\dd\\), the morphism  \\(\\epsilon_D: F(G(D)) \\to D\\) is universal from  \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\(F\\) to \\(D\\)}}</p> </li> </ul> <p>\\end{itemize} </p> <p> Since \\(F\\) is left adjoint to \\(G\\), we have the natural bijection  \\[ \\hom_{\\dd}(F(C), D) \\cong \\hom_{\\cc}(C, G(D)). \\] <p>This is natural in \\(C\\) and \\(D\\).</p> <p>By Proposition \\ref{proposition:universality_bijection},  the above bijection is natural in \\(D\\) if and only  if there exists a morphism \\(\\eta_C: C \\to G(F(C))\\) which is  universal from \\(C\\) to \\(G\\).  However, the bijection holds for all \\(C\\). Therefore, we obtain a family  of universal morphisms </p> \\[ \\eta_C: C \\to G(F(C)). \\] <p>Since this bijection is also natural in \\(C\\),  we ultimately obtain a natural transformation \\(\\eta: I_{\\cc} \\to G \\circ F\\). </p> <p>Using the same bijection from our adjunction, we can use Exercise \\ref{exercise:universality_bijection} to conclude the existence of  a family of morphisms \\(\\epsilon_D: F(G(D)) \\to D\\) which is universal from \\(F\\) to \\(D\\).  We then use the fact that the bijection is natural to form the natural  transformation \\(\\epsilon: F \\circ G \\to I_{\\dd}\\), as desired.</p> <p>As we used if and only if propositions, our work proves both directions, which  completes the proof. </p> <p> Let \\adjunction{\\cc}{F}{\\dd}{G} be an adjunction. We establish the  following terminology. <ul> <li> <p>The natural transformation \\(\\eta: I_{\\cc} \\to G \\circ F\\) is  the unit of the adjunction.</p> </li> <li> <p>The natural transformation \\(\\epsilon: F \\circ G \\to I_{\\dd}\\)  is the counit of the adjunction.</p> </li> </ul> <p></p> <p> We already saw this proposition in action in the introductory example.  In that example, we found a pair functors  \\begin{center} \\adjunction{Grp}{{R[-]}}{Ring}{(-)^{\\times}} \\end{center}   that formed an adjunction with universal morphisms  \\[ i_G: G \\to (R[G])^{\\times} \\qquad  \\epsilon_K: R[(K)^{\\times}] \\to K \\] <p>for all groups \\(G\\) and rings \\(K\\). Hence \\(i_G\\) is the unit of the adjunction, while  \\(\\epsilon_K\\) is the counit.  These units and counits are what allowed us to establish the bijection</p> \\[ \\hom_{**Ring**}(R[G], K) \\cong \\hom_{**Grp**}(G, (K)^{\\times}) \\] <p>natural in \\(G\\) and \\(K\\). Hence, the group ring functor \\(R[-]\\) is left adjoint  to the group of units functor \\((-)^{\\times}\\).  </p> <p>Using our previous work, we very quickly and (hopefully) painlessly established a connection between the natural  bijection that appears in the definition of an adjunction and the unit and counit morphisms.  However, we did not really describe what the bijection actually does on elements. The next proposition characterizes the bijection.</p> <p> Let \\(\\cc\\), \\(\\dd\\) be categories, and suppose  \\begin{tikzcd} \\cc \\arrow[r, shift right = -0.5ex, \"F\"] &amp; \\dd \\arrow[l, shift right = -0.5ex, \"G\"] \\end{tikzcd} form an adjunction with \\(F\\) left adjoint to \\(G\\).  Let \\(\\eta\\), \\(\\epsilon\\) be the unit and counit. <p>For each \\(C, D\\), the natural bijection</p> \\[ \\phi_{C,D}: \\hom_\\dd(F(C), D) \\isomarrow \\hom_\\cc(C, G(D)) \\] <p>is given by the function where for each \\(f: F(C) \\to D\\) and \\(g: C \\to G(D)\\), </p> \\[ \\phi(f) = G(f) \\circ \\eta_C \\qquad \\phi^{-1}(g) = \\epsilon_D \\circ F(g). \\] <p></p> <p>The proof is left to the reader. </p> <p> We have already encountered the pair of functors  \\  where \\(F\\) is the free monoid functor and \\(U\\) is the forgetful monoid functor. We previously saw that given a set \\(X\\), there exists an inclusion morphism \\[ i_X: X \\to U(F(X))  \\] <p>and this morphism is universal from \\universalDToF{\\(X\\) to \\(U\\)}.  In addition, we know that the monoid homomorphism </p> \\[ \\epsilon_M: F(U(M)) \\to M  \\] <p>and this morphism is from \\universalFToD{\\(F\\) to \\(M\\)}. Therefore, we see that  \\(F\\) and \\(U\\) are adjoint functors; specifically, \\(F\\) is left adjoint to \\(G\\) and \\(G\\)  is right adjoint to \\(F\\), and we have the natural bijection </p> \\[ \\hom_{**Mon**}(F(X), M) \\cong \\hom_{**Set**}(X, U(M)). \\] <p>Moreover, we know exactly how this bijection works. </p> <ul> <li> <p>For \\(f: F(X) \\to M\\), we send \\(\\phi(f)\\) to \\(U(f) \\circ i_X\\). </p> </li> <li> <p>For \\(g: X \\to U(M)\\), we send \\(\\phi^{-1}(g)\\) to \\(\\epsilon_M \\circ F(g)\\). </p> </li> </ul> <p>This data assembles into the commutative diagrams as below.  \\  </p> <p>Now we offer some sufficient conditions for establishing an adjunction. </p> <p> Let \\(G: \\dd \\to C\\) be a functor. Suppose that for each \\(C \\in \\cc\\), there exists an object \\(F_0(C) \\in \\dd\\) and a universal morphism \\(\\eta_C: C \\to G(F_0(C))\\) from \\(C\\) to \\(G\\).  Then there exists a functor \\(F: \\cc \\to \\dd\\) which is left-adjoint  to \\(G\\). </p> <p> To have universality from \\(C\\) to \\(G\\), the diagram  \\  must commute. Hence we have a bijection  \\[ \\hom_{\\dd}({F(C), D}) \\cong \\hom_\\cc({C, G(D)}).   \\] <p>Now suppose \\(h: C \\to C'\\). Then the dashed arrow  \\  must exist by universality; we simply utilize the previous diagram. In other words, if \\(h: C \\to C'\\), then there exists a morphism \\(f: F_0(C) \\to F_0(C')\\). With that said, we can then define a functor where \\(F: \\cc \\to \\dd\\) with \\(F(C) = F_0(C)\\) and \\(F(h) = F_0(C) \\to F_0(C')\\). By construction, this functor is left adjoint to \\(G\\).  </p> <p>A similar proposition holds for the establishing a right adjoint.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor. Suppose for each object \\(D \\in \\dd\\) there exists an object \\(G_0(D) \\in \\cc\\)  and a universal morphism \\(\\epsilon_d: F(G_0(D)) \\to D\\) from \\(F\\) to \\(D\\). <p>Then there exists a functor \\(G: \\dd \\to \\cc\\) which is right-adjoint  to \\(F\\).  </p> <p>We now introduce a proposition which offers sufficient conditions for an adjunction, although it is not parallel to either of our previous propositions. </p> <p> Let \\(F: \\cc \\to \\dd\\) and \\(G: \\dd \\to \\cc\\) be functors, and suppose we have the pair of natural transformations: \\[\\begin{align*} \\eta_{C} : I_C \\to G \\circ F \\quad \\epsilon_{D}: I_D \\to F \\circ G \\end{align*}\\] <p>such that the following composites are the identity:  \\  Then there exists a bijective \\(\\phi\\) such that \\((F, G, \\phi)\\) form an adjunction between \\(\\cc\\) and \\(\\dd\\).  </p> <p> Let \\(U: **R-Mod** \\to **Ab**\\) be the forgetful functor, which forgets the \\(R\\)-module structure on the underlying abelian group \\(M\\). Consider the functor \\(F: **Ab** \\to **R-Mod**\\), where \\(F(A) = R \\otimes A\\). We'll show that this is left-adjoint to \\(U\\) as follows.  <p>To show this, we'll propose a morphism which we will show to be universal. If \\(A\\) is an abelian group, then we let  \\(\\eta_A : A \\to U(F(A))\\) where \\(\\eta_A(a) = 1 \\otimes a\\). </p> <p>Thus let \\(M\\) be an \\(R\\)-module, and suppose there exists a morphism \\(f: A \\to U(M)\\). Then we can define a morphism \\(\\phi: F(A) \\to M\\) where </p> \\[ \\phi(r \\otimes a) = r\\cdot f(a). \\] <p>Our construction ensures that this is a well-defined \\(R\\)-module homomorphism. Hence we clearly have the equality \\(U(\\phi) \\circ \\eta_A = f\\). Visually, this becomes  \\  Since the construction of \\(\\phi\\) depends directly on the existence of \\(f\\), we see that it is unique. Hence we see that \\(\\eta_A: A \\to U(F(A))\\) is universal from \\(A\\) to \\(U\\). Then by Theorem 4.1, we see that we have an adjunction, so that \\(F\\) is truly left adjoint to the forgetful functor \\(U\\). </p> <p></p> <p>The following proposition is one of the main reasons why adjoint functors are  extremely useful.</p> <p> Let \\(F, F' : \\cc \\to \\dd\\) be two left adjoints of the functor \\(G : \\dd \\to \\cc\\). Then \\(F, F'\\) are naturally isomorphic.  </p> <p> Let \\((F, G, \\phi)\\) and \\((F', G, \\phi')\\) be two adjunctions between  \\(\\cc\\) and \\(\\dd\\). Then these adjoints give rise to the universal morphisms \\[ \\eta_C : C \\to G(F(C)) \\quad \\eta'_{C}: C \\to G(F'(C)) \\] <p>for every \\(C \\in \\cc\\). Since these are both universal morphisms from \\(C\\) to \\(G\\), we know that they are isomorphic. Hence there exists a unique isomorphism \\(\\theta_C :  F(C) \\to F'(C)\\) by universality such that \\(G(\\theta_C) \\circ \\eta_C = \\eta_C'\\) (think of a universal diagram). </p> <p>Now let \\(h: C \\to C'\\) be a morphism in \\(\\cc\\). Then  \\(F'(h) \\circ \\theta_C = \\theta_{C'} \\circ F(h)\\) so that the diagram  \\  commutes. Hence we see that \\(\\theta: F \\to F'\\) is a natural isomorphic transformation between \\(F\\) and \\(F'\\), so that these two functors are naturally isomorphic.  The other direction holds as well. That is, two right adjoints to one left adjoint are naturally isomorphic as well, and the proof is the same. We now have our last proposition for this section. </p> <p> Let \\(G: \\dd \\to \\cc\\) be a functor. Then \\(G\\) has a left-adjoint \\(F: \\cc \\to \\dd\\) if and only if for each \\(C \\in \\cc\\), the functor \\(\\hom_{\\cc}(C, G(-))\\) is representable as a functor of \\(D \\in \\dd\\). Furthermore, if \\(\\phi: \\hom_{\\dd}(F_0(C), D) \\cong \\hom_{\\cc}(C, G(D))\\) is a representation of this functor, then \\(F_0\\) is the object function of \\(F\\).  </p> <p>Finally, we end this section by realizing that we can actually form composition of adjoints. </p> <p> Let \\(\\cc, \\dd\\) and \\(\\ee\\) be categories. Suppose we have two adjunctions as below. \\  Then the functors \\(F'\\circ F\\), \\(G \\circ G'\\) form an adjunction between \\(\\cc\\) and \\(\\ee\\). Further, if \\((\\eta, \\epsilon)\\) and \\((\\eta', \\epsilon')\\)  are unit and counits of the adjunction  from \\((F, G)\\) and \\((F', G')\\), then the unit and counit of the new adjunction  is  \\[\\begin{align*} \\overline{\\eta}_C = G(\\eta'_{F(C)}) \\circ \\eta_C: C \\to (G\\circ G) \\circ (F'\\circ F(C))\\\\ \\overline{\\epsilon}_E = \\epsilon'_E \\circ F'(\\epsilon_{G'(E)})  : (F' \\circ F) \\circ (G \\circ G'(E)) \\to E \\end{align*}\\] <p>\\vspace{-0.7cm} </p> <p> First, observe that the two given adjunctions give rise to  \\[ \\hom_{\\dd}(F(C), D) \\cong \\hom_{\\cc}(C, G(D))  \\qquad  \\hom_{\\ee}(F'(D), E) \\cong \\hom_{\\dd}(D, G'(E)). \\] <p>which are relations that are natural in objects \\(C, D\\) and \\(E\\). Observe that in the second relation, we can set \\(D = F(C)\\). This then translates to </p> \\[ \\hom_{\\ee}(F'(F(C)), E) \\cong \\hom_{\\dd}(F(C), G'(E)). \\] <p>Using the first relation, we know that  \\(\\hom_{\\dd}(F(C), G(E)) \\cong \\hom_{\\cc}(C, G(G'(E)))\\). Putting this together, we then have the bijection of homsets </p> \\[ \\hom_{\\ee}(F'\\circ F(C)), E) \\cong \\hom_{\\cc}(C, G \\circ G'(E)) \\] <p>which is natural in \\(C\\) and \\(E\\). Now, describing the unit and counit is a bit ugly, and not exactly necessary, since in the end we know what these adjunctions look like. The punchline here is that we can write our new unit and counit in terms of the original ones. </p> <p>Observe that for any object \\(C\\) of \\(\\cc\\), we have the universal morphism </p> \\[ \\eta_C: C \\to G(F(C)). \\] <p>Since \\(F(C) \\in \\dd\\), we can use \\(\\eta'\\) that</p> \\[ \\eta'_{F(C)}: F(C) \\to G'(F'(F(C))).             \\] <p>Finally, note that \\(G(\\eta'_{F(C)}): G(F(C)) \\to G(G'(F'(F(C))))\\). However, we can precompose this with \\(\\eta_C\\) to have that </p> \\[ G(\\eta'_{F(C)}) \\circ \\eta_C: C \\to G(G'(F'(F(C)))). \\] <p>On the other hand, for any object \\(E\\) of \\(\\ee\\) that </p> \\[ \\epsilon'_E : F'(G'(E)) \\to E. \\] <p>We also have \\(\\epsilon_D : F(G(D)) \\to D\\) for any object \\(D \\in \\dd\\). Hence,  we can set \\(D = G'(E)\\) for some object \\(E\\) of \\(\\ee\\) to get</p> \\[ \\epsilon_{G'(E)} : F(G(G'(E))) \\to G'(E). \\] <p>We can then get that \\(F'(\\epsilon_{G'(E)}) : F'(F(G(G'(E)))) \\to F'(G'(E))\\). Composing this with the original \\(\\epsilon'_D\\), we get that </p> \\[ \\epsilon'_E \\circ F'(\\epsilon_{G'(E)}): F'(F(G(G'(E)))) \\to E \\] <p>as desired. Now showing that these remain universal is not hard. </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] Give a proof of Proposition \\ref{proposition:adjunction_isomorphism_behavior}.  </p> </li> <li> <p>[2.]  Let \\(U: **Ab** \\to **Grp**\\) be the forgetful functor, and suppose \\(F: **Grp** \\to **Ab**\\) is  the abelianization functor. That is, if \\(G\\) is a group and \\(\\phi: G \\to G'\\) is a group homomorphism then </p> </li> </ul> \\[ F(G) = G/[G,G] \\qquad F(\\phi) : G/[G, G] \\to G'/[G',G']. \\] <p>where \\([G,G]\\) is the commutator subgroup.</p> <p>Show that we have an adjunction \\adjunction{Grp}{F}{Ab.}{U} Give a description of the unit and counits. </p>"},{"location":"category_theory/Adjunctions./Reflective%20Subcategories./","title":"4.2. Reflective Subcategories.","text":"<p> Let \\(\\aa\\) be a full subcategory of \\(\\cc\\). We say \\(\\aa\\) is reflective in \\(\\cc\\) whenever the inclusion functor \\(I: \\aa \\to \\cc\\) has a left adjoint \\(F: \\cc \\to \\aa\\). We then say the functor \\(F\\) is the reflector, and the adjunction \\((F, I, \\phi)\\) is a reflection of \\(B\\).   In the case of a reflection, we obtain the bijection of hom-sets </p> \\[ \\hom_{\\aa}(F(C),A) \\cong \\hom_{\\cc}(C, I(A)) \\implies \\hom_{\\aa}(F(C),A) \\cong \\hom_{\\cc}(C, A)  \\] <p>which is natural in both \\(C\\) and \\(A\\). </p> <p> Let \\(F: **Grp** \\to **Ab**\\) be the abelianization functor, which sends a group \\(G\\) to its free abelian group \\(G/[G,G]\\). From Exercise \\ref{exercise:abelianization_functor_is_left_adjoint}, we know that this is left adjoint to the forgetful functor \\(U: **Ab** \\to **Grp**\\).  <p>However, the functor \\(U: **Ab** \\to **Grp**\\) is isomorphic to the inclusion  functor \\(I: **Ab** \\to **Grp**\\). Hence, \\(F\\) is also left adjoint  to the inclusion functor, so that Ab is a reflective subcategory  of Grp.  </p> <p> Let \\(**Top**\\) be the category of topological spaces with morphisms continuous functions. Let \\(**CHaus**\\), the category of compact Hausdorff spaces, which is a subcategory of \\(**Top**\\).  <p>If we let \\(X\\) be a topological space, then we denote \\(\\beta(X)\\) to be the Stone-Cech compactification. Let \\(I : **CHaus** \\to **Top**\\) be the inclusion functor.  Then the definition of the Stone-Cech compactification of a space \\(X\\) is the universal property: </p> <p> That is, the Stone-Cech compactification is a topological space \\(\\beta(X)\\) with a morphism \\(u: X \\to \\beta(X)\\) which is universal across all morphisms \\(f: X \\to C\\) where \\(C\\) is compact, Hausdorff. </p> <p>Thus we see that a Stone-Cech compactification gives rise to an object \\(\\beta(X) \\in **CHaus**\\) and a universal morphism \\(X \\to I(\\beta(X))\\) from \\(X\\) to \\(I\\). Now by Proposition 4.1, this makes \\(\\beta : **Top** \\to **CHaus**\\) a functor, which is left adjoint to the inclusion functor \\(I: **CHaus** \\to **Top**\\). </p> <p>This then makes \\(\\beta: **Top** \\to **CHaus**\\) a reflector, so that the adjunction is a reflection between \\(**Top**\\) and \\(**CHaus**\\). Consequently we have the bijection </p> \\[ \\hom_{**Top**}(X, I(C)) \\cong \\hom_{**CHaus**}(\\beta(X), C) \\implies  \\hom_{**Top**}(X, C) \\cong \\hom_{**CHaus**}(\\beta(X), C). \\] <p>since \\(I(C)\\) is technically no different than from \\(C\\). This bijection is natural in both \\(X\\) and \\(C\\).  </p> <p> Let \\(**Ab**_{**TF**}\\) represent the category of abelian groups with torsion free elements (for a lack of better notation). Then we have a natural inclusion functor  \\(I: **Ab**_{**TF**} \\to **Ab**\\). Now consider the functor \\(F : **Ab** \\to **Ab**_{**TF**}\\), which we define as follows: \\begin{description} \\item[Objects.] Let \\(G\\) be an abelian group. Then  \\(F(G) = G_{TF}\\) where  \\[ G_{TF} = \\{g \\in G \\mid g^n \\ne e \\text{ for } n = 1, 2, 3, \\dots\\}. \\] <p>That is, it sends \\(G\\) to its underlying abelian group of torsion-free elements. It's not hard to show this is an abelian group.</p> <p>\\item[Morphisms.] Suppose \\(\\phi: G \\to H\\) is a morphism between abelian groups. Then we set \\(F(\\phi) = \\phi_{TF}\\) where </p> \\[ \\phi_{TF}: G_{TF} \\to H_{TF} \\qquad \\phi_{TF}(g) = \\phi(g). \\] <p>Note that this definition will cause no issues, since \\(\\text{ord}(g) = \\text{ord}(\\phi(g))\\). Thus we simply obtain \\(\\phi_{TF}\\) by restricting \\(\\phi\\) to \\(G_{TF}\\). \\end{description}  To show that \\(F\\) is left adjoint to \\(I\\), we need to demonstrate that there exists a universal morphism \\(\\eta_{G} : G \\to I(F(G))\\) for every \\(G \\in **Ab**\\). Hence we propose \\(\\eta_{G}\\) takes on the form </p> \\[ \\eta_{G}(g) =  \\begin{cases} g &amp; \\text{ if } \\text{ord}(g) = \\infty\\\\ e &amp; \\text{ otherwise. } \\end{cases} \\] <p>To show this is universal from \\(G\\) to \\(I\\), suppose we have a morphism \\(\\phi: G \\to I(H)\\), where \\(H \\in **Ab**_{**TF**}\\). Then there exists a morphism \\(\\psi: F(G) \\to H\\) such that \\(I(\\phi) \\circ \\eta_{G} = \\phi\\). Visually, that is,  \\  Sure such a morphism exists, but why the equality?  \\begin{description} \\item[\\(\\bm{g \\in \\ker(\\eta_G)}\\).] If \\(g \\in \\ker(\\eta_G)\\), then \\(g\\) has finite order. Hence we see that \\(\\phi(g) = e\\); this is because \\(\\text{ord}(\\phi(g)) = \\text{ord}(g) &lt; \\infty\\), but the only element in \\(I(H)\\) with finite order is \\(e\\). We then have that \\(g \\in \\ker(\\phi)\\). Therefore, </p> \\[ I(\\psi)\\circ \\eta_G(g) = I(\\psi)(e) = e = \\phi(g). \\] <p>Hence \\(I(\\psi) \\circ \\eta_G = \\phi\\) if \\(g \\in \\ker(\\eta_{G})\\). </p> <p>\\item[\\(\\bm{g \\not\\in \\ker(\\eta_G)}\\).] if \\(g \\not\\in \\ker(\\eta_G)\\), then we know that \\(\\text{ord}(g) = \\infty\\). Therefore, we see that </p> \\[ I(\\psi) \\circ \\eta_G(g) = I(\\phi)(g) = \\phi(g). \\] <p>Hence \\(I(\\psi) \\circ \\eta_G = \\phi\\) for \\(g \\not\\in \\ker(\\eta_G)\\).  \\end{description} By our previous work, we then have that \\(I(\\psi) \\circ \\eta_G = \\phi\\), as desired. Now \\(\\psi\\) is of course unique based on its construction, since its definition depends directly on \\(\\phi\\). We then have that \\(\\eta_G: G \\to I(F(G))\\) is universal from \\(G\\) to \\(I\\) for each \\(G \\in **Ab**\\)! </p> <p>We then have by Theorem 4.1 that \\(F, I\\) form an adjunction, so that \\(F\\) is the left adjoint of \\(I\\). Hence by definition, we see that \\(**AB**_{**TF**}\\) forms a full reflective subcategory of \\(**Ab**\\). </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] Is FinSet a reflective subcategory of Set?  </p> </li> <li> <p>[2.] Let \\(G\\) and \\(H\\) be a groups. Prove that</p> </li> </ul> \\[ G*H/[G*H, G*H] \\cong G/[G,G]\\oplus H/[H,H] \\] <p>where \\(G*H\\) denotes the \\hyperref[example:free_product]{\\textcolor{blue}{free product}}  of \\(G\\) and \\(H\\). (What this is saying is that \\(F: **Grp** \\to **Ab**\\), the abelianization  functor, preserves coproducts. Eventually, this fact will immediately  follow by our knowledge of the adjunction \\adjunction{Grp}{F}{Ab.}{U}) </p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Category%20Theory%20Axioms./","title":"1.3. Category Theory Axioms.","text":"<p>Now we have an understanding of the fact that (1) there is no definitive foundation of mathematics, and therefore that (2) there is no definitive category theory, but rather a definitive set of axioms for categories. We also understand what things might look like under the axioms of category theory. </p> <p> A category \\(\\mathcal{C}\\) consists of  <ul> <li> <p>a collection of objects \\(\\ob(\\cc)\\)</p> </li> <li> <p>a collection of morphisms between objects; for any objects \\(A, B\\),  we denote the morphisms \\(f: A \\to B\\) from \\(A\\) to \\(B\\) as \\(\\hom_{\\cc}(A, B)\\)</p> </li> <li> <p>a binary operator \\(\\circ\\) known as composition, such that for any objects \\(A,  B, C\\), </p> </li> </ul> \\[\\begin{align*} \\circ &amp;: \\hom(A, B) \\times \\hom(B, C) \\to \\hom(A, C)\\\\ &amp;(f, g) \\mapsto (g \\circ f) \\end{align*}\\] <p>Furthermore, the following laws must be obeyed. \\begin{description} \\item[(1) Identity.] For each \\(A \\in \\ob(\\cc)\\), there exists a distinguished morphism, called the identity \\(\\id_A: A \\to A\\) in \\(\\hom(\\cc)\\). \\item[(2) Closed under Composition.] If \\(A, B, C\\) are objects, then for any  \\(f \\in \\hom(A,B)\\), \\(g \\in \\hom(B,C)\\), there exists a morphism \\(h \\in \\hom(A, C)\\) such that \\(h = g \\circ f\\).</p> <p></p> <p>\\item[(3) Associativity under Composition.] For objects \\(A, B, C\\) and \\(D\\) such that \\  we have the equality </p> \\[ h \\circ (g \\circ f) = (h \\circ g) \\circ f. \\] <p>\\item[(4) Identity action.] For any \\(f \\in \\hom(\\cc)\\) where \\(f:A \\to B\\) we have that</p> \\[ 1_B \\circ f = f = f \\circ 1_A. \\] <p>\\end{description} </p> <p>At this point, the reader is assumed to have never seen a category or has at least  some vague idea. Therefore, any reasonable person would next introduce examples  to clarify the above abstract nonsense. There are two types of examples we can  introduce: abstract and concrete examples. We first introduce the three canonical examples,  then three abstract examples. In the next section we introduce a barrage of more complicated, but  real examples of categories in mathematics.  The reader is at liberty to read the next two sections in order, in reverse, or she can skip  back and forth between them. </p> <p>Here we make a comment on notation. In what follows we are going to have to describe  categories. To describe them, we need to tell the reader (1) what the objects are  (2) what the morphisms are and (3) what composition is. Often times, (3) is implicit.  Therefore our preferred format of describing an arbitrary  category \\(\\cc\\) is using a bold-faced list.  An example: \\  This is simply to avoid a lot of unnecessary  words to describe a category (e.g. ''the objects of this category are... the morphisms of this category are...'').</p> <p> The canonical example of a category is the category of sets, denoted  as \\(\\Set\\), which we can describe as  \\begin{description} \\item[Objects.] All sets \\(X\\).\\footnote{There's a minor issue with saying this. We will address it, but not for now.}  \\item[Morphisms.] All functions between sets \\(f: X \\to Y\\).  \\end{description} Because most of mathematics is based in set theory, we shall see that while this is a fairly  simple category, it is one of the most useful.  </p> <p>A tip moving forward: When dealing with any abstract construction, it is a common  strategy to keep a \"canonical example\" of such an abstract construction in your head.  For many people, they often use \\(\\Set\\) as the image in their head when they imagine a category.  This is fine, but one should be cautioned:  in general, categorical objects are not sets.  Furthemore, morphisms are in general not functions. This might be strange, but you will get used to it and it will eventually become natural to you. The moral of the story is: \\begin{center} \\begin{minipage}{0.7\\textwidth} \\textcolor{NavyBlue}{The canonical example of a category is \\(\\Set\\), but  in general the objects of an arbitrary category \\(\\cc\\) are not sets,  and the morphisms are not functions. } \\end{minipage} \\end{center}    The second canonical example is the category of groups, denoted  as \\(\\grp\\). This can be described as  \\begin{description} \\item[Objects.] All groups \\((G, \\cdot)\\). Here, \\(\\cdot: G \\times G \\to G\\) is the group operation.  \\item[Morphisms.] All group homomorphisms \\(\\phi: (G, \\cdot) \\to (H, \\cdot)\\).  Specifically, set functions \\(\\phi: G \\to H\\) where \\(\\phi(g \\cdot g') = \\phi(g)\\cdot \\phi(g')\\).  \\end{description}       We again check this satisfies the axioms of a category.  \\begin{description} \\item[(1)] Each group \\((G, \\cdot)\\) has a identity group homomorphism \\(\\id_G: (G, \\cdot) \\to (G, \\cdot)\\) where  \\(\\id_G(g) = g\\).  \\item[(2)] The function composition of two group homomorphisms \\(\\phi: (G, \\cdot) \\to (H, \\cdot)\\) and \\(\\psi: (H, \\cdot) \\to (K, \\cdot)\\)  is again a group homomorphism where \\((\\psi \\circ \\phi)(g) = \\psi(\\phi(g))\\). This is because  \\[\\begin{align*} (\\psi \\circ \\phi)(g \\cdot g')&amp;= \\psi(\\phi(g \\cdot g'))\\\\  &amp;= \\psi(\\phi(g)\\cdot \\phi(g')) \\\\ &amp;= \\psi(\\phi(g)) \\cdot \\psi(\\phi(g'))\\\\ &amp;= (\\psi \\circ \\phi)(g) \\cdot (\\psi \\circ \\phi)(g). \\end{align*}\\] <p>\\item[(3)] Function composition is associative; therefore, composition of group homomorphisms is associative. \\item[(4)] If \\(\\phi: (G, \\cdot) \\to (H, \\cdot)\\) is a group homomorphism, then  \\(\\id_H \\circ \\phi = \\phi \\circ \\id_G = \\phi\\). \\end{description} Therefore we see that this is a category. We will later see that this category possesses  many convenient and interesting properties.  </p> <p> The third canonical example is the category of topological spaces,  denoted \\(\\top\\). We describe this as  \\begin{description} \\item[Objects.] All topological spaces \\((X, \\tau)\\) where \\(\\tau\\) is a topology on the set \\(X\\).  \\item[Morphisms.] All continuous functions \\(f: (X, \\tau) \\to (Y, \\tau')\\).  \\end{description} The reader can show that this too satisfies the axioms of a category. </p> <p>We now consider some abstract examples. While abstract, they are nevertheless important  examples in their own right. They also illustrate that categories can be finite,  which may counter the intuition the reader might have of categories being \"infinte.\"</p> <p> In this example we introduce the three most basic categorical structures.  The first, and most important of the three, is the single object or initial category \\(\\bm{1}\\), which is the category where:  \\begin{description} \\item[Objects.] A single object, abstractly denoted as \\(\\bullet\\). \\item[Morphisms.] A single identity morphism \\(\\id_{\\bullet}: \\bullet \\to \\bullet\\).  \\end{description} The identity of \\(\\bullet\\) does not matter; it is an abstract object. This is similar to how  a one point set is denoted as \\(\\{*\\}\\) and we don't really care what \\(*\\) is.  \\ <p>The second category is the arrow category, denoted as \\(\\bm{2}\\), which we can  describe as  \\begin{description} \\item[Objects.] Two objects \\(\\textcolor{NavyBlue}{\\bullet}\\) and \\(\\textcolor{Orange}{\\bullet}\\) \\item[Morphisms.] Two identity morphisms $\\id_{\\textcolor{NavyBlue}{\\bullet}}: \\textcolor{NavyBlue}{\\bullet}  \\to \\textcolor{NavyBlue}{\\bullet} $  and \\(\\id_{\\textcolor{Orange}{\\bullet} }: \\textcolor{Orange}{\\bullet}  \\to \\textcolor{Orange}{\\bullet}\\) and one  nontrivial morphism \\(f: \\textcolor{NavyBlue}{\\bullet} \\to \\textcolor{Orange}{\\bullet}\\).  \\end{description} Here we color our abstract objects to clarify that these objects are distinct. \\  Finally, we have the category triangle category, denoted as \\(\\bm{3}\\), which  can be describe as  \\begin{description} \\item[Objects.] Three distinct objects \\(\\textcolor{NavyBlue}{\\bullet}, \\textcolor{Orange}{\\bullet}, \\textcolor{Purple}{\\bullet}\\)  \\item[Morphisms.] Three identity morphisms, and three nontrivial morphisms:  \\(f: \\textcolor{NavyBlue}{\\bullet} \\to \\textcolor{Purple}{\\bullet}\\),  \\(g: \\textcolor{Purple}{\\bullet} \\to \\textcolor{Orange}{\\bullet}\\) and \\(h: \\textcolor{NavyBlue}{\\bullet} \\to \\textcolor{Orange}{\\bullet}\\). \\end{description} In this category, we define \\(h = g \\circ f\\) so that this is closed under composition. Note that  if we did not include the existence of \\(h\\), then this would not be closed under composition,  and hence it would not even be a category.</p> <p>We can picture all three categories as below.  \\  </p> <p>Our first step in category theory has been introducing the axioms and showing  some simple examples. We now take our second step by moving on to more basic  concepts of category theory by making a few comments about categories. </p> <p> Let \\(\\cc\\) be a category. We say that \\(\\cc\\) is  <ul> <li> <p>Finite if there are only finitely many objects  and finitely many morphisms. </p> </li> <li> <p>Locally Finite if, for every pair of objects  \\(A, B\\), the set \\(\\hom_{\\cc}(A, B)\\) is finite. </p> </li> <li> <p>Small if the collection of objects and collections of morphisms  assemble into a set. </p> </li> <li> <p>Locally Small if \\(\\hom_{\\cc}(A, B)\\) is a set for every  pair of objects \\(A, B\\). </p> </li> <li> <p>Large if \\(\\cc\\) is not locally small. That is, the objects and  morphisms do not form a set. </p> </li> </ul> <p></p> <p>Such terminology proves to be useful, since we have seen that categories come in different sizes.  For example, the categories \\(\\bm{1}, \\bm{2},\\) and \\(\\bm{3}\\) are finite categories. However, recall Russel's Paradox, so that the collection of all sets is not a set.  Therefore, \\(\\Set\\) is a large category.</p> <p>We now introduce the concept of a subcategory, which is also extremely useful to include in our vocabularly. </p> <p> Let \\(\\cc\\) be a category. We say a category \\(\\mathcal{S}\\) is a subcategory of \\(\\cc\\) if \\begin{description} \\item[(1)] \\(\\ss\\) is a category, with composition the same as \\(\\cc\\) \\item[(2)] The objects and morphisms of \\(\\ss\\) are contained in the collection of objects  and morphisms of \\(\\cc\\).  \\end{description} Furthermore, we say \\(\\ss\\) is a full subcategory if we additionally have that  \\begin{description} \\item[(3)] For each pair of objects \\(A, B \\in \\ss\\), we have that \\(\\hom_{\\ss}(A, B) = \\hom_{\\cc}(A, B)\\). \\end{description} More informally, \\(\\ss\\) is full if it \"contains all of its morphisms.\" </p> <p> Let \\(\\ab\\) be the category described as  \\begin{description} \\item[Objects.] All abelian groups \\((G, \\cdot)\\) \\item[Morphisms.] Group homomorphisms. \\end{description} Then \\(\\ab\\) is a subcategory of \\(\\grp\\). Futhermore, \\(\\ab\\)  is a full subcategory of \\(\\grp\\). This observation also applies to  <ul> <li> <p>FinGrp, the category of finite groups </p> </li> <li> <p>FindAb, the category finite abelian groups</p> </li> <li> <p>\\(\\ab_{\\text{TF}}\\), the category of torsion-free abelian groups </p> </li> </ul> <p>However, none of these categories are subcategories of \\(\\Set\\). In fact,  many categories which are based in set theory are not actually subcategories  of \\(\\Set\\). This is because the objects of categories such as \\(\\grp\\) or \\(\\top\\) are not just sets, but are sets with extra data (such as a binary operation or a topology).  </p> <p></p> <p> Let \\(\\ring\\) be the category described as  \\begin{description} \\item[Objects.] Unital rings \\((R, +, \\cdot)\\). That is, rings \\(R\\) with a multiplicative identity 1 that is not  equal to its additive identity 0.  \\item[Morphisms.] (Unit preserving) Ring homomorphisms \\(\\phi: R \\to R'\\).  That is, functions \\(\\phi: R \\to R'\\) such that  <ul> <li> <p>\\(\\phi(a + b) = \\phi(a) + \\phi(b)\\)</p> </li> <li> <p>\\(\\phi(a \\cdot b) \\phi(a) \\cdot \\phi(b)\\) </p> </li> <li> <p>\\(\\phi(0_R) = 0_{R'}\\) and \\(\\phi(1_R) = 1_{R'}\\).</p> </li> </ul> <p>\\end{description} For a ring \\(R\\) we know that \\((R, +)\\) is an abelian group, and we know that  every ring homomorphism is technically a group homomorphism between abelian groups. However, it is not  the case that \\(\\ab\\) is a subcategory of \\(\\ring\\). This is because while  every ring is technically an abelian group, abelian groups on their own are not rings.  </p> <p>We now introduce a convenient categorical construction which will serve to be useful to us  from here on out. </p> <p> Let \\(\\cc, \\dd\\) be categories. Then we can form the product category where  we have that  \\begin{description} \\item[Objects.] Pairs \\((C, D)\\) with \\(C \\in \\cc\\) and \\(D \\in \\dd\\). \\item[Morphisms.] Pairs \\((f, g)\\) where \\(f: C \\to C'\\) and \\(g: D \\to D'\\) are morphisms in \\(\\cc\\) and \\(\\dd\\).   \\end{description} To define composition in this category, suppose we have composable morphisms in \\(\\cc\\)  and \\(\\dd\\) as below.  \\  Then the morphisms \\((f, g)\\) and \\((f', g')\\) in \\(\\cc \\times \\dd\\)  are composable too, and their composition is defined as \\((f', g') \\circ (f , g) = ( f' \\circ f, g' \\circ g)\\). \\  <p></p> <p>Note that we can form even larger products of categories; we don't have to stop at two!  But this will be explored later. For now, we can just be happy with this  new tool because it allows us to be build new categories from the old ones that we already  know. </p> <p> A useful example of a product involves the category \\(\\Set\\times\\Set\\) which  we can describe as  \\begin{description} \\item[Objects.] Pairs of sets \\((X, Y)\\). \\item[Morphisms.] Pairs of functions \\((f, g)\\).  \\end{description} Such product constructions are useful because in general, algebraic operations of any kind  require a product. For example, to talk about a group \\((G, \\cdot)\\),  one needs a binary operator, i.e. a function \\(\\cdot: G \\times G \\to G\\). Hence to talk to generalize operations on categories, we need to talk about products.  For example, with \\(\\Set\\times\\Set\\), we can talk about the product of two sets  as a mapping \\(\\times: \\Set \\times \\Set \\to \\Set\\) where \\((A, B) \\mapsto A \\times B\\).  </p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Examples%20and%20Nonexamples%20of%20Functors/","title":"1.7. Examples and Nonexamples of Functors","text":"<p>Functors were not defined out of arbitrary interest. The definition of a functor  was motivated by constructions that were seen in mathematics (unlike constructions in say, number theory, which  are interesting in their own right).  Thus in this section, we include a wide variety of different constructions in  in different areas of mathematics which all fit the definition of a functor.  We present examples  from algebraic topology, differential geometry, topology, algebraic geometry,  abstract algebra and set theory.</p> <p>In short, this section is due to the fact that the only way to really understand what a functor does is to realize  the definition with examples.  It's not necessarily important to understand all  the examples, if for instance you have never worked with differential geometry,  but it would be good to get a few of them. What is more important is  witnessing how such a simple definition can be so versatile and prevalent in seemingly different fields of mathematics; thus, what is important is  witnessing the flexibility of functors (in addition to filling in the details of  the examples and doing the exercises at the end). \\</p> <p>{\\large\\noindent Algebraic Geometry.\\par}</p> <p> In algebraic geometry, it is often of interest to construct the affine \\(n\\)-space \\(A^n(k)\\) of a field \\(k\\). Usually, \\(k\\) is an algebraically closed field, but it doesn't have to be. \\[ A^n(k) = \\{(a_0, \\dots, a_{n-1}) \\mid a_i \\in k \\}. \\] <p>For example, when \\(k = \\rr\\), we have that \\(A^n(k) = \\rr^n\\).  Moreover, we claim that we have a functor \\(A^n(-): \\fld \\to \\Set\\). To see this, we need to figure out where \\(A^n(-)\\) sends objects and morphisms.</p> <p>We can first observe that \\(A^n(-)\\) sends fields \\(k\\) to sets \\(A^n(k)\\). Secondly, we can observe that for a field homomorphism \\(\\phi: k \\to k'\\),   we can define the function \\(A^n(\\phi): A^n(k) \\to A^n(k')\\) where for each \\((a_1, \\dots, a_n) \\in A^n(k)\\) we have that</p> \\[ A^n(\\phi)(a_0, \\dots, a_{n-1}) = (\\phi(a_0), \\dots,  \\phi(a_{n-1})). \\] <p>The reader can show that this satisfies the rest of the axioms of a functor. Overall,  we can say that we have a functor </p> \\[ A^n(-): \\fld \\to \\Set.   \\] <p></p> <p> Once the affine \\(n\\)-space is defined, the next step in algebraic geometry is to construct  the projective space \\(P^n(k)\\) for a field \\(k\\). To define this, we first  define an equivalence relation on \\(A^{n+1}(k)\\). We say  \\[ (a_0, \\dots,  a_n) \\sim (b_0, \\dots, b_n) \\text{ if there is a nonnzero } \\lambda \\in k \\text{ such that } a_i = \\lambda b_i. \\] <p>This defines an equivalence relation on the points of \\(A^n(k)\\). Geometrically, this equivalence relation  says two points are equivalent whenever they lie on the same line passing through the origin. With this equivalence relation, we then define</p> \\[ P^n(k) = A^{n+1}/\\sim = \\Big\\{[(a_0, \\dots, a_n)] \\mid (a_0, \\dots, a_n) \\in A^{n+1}(k)\\Big\\}. \\] <p>Hence we see that \\(P^n(k)\\) is the set of equivalence classes under this equivalence relation. Similar to  the previous example, this construction is also functorial. Consider a field homomorphism \\(\\phi: k \\to k'\\). Then we  define the function \\(P^n(\\phi): P^n(k) \\to P^n(k')\\) where </p> \\[ P^n(\\phi)([a_0, \\dots, a_n]) = [(\\phi(a_0), \\dots, \\phi(a_n))].            \\] <p>However, when defining functions on a set of equivalence classes, we need to be careful.  It's possible that the function could send equivalent objects to different things, so that such  a fuction would not be well-defined. In this case, the above function is in fact well-defined.  This is because \\(\\phi(\\lambda a_i)  =\\phi(\\lambda) \\phi(a_i)\\) for any \\(i = 0, 1, \\dots, n\\).  Therefore we can state that we have a functor </p> \\[ P^n(-): \\fld \\to \\Set.   \\] <p> \\vspace{0.5cm}  </p> <p>{\\large\\noindent Algebraic Topology.\\par}</p> <p> An important example of a functor arises in homology theory. For example, in singular homology theory, one considers a topological space \\(X\\) and associates this  with its \\(n\\)-th homology group. \\[ X \\mapsto H_n(X) \\] <p>In a typical  topology course, one then proves that if \\(f: X \\to Y\\) is a continuous  mapping between topological spaces, then \\(f\\) induces a group homomorphism </p> \\[ H_n(f): H_n(X) \\to H_n(Y)  \\] <p>in such a way that for a second mapping \\(g: Y \\to Z\\), \\(H_n(g \\circ f)  = H_n(g) \\circ H_n(f)\\) for all \\(n\\). Finally, we also know that  \\(H_n(1_X) = 1_{H_n(X)}\\). Therefore, what we see is that this process can be cast into the language  of category theory, so that we may define a \\textbf{singular  homology functor}</p> \\[ H_n: \\top \\to \\ab \\] <p>since this functorial process sends topological spaces in \\(\\top\\) to abelian groups in \\(\\ab\\). </p> <p> Another example from algebraic topology can be realized from the fundamental group \\[ \\pi_1(X, x_0) = \\{[x] \\mid x \\in X\\} \\] <p>with \\(x_0 \\in X\\), and  where \\([x]\\) is the equivalence class of loops based at \\(x_0\\), subject  to the homotopy equivalence  relation. First observe that \\(X \\mapsto \\pi_1(X)\\) is in fact a mapping  of objects between \\(\\top^*\\) and \\(\\grp\\). Second,  observe that if \\(f: X \\to Y\\) is a continuous function, then  we can define a group homomorphism </p> \\[ \\pi_1(f): \\pi_1(X) \\to \\pi_1(Y) \\qquad [x] \\mapsto [f(x)]. \\] <p>Note that this is well defined since if \\(x \\sim x'\\) then  there is a homotopy relation \\(H: X \\times [0, 1] \\to Y\\). However,  \\(f \\circ H\\) is also another homotopy relation that establishes that  \\(f(x) \\sim f(x')\\); hence our group homomorphism is well defined. </p> <p>Moreover, if \\(f: X \\to Y\\) and \\(g:Y \\to Z\\) are continuous, then  we can check that \\(\\pi_1(g \\circ f) = \\pi_1(g) \\circ \\pi_1(f)\\); if \\([\\alpha] \\in \\pi_1(X, x_0)\\), then</p> \\[ (g \\circ f)_*([\\alpha]) = [(g \\circ f) \\circ \\alpha]  = [g \\circ (f \\circ \\alpha)] = g_*([f_*([\\alpha])]) = g_* \\circ f_*([\\alpha])  \\] <p>so that \\((g \\circ f)_* = g_*f_*\\). Finally, we  can examine how the identity map \\(1_X\\) on a topological  space acts on an element \\([\\alpha] \\in \\pi_1(X, x_0)\\):</p> \\[ id_*([\\alpha]) = [id \\circ \\alpha] = [\\alpha]. \\] <p>so that it is sent to the identity homomorphism. All together, this allows  us to conclude that this process is entirely functorial, so we may summarize our  results by stating that </p> \\[ \\pi_1: \\top^* \\to \\grp    \\] <p>is a functor. </p> <p>We now present two examples from differential geometry, which aren't traditionally  presented as examples of functors but are nevertheless interesting in their  own right.  \\vspace{0.5cm}</p> <p>{\\large\\noindent Differential Geometry.\\par}</p> <p> Let \\(M^n\\) be a differentiable manifold of dimension \\(n\\). In general, this  means that there exists a family of open sets \\(U_\\alpha \\subset \\rr^n\\) and injective  mappings \\(\\bm{x}_\\alpha: U_\\alpha \\to M\\) for \\(\\alpha \\in \\lambda\\), \\(\\lambda\\) an indexing  set, with the mappings subject to various conditions\\footnote{There isn't  a universally agreed upon set of conditions, and we won't really need to worry about  them here. If the reader likes, they can consult Do Carmo's Riemannian Geometry,  which is, and has been for a long time, the go-to differential geometry text. }.  Recall from differential  geometry that we can associate each point \\(p \\in M^n\\) with its tangent space \\(T_p(M)\\), in the following manner.  <p>Suppose for \\(\\alpha' \\in \\lambda\\) we have that  \\(\\bm{x}_{\\alpha}: U_{\\alpha} \\to M\\) is a mapping whose image contains \\(p\\) (such an \\(\\alpha'\\) must exist). Then \\(T_p(M)\\) has a basis </p> \\[ \\left\\{ \\frac{\\partial}{\\partial x_1}, \\frac{\\partial}{\\partial x_2}, \\dots, \\frac{\\partial}{\\partial x_n} \\right\\} \\] <p>where \\(\\displaystyle \\frac{\\partial}{\\partial x_i}\\) is the tangent vector of the map  \\(\\bm{c}_i: U \\to M\\), which simply sends \\((0, \\dots ,0, x_i, 0, \\dots, 0)\\). </p> <p>Now suppose \\(\\phi: M_1^n \\to M_2^m\\) is a differentiable mapping. Recall  that the differential of \\(\\phi\\) establishes a  linear transformation between the vector spaces: </p> \\[ d\\phi_p: T: M^n_1 \\to T_{\\phi(p)}M^m_2. \\] <p>Consider the category \\(**DMan**^*\\) whose objects are pairs \\((M^n, p)\\) with \\(M^n\\) a differentiable manifold and  \\(p \\in M^n\\). The morphism are  \\((\\phi, p): (M_1^n,p) \\to (M_2^m, q)\\) with \\(\\phi: M_1^n \\to M_2^m\\) a differentiable  map and \\(\\phi(p) = q\\).  Then this process may be summarized as a functor  \\(T_p: **DMan**_n^* \\to \\vect_{\\rr}\\) where </p> \\[\\begin{gather*} T:(M, p) = T_p(M)\\\\ T(\\phi: (M_1^n,p) \\to (M_2^m, \\phi(p)) )  = d\\phi_{p}: T_p(M) \\to T_{\\phi(p)}M_2^m \\end{gather*}\\] <p>One can show that the identity map is sent to the identity linear transformation  on \\(T_p(M)\\) and that the differential respects composition, so that  that the association of a manifold \\(M\\) (with a specified point \\(p \\in M\\))  to its tangent space \\(T_p(M)\\) gives rise to a functor</p> \\[ T_p: **DMan**^* \\to \\vect_{\\rr}. \\] <p></p> <p> Consider again a differentiable manifold \\(M^n\\) of dimension \\(n\\). Recall that  we may consider the tangent bundle \\(TM\\) of \\(M\\), which is the set  \\[ TM = \\{(p, v) \\mid p \\in M^n \\text{ and } v \\in T_p(M)\\}. \\] <p>The set \\(TM\\) simply pairs each point \\(p \\in M^n\\) with its tangent space \\(T_p(M)\\).  However, \\(TM\\) is more than such a set; it inherits the structure of a differentiable manifold  from \\(M\\) as well. In fact, it is a manifold of dimension \\(2n\\). </p> <p>Now suppose we have a differentiable mapping \\(\\phi: M_1^n \\to M_2^m\\). Then  this induces a mapping </p> \\[\\begin{gather*} (\\phi, d\\phi): TM_1^{2n} \\to TM_2^{2m}\\\\ (\\phi, d\\phi)(p, v) = (\\phi(p), d\\phi_p(v)). \\end{gather*}\\] <p>One can show that \\((\\phi, d\\phi): TM_1^{2n} \\to TM_2^{2m}\\) is a differentiable mapping  between manifolds\\footnote{I wanted to show this here,  but it turned out to be just tedious definition-checking, so I don't think it's appropriate  to include here (\\textcolor{Red}{perhaps I could make/put it in an appendix...})}  At this point we may guess that we have a functor  \\(TB: **DMan** \\to **DMan**\\) (\"\\(TB\\)\" for \"tangent  bundle\")  where </p> \\[\\begin{gather*} TB(M^n) = TM\\\\ TB(\\phi: M_1^n \\to M_2^m) = (\\phi, d\\phi): TM_1^{2n} \\to TM_2^{2m}. \\end{gather*}\\] <p>To check this, we first observe that \\(TB(1_{M^n}) = 1_{TM^{2n}}\\). Next, suppose \\(\\phi: M_1^n \\to M_2^m\\) and \\(\\psi: M_2^m \\to M_3^p\\),  and observe that </p> \\[ TB(\\psi \\circ \\phi) =  (\\psi \\circ \\phi, d_{\\psi \\circ \\phi}) = (\\psi, d_{\\psi}) \\circ (\\phi, d_{\\phi}) = TB(\\psi) \\circ TB(\\phi). \\] <p>Note that above in the second step, we used the fact that  \\(d_{\\psi \\circ \\phi} = d_{\\psi} \\circ d_{\\phi}\\), which we know is true from  the previous example. As \\(TB\\) respects the identity and composition, we see that we  do in fact have a functor </p> \\[ T: **DMan** \\to **DMan**   \\] <p>as desired. </p> <p>\\vspace{0.5cm} {\\large\\noindent Topology.\\par}</p> <p> Let \\(X\\) be a set. Recall that we can turn \\(X\\) into a topological space  \\((X,\\tau_d)\\), where \\(\\tau^{(X)}_{d}\\) is the discrete topology, so that every subset of  \\(X\\) is an open set. We claim that this process is functorial, so that we  have a functor \\[ D: \\Set \\to \\top. \\] <p>This is because any function \\(f: X \\to Y\\) extends  to a continuous function \\(f: (X, \\tau^{(X)}_{D}) \\to (Y, \\tau^{(Y)}_D)\\)  (hopefully the abuse of notation in \\(f\\) is forgivable here). Hence this defines a functor, although in a simpler way than we've seen in the  previous examples.  </p> <p> Let \\((X, \\tau)\\) be a topological space and consider any \\(x_0 \\in X\\).  Then \\((X, x_0)\\) forms an element of \\(\\top^*\\).  With such a space, we can  consider the loop space of \\((X, x_0)\\) defined to be  \\[ \\Omega(X) = \\{\\phi: S^1 \\to X \\mid \\phi \\text{ is continuous and } \\phi(0) = x_0\\}.  \\] <p>Here \\(S^1\\) is the circle.  As this consists of a family of continuous functions between two topological spaces,  it can be endowed with the Compact Open topology to turn it into a topological space as well. Hence we claim we have a functor </p> \\[ \\Omega: \\top^* \\to \\top. \\] <p>To see this, one needs to first consider a morphism in \\(\\top^*\\), which  in this case is continuous function \\(f: (X, x_0) \\to (Y, y_0)\\)  such that  \\(f(x_0 )= y_0\\). This must then correspond with a continuous function  \\(\\Omega(f): \\Omega(X) \\to \\Omega(Y)\\). We can define this function  pointwise: for each continuous \\(\\phi: S^1 \\to X\\) such that \\(\\phi(0) = x_0\\), we have that  \\(\\Omega(f)(\\phi) = f \\circ \\phi: S_1 \\to Y\\). In this case we see that \\((f \\circ \\phi)(0) = y_0\\),  and is a continuous function, so it is well-defined. </p> <p>This example can be further generalized to higher loop spaces which consider continuous  functions \\(\\phi: S^n \\to X\\), rather than just having \\(n = 1\\).  </p> <p>\\vspace{0.5cm}</p> <p>{\\large\\noindent Algebras, Rings, Groups.\\par}</p> <p> Recall that a Lie Algebra \\(\\mathfrak{g}\\) is a vector space  \\(\\mathfrak{g}\\) (over a field \\(k\\)), equipped with a bilinear operation  \\([-, -]: \\mathfrak{g}\\times \\mathfrak{g} \\to \\mathfrak{g}\\) such that  <ul> <li> <p>[1.] \\([x, y] = -[y, x]\\)</p> </li> <li> <p>[2.] \\([x, [y, z]] + [y, [z, x]] + [z, [x,y]] = 0\\). </p> </li> </ul> <p>Condition (2) is referred to as the Jacobi identity, and many  familiar operations on vector spaces satisfy (1) and (2). For example, the cross  product on vector spaces in \\(\\rr^3\\) satisfy these properties. </p> <p>Consider an associative algebra \\(A\\) over a field \\(k\\) with (associative); recall  that this too has a bilinear operation \\(\\cdot: A \\times A \\to A\\) with unit \\(e \\in A\\).  Then we can use \\(A\\) to create a Lie algebra \\(L(A)\\), whose (1) underlying vector  space is \\(A\\) and (2) whose bilinear operation is \\([a, b] = a \\cdot b - b\\cdot a\\). </p> <p>Now suppose \\(\\phi: A \\to A'\\) is a morphism of algebras. Then we can associate  both \\(A, A'\\) with their Lie algebras \\(L(A), L(A')\\). Further, we can construct a Lie  Algebra morphism \\(L(\\phi): L(A) \\to L(A')\\), using \\(\\phi\\), by setting  \\(L(\\phi)(a) = \\phi(a)\\). This is a morphism of Lie algebras since </p> \\[ [\\phi(a), \\phi(b)] = \\phi(a)\\phi(b) - \\phi(b)\\phi(a) = \\phi(ab - ba) = \\phi([a, b]). \\] <p>One can then check that \\(L(1_A) = 1_{L(A)}\\) and \\(L(\\phi \\circ \\psi) = L(\\phi)\\circ L(\\psi)\\), so  that what we have is a functor </p> \\[ L: **Alg** \\to **LieAlg**             \\] <p>which associates each associative algebra with its Lie algebra structure. </p> <p> Let \\(R\\) be a commutative ring. Recall that \\(\\spec(R)\\) is the set of all prime ideals of \\(R\\). In addition, recall that if \\(\\phi: R \\to S\\) is a ring homomorphism and if \\(P\\) is a prime ideal of \\(S\\), then \\(\\phi^{-1}(P)\\) is also a prime  ideal in \\(R\\). This then allows us to define a functor  \\[ **Spec**: \\ring \\to \\Set \\] <p>where on objects \\(R \\mapsto \\spec(R)\\) and on morphisms  \\(\\phi: R \\to S \\mapsto \\phi^*: \\spec(S) \\to \\spec(R)\\) where  \\(\\phi^{*}(P) = \\phi^{-1}(P)\\).</p> <p>However, we can go even deeper than this. Recall from algebraic  geometry that \\(\\spec(R)\\) can be turned into a topological space,  using the Zariski topology. However, because  \\(\\phi^{-1}(P)\\) is a prime ideal whenever \\(P\\) is, we see that \\(\\phi^*:  \\spec(S) \\to \\spec(R)\\) is actually a continuous function between  the topological spaces. Hence we can view this  as a functor </p> \\[ **Spec**: \\ring \\to \\top. \\] <p>Usually this is phrased more naturally as a functor  \\(**Spec**: \\ring \\to **Sch**\\) where \\(**Sch**\\) is the category of schemes; this is simply  because schemes are isomorphic to \\(\\spec(R)\\) for some \\(R\\). </p> <p> Let \\(G\\) be a group, and \\(R\\) be a ring with identity. Recall  from ring theory that we can form the group ring \\[ R[G] = \\left\\{ \\sum_{g \\in G}a_gg\\mid a_g \\in R, \\text{ all but finitely many } a_g = 0 \\right\\}. \\] <p>Thus the elements are finite sums, but we have possibly infinitely many ways  of adding them. Now  for two elements \\(\\displaystyle \\alpha = \\sum_{g \\in G}a_kg\\) and  \\(\\displaystyle \\beta = \\sum_{g \\in G}b_gg\\), we define ring addition and multiplication  as </p> \\[ \\alpha + \\beta = \\sum_{g \\in G}(a_k + b_k)g \\qquad  \\alpha \\cdot \\beta  =  \\sum_{g \\in G}\\sum_{g_1 \\cdot g_2 = g}(a_{g_1}b_{g_2})g. \\] <p>Now suppose \\(\\phi: G \\to H\\) is any group homomorphism. With that said, we claim that \\(\\phi\\) induces a natural ring homomorphism \\(\\phi^{*}: R[G] \\to R[H]\\) between the group rings, where </p> \\[ \\sum_{g\\in G}a_gg \\mapsto \\sum_{g \\in G}a_g \\phi(g).  \\] <p>Clearly this is linear and preserves scaling; less obvious is if  this behaves on multiplication, although we check that below. If \\(\\alpha, \\beta\\) defined as above then </p> \\[ \\phi^*( \\alpha \\cdot \\beta ) = \\phi^*\\left( \\sum_{g \\in G}\\sum_{g_1 \\cdot g_2 = g}(a_{g_1}b_{g_2})g \\right) = \\sum_{g \\in G}\\sum_{g_1 \\cdot g_2 = g}(a_{g_1}b_{g_2})\\phi(g) = \\sum_{g \\in G}a_g\\phi(g)\\cdot \\sum_{g \\in G}b_g\\phi(g) = \\phi^*(\\alpha)\\cdot\\phi^*(\\beta). \\] <p>Hence we see that \\(\\phi^*\\) is a ring homomorphism. Therefore,  what we have on our hands is a functor </p> \\[ R[-]: \\grp \\to \\ring   \\] <p>Possibly, your brain may wonder: it looks like we have an assignment  of rings to functors. </p> \\[ R \\mapsto R[-]: \\grp \\to \\ring. \\] <p>Perhaps this process is functorial? The answer is yes, although  at the moment we don't have the necessary language to describe it;  we will go over this when we introduce functor categories.  \\vspace{0.5cm}</p> <p>{\\large\\noindent Set Theory\\par}</p> <p> Consider the power set \\(\\mathcal{P}(X)\\)\u00a0on a set \\(X\\). Then we can create a functor \\(\\mathcal{P}: \\Set \\to \\Set\\) as follows.  <p>Observe that for any set \\(X\\), \\(\\mathcal{P}(X)\\) is of course another set. Therefore objects of \\(\\Set\\) are sent to \\(\\Set\\), as we claim. </p> <p>Now let \\(f: X \\to Y\\) be a function between two sets \\(X\\) and \\(Y\\). Then we define \\(\\mathcal{P}(f) : \\mathcal{P}(X) \\to \\mathcal{P}(Y)\\) to be the function where </p> \\[ P(f)(S) = \\{f(x) \\mid x \\in S\\}. \\] <p>which is clearly in \\(\\mathcal{P}(Y)\\). Now we must show that this function respects identity and composition properties. \\begin{description} \\item[Identity.] Consider the identity function  \\(\\id_X: X \\to X\\) on a set \\(X\\).  Then observe that for any \\(S \\in \\mathcal{P}X\\), we have that  </p> \\[ \\mathcal{P}(\\id_X)(S)  = \\{\\id_X(x) \\mid x \\in X\\} = S. \\] <p>Therefore, \\(\\mathcal{P}(\\id_X) = 1_{\\mathcal{P}X}\\) so that \\(\\mathcal{P}\\) respects identities. </p> <p>\\item[Composition.] Let \\(X, Y, Z\\) be sets and \\(f: X \\to Y\\) and \\(g: Y \\to Z\\) be functions. Let \\(S \\in \\mathcal{P}(X)\\). Observe that </p> \\[\\begin{align*} \\mathcal{P}(g \\circ f)(S)  &amp;= \\{(g \\circ f)(x) \\mid x \\in S \\}\\\\ &amp;= \\{ g(f(x)) \\mid x \\in S \\}\\\\ &amp;= \\{ g(y) \\mid y = f(x) \\text{ and } x \\in S\\} &amp;= \\mathcal{P}(g)(\\{f(x) \\mid x \\in S\\})\\\\ &amp;= \\mathcal{P}(g)( \\mathcal{P}(f)(S))\\\\ &amp;= (\\mathcal{P}(g) \\circ \\mathcal{P}(f)) (S). \\end{align*}\\] <p>Therefore we see that \\(\\mathcal{P}(g \\circ f) = \\mathcal{P}(g) \\circ \\mathcal{P}(f),\\) so that \\(\\mathcal{P}\\) describes a functor from \\(\\Set\\) to \\(\\Set\\). \\end{description} </p> <p>As we just encountered a mass of different examples of functors from  different fields, one might wonder: are there other mathematical constructions  which simply do not behave exactly as a functor? The answer is yes, although  finding these examples is a bit tricky. The following is a well-known  example, while the one after is one I haven't seen presented elsewhere. \\vspace{0.5cm}</p> <p>{\\large\\noindent Non-functor Examples.\\par}</p> <p> Recall from group theory that, with every group \\(G\\),  there is an associated subgroup of \\(G\\) called the center: \\[ Z(G) = \\{ z \\in G \\mid zg = gz \\text{ for all } g \\in G\\}. \\] <p>By definition, \\(Z(G)\\) is an abelian group. As every group \\(G\\) may be  associated with an abelian group \\(Z(G)\\), one might expect that this  process is functorial. One might prematurely denote this as</p> \\[ Z: \\grp \\to \\ab. \\] <p>However, this is not a functor, as an issue arises with the morphisms.  Consider a group homomorphism \\(\\phi: G \\to H\\). Then for this to be a functor,  we'd naturally desire a group homomorphism \\(Z(\\phi): Z(G) \\to Z(H)\\) between  the abelian groups. The only issue is that there is no consistent way to  define such a morphism from \\(\\phi\\). The most natural way we would attempt to achieve this is by considering the restriction, but in general  \\(\\phi\\big|_{Z(G)}: G \\to H\\) does not map into \\(Z(H)\\).  For example, consider the Heisenberg Group</p> \\[ H_3(R) =  \\left\\{ \\begin{pmatrix} 1 &amp; a &amp; b\\\\ 0 &amp; 1 &amp; c\\\\ 0 &amp; 0 &amp; 1  \\end{pmatrix} \\Bigg| a,b,c \\in R \\right\\} \\] <p>where \\(R\\) is a commutative ring with identity. Observe that  we can create an inclusion group homomorphism \\(i: H_3(R) \\to \\text{GL}_3(R)\\).  One can show that </p> \\[ Z(H_3(R)) =  \\left\\{ \\begin{pmatrix} 1 &amp; 0 &amp; a\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1  \\end{pmatrix} \\Bigg| a \\in R \\right\\} \\qquad  Z(\\text{GL}_3(R)) = \\left\\{   \\begin{pmatrix} a &amp; 0 &amp; 0\\\\ 0 &amp; a &amp; 0\\\\ 0 &amp; 0 &amp; a  \\end{pmatrix} \\Bigg| a \\in R  \\right\\}. \\] <p>Hence restricting the inclusion \\(i: H_3(R) \\to \\text{GL}_3(R)\\)  to \\(Z(H_3(R))\\) results in a group homomorphism that  does not even hit \\(Z(\\text{GL}_3(R))\\) (except of course when \\(a = 0\\)).  Thus there is not a general way to relate these two quantities in a functorial  fashion. </p> <p>What follows is a second example in which a process which may appear to be  functorial does not turn out to be. It can, however, be adjusted to  become a functor. </p> <p> Let \\(X\\) be a set. Recall from topology that we can treat \\(X\\) as a topological space by associating to it the \\textbf{finite complement  topology:}  \\[ \\tau^X_{FC}= \\{U \\subset X \\mid X - U \\text{ is finite.}\\} \\] <p>With that said, one may suppose that we have a functor \\(\\text{FinC}: \\Set \\to  \\top\\) where \\(X \\mapsto (X, \\tau^X_{FC})\\). This would require that each function \\(f:X \\to Y\\)  extends to a continuous function \\(f: (X, \\tau_{FC}^X) \\to (Y, \\tau_{FC}^Y)\\).  However, for such a function to be continuous we would need that </p> \\[ \\text{if } Y - V \\text{ is finite then } X - f^{-1}(V) \\text{ is finite.} \\] <p>In general, this is not true. For example suppose \\(X\\) is infinite and \\(Y\\) is finite.  Then \\(Y - \\varnothing\\) is finite, but \\(X - f^{-1}(\\varnothing) = X\\) is infinite.  Hence this cannot define a functor \\(F: \\Set \\to \\top\\). </p> <p></p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.]  \\begin{itemize}</p> </li> <li> <p>[(i.)] Let \\(X\\) and \\(Y\\) be two sets. Regard each set as a discrete category.  Interpret what a functor \\(F: X \\to Y\\) means in this case.</p> </li> <li> <p>[(ii.)]  Let \\(G\\) and \\(H\\) be two groups. Regard each group as a one-object  category whose morphisms sets correspond to their group elements, with composition  their group product. Interpret what a functor \\(F: G \\to H\\) means in this case.</p> </li> <li> <p>[(iii.)]  Let \\(X\\) and \\(Y\\) be a pair of thin categories. Interpret  what a functor \\(F: X \\to Y\\) means in this case. (Use (i)  to get you started.) </p> </li> </ul> <p>\\vspace{0.2cm}</p> <p>\\item[2.] Let \\(G\\) be a group. Then for any two elements \\(a, b \\in G\\), we define the commutator of \\(a, b\\) to be the element </p> \\[ aba^{-1}b^{-1}. \\] <p>Define \\([G, G]\\) to be the set </p> \\[ \\{x_1x_2\\cdots x_n \\mid n \\in \\mathbb{N}, x_i \\text{ is a commutator in } G\\}     \\] <p>which we call the commutator subgroup. Its underlying set consists of all possible products, with factors that are of the form \\(a_ib_ia_i^{-1}b_i^{-1}\\).  One can show that \\([G, G] \\normal G\\) for any group \\(G\\), which implies that we may discuss the quotient group \\(G/[G, G]\\), which is abelian in this case. </p> <p>So, show that we have a functor \\(F: \\grp \\to \\ab\\)  where </p> \\[ F(G) = G/[G,G]  \\] <p>Deduce the action of \\(F\\) on the morphism of \\(\\grp\\)  (i.e., the group homomorphisms.) and show that it is well-defined. \\vspace{0.2cm}</p> <p>\\item[3.] Let \\(R\\) be a unital ring. Recall that \\(GL_n(R)\\) is the group consisting of  \\(n \\times n\\) matrices with entries in \\(K\\). Show that this construction more  generally is that of a functor</p> \\[ **GL**_n: \\ring \\to \\grp. \\] <p>In addition, with such a ring \\(R\\), we may associate it with  its group of units \\(R^{\\times}\\), which you may recall is </p> \\[ R^{\\times} =\\{ u \\in R \\mid ur = ru = 1 \\text{ for some } r\\in R\\}. \\] <p>Show that this also defines a functor </p> \\[ (-)^{\\times}: \\ring \\to \\grp. \\] <p>We will see in the next section that there is an interesting  relationship between these two functors.</p> <p>\\item[4.] Recall the category \\(\\Set_{FTO}\\)  is the category whose objects are sets and whose morphisms are functions  with the finite-to-one property (See Exercise 1.3.3). While we saw  that \\(\\text{FinC}: \\Set \\to \\top\\) where </p> \\[ X \\mapsto (X, \\tau^X_{FC}) \\] <p>does not define a functor, show that upon changing the domain category from \\(\\Set\\) to \\(\\Set_{FTO}\\), it does  define a functor \\(\\text{FinC}: \\Set_{FTO} \\to \\top\\). \\vspace{0.2cm}</p> <p>\\item[5.]</p> <ul> <li>[(i.)] Let \\(X = \\{x_1, x_2, \\dots, x_n\\}\\) be a finite set. With such  a finite set, we can pick a field \\(k\\) and build \\(X\\) into a finite-dimensional  vector space \\(V_X\\) over \\(k\\). Explicitly, we can create the vector space</li> </ul> \\[ V_X= \\{c_1x_1 + \\cdots + c_nx_n \\mid c_i \\in k\\}. \\] <p>We define addition in the intuitive way of adding coefficients of the  same basis, so this is truly a vector space. Note that when \\(k = \\rr\\),  we get that \\(V_X \\cong \\rr^n\\). </p> <p>Prove that this process is functorial. That is, show that the functor </p> \\[ F: \\finset \\to \\vect_k \\qquad F(X) = V_X \\] <p>is a functor. </p> <ul> <li>[(ii).] From any set \\(X\\), we may construct the free group \\(F(X)\\) generated  by \\(X\\). The elements of \\(F(X)\\) are (1) the elements of \\(X\\), (2) a new  element \\(e\\), and (3) all elements \\(xy\\) whenever \\(x, y \\in X\\).  In this way, \\(F(X)\\) is a group with the product being string concatenation,  and we require that \\(xe = x = ex\\).  . Below, two words (elements of \\(F(X)\\)) are combined.        </li> </ul> \\[ (x^2yz^{-1}) \\cdot (zy^2x) = x^2y^2x. \\] <p>Show that we have a functor \\(F: \\Set \\to \\grp\\) where  sets are mapped to their free groups, that is, \\(X \\mapsto F(X)\\).  </p> <ul> <li>[(iii).] For any set \\(X\\), we can build the free ring \\((R\\{X\\}, +, \\cdot)\\)  as follows. Let \\((F(X), \\cdot)\\) be the free group with the added relation that  \\(xy = yx\\) for any \\(x, y  \\in F(X)\\). We can then define</li> </ul> \\[ R\\{X\\} = \\left\\{ \\sum_{x_i \\in F(X)} x_i^{n_i} \\mid \\right\\} \\] <p>Note: This example becomes particularly important later.  It can also be generalized to functors \\(F:  \\Set \\to \\mon\\),  \\(F: \\Set \\to \\ring\\), and other algebraic systems,  since sets can also be turned into free monoids, free rings, or other  free \"objects.\" \\vspace{0.2cm}</p> <p>\\item[6.] Let \\(V\\) be a vector space over a field \\(k\\).  Recall that we can associate \\(V\\) with its  projective space \\(P(V)\\) which is defined as the set of equivalence  classes of element in \\(V\\), subject to the relation \\(v \\sim w\\) if \\(v = \\lambda w\\)  for some nonzero \\(\\lambda \\in k\\). That is,</p> \\[ P(V) =  \\Big\\{[v] \\mid v \\in V \\Big\\} \\] <p>where \\([v]\\) denotes the equivalence class of \\(v\\). Show that this process  is functorial, so that we have a functor </p> \\[ P: \\vect_k \\to \\Set. \\] <p>\\item[7.] Let \\(R\\) be a ring with ideal \\(I\\). Recall that we can construct the  radical of the ideal \\(I\\) as the ideal </p> \\[ \\sqrt{I} = \\{r \\in R \\mid r^n \\in I \\text{ for some } n \\ge 1  \\}. \\] <p>Show that we have a functor </p> \\[ \\sqrt{-}: **Ideals**(R) \\to **Ideals**(R)     \\] <p>where \\(**Ideals**(R)\\) is the partial order of ideals on \\(R\\),  whose ordering is given by subset containment. </p> <p>\\item[8.]  Let \\(X\\) be a topological space, and denote \\(\\open(X)\\) as the category  where the objects are open sets \\(U \\subset X\\) and morphisms are inclusion morphisms.  Create a functor </p> \\[ F: \\open(X) \\to \\Set   \\] <p>where on objects \\(F(U) = \\{f: U \\to \\rr \\mid f \\text{ is continuous}\\}\\). That is, how  should \\(F\\) act on the morphisms for this to be a functor?</p> <p>\\item[9.]  Let \\(k\\) be a field. With each field, we may associate \\(k\\) with the category  \\(\\vect_k\\) which consists of finite dimensional vector spaces \\(V\\)  over \\(k\\). Is this process functorial? That is, do we have a functor </p> \\[ \\vect: \\fld \\to \\cat  \\] <p>where \\(\\vect(k) = \\vect_k\\)?\\ Hint: No. But explain why it breaks. \\end{itemize}</p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Examples%20of%20Categories/","title":"1.4. Examples of Categories","text":"<p>Now that we have some idea of basic categories and a few examples in mind on  how they work, we introduce more examples in this section to deepen our understanding. Categories are extremely abundant in mathematics, so it is not difficult to  find examples.</p> <p>Without proof, we comment that the categories below truly form categories.  To discuss these categories, we will use the notation in the leftmost column. \\begin{center}</p> Category Objects Morphisms \\(\\finset\\) Finite sets \\(X\\) Functions \\(f: X \\to Y\\) \\(\\vect_K\\) Vector spaces over \\(k\\) Linear transformations \\(T: V \\to W\\) \\(\\mon\\) Monoids \\((M, \\cdot)\\) Monoid homomorphisms \\(\\psi: M \\to M'\\) FinGrp Finite Groups Group homomorphisms \\(\\phi: (G, \\cdot) \\to (H, \\cdot)\\) \\(\\ab\\) Abelian Groups \\((G, \\cdot)\\) Group homomorphisms FinAb Finite Abelian Groups \\((G, \\cdot)\\) Group homomorphisms Ring Rings \\((R, \\cdot, +)\\) Ring homomorphisms \\(\\phi: (R, \\cdot, +) \\to (S, \\cdot, +)\\) CRing Commutative Rings \\((R, \\cdot, +)\\) Ring homomorphisms \\(\\ring\\) Rings \\((R, \\cdot, +)\\) with identity \\(1 \\ne 0\\) Ring homomorphisms \\(R\\rmod\\) \\(R\\)-modules \\((M, +)\\) \\(R\\)-module homomorphisms \\(\\fld\\) Fields \\(k\\) Field homomorphisms \\(\\top^*\\) Topological spaces \\((X, x_0)\\) with basepoint \\(x_0 \\in X\\) Continuous functions preserving basepoints Toph Topological spaces \\((X, \\tau)\\) Homotopy equivalence classes Haus Hausdorff topological spaces \\((X, \\tau)\\) Continuous functions CHaus Compact Hausdorff topological spaces \\((X, \\tau)\\) Continuous functions DMan Differentiable manifolds \\(M\\) Differentiable functions \\(\\phi: M \\to M'\\) LieAlg Lie algebras \\(\\mathfrak{g}\\) Lie algebra homomorphisms Grph Graphs \\((G, E, V)\\) Graph homomorphisms <p>\\end{center} Now that we are aquainted with some of the categories that we'll be working with,  we'll introduce more interesting categories that become useful. However,  these categories are less trivial than the ones above, i.e it takes a bit of work  to see how they form into categories. </p> <p> Let \\(X\\) be a nonempty set. We can regard \\(X\\) as a  category where  \\begin{description} \\item[Objects.] All elements of \\(X\\). \\item[Morphisms.] All morphisms are identity morphisms, and there are  no morphisms between any two distinct objects. \\end{description} This category, while fairly trivial, is called a discrete category. </p> <p> Consider any of the categories \\(\\mon\\), \\(\\grp\\), \\(\\ring\\), or  \\(R\\rmod\\). For any object of these categories, we can create  the notion of a grading. Such a concept is a useful algebraic construction which  appears in different areas of mathematics. For simplicity, we'll consider a grading  on a group. <p>A group \\(G\\) is said to be **\\(\\mathbb{N**\\)-graded} if there exists a family  of groups \\(G_1, G_2, \\dots, G_n, \\dots\\) such that \\(G = \\bigoplus_{i=1}G_i\\). An example of this is the group \\((\\mathbb{R}[x], +)\\), the single variable polynomials  in one variable. To see that this is graded, observe that any  polynomial \\(p(x)\\) is of the form</p> \\[ p(x) = a_0 + a_1x + a_2x^2 + \\cdots + a_{n}x^n. \\] <p>Note that \\(p(x)\\) consists of \"components\", i.e., different powers  of \\(x\\). If we let </p> \\[ \\mathbb{R}_n[x] = \\{ax^n \\mid a \\in \\mathbb{R}\\}     \\] <p>then we see that \\(\\mathbb{R}[x] = \\bigoplus_{i = 0}\\mathbb{R}_n[x]\\). </p> <p>More generally, if \\(\\lambda\\) is an indexing set, we say a group \\(G\\) is \\(\\lambda\\)-graded  if there is a family of groups \\(G_i, i \\in \\lambda\\) such that \\(G = \\bigoplus_{i \\in \\lambda}G_i\\). In addition, if \\(G = \\bigoplus_{i \\in \\lambda}G_i\\) and \\(H = \\bigoplus_{i \\in \\lambda}H_i\\)  are two graded groups such that \\(\\phi_i:G_i \\to H_i\\) is a group homomorphism,  then we say \\(\\phi: G \\to H\\) is a \\(\\lambda\\)-graded homomorphism.</p> <p>With that said, we can define the category of graded groups to be the category  GrGrp, (read as \"graded groups\") described as  \\begin{description} \\item[Objects.] \\(\\lambda\\)-graded groups \\(G = \\bigoplus_{i \\in \\lambda}\\)  for some set \\(\\lambda\\)  \\item[Morphisms.] Graded homomorphisms between graded groups. \\end{description} As we said before, this produces many graded categories, including GrMon,  GrRing, \\(**GrMod**_R\\) etc. </p> <p> A monoid is a set \\(M\\) equipped with an operation \\(\\cdot: M \\times M \\to M\\) and  an identity \\(e\\) such that \\(e\\cdot m = m \\cdot e = m\\) for all \\(m \\in M\\). In other words,  monoids are like groups, in that we drop the requirement of an inverse.  <p>Let \\(\\cc\\) be a category with one object; denote this object as \\(\\bullet\\).  As we have one object, we have one homset. We can then interpret \\(M\\) as a category by setting </p> \\[ \\hom_{\\cc}(\\bullet, \\bullet) = M.    \\] <p>Thus each \\(m \\in M\\) corresponds to a morphism. So, we can write each morphism in the category as \\(f_{m}: \\bullet \\to \\bullet\\)  for some \\(m \\in M\\).  We then write \\(f_{e} = 1_{\\bullet}\\), the identity, and more generally  define composition in the category as</p> \\[ f_m \\circ f_{m'} = f_{m \\cdot m'}.    \\] <p>Since \\(M\\) is a monoid, and its multiplication is associative, we see that composition defined in this way is also associative.  Further, for each \\(f_m\\), we have that </p> \\[ f_{e} \\circ f_m =  f_m \\circ f_{e} = f_m \\] <p>since \\(e \\cdot m = m \\cdot e = m\\) in the monoid \\(M\\). Thus we can interpret monoids  as one object categories.  </p> <p> A category \\(\\mathcal{P}\\) is said to be thin or a preorder if there is at most one morphism \\(f:  A \\to B\\) for each \\(A, B \\in \\mathcal{P}\\).   The simplest thin categories are of the form below  \\  but they may also have more complex shapes such as the category below.  \\  Thin categories are very common since we often times only care about one single  type of relation between any two objects. An example of such a relation is a binary relation;  for any two real numbers \\(x, y \\in \\rr\\), we know that either \\(x \\le y\\) or \\(y \\le x\\). </p> <p>This intuition is actually not very far off. Given a thin category \\(\\mathcal{P}\\), define the binary relation \\(\\le\\) on the objects \\(\\ob(\\mathcal{P})\\) as follows.  For any pair of objects \\(A, B \\in \\mathcal{P}\\), we have that </p> \\[ A \\le B \\text{ if and only if there exists an morphism } A \\to B.  \\] <p>Some things are to be said about this relation:</p> <ul> <li> <p>For each object \\(A\\),  there always exists a morphism \\(A \\to A\\) (namely, the identity). This implies that \\(A \\le A\\) for all objects \\(A\\), so that \\(\\le\\) is reflexive.</p> </li> <li> <p>If \\(f: A \\to B\\) and \\(g: B \\to C\\), then we have that  \\(A \\le B\\) and \\(B \\le C\\). Since we may compose morphisms, we  have that \\(g \\circ f: A \\to C\\). Therefore, \\(A \\le C\\), so that \\(\\le\\) \\(\\le\\) is transitive.</p> </li> </ul> <p>Hence, \\(\\mathcal{P}\\) is really just a set with a reflexive and transitive binary relation. However, this is exactly the definition of a preorder! Therefore, preorders \\(P\\) can be regarded as categories with at most  one morphism between any two objects, and vice versa.</p> <p>Preorders can also turn into partial orders, which have the axiom that </p> \\[ \\text{if } p \\le p' \\text{ and } p' \\le p \\text{ then } p = p'. \\] <p>or linear orders, where for any \\(p,  p'\\) we have that \\(p \\le p'\\) or \\(p' \\le p\\).</p> <p> Here we introduce some examples of thin categories. \\begin{description} \\item[Natural Numbers.]  The sets \\(\\{1, 2, \\dots, n\\}\\) for any \\(n \\in N\\) are linear orders, each of which forms a category as pictured below. \\  In this figure, the loops represent the trivial identity functions. <p>This example can also be generalized to include \\(\\mathbb{N}, \\mathbb{Z},  \\mathbb{Q},\\) and \\(\\mathbb{R}\\). </p> <p>\\item[Subsets.]  Let \\(X\\) be a set. Then one can form a category \\(\\text{Subsets}(X)\\) where the  objects are subsets of \\(X\\) and the morphisms are inclusion morphisms. Hence, there is at most one morphism between any two sets.</p> <p>Since there is at most one morphism between any two objects of the  category, we see that this forms a thin category, and hence a partial ordering.  What this then tells us is that subset containment determines an ordering,  specifically a partial ordering. </p> <p>\\item[Open Sets.]  Let \\((X, \\tau)\\) be a topological space. Define the category  \\(\\open(X)\\) to be the category whose objects are the open sets of \\(X\\)  and morphisms \\(U \\to V\\) are inclusion morphisms \\(i: U \\to V\\) whenever \\(U \\subset V\\). Hence, there is at most one morphism between any two open sets, so that this  also forms a preorder.</p> <p>\\item[Subgroups.]  Let \\(G\\) be a group. We can similarly define the category  \\(**SbGrp**(G)\\) to be the category whose objects consists  of subgroups \\(H \\le G\\), and whose morphisms are inclusion homomorphisms. This is just like the last example; and, as in the last example,  there is at most one morphism between any two subgroups \\(H, K\\) of \\(G\\)  (either \\(i: H \\to K\\) or \\(i: K \\to H\\)). Hence, we can place a partial ordering  on this, so that subgroup containment is a partial ordering.</p> <p>\\item[Ideals.] Let \\(R\\) be a ring. Then we can form a category \\(**Ideals**(R)\\)  whose objects are the ideals \\(I\\) of \\(R\\) and whose morphisms are inclusion  morphisms. As we've seen, this forms a thin category.  \\end{description}</p> <p></p> <p> Let \\(B_n\\) be the set of braids on \\(n\\) strands. Recall that  \\(B_n\\) forms a group where the group product is composition, and where the identity is simply  \\(n\\) parallel strands. Each braid group actually has a nice presentation: \\[ B_n = \\left&lt; \\sigma_1, \\dots, \\sigma_{n-1} \\mid \\sigma_i\\sigma_{i+1}\\sigma_{i} = \\sigma_{i+1}\\sigma_{i}\\sigma_{i+1}^{(\\texttt{1})}, \\sigma_i\\sigma_j = \\sigma_j\\sigma_i^{(\\texttt{2})} \\right&gt;    \\] <p>where (\\texttt{1}) holds only when \\(1 \\le i \\le n - 2\\) and (\\texttt{2}) hold only when  \\(|i - j| &gt; 1\\). These two laws are imposed so that they match our geometric intuition, so that if we were to replace the strands with real, phyiscal  ropes then they would behave the same way.</p> <p>Each generator \\(\\sigma_i\\) is interpreted as swapping the \\(i\\)-th strand over the \\((i+1)\\)-th strand, while \\(\\sigma_i\\) is swapping the \\((i+1)\\)-th strand over the \\(i\\)-th strand. Below are some example generators. \\  The reason why we care about these generators is because every braid can be expressed  by over and under crossings (although such an expression may not be unique). Now, the group multiplication in this group is simply stacking of braids. For example, the braid \\  can be obtained by stacking \\(\\sigma_1, \\sigma_2\\) and then \\(\\sigma_1\\) again.  Hence, the  braid \\(\\sigma_1\\sigma_2\\sigma_1\\). </p> <p>Now with the family of braid groups \\(B_1, B_2, \\dots,\\) we can form a category \\(\\mathbb{B}\\) as follows.  \\begin{description} \\item[Objects.] Positive integers \\(1,2, \\dots,\\) \\item[Morphisms.] For any pair of positive integers \\(n,m\\), we have that </p> \\[ \\hom_{\\mathbb{B}}(n,m) =  \\begin{cases} B_n &amp; \\text{if } n = m\\\\ \\varnothing &amp; n \\ne m \\end{cases} \\] <p>\\end{description} Hence we only have morphisms \\(f: n \\to m\\) when \\(n = m\\). Furthermore, each  morphism is a braid. Composition is then group multiplication. The identity for each  object \\(n\\) is the identity braid of \\(n\\) parallel strands. As group multiplication  is associative, the composition in this category is associative, so we see that this truly  does form a category.  </p> <p>The following examples demonstrates again that morphisms are not always  functions, or mappings of some kind.</p> <p> Let \\(R\\) be a ring with identity \\(1 \\ne 0\\). For every pair of positive  integers \\(m, n\\), let \\(M_{m, n}(R)\\) be the set of all \\(m \\times n\\) matrices.  Now recall that for an \\(m \\times n\\) matrix \\(A\\) and a \\(n \\times p\\) matrix  \\(B\\),  the product \\(AB\\) is an \\(m \\times p\\) matrix.   \\[ \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n}\\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\begin{pmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1p}\\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ b_{n1} &amp; b_{n2} &amp; \\cdots &amp; b_{np} \\end{pmatrix}   = \\begin{pmatrix} c_{11} &amp; c_{12} &amp; \\cdots &amp; c_{1p}\\\\ c_{21} &amp; c_{22} &amp; \\cdots &amp; c_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ c_{n1} &amp; c_{n2} &amp; \\cdots &amp; c_{np} \\end{pmatrix}   \\] <p>where \\(\\displaystyle c_{ij} = \\sum_{k= 1}^{n}a_{ik}b_{kj}\\). This can rephrased as saying that  we have a multiplication map as below.</p> \\[ M_{m,n}(R)\\times M_{n,p}(R) \\to M_{m, p}(R) \\] <p>Since matrix multiplication is associative, we can also say that the above mapping  is associative. </p> <p>This however should feel sort of similar to the process of composition, say for example in \\(\\Set\\),  where if we  have functions \\(f: X \\to Y\\)  and \\(g: Y \\to Z\\) we obtain a function \\(g \\circ f: X \\to Z\\). If we follow this intuition, we can consider a matrix \\(A\\)  of shape \\(m \\times n\\) as a morphism from \\(m \\to n\\). Similarly, \\(B\\) can be regarded a morphism from \\(n \\to p\\). This together implies that  \\(AB\\) is a morphism from \\(m \\to p\\).  This should feel strange, because we are used to thinking of a morphism as some kind  of function. But it works; we can form a category where \\begin{description} \\item[Objects.] The objects are positive integers \\(m\\). \\item[Morphisms.] The morphisms are matrices. Specifically, for any pair of  objects \\(m,n\\), </p> \\[ \\hom_{\\cc}(m, n) = M_{m, n}(R). \\] <p>Here, composition is simply matrix multiplication. \\end{description} Observe now that our initial observation regarding matrix multiplication translates to a statement regarding whenever two matrices \\(A\\) and \\(B\\) are  \"composable\" (i.e., whenever we can multiply them). That is,  our mapping \\(M_{m,n}(R)\\times M_{n,p}(R) \\to M_{m, p}\\) can be rephrased as composition</p> \\[ \\circ: \\hom_{\\cc}(m, n) \\times \\hom_{\\cc}(n, p) \\to \\hom_{\\cc}(m, p) \\] <p>Associativity of matrix multiplication translates to associativity of composition.  Finally, note that for each object (positive integer) \\(n\\), the identity morphism is  simply the identity matrix. </p> \\[ 1_n  := I_n =  \\begin{pmatrix} 1 &amp; 0 &amp; 0  &amp; \\cdots &amp; 0\\\\ 0 &amp; 1 &amp; 0  &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1\\\\ \\end{pmatrix}. \\] <p>Thus we see that we have all the necessary ingredients to declare this to be a category.  </p> <p> Let \\(G\\) be a group, and recall that \\(G\\) is equipped with some binary  operator \\(\\cdot: G \\times G \\to G\\) which satisfies associativity.  Because this is a two-variable function on \\(G\\) every  \\(g \\in G\\) induces a map \\[ (-) \\cdot g := f_g: G \\to G \\qquad  \\] <p>This then gives rise to a collection of maps \\(f_g: G \\to G\\) for each  \\(g \\in G\\), which we can picture as below.  \\  In particular, if \\(e\\in G\\) is the identity, then \\(f_e = 1_G\\).  Moreover, composition of these maps is associative. Thus we can think of this as a category, specifically  one with one object, whose morphisms \\(f:G \\to G\\) are induced by the elements \\(g \\in G\\). Also, note that each such map is an isomorphism, since its inverse  is given by \\((-) \\cdot g^{-1}: G \\to G\\). </p> <p>Now we can step up a level of generality. Let \\(X\\) be a set, and  suppose we have a group action \\(\\phi: X \\times G \\to X\\). If we denote  \\(\\phi(g, -):= \\phi_h: X \\to X\\) for each \\(g \\in G\\), then since \\(\\phi\\)  is a group action we have that \\(\\phi_g \\circ \\phi_{g'} = \\phi_{g\\cdot g'}\\)  and \\(\\phi_e = 1_X\\). Hence composition is associative and we have a well-behaved identity morphism. Usually, when we draw group actions, we think of something  like this:  \\  What we're seeing is that group actions can be phrased as a category with one object,  with morphisms as isomorphisms. This generalizes our previous discussion, which makes  sense since groups are trivial examples of group actions by setting \\(X = G\\).  </p> <p>{\\large Exercises \\vspace{0.2cm}}</p> <ul> <li> <p>[1.]  Let \\(n\\) be a positive integer, and consider a group \\(G\\) such that  \\(g^n = 1\\) for all elements \\(g \\in G\\). Show that if we take these groups to be  our objects, and group homomorphisms to be our morphisms, then  this forms a category \\(\\grp_n\\). \\vspace{0.2cm}</p> </li> <li> <p>[2.]  Consider an infinite family of groups \\(G_1, G_2, \\dots, G_n, \\dots\\) Show that we have a category \\(**G**\\) where \\begin{description}</p> </li> <li> <p>[Objects.] The positive integers \\(1, 2, \\dots, n, \\dots\\)</p> </li> <li> <p>[Morphisms.] For any two positive integers \\(n,m\\), we define </p> </li> </ul> \\[ \\hom_{**G**}(n,m) =  \\begin{cases} G_n &amp; \\text{if } n = m\\\\ \\varnothing &amp; \\text{otherwise}. \\end{cases}   \\] <p>\\end{description} This example can be applied to many interesting families of groups, since they  often come graded (i.e., they often are indexed by the positive integers.) For instance, the braid groups \\(B_1, B_2, \\dots,\\) are such an example. \\vspace{0.2cm}</p> <ul> <li> <p>[3.]  Let \\(f: X \\to Y\\) be a function between two sets.  We say \\(f\\) has the \"finite-to-one\" property if \\(f^{-1}(y)\\) is always a  finite set for all \\(y \\in Y\\). Show that we have a (large) category,  denoted  \\(\\Set_{FTO}\\), where  \\begin{description}</p> </li> <li> <p>[Objects.] All sets \\(X\\).</p> </li> <li> <p>[Morphisms.] functions \\(f\\) with the finite-to-one property.  \\end{description} \\vspace{0.2cm}</p> </li> <li> <p>[4.] Let \\(X\\) and \\(Y\\) be sets. A binary relation \\(R\\) on \\(X\\) and \\(Y\\) is any subset of \\(X \\times Y\\). For two elements  \\(x \\in X, y \\in Y\\), we then write  \\(xRy\\) if \\((x,y) \\in R\\). Binary relations can be specialized to describe  functions and order relations in set theory.</p> </li> </ul> <p>Show that we can form a category where  \\begin{description}</p> <ul> <li> <p>[Objects.] All sets \\(X\\). </p> </li> <li> <p>[Morphisms.] For any two sets \\(X,Y\\), we write, by abuse of notation, \\(R: X \\to Y\\) as a morphism for each relation \\(R\\) on \\(X\\) and \\(Y\\). \\end{description} This category is called Rel, to indicate that it is the  category of relations.\\ Hint:  Define composition in this category as follows. Suppose  \\(R:X\\to Y\\) is a relation on \\(X\\) and \\(Y\\) and \\(P: Y \\to Z\\) is a binary relation on \\(Y\\) and \\(Z\\). Then the composite relation \\(Q: X \\to Z\\) is given by </p> </li> </ul> \\[ Q = \\{(x, z) \\mid \\text{there exist } y \\in Y \\text{ such that } (x,y)\\in R, (y,z) \\in P \\}. \\] <ul> <li>[5.] Recall that for a two metric spaces \\((M, d_M)\\) and \\((N, d_N)\\), where  \\(d_M: M \\times M \\to M\\) and \\(d_N: N \\times N \\to N\\) are the metrics,  we say a function \\(f: M \\to N\\) is a Lipschitz-1 map  with Lipschitz constant 1 if </li> </ul> \\[ d_N(f(x), f(y)) \\le d_M(x, y)   \\] <p>for all \\(x, y \\in M\\). Using this concept, show that we have a category  where  \\begin{description}</p> <ul> <li> <p>[Objects.] Metric spaces \\(M\\) </p> </li> <li> <p>[Morphisms.] Lipschitz-1 maps with Lipschitz constant 1.  \\end{description} This category is commonly denoted as Met.</p> </li> <li> <p>[6.] Let \\(G\\) be a group. We say that \\(G\\) acts on a set \\(X\\) if we have a  function \\(\\phi: G \\times X \\to X\\) such that  \\begin{itemize}</p> </li> <li> <p>[\\(\\bullet\\)] \\(e \\cdot x = x\\)</p> </li> <li> <p>[\\(\\bullet\\)] \\(h\\cdot (g \\cdot x) = (hg)\\cdot x\\)</p> </li> </ul> <p>Such an \\(X\\) is sometimes called a G-set. Note here that we represent \\(\\phi(g, x)\\) as \\(g \\cdot x\\). Now suppose \\(X, Y\\) are  two sets for which \\(G\\) acts on. Then we define a morphism of \\(G\\) sets to be a  function \\(f: X \\to Y\\) such that \\(f(g \\cdot x) = g \\cdot f(x)\\). Such a map is called  \\(G\\) equivariant. Show that we have  a category \\(G**-Sets**\\) where  \\begin{description} \\item[Objects.] All \\(G\\)-sets (i.e., sets with a group action by \\(G\\)) \\item[Morphisms.] \\(G\\) equivariant maps.  \\end{description} \\end{itemize}</p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Forgetful%2C%20Full%20and%20Faithful%20Functors./","title":"1.8. Forgetful, Full and Faithful Functors.","text":"<p>Like functions, functors can be composed to form new functors.</p> <p> If \\(\\mathcal{A}, \\mathcal{B}\\) and \\(\\mathcal{C}\\) are categories where  <p> are functors, then we can define the \\textbf{composite functor} \\(G \\circ F: \\mathcal{A} \\to \\mathcal{C}\\) where </p> \\[ C \\mapsto G(F(C)) \\in \\cc \\quad\\quad (f: A \\to B) \\mapsto G(F(f)) \\in \\hom_{\\cc}(G(F(A)), G(F(B))). \\] <p> We've now reached something quite important. We have the notion of  a category, as well as the notion of a functor which acts as a map between  categories. Moreover, every category \\(\\cc\\) is equipped with an identity functor  \\(1_{\\cc}: \\cc \\to \\cc\\), functor composition is associative, and so we  may  form the category of categories CAT where  \\begin{description} \\item[Objects.] All categories (large and small) \\item[Morphisms.] All functors between such categories. \\end{description} If we instead restrict our objects to all small categories,  we obtain the category \\(\\cat\\), which is usually what we'll work with. \\textcolor{NavyBlue}{Overall, what we see is that functors are the rightful \"morphisms\" between categories.}</p> <p>Since functors are, in an abstract sense, morphisms, and we know that for general  morphisms, there exists a concept of an isomorphism, we can directly apply  such a notion to define what an isomorphic functor is.</p> <p> Let \\(\\cc\\) and \\(\\dd\\) be two categories. Then a functor \\(F: \\cc \\to \\dd\\) is said to be a isomorphism if it is bijective on both objects and arrows. \\ <p>Equivalently, \\(F\\) is an isomorphic functor if and only if there exists a functor \\(G : \\dd \\to \\cc\\) such that \\(F \\circ G\\) is the identity on \\(\\cc\\) and \\(G \\circ F\\) is the identity on \\(\\dd\\) (both in terms of objects and arrows). </p> <p>\\textcolor{MidnightBlue}{Sometimes when a functor maps objects  from one category to another, the underlying structure of the objects in the first category gets lost. Or perhaps a binary operation acting on the elements in the first set of objects becomes lost.  For this, we have a special name.}</p> <p> Let \\(\\cc\\) and \\(\\dd\\) be categories and suppose \\(F: \\cc \\to \\dd\\) is a functor. Then \\(F\\) is said to be forgetful whenever \\(F\\) does not preserve the axioms and structure present in the objects of \\(\\cc\\) (whether it be algebraic or some kind of ordering). </p> <p>The above definition isn't precise, although it is a useful notion to have. It will eventually become precise, but we'll comment  more on that after a few examples.</p> <p> Consider a group \\((G, \\cdot)\\) with \\(\\cdot\\) the binary operation. In some sense, groups are simply sets with added structure, while  group homomorphisms are simply functions that respect group structure. Hence we can create a map between \\(\\grp\\) and \\(\\Set\\) that  forgets this structure: \\[ (G,  \\cdot) \\mapsto G \\qquad \\phi: (G, \\cdot) \\to (H, +) \\mapsto \\phi: G \\to H. \\] <p>We can demonstrate that this process is functorial. Observe that if \\(1_G: (G, \\cdot) \\to (G, \\cdot)\\) is the identity group homomorphism,  then one can readily note that \\(1_G(g) = g\\) for all \\(g \\in G\\), so that it is also an identity function  on the underlying set \\(G\\). Therefore, \\(F(1_G) = 1_{F(G)}\\)</p> <p>Next, if \\(\\phi: G \\to H\\) and \\(\\psi: H \\to K\\) are group homomorphisms, then \\(F(\\psi \\circ \\phi)\\) is the underlying function \\(\\psi \\circ \\phi: G \\to K\\).  Note however that for each \\(g \\in G\\),</p> \\[ F(\\psi \\circ \\phi)(g) = \\psi(\\phi(g)) = F(\\psi) \\circ F(\\phi)(g) \\implies  F(\\psi \\circ \\phi) = F(\\psi) \\circ F(\\phi). \\] <p>Hence, we see that we have a forgetful functor \\(F: \\grp \\to \\Set\\) which  leaves behind group operations, and moreover regards every group  homomorphism as a function. </p> <p> Let \\((R, +, \\cdot)\\) be a ring. Recall that \\((R, +)\\) (alone with its  addition) is an abelian group. Hence we can forget the structure  of \\(\\cdot: R\\times R \\to R\\) and, in a forgetful sense, treat every  ring as an abelian group.   <p>This then defines a forgetful functor  \\(F: **Rng** \\to \\ab\\) which simply maps a ring to its abelian group. This works on the morphisms, since every ring homomorphism \\(\\phi: (R, +, \\cdot) \\to (S, +, \\cdot)\\) is a group homomorphism \\(\\phi: (R, +) \\to (S, +)\\) on the abelian groups.  </p> <p> Consider the category \\(\\top\\). Each object in top is a pair \\((X, \\tau)\\) where \\(\\tau\\) is a topology on \\(X\\).  Moreover, continuous functions are simply functions. This forgetful process is  also functorial: \\[ (X, \\tau) \\mapsto X \\qquad f: (X, \\tau) \\to (Y, \\tau') \\mapsto f: X \\to Y. \\] <p>This then gives us the forgetful functor \\(F: \\top \\to \\Set\\). </p> <p>Some things need to be said about a forgetful functors. You might have  noticed that our definition of a forgetful functor was not at all mathematically  rigorous. This is because to define forgetful functors we have two main options:</p> <ul> <li> <p>[1.] Use very deep set theory and logic to characterize  the data of a category; then define forgetfulness as forgetting some  of the data. </p> </li> <li> <p>[2.] Define a forgetful functor to be the left adjoint  of a free functor \\(F: \\cc \\to \\dd\\) (usually, \\(\\cc = \\Set\\))   </p> </li> </ul> <p>Option 1. sounds like a pain, and I don't know any logic. I'm sure  the reader is probably not interested in going on that kind of a ride anyways.  Option 2. is not possible right now, but it will be once we learn about  adjunctions.</p> <p>Thus, using the tools we have right now, we cannot create a rigorous mathematical definition  of a forgetful functor. This does not mean what we're doing is nonsense; it just  means we're being sloppy in the interest of pedagogy. Once we learn about adjunctions  things will make more sense, so the reader is urged to not worry too much about the  rigor of a forgetful functor. </p> <p>The sloppiness of our work regarding forgetful functors (i.e., us non-rigorously  being like \"Hey! See this piece of data? Let's throw it away!\") might nevertheless be of some discomfort for the pedantic reader. This is because we cannot rigorously demonstrate what a forgetful functor is at  this point; hence a reader interested in true understanding won't be able to fully  do so at this point. Sometimes, however, understanding how something works is aided by  understanding when something doesn't work. Hence to comfort the pedantic reader, we introduce an example where one might  intuitively think such a forgetful functor exists, but it in fact does not. </p> <p> Recall that the category hTop has objects as topological spaces  and morphisms as homotopy classes between topological spaces. One might  prematurely believe that there is a forgetful functor \\(**hTop** \\to \\Set\\),  but that is not possible. <p>In trying to do so, we naturally associate topological spaces \\((X, \\tau)\\) with its  underlying set \\(X\\). On morphisms, it's trickier. Suppose \\([f: X \\to Y]\\) is  a homotopy equivalence class with \\(f: X \\to Y\\) as the continuous function representing  the class. Choose any \\(f': X \\to Y \\in [f]\\);  we may very well choose \\(f\\) itself  in which case \\(f' = f\\), and set \\(F(f') = f'\\), where \\(f' \\in \\Set\\) is regarded  as a function. </p> <p>This breaks when we encounter composition. Suppose \\(f: X \\to Y\\) and \\(g: Y \\to Z\\)  are continuous functions. Let \\(F([f]) =f'\\), \\(G([g]) = g'\\),  and \\(F([g \\circ  f]) = (g \\circ f)'\\) where \\(f', g',\\) and \\((g\\circ f)\\) are any  elements of \\([f], [g], [g' \\circ f ']\\) respectively. Then in no case can we always expect that </p> \\[ F(g \\circ f) = F(g) \\circ F(f) \\implies (g \\circ f)' = g' \\circ f'.   \\] <p>Hence this forgetful process cannot behave functorially. </p> <p>Next, we introduce the notion of full and  faithful functors. Towards that goal, consider a functor  \\(F: \\cc \\to \\dd\\) between locally small categories. Then for every  pair of objects \\(A, B \\in \\cc\\), there is a function </p> \\[ F_{A,B}: \\hom_{\\cc}(A, B) \\to \\hom_{\\dd}(F(A), F(B)) \\] <p>where a morphism \\(f:A \\to B\\) is sent to its image \\(F(f): F(A) \\to F(B)\\)  under the functor \\(F\\).  \\ </p> <p>As we have a family of functions \\(F_{A,B}\\), we can ask: when is this function surjective or injective? This motivates the following definitions. </p> <p> Let \\(F: \\cc\\to \\dd\\) be a functor between locally small categories.  We say \\(F\\) is  <ul> <li> <p>Full if \\(F_{A,B}\\) is surjective</p> </li> <li> <p>Faithful if \\(F_{A,B}\\) is injective.</p> </li> </ul> <p>If \\(F_{A,B}\\) is an isomorphism, we say \\(F\\) is fully faithful. </p> <p>Now we completely ignored the situation for when \\(\\cc, \\dd\\) are not locally small.  This is out of pedagogical interest; if \\(\\cc, \\dd\\) are not locally small then we do  not have the function described above. However, the concept of full and faithful  can still be described; it's just not as nice of a description as before.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor. <ul> <li> <p>Full if for all \\(A, B\\), every morphism  \\(g: F(A) \\to F(B)\\) in \\(\\dd\\) is the image of some \\(f: A \\to B\\) in \\(\\cc\\) </p> </li> <li> <p>Faithful if for all \\(A, B\\), we have that if \\(f_1, f_2: A \\to B\\) with \\(F(f_1) = F(f_2)\\), then\\(f_1 = f_2\\).</p> </li> </ul> <p>We then say \\(F\\) is a fully faithful if it is both full and faithful. </p> <p> Consider the forgetful functor \\(F: \\top \\to \\Set\\)  which we introduced earlier; topological spaces \\((X, \\tau)\\) are  sent to their underlying sets \\(X\\) while continuous functions  \\(f: (X, \\tau) \\to (Y, \\tau')\\) are regarded as functions \\(f: X \\to Y\\). This functor is faithful, since if two continuous functions  are equal as set maps, then they are equal as continuous functions.  The fact that this functor is faithful is simply due to the fact that the  extra data on a continuous function, i.e., its continuity, does not  interfere with its behavior of being a set function in  sending points \\(X\\) to  \\(Y\\). <p>Note however that this function is clearly not full, because not every  function \\(g: X \\to Y\\) can be regarded as a continuous function between  the topological spaces.  </p> <p> Let \\((G, \\cdot)\\) and  \\((H, \\cdot)\\) be a group.  Regard both groups as one object categories \\(\\cc\\) and \\(\\dd\\) with objects \\(\\textcolor{NavyBlue}{\\bullet}\\) and \\(\\textcolor{Orange}{\\bullet}\\)  where we set  \\[ \\hom_{\\cc}(\\textcolor{NavyBlue}{\\bullet}, \\textcolor{NavyBlue}{\\bullet}) = G \\qquad \\hom_{\\cc}(\\textcolor{Orange}{\\bullet}, \\textcolor{Orange}{\\bullet}) = H \\] <p>so that each \\(g \\in G\\) is now a morphism \\(g: \\textcolor{NavyBlue}{\\bullet} \\to \\textcolor{NavyBlue}{\\bullet}\\),  and vice versa for every \\(h \\in G\\), so that composition is given by the group structure. If we have a functor \\(F: \\cc \\to \\dd\\) between these categories,  then the function we introduced simply becomes a set function </p> \\[ F_{\\textcolor{NavyBlue}{\\bullet}, \\textcolor{NavyBlue}{\\bullet}} :  \\hom_{\\cc}(\\textcolor{NavyBlue}{\\bullet}, \\textcolor{NavyBlue}{\\bullet}) \\to  \\hom_{\\dd}(\\textcolor{Orange}{\\bullet}, \\textcolor{Orange}{\\bullet}). \\] <p>However, the functorial properties allow this to extend to a group homomorphism from \\(G\\) to \\(H\\). Therefore, we see that if \\(F: \\cc \\to \\dd\\) is full, it extends to a surjective  group homomorphism. If it is faithful, it extends to an injective group homomorphism.  </p> <p> Consider the category of \\(\\grp\\), and recall it has a  forgetful functor \\(F: \\grp \\to \\Set\\). This functor is actually  fully faithful; to see this, consider two group homomorphisms  \\(\\phi,\\psi: (G, \\cdot) \\to (H, \\cdot)\\), and suppose that  \\(F(\\phi) = F(\\psi)\\). Then this implies that  \\(F(\\phi)(g) = F(\\psi)(g)\\) for each \\(g \\in G\\). However,  \\(F(\\phi)(g) = \\phi(g)\\) and vice versa for \\(\\psi\\). Therefore, we  have that \\(\\phi = \\psi\\), so that the forgetful functor \\(F\\) is a  faithful functor.  </p> <p>The above example can be repeated for many familiar categories, which motivates the  following definition. </p> <p> A category \\(\\cc\\) is said to be concrete if there is a faithful  functor \\(F: \\cc \\to \\Set\\). </p> <p>Examples of concrete categories includ \\(\\grp\\), \\(\\top\\), \\(R\\rmod\\), and many others since these categories are, in some sense, built from \\(\\Set\\).  Their objects are sets, and their morphisms are functions with extra properties; nevertheless,  at the end of the day the morphisms are still functions. Note in particular  that these categories are not subcategories of \\(\\Set\\), but they are still deeply  related to this category in a way that the above definition illuminates. </p> <p>We don't have the tools right now, but we will later show that every small category \\(\\cc\\) is  a concrete category. </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.]  In this exercise, you'll demonstrate that the image of a functor is generally  not a category, but that full functors remedy the situation.  \\begin{itemize}</p> </li> <li> <p>[(i.)] Let \\(F: \\cc \\to \\dd\\). Define the image of \\(F\\)  in \\(\\dd\\) to consist of  \\begin{description}</p> </li> <li> <p>[Objects.] All \\(F(A)\\) with \\(A \\in \\cc\\) </p> </li> <li> <p>[Morphisms.] For any two objects \\(F(A)\\) and \\(F(B)\\), we have that </p> </li> </ul> \\[ \\hom_{\\dd}(F(A),  F(B))=\\{ F(f) \\mid f: A \\to B \\}. \\] <p>\\end{description} Show that this is not always a category. In general, the image of a functor  is not a category.  \\ Hint: Picture two categories \\(\\cc\\) and \\(\\dd\\) below \\  and consider the functor \\(F(A) = X, F(B) = F(C) = Y\\), and \\(F(D) = Z\\).  Explain what goes wrong, and more generally why the image of a functor  is not a category.</p> <ul> <li> <p>[(ii.)] Let \\(F: \\cc \\to \\dd\\) be a full functor.  Show that the image of \\(\\cc\\) under \\(F\\) forms a full subcategory of  \\(\\dd\\).  </p> </li> <li> <p>[(iii.)] By (ii), it is sufficient for \\(F\\) to be full in order for the image to be a category. Is this condition necessary for the image to form a category?  In other words, suppose the image of a functor \\(F\\) is a category. Is \\(F\\) full?</p> </li> </ul> <p>\\end{itemize}</p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Functors/","title":"1.6. Functors","text":"<p>At this point, we really have no significant reason to care about categories.  They have only so far proved to be an organizatonal tool for concepts of mathematics,  but that is about it. In this section, we introduce the abstract notion of a functor which is prevalent everywhere in mathematics. Functors are ultimately  a helpful notion which we care a lot about, but in order to define a functor  we first needed to define categories.  But as we have defined categories, we move on to defining functors. </p> <p> Let \\(\\cc\\) and \\(\\dd\\) be categories. A (covariant) functor \\(F: \\cc \\to \\dd\\) is a \"mapping\" such that <ul> <li> <p>[1.] Every \\(C \\in \\ob(\\cc)\\) is assigned uniquely to some \\(F(C) \\in \\dd\\)</p> </li> <li> <p>[2.] Every morphism \\(f: C \\to C'\\) in \\(\\cc\\) is assigned uniquely to some morphism \\(F(f): F(C) \\to  F(C')\\) in \\(\\dd\\) such that  \\begin{statement}{ProcessBlue!10}</p> </li> </ul> \\[\\begin{align} F(1_C) = 1_{F(C)} \\quad\\quad F(g \\circ f) = F(g) \\circ F(f) \\end{align}\\] <p>\\end{statement}</p> <p></p> <p>If you have seen a graph homomorphism before, this definition might seem similar.  This is no coincidence, and we'll see later on what the relationship between categories  and graphs really are. But with that intuition in mind, we can visualize the action of a functor. Below we have arbitrary categories \\(\\cc\\), \\(\\dd\\), and a functor \\(F: \\cc \\to \\dd\\).</p> <p></p> <p>In what follows, we offer some simple and abstract examples that  can get us familiar with the behavior of functors. In the next section,  we do the opposite, and instead use our abstract understanding of functors to  witness functors in real mathematical constructions\\footnote{I chose to separate this section and the  next to ease the learning curve for functors; both perspectives are necessary for true understanding  of a functor.}.  </p> <p> Denote \\(\\bm{1}\\) as the category with one  object \\(\\bullet\\) and one identity morphism \\(1_\\bullet: \\bullet \\to \\bullet\\).  Then for any category \\(\\cc\\), there exists  a unique functor \\(F: \\cc \\to \\bm{1}\\) which sends every object to \\(\\bullet\\)  and every morphism to \\(1_\\bullet\\).  <p>Conversely, there are many functors \\(F: \\bm{1} \\to \\bm{\\cc}\\). Since we only have  \\(F(\\bullet) = A\\) for some \\(A \\in \\cc\\), and \\(F(1_\\bullet) = 1_A\\), we see that this functor  simply picks out one element of \\(\\cc\\). So these functors are in correspondence  with the objects of \\(\\cc\\); the picture below may help.</p> <p>\\  </p> <p> Let \\(\\bm{2}\\) be the category with two objects \\(\\textcolor{Blue}{\\bullet}\\)  and \\(\\textcolor{Orange}{\\bullet}\\) with one nontrivial \\(f: \\textcolor{Blue}{\\bullet} \\to \\textcolor{Orange}{\\bullet}\\).  The category can be pictured as below.  \\  Suppose now that \\(\\cc\\) is an arbitrary category, and that we  have a functor \\(F: \\bm{2} \\to \\cc\\). Then note that \\(F(\\textcolor{Blue}{\\bullet}) = A\\)  and \\(F(\\textcolor{Orange}{\\bullet}) = B\\) for some objects \\(A, B \\in \\cc\\). Hence we have that  \\(F(f) = \\phi: A \\to B\\) for some \\(\\phi \\in \\cc\\). Below we have the functor pictured.  \\  Note we suppressed the identity morphisms.  Therefore, we see that this functor simply picks out morphisms \\(\\phi: A \\to B\\) in \\(\\cc\\).  So we can say that functors \\(F: \\bm{2} \\to \\cc\\) are in correspondence with the  morphisms of \\(\\cc\\).  </p> <p>Consider the very first figure of this section, Figure \\ref{figure:functor_def}. In that image we saw three objects \\(A,B,C\\) get sent to \\(F(A),F(B),F(C)\\). However, the  original commutative diagram involving \\(f, g\\) and \\(g \\circ f\\) was translated into another  commutative diagram in \\(\\dd\\) involving \\(F(f), F(g)\\) and \\(F(g \\circ f)\\).  This is because of the critical property \\(F(g \\circ f) = F(g) \\circ F(f)\\)  given by a functor. In fact, any commutative diagram translates to a commutative  diagram under a functor. </p> <p> Let \\(\\cc, \\dd\\) be categories with \\(F: \\cc \\to \\dd\\) a functor.  Suppose \\(J\\) be a commutative diagram in \\(\\cc\\). Then the diagram obtained  from the image of \\(J\\) under \\(F\\), which we denote as \\(F(J)\\), is commutative in \\(\\dd\\). </p> <p> It suffices to prove that, for any complete subdiagram \\(J'\\) of \\(J\\) involving any  two distinct paths \\[ p = f_n \\circ f_{n-1} \\circ \\cdots \\circ f_1 \\qquad q = g_m \\circ g_{m-1} \\circ \\cdots \\circ g_1 \\] <p>in \\(J\\), we have that \\(F(J')\\) is commutative in \\(\\dd\\). But this immediate.  Since \\(J'\\) is commutative in \\(\\cc\\), we have that \\(p = q\\). Hence  we see that </p> \\[ F(p) = F(q) \\implies F(f_n) \\circ F(f_{n-1}) \\cdots F(f_1) = F(g_m) \\circ F(g_{m-1}) \\circ \\cdots \\circ F(g_1). \\] <p>by repeatedly applying the composition property of a functor. Hence \\(F(J')\\) is commutative of \\(J\\). Since  </p> <p>Finally, before we move onto the next section and introduce various examples of  functors across mathematics, we introduce one of the most important  functors in basic category theory. </p> <p> Let \\(\\cc\\) be a locally small category. Then for every object \\(A\\),  we obtain the covariant hom-functor denoted as  \\[ \\hom_{\\cc}(A, -): \\cc \\to \\Set. \\] <p>where on objects \\(C \\mapsto \\hom_{\\cc}(A, C)\\) and on morphisms  \\((\\phi: C \\to C') \\mapsto \\phi^*: \\hom_{\\cc}(A, C) \\to \\hom_{\\cc}(A, C'))\\) where  \\(\\phi^*\\) is a function defined pointwise as </p> \\[ \\phi^*(f: A \\to C) = \\phi \\circ f: A \\to C'. \\] <p>Such a functor is naturally of interest in mathematics since it is often  of interetst to consider the hom set \\(\\hom_{\\cc}(A, B)\\) for some objects \\(A, B\\)  in a category \\(\\cc\\), as it is usually the case that this set contains extra structure. For example, within topology this set is always a topological space, since  families of continuous functions can be endowed with the compact open topology. In the setting of abelian groups, this set also forms an abelian group. Much of category  theory can actually be done by simply \"enriching\" hom sets of a category with  some extra structure; this is the object of enriched category theory, which we'll introduce later.</p> <p>This functor in general also exhibits nice properites. For example,  let \\(R\\) be a ring. Then the sequence below \\  is exact if and only if, for every \\(R\\)-module \\(N\\), the sequence  \\  is exact. This result even extends to split short exact sequences. We  also have that for \\(R\\)-modules \\(N\\), \\(M_1, M_2\\) that </p> \\[ \\hom(N, M_1\\oplus M_2)\\cong \\hom(N, M_1)\\oplus \\hom(N, M_2). \\] <p>This result also holds for arbitrary direct sums, so that the hom functor distributes over all direct sums. Even better, we cannot forget that the hom-functor exhibits the  tensor-hom adjunction which states that for \\(R\\)-modules \\(N, M_1, M_2\\) </p> \\[ \\hom(N \\otimes M_1, M_2) \\cong \\hom(N, \\hom(M_1, M_2)). \\] <p>More is to be said about this property; we'll later  see that this is an example of an adjunction.</p> <p></p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Initial%2C%20Terminal%2C%20and%20Zero%20Objects/","title":"1.11. Initial, Terminal, and Zero Objects","text":"<p>We can also be more specific in discussing the nature of the objects of a given category \\(\\cc\\).</p> <p> Let the following objects exist in some category \\(\\cc\\).  <ul> <li> <p>Let \\(T\\) be an object. Then \\(T\\) is terminal if for each object \\(A\\), there exists exactly one morphism \\(f_A\\) such that \\(f_A: A \\to T\\). </p> </li> <li> <p>Let \\(I\\) be an object. Then \\(I\\) is said to be initial if for each object \\(A\\) there exists exactly one  morphism \\(f_A : I \\to A\\). </p> </li> <li> <p>An object \\(Z\\) is said to be a zero object if it is both terminal and initial. Since terminal and initial objects  are unique, so is a zero object.</p> </li> </ul> <p>Equivalently, it is zero if for any objects \\(A, B\\), there exists exactly one morphism \\(f: A \\to Z\\) and exactly one morphism \\(g: Z  \\to B\\). Hence, for any two objects there exists a morphism between them, namely given by by \\(g \\circ f\\), called the zero morphism from \\(A\\)  to \\(B\\). </p> <p>If an object \\(T\\) is terminal, then there is one and only morphism to itself (namely, its identity). Therefore, for any two terminal objects \\(T\\) and \\(T'\\), they are isomorphic, since by assumption there exists unique morphisms \\(f: T \\to T'\\) and \\(g: T' \\to T\\) and we have no choice but to say </p> \\[ f \\circ g = 1_T \\quad g \\circ f = 1_{T'}. \\] <p></p> <p> Recall that in the category \\(\\grp\\), there exists a trivial group  \\(\\{e\\}\\). Moreover, for each group \\(G\\), there exist unique group homomorphisms  \\[ i_G: \\{e\\} \\to G \\qquad e \\mapsto e_G  \\] <p>and </p> \\[ t_G: G \\to \\{e\\} \\qquad g \\mapsto e_G. \\] <p>Note that both are group homomorphisms since they both behave on identity elements  and are trivially distributive across group operations. This then shows that  \\(\\grp\\), the trivial group is initial and terminal and hence a zero object. </p> <p>This makes sense since for any two groups \\(G, H\\), there exists a unique map </p> \\[ z: G \\to H \\qquad g \\mapsto e_H \\] <p>which could be factorized as </p> <p> which demonstrates the existence of a zero object (the name \"zero\" makes sense now, right?),  which we already know is \\(\\{e\\}\\). Note in this example, we did not actually use much group theory. In fact, this could be repeated  for the categories \\(R\\rmod\\), \\(\\ab\\), and other similar categories.  </p> <p>The next two examples demonstrate that terminal and initial objects of course don't  always have to coincide like they did in the previous example. </p> <p> Let \\(n\\) be a positive integer. Recall that we can create a category, specifically a  preorder, by taking our objects to be positive integers less than \\(n\\),  and allowing one morphism \\(f: k \\to m\\) whenever \\(k \\le m\\). \\  Then 1 is an initial object while \\(n\\) is a terminal object. This  is because for any number \\(1 \\le m \\le n\\), there exists a unique morphism  from \\(1\\) to \\(m\\), and a unique morphism \\(m\\) to \\(n\\), both which may  be obtained by repeated composition.  </p> <p> Consider the category \\(\\Set\\). Let \\(X\\) be a given set in this category.  Then there are two unique functions which we may construct. First, there is the function  \\[ t_X: X \\to \\{\\bullet\\} \\] <p>where everything in \\(X\\) is mapped to the one element \\(\\bullet\\) of the one point set.  Secondly, we may construct a function whose domain is the empty set,  and whose codomain is \\(X\\), as below.</p> \\[ i_X: \\varnothing \\to X \\] <p>Thus we have that, in \\(\\Set\\), the one point set is a terminal  object \\(\\{\\bullet\\}\\) while the empty set \\(\\varnothing\\) is an initial object.</p> <p>One may wonder at this point: How exactly is \\(i_X\\) a true, set theoretic function? And why can't we also obtain a unique morphism \\(i'_X: X \\to \\varnothing\\),  so that \\(\\varnothing\\) is a terminal object as well?</p> <p>The second question is easy to answer; if \\(\\varnothing\\) was also terminal, then  we'd have that \\(\\{\\bullet\\} \\cong \\varnothing\\) which is not true. Since this is a bit of a boring  answer, we'll explain in detail.</p> <p>Recall that a function in \\(f: A \\to X\\) between two sets \\(A\\) and  \\(X\\) is a relation \\(R \\subset A \\times X\\) which satisfies two properties. </p> <ul> <li> <p>[1.] (Existence.) For each \\(a \\in A\\), there exists a \\(x \\in X\\) such that \\((a, x) \\in R\\) </p> </li> <li> <p>[2.] (Uniqueness. Or, if you'd like, the vertical line test.)  If \\((a, x) \\in R\\) and \\((a, x') \\in R\\) then \\(x = x'\\). </p> </li> </ul> <p>Now observe that if \\(A = \\varnothing\\), then \\(R \\subset \\varnothing \\times X = \\varnothing\\).  Hence (1) and (2) are satisfied because each is trivially true. However, we don't get  a function \\(f: X \\to \\varnothing\\), since in this case (1) fails. Specifically, (1) demands  the existence of elements in our codomain, a demand we cannot meet if it is empty. </p> <p>Thus we see that \\(\\varnothing\\) is initial, but not terminal as our intuition may suggest,  and that \\(\\{\\bullet\\}\\) is terminal. </p> <p> Consider the category of fields \\(\\fld\\). Suppose we ask if this has an initial  or terminal object.  <p>We might guess that the smallest field </p> \\[ \\mathbb{F}_2 \\cong (\\zz/2\\zz, +, \\cdot) = \\{0, 1\\}  \\] <p>which has characteristic 2 is an initial object. However, this fails to be initial.  Observe that the only homomorphism between \\(\\mathbb{F}_2\\) and \\(\\mathbb{F}_3\\) is the zero  homomorphism, which is not in our category. (Recall that \\(\\fld\\) is a full subcategory  of \\(\\ring\\), a category whose morphisms we require to be unit preserving.)</p> <p>The reason why it must be the zero homomorphisms is because \\(\\mathbb{F}_3\\) has characteristic three,  and in general, two fields will only share a (nonzero) field homomorphisms if they  have the same characteristic. </p> <p>By a similar argument, we can state that terminal objects also do no exist. Overall,  these objects fail to exist in \\(\\fld\\) because fields have a large set of  restictions imposed by their numerous axioms. Hence, this category lacks initial and terminal objects. </p> <p>{\\large Exercises \\vspace{0.5cm}}\u00a0</p> <ul> <li> <p>[1.]  \\begin{itemize}</p> </li> <li> <p>[(i.)] Let \\(\\cc\\) be a category with initial object \\(I\\).  For any two objects \\(A, B \\in \\cc\\), define for each \\(f \\in \\hom_{\\cc}(A, B)\\) the functor</p> </li> </ul> \\[ P_f: **2** \\to \\cc \\] <p>such that \\(P(\\textcolor{NavyBlue}{\\bullet}) = A\\), \\(P(\\textcolor{Orange}{\\bullet}) = B\\),  and \\(P_f(\\textcolor{NavyBlue}{\\bullet} \\to \\textcolor{Orange}{\\bullet}) = f: A \\to B\\).  Show that for each \\(f: A \\to B\\) in \\(\\cc\\), we have a natural transformation </p> \\[ \\eta: P_{1_I} \\to P_f. \\] <p>Note that \\(1_I: I \\to I\\) is the identity on the initial object.</p> <ul> <li>[(ii.)] Suppose we don't know if \\(\\cc\\) has an initial object,  but we have a distinguished object  \\(I'\\) with the property that for each \\(f \\in \\hom_{\\cc}(A,B)\\) there is a natural  transformation </li> </ul> \\[ \\eta: P_{1_{I'}} \\to P_f. \\] <p>Is \\(I'\\) an inital object?</p> <ul> <li>[(iii.)] Dualize your work for terminal objects.\\ (Hint: We now want a natural transformation \\(\\eta': P_f \\to P_{1_I})\\).  </li> </ul> <p>\\end{itemize}</p> <p>\\chapterimage{chapter2_pic/chapt2head.pdf} </p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Introduction%3A%20What%20are%20the%20Foundations%20of%20Math%3F/","title":"1.1. Introduction: What are the Foundations of Math?","text":"<p>Category theory attempts to \"zoom out\" of mathematical constructions and to point out the higher level relationships between different mathematical constructions. The three main concepts are categories, functors, and natural transformations, although the theory grew out of implications of these main concepts. </p> <p>These main concepts were first seen in the study of algebraic topology, since  it was observed that topological problems could be reduced to algebraic, and vice versa. But how? Since there was no formal notion for what it really meant to take a topological space \\(X\\) and associate it with some group \\(\\pi(X)\\),  category theory came about to formalize this. </p> <p>However, as we shall soon see, category theory has a big problem.  Specifically, there isn't a universally agreed upon foundation for category theory, or for mathematics in general.  \\begin{center} What do we mean by foundations? \\end{center}    Well, consider a topological space \\(X\\), or a group \\(G\\), or a domain \\(\\mathbb{R}\\). Then suppose I ask you \"What is \\(X\\)?\" or \"What is \\(G\\)\" or \"What is \\(\\mathbb{R}\\)?\"  Well, you'll tell me it's a topological space, a group, or the set of real numbers and list the axioms for each object. </p> <p>That is, a correct answer will characterize \\(X\\), \\(G\\) or \\(\\mathbb{R}\\) as a set which satisfies  some axioms. But really, that's what all our mathematical objects are. So at this point, our foundations \\textbf{are grounded in set theory.}</p> <p>\\begin{center} What is set theory? \\end{center}   Suppose I ask you what is set theory. While we all know there are  different set theories, most people don't think about set theory axioms on a daily  and won't know (like myself). But answering this question requires answering the next. </p> <p>\\begin{center} What is a set? \\end{center}   We usually never have to face this question. But in developing a theory that considers relationships between different sets, we have to. </p> <p>Our intuition tells us that sets \\(X\\) are a \\textbf{collection of objects, and that every collection of objects is a set}. We intuitively think that we can form collections of objects to create a set \\(X\\), and that we can form  intersection and unions between sets, or even compute power sets, to produce other sets. We also think we can also form sets such as </p> \\[ X = \\{x \\mid \\phi(x) \\} \\] <p>where \\(\\phi\\) is some logical condition of inclusion. However, this leads to paradoxes, one of the most famous known as Russel's Paradox which we can describe as follows. </p> <p>\\noindentRussel's Paradox. Let \\(X\\) be a set such that </p> \\[ X = \\{A \\text{ is a set }\\mid A \\text{ is not a member of itself.}\\} \\] <p>Now observe the following. </p> <ul> <li>[1.] If \\(X \\in X\\), then consequently \\(X\\) is not a member of itself. In other words, if \\(X \\in X\\), then \\(X \\not\\in X\\).</li> </ul> <p>Clearly, this is a contradiction. Since \\(X \\in X\\) is nonsense, \\(X \\not\\in X\\), right? </p> <ul> <li>[2.] Suppose \\(X \\not\\in X\\). Then \\(X\\) is not a member of itself, so \\(X \\in X\\) by the condition of member of \\(X\\). In other words, \\(X \\not\\in X \\implies X \\in X\\).</li> </ul> <p>See the problem here? \\textbf{Not every collection of objects is a set.} So our previous notions of sets aren't correct.</p> <p>\\textcolor{MidnightBlue}{Note that our trouble arose when we said that \\textbf{a set is a collection of objects, and a collection of objects is a set.}  This is because no, not every collection of objects is a set. Thus we need to go back and fix our definition of a set. }</p> <p>\\begin{center} What do we do? \\end{center}   This is what many mathematicians asked in the early 1900s when they identified the paradoxes that arise from our notion of a set. The result has been multiple different types of set theories,  and so there isn't a clear  choice on what to make our foundations. However, this isn't a huge problem  for category theory. Category theory  has its own core axioms, but the fact that there are different set theories  simply means that such core axioms will be phrased differently under different  set theories (although there are some cases where  one does need to be careful with their foundations). In this text, we'll be a bit sloppy with the foundations of category theory,  although  we will point out where we need to be careful.</p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Monic%2C%20Epics%2C%20and%20Isomorphisms/","title":"1.10. Monic, Epics, and Isomorphisms","text":"<p>In category theory the ultimate focus is placed on the morphisms within a category. What we really care about are the relationships between the objects. Thus in this section we'll go over types of morphisms that exist between objects.</p> <p>The way that this is done in set theory is to consider injective functions, surjective  functions, and isomorphisms. This can also be done in topology, and in group, ring, and module  theory. However, these concepts make no sense in general. This is because in general, the morphisms of a category are not functions because in general,  the objects of a category are not sets (even if the objects are sets, the morphisms  can still be different than functions). </p> <p>We can nevertheless abstract the concept of injections and surjections by  expressing their properties categorically; that is, without reference to specific  elements in any objects. This leads to the concepts of monomorphisms and epimorphisms. \\vspace{0.3cm}</p> <p>\\begin{minipage}{0.8\\textwidth}</p> <p> Let \\(f: A \\to B\\) be a morphism. Then  <ul> <li>[1.] \\(f\\) is a monomorphism (or is monic) if</li> </ul> \\[  f \\circ g_1 = f \\circ g_2 \\implies g_1 = g_2 \\] <p>for all \\(g_1,g_2 : C \\to A\\), where \\(D\\) is arbitrary.</p> <ul> <li>[2.] \\(f\\) is a epimorphism (or is epic) if </li> </ul> \\[  g_1 \\circ f  = g_2 \\circ f \\implies g_1 = g_2 \\] <p>for all \\(g_1, g_2 : B \\to C\\), where \\(C\\) is an arbitrary object. </p> <ul> <li>[3.] $f $ is a split monomorphism (or retraction) if, for some\\ \\(g: B \\to A\\), </li> </ul> \\[ f  \\circ g = 1_B.   \\] <ul> <li>[4.] $f $ is a split epimorphism (or section) if, for some \\(g: B \\to A\\), </li> </ul> \\[ g \\circ f  = 1_A. \\] <p> \\end{minipage} \\hspace{-2cm} \\begin{minipage}{0.1\\textwidth} \\vspace{-2.2cm}</p> <p>\\begin{tikzpicture} \\filldraw[yellow!30, rounded corners] (-2.25,-1.5) rectangle (2.25,1.5); \\node at (0,0){ \\begin{tikzcd}[row sep = 1.2cm, column sep = 1.4cm] C  \\arrow[dr,swap, \"f \\circ g_1 = f \\circ g_2\"] \\arrow[r, yshift=0.7ex, \"g_1\"] \\arrow[r, yshift=-0.7ex,swap,  \"g_2\"]    &amp; A \\arrow[d, \"f\"]\\ &amp; B  \\end{tikzcd}  }; \\end{tikzpicture} \\vspace{0.2cm}</p> <p>\\begin{tikzpicture} \\filldraw[yellow!30, rounded corners] (-2.25,-1.5) rectangle (2.25,1.5); \\node at (0,0){ \\begin{tikzcd}[row sep = 1.2cm, column sep = 1.4cm] A \\arrow[d, swap, \"f\"] \\arrow[dr, \"g_1 \\circ f = g_2 \\circ f\"]\\ B \\arrow[r, yshift=0.7ex, \"g_1\"] \\arrow[r,yshift=-0.7ex,swap,  \"g_2\"] &amp; C \\end{tikzcd} }; \\end{tikzpicture} \\end{minipage} \\vspace{1cm}</p> <p>Monomorphisms and epimorphisms are an abstraction that take advantage key properties  of both injective and surjective functions. We illustrate this with a few examples. </p> <p> In \\(\\Set\\), an injective function \\(f: X \\to Y\\) is \"one-to-one\" in  the sense that \\(f(x) = f(y)\\) if and only if \\(x = y\\). With that said, suppose that  \\(g_1, g_2: Z \\to X\\) are functions and moreover that \\(f\\circ g_1 = f \\circ g_2\\).  Then this means that, for all \\(z \\in Z\\), we have that  \\[ f(g_1(z)) = f(g_2(z)) \\implies g_1(z) = g_2(z)     \\] <p>since \\(f\\) is one-to-one. Hence we see that injective functions are  monomorphisms in \\(\\Set\\);  one can then conversely show that a monomorphism in \\(\\Set\\)  are injective functions.  </p> <p> Let \\((G, \\cdot)\\) be a group, and suppose \\((H, \\cdot)\\) is a normal subgroup of  \\(G\\). Then with such a construction, we always have access to the  inclusion and projection homomorphisms \\[\\begin{align*} i: H \\to G \\qquad &amp;i(h) = h \\\\ \\pi: G \\to G/H \\qquad &amp;\\pi(g) = g + H. \\end{align*}\\] <p>It is not hard to see that \\(i\\) is a monomorphism and \\(\\pi\\) is an epimorphism;  for suppose \\(\\phi, \\psi:K \\to G\\) are two group homomorphisms from some group \\(K\\)  where \\(i \\circ \\phi = i \\circ \\psi\\). Then for each \\(k \\in K\\), \\(i(\\phi(k)) = i(\\psi(k)) \\implies \\phi(k) = \\psi(k)\\), so that \\(\\phi = \\psi\\). Conversely, if \\(\\sigma, \\tau: G \\to M\\)  are two group homomorphisms to some group \\(M\\) such that  \\(\\sigma \\circ \\pi = \\tau \\circ \\pi\\), then because \\(\\pi\\) is surjective we have that  \\(\\sigma = \\tau\\). Hence, we see \\(\\pi\\) is an epimorphism.</p> <p>Since the above constructions can be repeated in the  categories \\(\\ab\\), \\(\\ring\\),  and \\(R\\rmod\\), so can the above argument. We'll see more generally  the deeper reason for why this is the case later on. </p> <p> In the category of fields, \\(\\fld\\), every nonzero morphism is a  monomorphism. This is due to the classic argument: the only nontrivial ideal  of a field \\(k\\) its itself; hence the kernal of any map \\(\\phi: k \\to k'\\)  is either trivial or all of \\(k\\). If we suppose \\(\\phi\\) is nonzero, then we see  that it must be injective, and hence a monomorphism.  </p> <p> Let \\(f: A \\to B\\) be a morphism between two objects \\(A\\) and \\(B\\). We say that \\(f\\) is an isomorphism if there exists a morphism \\(f^{-1}:B \\to A\\) in \\(\\cc\\)! such that  \\[ f \\circ f^{-1} =\\id_A \\quad \\quad f^{-1} \\circ f = \\id_B. \\] <p>In this case, \\(f^{-1}\\) is unique, and for any two isomorphisms \\(f:A \\to B\\) and  \\(g:B \\to C\\) we have</p> \\[ (g \\circ f)^{-1} = f^{-1}\\circ g^{-1}. \\] <p>In this case we say that \\(A\\) and \\(B\\) are isomorphic and denote  this as \\(A \\cong B\\).  </p> <p>This is a generalization of the familiar concept of isomorphisms in abstract algebra and in set theory that one usually encounters. </p> <p>Next, we illustrate a few properties of these types of morphisms.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor. Then  if \\(f: A \\to B \\in \\cc\\) <ul> <li> <p>is an isomorphism, then  \\(F(f)\\) is an isomorphism in \\(\\dd\\). </p> </li> <li> <p>is a split monomorphism, then \\(F(f)\\)  is a split monomorphism in \\(F(f)\\) </p> </li> <li> <p>is a split epimorphism, then \\(F(f)\\) is a split  epimorphism.</p> </li> </ul> <p>That is, functors preserve isomorphisms, split monomorphism,  and split epimorphisms.    In general, functors do not reflect isomorphisms, split monomorphisms,  and split epimorphisms. That is, if \\(F(f): F(A) \\to F(B)\\) is an isomorphism  it is not the case that \\(f\\) is an isomorphism.</p> <p>We demonstrate this with the following example. </p> <p> Recall that \\(\\text{Spec}(-): \\cring \\to \\Set\\) is a functor  that appears in algebraic geometry. It sends every commutative ring \\(A\\) to  its ring spectrum \\(\\spec(A)\\), which consists of all prime ideals of \\(A\\). <p>Let \\(\\displaystyle N =\\bigcap_{P \\in \\spec(A)}P\\) be the intersection of all prime ideals. An equivalent way to speak of \\(N\\)  is the set \\(N = \\{a \\in A \\mid a^m =0 \\text{ for some positive integer }m\\}\\);  that is, \\(N\\) is equivalently the nilradical elements of \\(A\\).</p> <p>Now the projection ring homomorphism </p> \\[ \\phi: A \\to A/N \\] <p>is certainly not an isomorphism (unless \\(A\\) has no nontrivial  nilradical elements), but the image of this map under \\(\\spec\\) </p> \\[ \\spec(\\phi): \\spec(A/N) \\isomarrow \\spec(A) \\] <p>is always an isomorphism. In fact, if we impose the Zarisky topology on these prime spectrums,  the functor becomes one which goes to topological spaces</p> \\[ \\text{Spec}(-): \\cring \\to \\top \\] <p>and the map \\(\\phi\\) becomes a homeomorphism. Hence, this functor does not reflect isomorphisms  in either the set or topological senses, because the image \\(\\spec(\\phi)\\)  is an isomorphism, but \\(\\phi\\) is not. Despite this, the interpretation of this result is a useful one because it demonstrates that algebraic  geometrists can \"throw away\" their nilradical elements without changing  their Zariski topology. </p> <p> The composition of monomorphisms (epimorphisms) is a (an) monomorphism (epimorphism). </p> <p> Let \\(f: A \\to B\\) and \\(g: B \\to C\\) be monomorphisms, and suppose \\(h_1, h_2 : D \\to A\\) are two parallel morphisms. Suppose that  \\((g \\circ f) \\circ h_1 = (g \\circ f) \\circ h_2.\\) Note that we can rewrite the equation to obtain that  \\[ g \\circ (f \\circ h_1) = g \\circ (g \\circ h_1) \\implies f \\circ h_1 = f \\circ h_2.  \\] <p>as \\(g\\) is monic, and hence it is left cancellable.  But once again, \\(f\\) is monic, so we cancel on the left to obtain that \\(h_1 = h_2\\) as desired. </p> <p>\\textcolor{MidnightBlue}{Note: it is not always the case that a monic, epic morphism is an isomorphism (that is, it's not always invertible.)}</p> <p> Consider the category \\(\\top\\), consisting of (small) topological spaces as our objects with continuous functions between them as morphisms. Let \\(D\\) be a dense subset of a topological space \\(X\\) and let \\(i: D \\to X\\) be the inclusion map. We'll show that this function is both epic and monic. <p>To show it is epic, let \\(f_1, f_2: X \\to Y\\) be continuous maps form \\(X\\) to another topological space \\(Y\\). Let \\(Y\\) be Hausdorff, and suppose that </p> \\[ f_1 \\circ i = f_2 \\circ i. \\] <p>Now \\(\\im(i) = D\\), so the above equation tells us that \\(f_1(d) = f_2(d)\\) for all \\(d \\in D\\). That is, the functions agree on the dense subset. However, we know from topology that this implies that \\(f_1 = f_2\\).</p> <p> \\textcolor{MidnightBlue}{ Suppose that \\(f_1(x) \\ne f_2(x)\\) for some \\(x \\notin D\\). Since the points are distinct, and since \\(Y\\) is Hausdorff, there must exist disjoint open sets  \\(U, V\\) in \\(Y\\) such that \\(f_1(x) \\in U\\) and \\(f_2(x) \\in V\\). Since both \\(f_1, f_2\\) are continuous, there must exist open sets \\(U', V'\\) in \\(X\\) such that  \\(f(U') \\subset U\\) and \\(g(V') \\subset V\\).  \\ \\indent However, since \\(D\\) is dense in \\(X\\), both \\(U'\\) and \\(V'\\) must intersect with some portion of \\(D\\); that is,  there is some \\(y \\in U'\\) and \\(z \\in V'\\) such that \\(y, z \\in D\\). Therefore,  we see that \\(f_1(y) \\in U\\) and \\(f_2(z) \\in V\\), and since \\(y, z \\in D\\) we have that \\(f_1(y) = f_2(z)\\). But this contradicts the fact that \\(U \\cap V = \\emptyset.\\) Therefore, we have a contradiction and it must be the case that  \\(f_1(x) = f_2(x)\\) for all \\(x \\in X\\), as desired. }  \\noindent Therefore, we see that \\(i\\) is epic. To show that it is monic, suppose \\(g_1, g_2: Y \\to D\\) are two  parallel, continuous functions, and that </p> \\[ i \\circ g_1 = i \\circ g_2. \\] <p>Since \\(i\\) is nothing more than an inclusion map, we immediately have that \\(g_1 = g_2\\). Therefore, \\(i\\) is also monic.</p> <p>However, note that \\(i: D \\to X\\) is not an isomorphism, since  it is not necesasrily always surjective. Hence \\(i\\) is an example of a monic, epic morphism which is not an isomorphism. </p> <p> </p> <p>We finish our discussion on monics and epics by considerig the automorphism groups  of a category. </p> <p> Let \\(\\cc\\) be a locally small category. For each object \\(A\\) in \\(\\cc\\), we can consider the  automorphism group \\(\\aut(A)\\) whose objects consist of isomorphisms  \\(\\phi: A \\isomarrow A\\), whose product is composition, and whose identity is \\(1_A\\).  </p> <p>Note that despite the notation, this does not generally define a functor. </p> <p> Some examples of the above construction include familiar and useful examples  in mathematics. <ul> <li> <p>For any group \\((G, \\cdot)\\) in \\(\\grp\\), we can formulate the automorphism  group \\(\\aut(G)\\) which is the group of isomorphisms from \\(G\\) to itself.  Depending on \\(G\\), this can have all kinds of behavior. For example, if  \\(\\aut(G)\\) is cyclic, then  \\(G\\) is abelian. If \\(G\\) is an abelian group  of order \\(p^n\\), then \\(\\aut(G) = GL(n, F)\\) where \\(F\\) is the finite field of order \\(p\\).</p> </li> <li> <p>For any set \\(X\\) in \\(\\Set\\), the automorphism group \\(\\aut(X)\\) consists of the bijections  on \\(X\\) to itself; by definition in set theory, these are just permutations. Hence the automorphism group  is the permutation group of the elements of \\(X\\). </p> </li> <li> <p>For any field \\((k, \\cdot, +)\\) in \\(\\fld\\), the automorphism  group \\(\\aut(k)\\) also consists of field isomorphisms to itself.  In this setting, what is often  of more interest is considering the subgroups of \\(\\aut(k)\\), often  denoted as \\(\\aut(k/L)\\), which are automorphisms that fix the subfield  \\(L\\). These subgroups are key  to studying polynomial roots and hence are prevalent in Galois theory.</p> </li> <li> <p>For any graph \\((G, E, V)\\) in Grph, one can construct the automorphism group \\(\\aut(G)\\),  which tracks the symmetries of the graph. Interestingly, there is a theorem known as  Frucht's Theorem which states that every finite  group is the automorphism group of a finite (undirected) graph; this was later  extended and shown that every group is the automorphism group of a directed  graph [Groups represented by homeomorphism groups.]. </p> </li> <li> <p>For any topological space \\((X, \\tau)\\) in \\(\\top\\),  the autormorphism group \\(\\aut(X)\\)  consists of the homeomorphisms to itself. Geometrically, these record the possible  ways of continuously deforming a space back into itself. It is a theorem  that every group is the automorphism group of some complete, connected,  locally connected metric space \\(M\\) of any dimension.         </p> </li> </ul> <p></p> <p>With the automorphism group in mind, we might ask the same question on the object  level: Given an object \\(A\\) in \\(\\cc\\), what objects are isomorphic to \\(A\\) in \\(\\cc\\)? To answer this, we define the relation  \\(\\sim\\) on \\(\\ob(\\cc)\\), the objects of \\(\\cc\\), where we say </p> \\[ A \\sim B \\text{ if } A \\cong B. \\] <p>Such an equivalence relation divides the objects of \\(\\cc\\) into disjoint  isomorphsm classes, which reduces the structure of \\(\\cc\\). </p> <p> Let \\(\\cc\\) be a category and \\(A\\) any object. We call the equivalence class  of \\(A\\) under \\(\\sim\\), defined previously, as the isomorphism class  which we denote as  \\[ \\text{Isom}(A) = \\{X \\in \\ob(\\cc) \\mid X \\cong A\\}. \\] <p></p> <p>This leads to the following categorical construction which preserves a great  deal of information within the category.</p> <p> Let \\(\\cc\\) be a category, and assume the axiom of choice. Then we can construct  a skeleton of a category \\(\\cc\\), denoted \\(\\text{sk}(\\cc)\\), as  the category where  \\begin{description} \\item[Objects.] For each \\(A \\in \\cc\\), we select one  representative of each isomorphism class \\(\\text{Isom}(A)\\). \\item[Morphisms.] For two representatives of isomorphism  classes \\(A, B\\), we take  \\[ \\hom_{\\text{sk}(\\cc)}(A,B)= \\hom_{\\cc}(A,B) \\] <p>\\end{description}  We note three things regarding this construction.  \\begin{description} \\item[(1)] We used the axiom of choice to build the objects of the  category, since we needed to select one element from each isomorphism class. \\item[(2)] The category \\(\\text{sk}(\\cc)\\) is a full subcategory of \\(\\cc\\) by definition. \\item[(3)] We note that this construction builds a skeleton. In general,  a category will have different skeletons because there are many ways to construct the  objects of such a skeleton.  \\end{description} As noted, a category will have different skeletons. However, up to isomorphism, it does  not really matter which skeleton we build as we will see. </p> <p> Let \\(\\cc\\) be a category, and let \\(\\text{sk}(\\cc)\\) and \\(\\text{sk}'(\\cc)\\)  be two skeletons built from \\(\\cc\\). Then \\(\\text{sk}(\\cc) \\cong \\text{sk}'(\\cc)\\).  </p> <p>The prove is left as an exercise for the reader. We will see late that there  are more enjoyable properties of \"skeletal\" categories, which we define as categories  exhibiting this type of behavior.</p> <p> A category \\(\\cc\\) is called skeletal if no two distinct objects are isomorphic in \\(\\cc\\). </p> <p>Categorical skeletons are inadvertently studied everywhere in mathematics.  For example, asking for a classification of abelian groups, of manifolds, or  even of the cardinality of every set is the same thing as asking for the  skeletons of \\(\\ab\\), \\(**DMan**\\), and \\(\\Set\\). We give a few  examples. </p> <p> Consider the category \\(**FinCard**\\) (read: \"finite cardinals\") which we describe as  \\begin{description} \\item[Objects.] The set \\(\\varnothing\\) and the sets \\(\\{1, 2, ..., n\\}\\) for each \\(n \\in \\mathbb{N}\\). \\item[Morphisms.] All functions between these finite sets. \\end{description} Clearly this is a full subcategory of \\(\\finset\\). Moreover, it is skeletal;  no two sets are isomorphic because each object is of different size. Therefore, it  is skeletal. In fact, \\(**FinCard**\\) is a skeleton  of \\(\\finset\\) because any finite set (in some universe \\(U\\))  can be ordered in some way, which provides an enumeration on its objects. In other words, every finite set is of some finite size, making it isomorphic  to some set \\(\\{0, 1, 2, \\dots, n\\}\\).  </p> <p> One can try to generalize the previous example to \\(\\Set\\), but this  is in general not possible unless we assume ZFC with the  generalized continuum hypothesis, as such a posulate is independent of ZFC.  <p>Assuming such an axiom, we can construct the category \\(**Card**\\) where  \\begin{description} \\item[Objects.] The sets \\(\\varnothing, \\{1, 2, \\dots, n\\}\\) for each \\(n \\in \\mathbb{N}\\), and \\(\\omega_0, \\omega_1, \\omega_2, \\dots\\)  \\item[Morphisms.] All functions between such sets.  \\end{description} Here we see that this is again a skeleton \\(\\Set\\), since by our assumptions  (which is assuming a lot), any set is of some cardinality \\(1, 2, \\dots, n, \\dots, \\aleph_0, \\aleph_1, \\dots\\). However, for each such  cardinal we have a corresponding set with that cardinality. Hence each element  in \\(\\Set\\) is isomorphic to some element of \\(**Card**\\). Overall,  we see that Card forms a skeleton of \\(\\Set\\). </p> <p>The above example can be repeated for Cycl, the category of cyclic groups. This is because any two cyclic groups of the same order are isomorphic. Hence, one  can find a skeleton of Cycl by finding a family of cylic groups of every set  size (again, using the generalized continuum hypothesis).  </p> <p> Consider the category \\(**Ecld**\\) of Euclidean spaces, which we may describe as  \\begin{description} \\item[Objects.] The vector spaces \\(\\rr^n\\) for each $n = 0, 1, 2, \\dots, $ \\item[Morphisms.] Linear transformations between vector spaces.  \\end{description} Then we see that \\(**Ecld**\\) is the skeleton of \\(**FinVect**_k\\),  which is the category of finite-dimensional vector spaces. The reason why this  works is because every finite dimensional vector space is isomorphic to \\(\\rr^n\\) for  some \\(n\\).  </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] Prove Lemma \\ref{lemma:composition_of_epis} for epimorphisms.</p> </li> <li> <p>[2.] Prove Lemma \\ref{lemma:skeletons_are_isomorphic}.</p> </li> <li> <p>[3.] Describe the monomorphisms and epimorphisms  in the category of \\(\\cat\\).\\footnote{Classifying epimorphisms  in \\(\\cat\\) is actually nontrivial, although not impossible.  However, the task here is to just interpret  the definition of monics and epics \\(\\cat\\). }</p> </li> <li> <p>[4.] In the category of \\(\\ring\\), give an example of a morphism  which is both a monomorphism and epimorphism, but not an isomorphism. \\ (Hint: Consider the inclusion \\(i: \\zz \\to \\qq\\).)</p> </li> <li> <p>[5.] Recall from Exercise ? that, in any category, if we have two commutative  diagrams, we can always stack them together to obtain a larger commutative diagram.  We saw, however, that converse is not always true: subdividing a commutative  diagram does not produce smaller commutative diagrams. </p> </li> </ul> <p>Prove that the converse is true when all morphisms are isomorphisms.     </p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Motivation%20for%20Category%20Theory/","title":"1.2. Motivation for Category Theory","text":"<p>What do groups \\(G\\), topological spaces \\(X\\) and vector spaces \\(V\\) have  in common? We use different letters to describe them! Seriously, that is one major  difference. Why? Because our brains are organizational and thrive off of associations,  e.g., \\(G\\) with group, \\(X\\) with topological spaces, etc. This is great  for thinking, but the mental separation of these constructions hides a bigger  picture.</p> <p>Let's look at what these things look like.  With groups, we are often mapping between groups via group homomorphisms. For example, below we have the chain complex of abelian groups with boundary  operator \\(\\partial_n: C_n \\to C_{n-1}\\), with the familiar property that \\(\\partial_n \\circ \\partial_{n-1} = 0\\).</p> <p></p> <p>Within topology, we are often mapping topological spaces via continuous functions. \\ </p> <p>With vector spaces, we often use linear transformations to map  from one to another.  \\  At some point when we're learning different basic constructions in  pure mathematics, we often realize that we're just  repeating the same story over and over. The professor tells you about  an object (usually a set) equipped with some axioms. The next thing you learn  are \"mappings\" between such objects, which can abstractly be called morphisms. The characteristics of these morphism are generally the following: \\begin{description} \\item[1.] There's an identity morphism. \\item[2.] There's a notion of composition.  \\item[3.] Composition is associative.  \\item[4.] Composing identities in any order with a morphism  returns the same morphism.  \\end{description}</p> <p>What is it that I just described? It sounds just like  a monoid! In the most  basic sense, a monoid \\(M = \\{x_1, x_2, \\dots, \\}\\) is a set of elements equipped with  a multiplication map </p> \\[ \\cdot: M \\times M \\to M \\qquad (x, y) \\mapsto x\\cdot y \\] <p>which is associative, and with a multiplicative identity \\(e\\). With a monoid we see that  \\begin{description} \\item[1.] There's an identity \\(e\\). \\item[2.] There's a notion of multiplication. \\item[3.] Multiplication is associative. \\item[4.] Multiplying \\(e\\) in any order with an element \\(x\\) returns \\(x\\).    \\end{description}   The concept  of a monoid is one of the most underrated yet powerful concepts of mathematics,  and for some reason it's usually ignored in algebra courses. It's an innate, fundamental human concept, a consequence of our physical  reality. How many years have our ancestors been saying: ``Let's stack stuff together and see what  happens!'' \\emph{Stacking three things in two different ways is the same.  Stacking nothing is an \"identity\"}. Thus what we see is that groups, topological  spaces and vector spaces are all similar in that (1) we have morphisms of interest  and (2) the morphisms behave like a monoid. This notion  is what category theory takes care of.</p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Natural%20Transformations/","title":"1.9. Natural Transformations","text":"<p> Suppose we have a pair of functors \\(F, G: \\cc \\to \\Set\\).  In particular, suppose that \\(F(A) \\subset G(A)\\) for all objects \\(A\\).  This means that for each \\(A\\), there exists an injection \\(i_A: F(A) \\to G(A)\\). <p>Now this is a bit of an interesting construction since for any morphism  \\(f: A \\to B\\) in \\(\\cc\\), there are now two ways we can get from \\(F(A)\\)  to \\(G(B)\\). </p> <p></p> <p>As we have two different ways of traversing this diagram, are they equivalent?  That is, is it the case that </p> \\[ G(f) \\circ i_A = i_B \\circ F(f) \\quad \\text{or, spelled out,} \\quad      F(f)(x) = G(f)(x)? \\] <p>In general, this isn't true. But one way  (and as we'll see in the future, the only way)  we can make this diagram commute is if</p> \\[ F(f) = G(f)\\big|_{F(A)}. \\] <p>That is, if \\(F(f)\\) is a restriction of \\(G(f)\\). We summarize this  observation by stating that, if \\(F(f) = G(f)\\big|_{F(A)}\\) for all \\(f\\),  then the inclusion \\(i_A: F(A) \\to G(A)\\) is natural. </p> <p> Let \\(X\\) be a topological space. Then we can create the abelian groups  \\[ C_0(X), C_1(X), \\dots, C_n(X), \\dots          \\] <p>Here, \\(C_n(X)\\) is the free abelian  group generated by continuous functions of the form \\(\\phi: \\Delta^n \\to X\\), where  where \\(\\Delta^n\\) is the \\(n\\)-simplex. Hence, elements  are of \\(C_n\\) are of the form </p> \\[ \\sum_{\\phi} n_{\\phi} \\cdot \\phi        \\] <p>where all but finitely many of the integer coefficients \\(n_\\phi\\)  are zero. </p> <p>In algebraic topology, one observes that these abelian groups assemble into a  chain via a boundary operator \\(\\partial_n: C_n \\to C_{n-1}\\) with the property that \\(\\partial_{n+1}\\circ\\partial_n = 0\\) for all \\(n\\). \\  Now suppose that \\(f: X \\to Y\\) is a continuous map between topological spaces. Then for each \\(n\\), there is an evident mapping between the chain complexes.</p> \\[ C_n(f): C_n(X) \\to C_n(Y) \\qquad \\sum_{\\phi} n_{\\phi} \\cdot \\phi \\mapsto \\sum_{\\phi} n_{\\phi} \\cdot f \\circ \\phi. \\] <p>This is because if \\(\\phi: \\Delta^n \\to X\\) is a singular map then \\(f \\circ \\phi: \\Delta^n \\to Y\\)  is also a singular map because \\(f\\) is continuous.  However this presents us with an issue, one we faced in the earlier example.  On one hand, we have a map \\(C_{n-1}(f) \\circ \\partial_n: C_n(X) \\to C_n(Y)\\).  On the other hand, we have a map \\(\\partial_n \\circ C_n(f): C_n(X) \\to C_n(Y)\\).  But are these equivalent maps?  \\  It's a simple exercise to show that this diagram does in fact commute, i.e., that  \\(C_{n-1}(f) \\circ \\partial_n = \\partial_n \\circ C_n(f)\\) for all \\(n\\). </p> <p>As a result, this \"natural\" result (again pun intended)  gives us intuition on how to define a mapping between two chain complexes \\(\\{C_n\\}_{n \\in \\mathbb{N}}\\) and \\(\\{C_n\\}_{n \\in \\mathbb{N}}\\): : it is any family of maps \\(\\psi_n: C_n \\to C'_n\\) such that  \\(\\psi_{n-1} \\circ \\partial_n = \\partial_n \\circ \\psi_n\\). Moreover,  since we have a notion  of objects (i.e, chain complexes  \\(\\{C_n\\}\\) ) and morphisms (chain maps) this  gives rise to a category Ch(Ab), the category of chain complexes of abelian groups.</p> <p></p> <p>When the two ways to traverse the diagram are  equivalent, we call this behavior natural and it makes mathematicians very happy.  Naturality, which is what we will refer  to this property as, is ubiquitous in mathematics and functors give us a convenient  way of conceptualizing this useful property. </p> <p> Let \\(F, G: \\cc \\to \\dd\\) be two functors. Then we define a mapping\\footnote{Think morphism, because the word mapping here doesn't rigorously  mean anything. That's because  we don't really have a word to describe what a natural transformation  really is. We have axioms, which we present, but we don't have a nice word.  That nice word will turn out to be morphism, and you will see soon why.  } between the functors \\[ \\eta: F \\to G \\] <p>to be a natural transformation if it associates each \\(C \\in \\ob(\\cc)\\) with a morphism</p> \\[ \\eta_C : F(C) \\to G(C) \\] <p>in \\(\\dd\\) such that for every \\(f: A \\to B\\), we have that  \\begin{statement}{ProcessBlue!10} \\  \\end{statement} which amounts to \\(\\eta_B \\circ F(f) = G(f) \\circ \\eta_A\\).  Thus we can imagine that \\(\\eta\\) translates the diagram produced by the functor \\(F\\) to a diagram produced by \\(G\\). For example; if \\(\\eta\\) is a natural transformation between \\(F\\) and \\(G\\), then we also see that the following diagram commutes: \\  and this diagram commutes \\  if the above diagram on the left commutes. Colors are added to aid the visualization in seeing how the natural transformation translates the diagram produced by \\(F\\) to the diagram produced by \\(G\\). </p> <p> Let \\(\\eta: F \\to G\\) be a natural transformation. If \\(\\eta_A: F(A) \\to G(A)\\) \\ is an isomorphism for each object \\(A\\), then we say \\(\\eta\\) is a natural isomorphism. </p> <p> Let \\(K\\) be a ring in CRng. Recall from Exercise 1.3.3 that  \\[ GL_n(-): \\cring \\to \\grp \\quad (-)^{\\times}: \\cring \\to \\grp   \\] <p>are functors. In that exercise we actually showed that the domain categories  were \\(\\ring\\), but for our purpose we can restrict these functors to  the full subcategory \\(\\cring\\).</p> <p>Consider a commutative ring \\(K\\). Recall that for matrix \\(M \\in GL_n(K)\\),  we can take the determinant of \\(K\\); we are usually more familiar with this concept  when \\(K = \\mathbb{R}\\). However, it is a fact from ring theory  that a matrix \\(M\\) is invertible if and only if the determinant \\(\\det(M)\\) of \\(M\\) is in  \\(K^{\\times}\\). Since \\(GL_n(K)\\) is the set of all such invertible matrices,  we see that we may associate each \\(K\\) with its determinant function</p> \\[ \\text{det}_K: GL_n(K) \\to K^{\\times} \\] <p>which sends an invertible \\(M\\in GL_n(K)\\) to its determinant in \\(K^{\\times}\\).  To see that this morphism is a group homomorphism, we simply recall the determinant property </p> \\[ \\det(AB) = \\det(A)\\det(B). \\] <p>The claim is now that this family of morphisms assembles into a natural transformation.  Specifically, that \\(\\det: GL_n(-) \\to (-)^{\\times}\\). To see, this, let \\(f: K \\to K'\\) be a homomorphism between commutative rings.  Recall from ring theory that the determinant of a matrix \\(M = [a_{ij}]\\) with \\(a_{ij} \\in K\\)  is given by </p> \\[ \\det(M) = \\sum_{\\sigma \\in S_n}\\text{sgn}(\\sigma)a_{1\\sigma(1)}\\cdots a_{n\\sigma(n)}. \\] <p>where \\(S_n\\) is the symmetric group, and \\(\\text{sgn}(\\sigma)\\) is the sign  of a permutation. Now for \\(\\det\\) to form a natural transformation, we'll need that the diagram below commutes. \\  Note that \\(f: K \\to K'\\) is a commutative ring homomorphism. To show this diagram commutes, consider any \\(M = [a_{ij}] \\in GL_n(K)\\). Observe that </p> \\[\\begin{align*} (f^{\\times} \\circ \\text{det}_K)(M) &amp;= f^{\\times}(\\text{det}_K(M))\\\\ &amp;= f^{\\times}\\left( \\sum_{\\sigma \\in S_n}\\text{sgn}(\\sigma)a_{1\\sigma(1)}\\cdots a_{n\\sigma(n)} \\right) \\\\ &amp;= \\sum_{\\sigma \\in S_n}\\text{sgn}(\\sigma)f(a_{1\\sigma(1)})\\cdots f(a_{n\\sigma(n)})\\\\ &amp;= \\text{det}_{K'}([f(a_{ij}]))\\\\ &amp;= \\text{det}_{K'} \\circ GL_n(f)(M). \\end{align*}\\] <p>Hence we see that the diagram commutes, so that the determinant \\(\\det: GL_n(-) \\to (-)^{\\times}\\) assembles into a natural transformation  between the functors. </p> <p> For a field \\(k\\), recall that we have two functors \\(A^n(-), P^n(-): \\fld \\to \\Set\\) where  \\[ A^n(k) = \\{(a_0, \\dots, a_{n-1}) \\mid a_i \\in k\\}  \\qquad P^n(k) = A^{n+1}(k)/\\sim \\] <p>where  \\(\\sim\\) is the equivalence relation on the set \\(A^{n+1}(k)\\) described as follows: \\((a_0, \\dots, a_n) \\sim (a'_0, \\dots, a'_n)\\)  if \\((a_0, \\dots, a_n) = \\lambda(a'_0, \\dots, a'_n)\\) for some nonzero \\(\\lambda \\in k\\). Geometrically, the equivalence relation identifies  points which are lying on the same line passing through the origin. </p> <p>As we noted before, these functors  are particularly important in algebraic geometry. Now for each point \\((a_0, \\dots, a_n)\\),  denote \\([(a_0, \\dots, a_n)]\\) as its equivalence class. Let \\(\\theta_k: A^{n+1}(k) \\to P^n(k)\\) be the function that maps  a point \\((a_0, \\dots, a_n)\\) to its equivalence class \\([(a_0, \\dots, a_n)]\\).  Our claim is that for each \\(k\\), the functions \\(\\theta_k\\) assemble into a  natural transformation. </p> <p>That is, for a field homomorphism \\(\\phi: k \\to k'\\), the diagram \\  commutes.  The reader is encouraged to fill in the details for this one. It's quite  surprising that this does assemble into a natural transformation, because in general  there is no reason to ever expect that the projection map, \\(\\pi: X \\to X/\\sim\\) with \\(\\sim\\) an equivalence relation, is, in any sense, natural. Its  because most functions mess things up, and disorganize the equivalence classes! </p> <p>The above morphism, \\(\\theta: A^{n+1} \\to P^n\\), actually has a very interesting geometric  realization\\footnote{This isn't important for the reader to understand. However, I do want to avoid blabbering abstract nonsense so that the reader knows we're doing real, relevant mathematics. And  perhaps it might be motivation for the reader to check out an algebraic geometry text!}.  If \\(Y\\) is an algebraic subset of \\(P^n(k)\\), then we can build the affine cone \\(C(Y) = \\theta^{-1}(Y) \\cup\\{(0,\\dots, 0)\\}\\). With \\(n = 2\\), \\(Y\\) corresponds to  a curve in \\(P^2(k)\\), which generates the surface \\(C(Y)\\) in in \\(A^3(k)\\). \\  </p> <p> Earlier, we showed that \\(p_G: \\grp \\to \\ab\\) in which \\(G \\mapsto G/[G, G]\\) was a functor. It turns out that the projection  \\[  T_G : G \\to G/[G, G] \\qquad g \\mapsto g + [G, G] \\] <p>forms a natural transformation between the identity functor \\(1_{\\grp}: \\grp \\to \\grp\\) on \\(\\grp\\) and the  functor \\(p_G\\). </p> <p>To show this, consider the morphism \\(f: G \\to H\\) in \\(\\grp\\). We know that \\(p_G\\) induces a function \\(f^*: G/[G, G] \\to H/[H, H]\\) defined as </p> \\[ f^*(g + [G, G]) = f(g) + [H,H]. \\] <p>Now let \\(g \\in G\\).  \\begin{description} \\item[\\(\\bm{T_H\\circ f(g)}\\).] On one hand, observe that </p> \\[ T_H \\circ (f(g)) =  f(g) + [H, H].  \\] <p>\\item[\\(\\bm{f^*\\circ (T_G(g))}\\).] On the other hand, observe that </p> \\[ f^*\\circ T_G(g) = f^*(g + [G, G]) = f(g) + [H, H].   \\] <p>\\end{description} Hence, we see that </p> \\[ T_H\\circ f = f^*\\circ T_G \\] <p>so that the following diagram commutes  \\  and hence \\(T\\) is a natural transformation. </p> <p> The categories \\(\\finord\\) and \\(Set\\bm{_F}\\), are closely related categories. Recall that \\(\\finord\\) has finite ordinals \\(n = \\{0, 1, 2, \\dots, n-1 \\}\\) as objects with morphisms all functions \\(f: m \\to n\\) where \\(m, n\\) are natural numbers, and the objects of \\(\\Set\\bm{_F}\\) are all finite sets  (of some universe \\(U\\)) with morphisms all functions between such sets.  <p>Obviously the objects and morphisms of \\(\\finord\\) are in \\(\\Set\\bm{_F}\\). Thus, let \\(S : **Findord** \\to \\Set\\bm{_F}\\) be the inclusion functor.</p> <p>Define a functor \\(\\#: \\Set\\bm{_F} \\to \\finord\\) as follows. Assign each \\(X \\in \\Set\\bm{_F}\\) to the ordinal \\(\\# X = n\\), the number of elements in \\(X\\). We can represent this bijective mapping as </p> \\[ \\theta_X : X \\to \\#X. \\] <p>Furthermore, if \\(f: X \\to Y\\) is a morphism in \\(\\Set\\bm{_F}\\), associate \\(f\\) with the morphism \\(\\#f: \\#X \\to \\#Y\\) in \\(\\finord\\) defined by </p> \\[ \\#f =\\theta_Y \\circ f \\circ \\theta_X^{-1}. \\] <p>Thus we have that the following diagram is commutative: \\  and \\(\\theta\\) acts a natural transformation between the two functors.</p> <p>Note that if \\(X\\) is an ordinal number, we define \\(\\theta_X\\) to be the identity function, which ensures that \\(\\# \\circ S\\) is the identity functor on \\(\\finord\\). However, \\(S \\circ \\#\\) is not the identity on \\(\\Set\\bm{_F}\\), since the input will be \\(X\\) while the output will just be \\(\\#X\\) (as \\(S\\) is just the inclusion functor.)  </p> <p>To end this section, we offer a topological interpretation of the concept of  a natural transformation, one which has been known by category theorists since the 1960's, but a perspective which usually is not introduced since it does not really offer  signficant pedagogical advantagous unless the reader is already aware of basic homotopy theory (in which  case, they probably already know what a natural transformation is). I've nevertheless decided to include it because it is an interesting perspective.</p> <p>Let \\(X\\) and \\(Y\\) be topological spaces. Consider two functions \\(f: X \\to Y\\).  Recall that a homotopy \\(H\\) from \\(X\\) to \\(Y\\) is a continuous function \\(H: [0, 1] \\times X \\to Y\\)  such that \\(H(0, x) = f(x)\\) and \\(H(1,x) = g(x)\\). A simple example of a homotopy is when \\(X = [0, 1]\\). In this case,  \\(f, g: [0, 1] \\to Y\\) are simply two continuous paths in \\(Y\\). A  homotopy, in this situation, between \\(f,g\\) is pictured on  the bottom left. </p> <p>\\  On the above right we have the situation for when \\(f, g\\) start and end at the same point; this homotopy  is know as a path homotopy. </p> <p>Of course, a homotopy doesn't always exist. When it does, a homotopy can be  interpreted as parameterizing, via \\(t \\in [0, 1]\\), a family of continuous functions \\(H_t: X \\to Y\\) which continuously deform \\(f\\) into \\(g\\)\\footnote{Caution: a family of continuous functions does not conversely  define a homotopy.}. </p> <p>But this story is familar! A natural transformation \\(\\eta: F \\to G\\) between two functors  \\(F, G: \\cc \\to \\dd\\) give rise to a family of morphisms \\(\\eta_A: F(A) \\to G(A)\\) which are parameterized by the objects of \\(\\cc\\) (which also satisfy the naturality property). Below we have this pictured of what this generally looks like.  \\  So, what gives? Is the concept of a natural transformation somewhat logically  and conceptually analogous to the concept of a homotopy? The answer is yes, and we can define  a natural transformation in the following manner which is strikingly similar to the  definition of a homotopy. </p> <p> Let \\(F, G: \\cc \\to \\dd\\) be functors. Let \\(\\bm{2}\\) be the  category with two objects \\(0, 1\\) and a single nontrivial morphism. A natural transformation \\(\\eta: F \\to G\\) is a functor  \\(\\eta: \\cc \\times (\\bm{2}) \\to \\dd\\) such that  \\(\\eta(-, 0) = F\\) and \\(\\eta(-, 1)= G\\).  </p> <p>Proving this is left as an exercise.</p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] In what follows, let \\(F,G: \\cc \\to \\dd\\) be a pair of functors. Interpret what a natural transformation \\(\\eta: F \\to G\\) is in each case. \\begin{itemize}</p> </li> <li> <p>[(i.)] Where \\(\\cc\\) is a discrete category, and \\(\\dd\\) is arbitrary. Separately, can we have a natural transformation when \\(\\dd\\) is discrete?</p> </li> <li> <p>[(ii.)] Where \\(\\cc\\) and \\(\\dd\\) are preorders.</p> </li> <li> <p>[(iii.)]  Where \\(\\cc\\) and \\(\\dd\\) are one-object categories whose morphisms are group. </p> </li> <li> <p>[(iv.)] Where \\(\\cc\\) is arbitrary and \\(\\dd\\) is \\(\\cat\\).</p> </li> </ul> <p>\\item[2.] Show that Definition \\ref{definition:nat_trans_homotopy} and Definition \\ref{definition:nat_trans} are equivalent.</p> <p>\\item[3.] Consider the initial discussion of this section. Prove that for two functors  \\(F, G : \\cc \\to \\Set\\) such that \\(F(A) \\subset G(A)\\) for all \\(A \\in \\cc\\),  the inclusion morphisms \\(i_A: F(A) \\to G(A)\\) form a natural transformation  \\(i: F \\to G\\) if and only if, for each \\(f: A \\to B\\) in \\(\\cc\\), we have that \\(F(f) = G(f)|_{F(A)}\\). </p> <p>\\item[4.] Let \\(\\cc\\) be a category, and consider two objects \\(A,B\\)  so that we  have the functors </p> \\[ \\hom_{\\cc}(A, -), \\hom_{\\cc}(B, -): \\cc  \\to \\Set. \\] <ul> <li>[(i.)] Let \\(\\phi\\in \\hom_{\\cc}(B,A)\\). Show  that the family of functions </li> </ul> \\[ \\phi^*_C: \\hom_{\\cc}(A,C) \\to \\hom_{\\cc}(B,C) \\] <p>indexed by each object \\(C \\in \\cc\\), where \\(\\phi^*_C(f: A \\to  C) = f \\circ \\phi: B \\to C\\), forms a natural transformation \\(\\phi^*: \\hom_{\\cc}(A, -) \\to \\hom_{\\cc}(B, -)\\).</p> <ul> <li>[(ii.)] Show that every natural transformation  \\(\\eta:  \\hom_{\\cc}(A, -)\\to \\hom_{\\cc}(B, -)\\) is constructed  in this way. </li> </ul> <p>\\item[5.] Let \\(F: \\cc \\to \\Set\\) be any other functor. Interpret what a  natural transformation \\(\\eta: \\bullet \\to F\\) is.  What about \\(\\epsilon: F \\to \\bullet\\)? </p> <p>\\item[6.] For every ring \\(R\\) there is a natural inclusion homomorphism \\(i_R: R \\to R[x]\\). Thus, let \\((-)[x]: \\ring \\to \\ring\\) be the functor that sends a ring  \\(R\\) to its single-variable polynomial ring \\(R[x]\\). Show that we have a natural transformation </p> \\[ i: I \\to (-)[X] \\] <p>where \\(I: \\ring \\to \\ring\\) is the identity on \\(\\ring\\). </p> <p>\\item[7.] Recall the category of \\(G\\)-sets is the category where  \\begin{description} \\item[Objects.] All \\(G\\)-sets \\(X\\) (i.e., sets \\(X\\) such that \\(G\\) has a  group action \\(\\phi:X \\times G \\to X\\)) \\item[Morphisms.] All \\(G\\)-equivariant morphisms (i.e., functions \\(f:X \\to Y\\) such that \\(f(g \\cdot x) = g \\cdot f(x)\\)). \\end{description} (Also see Exercise 1.3.6). Let \\(X\\) be a \\(G\\)-set with action map \\(\\phi: X \\times G \\to X\\)  and fix an element \\(g \\in G\\). For such an \\(X\\), define the map  \\(\\phi_X^g: X \\to X\\) where \\(\\phi_X^g(x) = \\phi(g, x)\\). </p> <p>Show that for each \\(g\\), the maps \\(\\phi^g\\) form a natural transformation  \\(I \\to I\\), where \\(I: **G****-sets** \\to **G****-sets**\\)  is the identity  functor on this category. (Note that this is a nontrivial example of a natural  transformation between a functor and itself!)</p> <p>\\end{itemize}</p>"},{"location":"category_theory/Categories%2C%20Functors%20and%20Natural%20Transformations./Paths%20and%20Diagrams%20in%20Categories/","title":"1.5. Paths and Diagrams in Categories","text":"<p>In this section we give an overview of the concept of a path and  of a diagram within a category, which are concepts that are exactly  what they sound like.  This is usually a discussion that is usually glossed over, which is a huge mistake since diagrams are used everywhere in mathematics.  They'll appear in nearly every section from this point on, and any good book  on category theory will have dozens of diagrams. In short, they  are extremely indespensible. </p> <p>So, we set off to do a justice to the important concepts of paths and diagrams.  However, I've kept the pragmatic reader in mind and have avoided making  this discussion abstract and irrelevant. </p> <p>First, we form some intuition on what exactly a diagram is.  Informally, a diagram in a category \\(\\cc\\) consists of a finite sequence  of arrows between objects. Below are some diagrams. </p> <p> We can also have more complicated diagrams such as the diagrams below.  \\  Of course, a diagram does not really mean anything on its own; it is simply  a graph\\footnote{Technically, since a diagram can have multiple morphisms between  two objects, every diagram is a \"quiver.\" This is explored more in Chapter 2.}.  A diagram requires the context of a category to have any meaning.  Despite this, we can still abstract the core ingredients of what a diagram really  is for a general category \\(\\cc\\). To do so requires observing that in the diagrams  above (which are the ones we care about), there are certain paths given  by iterated composition. Thus we start at this concept and build upwards to define  a diagram.</p> <p> Let \\(\\cc\\) be a category and consider two objects  \\(A\\) and \\(B\\). A path \\(p\\) in \\(\\cc\\)  of length \\(n\\) from \\(A\\) to \\(B\\) consists of  <ul> <li> <p>distinct objects  \\(A_1, A_2, \\dots, A_{n+1}\\) with \\(A_1=  A\\) and \\(A_{n+1} = B\\)</p> </li> <li> <p>a chain of morphisms \\(f_1: A_1 \\to A_2, \\dots,  f_{n}: A_{n} \\to A_{n+1}\\)</p> </li> </ul> <p>and we say \\(p = f_n \\circ \\cdots \\circ f_1\\). If two paths \\(p = f_n \\circ \\cdots f_1\\)  and \\(q = g_m \\circ g_{m-1} \\circ \\cdots \\circ g_1\\) start and end at the same objects  \\(A\\) and \\(B\\), we say \\(p\\) and \\(q\\) are parallel paths.   For example, we have a path of length five from \\(A_1\\) to \\(A_6\\)  in some category \\(\\cc\\) displayed below in blue.</p> <p>\\  Note that in the above picture, we will in general have many possible paths  between two different objects. We now face the question: is there a way to  organize this data without getting too complicated? </p> <p>To answer that question, we must work with a small category in order  to avoid contradictions that arise due to size issues in set theory.  With that said, we propose the following definition.         </p> <p> Let \\(\\cc\\) be a small category. For any two objects \\(A, B\\),  and for any positive integer \\(n\\), define the path set of order \\(n\\)  from \\(A\\) to \\(B\\) as \\[ \\path^n(A, B) =  \\{ \\text{all paths } p: A \\to B \\text{ of length } n \\}. \\] <p>The above definition makes sense, but admittedly it is not illuminating.  Is there another perspective we can make from this?  </p> <p>Yes! Because paths are made of components which are inherently ordered, one way to imagine a path is as a tuple  \\((f_1, \\dots, f_n)\\) of \\(n\\)-morphisms where the codomain of \\(f_i\\) is the domain of \\(f_{i+1}\\). In other words, a path from \\(A\\) to \\(B\\) is an element of </p> \\[ \\hom(A, A_1)\\times \\hom(A_1, A_2)\\times \\cdots \\times \\hom(A_n, B).   \\] <p>for some objects \\(A_1, \\dots, A_n\\) in \\(\\cc\\). Therefore, we can say that </p> \\[ \\path^n(A,B) = \\bigcup_{A_1, \\dots A_n \\in \\text{Ob}(\\cc)} \\hom(A, A_1)\\times \\hom(A_1, A_2)\\times \\cdots \\times \\hom(A_n, B). \\] <p>where in the above union we vary across all objects \\(A_1, \\dots, A_n \\in \\ob(\\cc)\\).  Note that when \\(n = 1\\), we have that \\(\\path^n(A, B) = \\hom(A, B)\\). In this way,  the path set can be thought of as a generalized hom-set. </p> <p> A simple diagram \\(J\\) in a category \\(\\cc\\) consists of  two distinguished objects  \\(\\textcolor{NavyBlue}{A}\\) and \\(\\textcolor{Orange}{B}\\), referred to as  the source and target of \\(J\\),  and any finite collection of parallel paths \\(p_1: A \\to B, p_2: A \\to B, \\dots, p_n: A \\to B\\)  of any length. </p> <p>Some simple diagrams are pictured below. In the first diagram, the  source and targets are \\(X\\) and \\(Z\\); in the second, they are \\(A\\) and \\(F\\);  in the third, they are \\(V\\) and \\(V_7\\). \\  In many situations, simple diagrams are what we really care about. This is because  often times we have two objects of interests, and we consider many possible paths between  them. And in those situations, we are generally asking: are all such paths equivalent? </p> <p>This is something high schoolers ask themselves all the time, and a mistake  they make all the time. Let \\(n \\ge 2\\). Consider the functions</p> <ul> <li> <p>\\(e: \\mathbb{N} \\to \\mathbb{N}\\) where \\(f(a) = a^n\\) (\\(e\\) for exponent)</p> </li> <li> <p>\\(p: \\mathbb{N}\\times \\mathbb{N} \\to \\mathbb{N}\\) where  \\(f(a,b) = a + b\\) (\\(p\\) for plus)</p> </li> </ul> <p>Often times, they get confused and think that the paths of the diagram below are equivalent. \\  Sadly, this equation does not hold generally, and the two paths of the diagram  are not equivalent. Thus at this point we introduce terminology for  discussing when paths are equivalent.</p> <p> Let \\(J\\) be a simple diagram in \\(\\cc\\). If every parallel path is equal, then we say \\(J\\) commutes and is a commutative diagram.  </p> <p>At this point, we should note that there is still some work to be done, since  of course not all \"diagrams\" that we care about are simple.  For example, an extremely important diagram that will  eventually become engrained in your brain is pictured below on the left.\\footnote{Understanding this diagram right now is not important; there is a lot  more stuff one needs to learn before we get into what this means. Long story short,  it is the universal property of a product.} \\  Here, the objects are sets, and the morphisms are functions; the underlying function  maps are pictured above on the right. </p> <p>Clearly this diagram is not simple. However, note that it is built from simple  diagrams; specifically, the left and right triangles are simple diagrams.  At this point, it is clear that the task of rigorously defining the notion of a diagram is reduced to defining what exactly we mean by \"building\" such  diagrams.</p> <p>{\\large Exercises \\vspace{0.2cm}}</p> <ul> <li>[1.] Consider a category \\(\\cc\\) with objects \\(A, A_0, \\dots, A_n, B, B_0, B_1, \\dots, B_m\\). Let \\(A_0 = B_0 = A\\) and \\(A_n = B_m = B\\), and suppose we have a family of isomorphisms \\(f_i: A_{i-1} \\isomarrow A_i\\) and \\(g_i: B_{i-1} \\isomarrow B_i\\) as below.  \\  Suppose we have another object \\(C\\) and isomorphisms \\(\\phi_i: A_i \\isomarrow C\\), \\(\\psi_i: B_i \\isomarrow C\\) with \\(\\psi_0 = \\phi_0\\) and \\(\\phi_n = \\psi_m\\).  Prove that if \\(\\phi_{i} \\circ f_i = \\phi_{i+1}\\)  and \\(\\psi_{i} \\circ g_i = \\psi_{i+1}\\), then the above diagram is commutative in \\(\\cc\\). </li> </ul>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/%24%5Cmathcal%7BC/","title":"2.1. $\\mathcal{C","text":"<p>op$ and Contravariance}</p> <p> Consider a category \\(\\mathcal{C}\\). Then we define the opposite category of \\(\\cc\\), denoted \\(\\mathcal{C}\\op\\),  to be the category where  \\begin{description} \\item[Objects.] The same objects of \\(\\cc\\). \\item[Morphisms.] If \\(f: A \\to B\\) is a morphism of \\(\\cc\\), then we let  \\(f\\op : B \\to A\\) be a morphism of \\(\\cc\\op\\). \\end{description} In this case, composition isn't exactly obvious, so we will explain how that  works.  <p>Let \\(f: A \\to B\\) and \\(g: B \\to C\\) be morphisms of \\(\\cc\\). Then we obtain  morphisms \\(f\\op: B \\to A\\) and \\(g\\op: C \\to B\\). In this case \\(f\\op, g\\op\\) are composable,  and we define composition of \\(\\cc\\op\\),  denoted as \\(\\circ\\op\\),  to be the morphism </p> \\[ f\\op \\circ g\\op: C \\to A. \\] <p>Moreover, we have the relation \\((g \\circ f)\\op = f\\op \\circ g\\op\\). </p> <p>Taking the opposite category might seem very strange,  but we are doing nothing more than just taking the same category  and swapping the domain and codomain of every morphism. </p> <p>Consequently, many properties of morphisms are similarly reversed. For example, if \\(f: A \\to B\\) is monomorphism in \\(\\cc\\), then \\(f\\op: B \\to A\\) is an epimorphism in  \\(\\cc\\op\\).  More generally, every logically valid statement that can be made in \\(\\cc\\) using its objects and morphisms can be dualized to achieve an equivalent, logically valid statement in \\(\\cc\\op\\) using its objects and morphisms. </p> <p> Consider a category \\(\\cc\\) containing \\(3\\) objects whose morphisms are arranged as follows: <p> What does the dual category \\(\\cc\\op\\) look like? Well, \\(\\cc\\op\\) contains the same objects \\(A, B\\) and \\(C\\). As for the morphisms, \\(\\cc\\) has the three morphisms \\(f, g, h\\), in addition to their composites. Therefore, \\(\\cc\\op\\) also has three morphisms  \\(f\\op:B \\to A\\), \\(g\\op: C \\to B\\) and \\(h\\op: A \\to C\\) and their composites. Hence, \\(\\cc\\op\\) looks like this: \\  </p> <p> Let \\(P\\) be a preorder, specifically a partial order.  Recall that this means that \\(P\\) has  a binary relation  \\(\\le\\) and if \\(p \\le p'\\) and \\(p' \\le p\\), then \\(p = p'\\).  <p>We claim that that \\(P\\op\\) is still a partial order. But first,  what does \\(P\\op\\) even look like? If we have some elements \\(p_1, p_2, p_3\\) in \\(P\\) such that </p> \\[ p_1 \\le p_2 \\le p_3 \\] <p>Then, as a category, \\(P\\) has the unique morphisms \\(f: p_1 \\to p_2\\) and  \\(g: p_2 \\to p_3\\). Hence, in \\(P\\op\\), we have the unique morphisms  \\(g\\op : p_3 \\to p_2\\) and \\(f\\op:p_2 \\to p_1\\),  so that we obtain a reversed binary relation \\(\\le\\op\\) in \\(P\\), which  reorder \\(p_1, p_2, p_3\\) as below.</p> \\[ p_3 \\le\\op p_2 \\le\\op p_1 \\] <p>This is kinda weird to write, and in fact, it makes  more sense if we  write \\(\\le\\op = \\ge\\) as the binary relation in \\(P\\op\\). We then have that </p> \\[ p_1 \\le p_2 \\le p_3 \\text{ in } P  \\implies p_3 \\ge p_2 \\ge p_1 \\text{ in } P\\op \\] <p>which is nice! Things are even nicer in a linear order, for if \\(P = \\{p_1, p_2, p_3, \\dots \\}\\) is a linear order, then we can write that </p> \\[ \\cdots p_i \\le p_j \\le p_k \\cdots    \\] <p>and hence in \\(P\\op\\) this becomes </p> \\[ \\cdots p_i \\ge p_j \\ge p_k \\cdots. \\] <p></p> <p> Let \\((G, \\cdot)\\) be a group.  In group theory one can formulate the opposite group \\((G\\op, \\cdot\\op)\\)  as follows. Define \\((G\\op, \\cdot\\op)\\) to be group with the same set of elements as \\(G\\),  whose product \\(\\cdot\\op\\) works as  \\[ g_1 \\cdot\\op g_2 = g_2 \\cdot g_1.    \\] <p>Since both \\((G, \\cdot)\\) and \\((G, \\cdot\\op)\\) are groups, we can regard them both  as one object categories. What is interesting to realize is that under the  categorical interpretation, they are opposite categories of each other.</p> <p></p> <p>We thus see that dualizing a category simply involves changing the  directions of the morphisms  on the objects. But can we dualize a  functor?</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor and suppose \\(f: A \\to B\\) is morphism in \\(\\cc\\). We say \\(F\\) is a contravariant functor if \\(F(f): F(B) \\to F(A)\\).  This is in sharp contrast to a covariant functor, in which \\(f: A \\to B\\) is sent to \\(F(f): F(A) \\to F(B)\\). </p> <p>We next introduce a few examples to demonstrate a contravariant functor. </p> <p> Let \\(k\\) be an algebraically closed field. Recall that \\(A^n(k)\\) is the set of tuples  \\((a_1, a_2, \\dots, a_n)\\) with \\(a_i \\in k\\). In algebraic geometry, it  is of interest to associate each subset \\(S \\subset A^n(k)\\) with the ideal \\[ I(S) = \\bigg\\{f \\in k[x_1, \\dots, k_n] \\;\\bigg|\\; f(s) = 0 \\text{ for all } s \\in S \\bigg\\}.   \\] <p>of \\(k[x_1, \\dots, x_n]\\). Observe that this is always non-empty since \\(0 \\in I(S)\\) for any \\(S\\).  In additional, it is clearly an ideal of \\(k[x_1, \\dots, x_n]\\),  since for any \\(p \\in k[x_1, \\dots, x_n]\\),\\(q \\in I(S)\\), we have that </p> \\[ (p \\cdot q)(s) = p(s)\\cdot q(s) = p(s) \\cdot 0 = 0 \\text{ for all } s \\in S. \\] <p>so that \\(p\\cdot q \\in I(S)\\). Now it's usually an exercise to show that  if \\(S_1 \\subset S_2\\) are two subsets of \\(A^n(k)\\), then one has that  \\(I(S_2)\\subset I(S_1)\\). Hence this defines a contravariant functor</p> \\[ I: **Subsets**(A^n(k)) \\to **Ideals**(k[x_1,\\dots, x_n]). \\] <p>where \\(**Subsets**(A^n(k))\\) is the category of subsets with inclusion morphisms,  and \\(**Ideals**(k[x_1,\\dots, x_n])\\) is the category of ideals with inclusion  ring homomorphisms. </p> <p> Consider again \\(k\\) as an algebraically closed field. In algebraic geometry,  one often wishes to associated each ideal of \\(k[x_1, \\dots, x_n]\\) with its \"zero set\"  \\[ Z(I) = \\bigg\\{s = (a_1, \\dots, a_n) \\in A^n(k) \\;\\bigg|\\; f(s) = 0 \\text{ for all } s \\in I\\bigg\\}. \\] <p>It is usually an exercise to show that if \\(I_1 \\subset I_2\\) are two ideals,  then \\(Z(I_2) \\subset Z(I_1)\\). Hence we see that this defines a contravariant functor </p> \\[ Z: **Ideals**(k[x_1, \\dots, x_n]) \\to **Subsets**(A^n(k)). \\] <p></p> <p>It is usually at the beginning of an algebraic geometry course that one  will understand the relationship between these two constructions, which themselves  are secretly functors.</p> <p>What follows is a very interesting example. In fact, this example is an example of a  beautiful concept of a sheaf, and it is usually used as a motivating  example. But that is for later. </p> <p> Let \\(X\\) be a topological space, and  consider the thin category \\(**Open**(X)\\), which contains all open sets \\(U \\subset X\\), equipped with the inclusion function \\(i_{U, X}: U \\to X\\).  <p>For each \\(U \\in **Open**(X)\\), define the set</p> \\[ C(U) = \\{f: U \\to \\rr \\mid f \\text{ is continuous.}\\} \\] <p>Note that if \\(U \\subset V\\) are in \\(**Open**(X)\\), then  we define the function  \\(\\rho_{U,V}: C(V) \\to C(U)\\) where </p> \\[ \\rho_{U,V}(f: V \\to \\rr) = f\\big|_{U}: U \\to \\rr. \\] <p>That is, \\(\\rho_{U,V}\\) sends continuous, real-valued functions  on \\(V\\) to such functions on \\(U\\) by restriction.  It is not difficult to show that this respects identity and composition  requirements, so that we have a contravariant functor</p> \\[ C(-) : **Open**(X) \\to **Set** \\] <p>for each topological space \\(X\\).  </p> <p>What follows is another very important example. </p> <p> Let \\(\\cc\\) be a locally small category. In this case, we know that  each \\(A \\in \\cc\\) induces the covariant functor \\[ \\hom_{\\cc}(A, -) : \\cc \\to **Set** \\] <p>which sends objects \\(C\\) to the set \\(\\hom_{\\cc}(A, C)\\).  It is natural to ask if we may similarly define a functor  </p> \\[ \\hom_{\\cc}(-, A): \\cc \\to **Set**. \\] <p>The answer is yes. We did not make this observation in the past  for pedagogical reasons, since it's actually a contravariant functor  (and we didn't know what that was until now). We can now safely say  that \\(\\hom_{\\cc}(-, A)\\) is a contravariant functor.  </p> <p>We now comment on the relationship between contravariant and covariant functors. </p> <p> Let \\(\\cc\\), \\(\\dd\\) be categories.  <ul> <li>Let \\(F: \\cc \\to \\dd\\) be a contravariant functor. Then \\(F\\) corresponds to a  contravariant functor \\(\\overline{F}: \\cc\\op \\to \\dd\\) where for a \\(f\\op : B \\to A \\in \\cc\\op\\), </li> </ul> \\[ \\overline{F}(f\\op : B \\to A) = F(f: A \\to B) = F(f): F(B) \\to F(A). \\] <ul> <li>Conversely, let \\(F: \\cc \\to \\dd\\) be a covariant functor. Then \\(F\\)  corresponds to a contravariant functor \\(\\overline{F}: \\cc\\op \\to \\dd\\)  where </li> </ul> \\[ \\overline{F}(f\\op : B \\to A) = F(f: A \\to B) = F(f): F(A) \\to F(B) \\] <p></p> <p>The above proposition allows us to treat any functor as covariant or  contravariant. Thus, if we don't like the behavior of our functor on morphisms, we can  find an equivalent functor that behaves on morphisms in our preferred way. </p> <p>Generally, covariant functors are easier to think about, so we often like  to turn contravariant functors into covariant functors. </p> <p> Recall that the functor  \\[ C(-): **Open**(X) \\to **Set** \\] <p>is contravariant. What if we want to treat this as a covariant functor?  Well, we can define the functor </p> \\[ \\overline{C}(-): **Open**(X)\\op \\to **Set**             \\] <p>as follows. If \\(U \\subset V\\) are open subsets of the topological space \\(X\\),  then let \\(i: U \\to V\\) be the inclusion. This is a morphism in \\(**Open**(X)\\). Hence, \\(i\\op: V \\to U\\) is a morphism in \\(**Open**(X)\\op\\).  Therefore, we define</p> \\[ \\overline{C}(i\\op: V \\to U) = C(i: U \\to V) = \\rho_{U,V}: C(V) \\to C(U). \\] <p>Thus we see that this functor \\(\\overline{C}\\) acts the same way as \\(C\\), except  it behaves covariantly on the morphisms now instead of contravariantly.  </p>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Functor%20Categories/","title":"2.3. Functor Categories","text":"<p>In the proof for the last proposition, we used a trick of forming a desired natural transformation by composing two composable natural transformations. Hence, we see that natural transformations can be \"composed.\" We refine this notion as follows.</p> <p>Let \\(\\cc\\) and \\(\\dd\\) be categories and consider three functors \\(F, G, H: \\cc \\to \\dd\\). Suppose further that we have two natural transformations \\(\\sigma, \\tau\\) as below:</p> <p> (This might seem like a weird way to write this, but we are trying to hint at something.) Using these two natural transformations, we can define a natural transformation </p> \\[ \\tau \\cdot \\sigma: F \\to H \\] <p>where, for each \\(C \\in \\cc\\), we define </p> \\[ (\\tau \\cdot \\sigma)_C = \\tau_C \\circ \\sigma_C: F(C) \\to H(C). \\] <p>Visually, we can picture what we are doing as follows. For a given morphism \\(f: A \\to B\\) in \\(\\cc\\), we define the morphism \\((\\tau \\cdot \\sigma)_C\\) as \\  Thus, we see that natural transformations can be \"composed,\" and we can thus ask: If we view functors as objects, and view natural transformations  as morphisms, do we get a category? The answer is yes. </p> <p> Let \\(\\cc\\) and \\(\\dd\\) be small categories and consider set of all functors \\(F: \\cc \\to \\dd\\). Then the \\textbf{functor category}, denoted as \\(\\dd^\\cc\\) or \\(\\fun(\\cc, \\dd)\\), is the category where  \\begin{description} \\item[Objects.] Functors \\(F: \\cc \\to \\dd\\) \\item[Morphisms.] Natural transformations \\(\\eta: F \\to G\\)  \\end{description}  Functor categories are extremely useful, as we shall see that  they're the categorical version of representations. </p> <p>When we think of  representations, we usually think of a group homomorphism \\(\\rho: G \\to \\text{GL}_n(V)\\)  for some vector space \\(V\\) over a field \\(k\\). However, suppose we wanted to be  a real smart-ass and say ``Well, can't we regard \\(\\rho\\) as actually a functor  between two one-object categories whose morphisms are all isomorphism?'' The answer is yes! </p> <p>What this then means is that the category of representations of a group \\(G\\) is actually a functor category. Specifically, </p> \\[ \\fun(G, \\text{GL}_n(V)) \\cong R**-Mod**. \\] <p>Hence in some cases it helps to think of \\(\\fun(\\cc, \\dd)\\) as a  category of representations of \\(\\cc\\). This makes sense, since that is really  what a functor is. A functor preserves composition; and if we stop thinking  like the set theorists, we can realize that composition controls a great deal  of structure  in a category \\(\\cc\\). Hence a functor \\(F: \\cc \\to \\dd\\) ``represents\"\" that structure in a  category \\(\\dd\\). </p> <p> Let \\(\\bm{1}\\) be the one element category with a single identity arrow. Then for any category \\(\\cc\\), the functor category \\(\\cc^{\\bm{1}}\\) is isomorphic to \\(\\cc\\). This is because each functor \\(F: \\bm{1} \\to \\cc\\) simply associates the element \\(1 \\in \\bm{1}\\) to an element \\(C \\in \\cc\\), and the identity \\(1_1: 1 \\to 1\\) to the identity morphism \\(1_C\\) in \\(\\cc\\).  </p> <p> Let \\(\\bm{2}\\) be the category consisting of two elements, containing the two identities and one nontrivial morphism between the objects.  \\  Now consider the functor category \\(\\cc^{\\bm{2}}\\) where \\(\\cc\\) is any category. Each functor \\(F:\\bm{2} \\to \\cc\\) maps the pair of objects to objects \\(F(1)\\) and \\(F(2)\\) in \\(\\cc\\). However, since functors preserve morphisms, we see that  \\[  f: 1 \\to 2 \\implies F(f): F(1) \\to F(2). \\] <p>This is what each \\(F \\in \\cc^{\\bm{2}}\\) does. Hence, every morphism \\(g \\in \\hom(\\cc)\\) corresponds to an element in \\(\\cc^{\\bm{2}}\\). Hence, we call \\(\\cc^{\\bm{2}}\\) the category of arrows of \\(\\cc\\). </p> <p> Let \\(g: C \\to C'\\) be any morphism between objects \\(C, C'\\) in \\(\\cc\\). Construct the element \\(G \\in \\cc^{\\bm{2}}\\) as follows: \\(G(1) = C\\), \\(G(2) = C'\\) and \\(G(f) : G(1) \\to G(2) = g\\). Hence, \\(\\hom(\\cc)\\) and \\(\\cc^{\\bm{2}}\\) are isomorphic. Moreover, \\(\\hom(\\cc)\\) determines the members of \\(\\cc^{\\bm{2}}\\).  <p>A crude way to visualize this proof is imaging \\(1 \\to 2\\) is a \"stick\" with 1 and 2 on either end, and so the action of any functor is simply taking the stick and applying it  to anywhere on the direct graph generated by the category \\(\\cc\\). Hence, this is why we say \\(\\hom(\\cc)\\) determines the functor category \\(\\cc^{\\bm{2}}\\).  </p> <p> Let \\(X\\) be a set. Hence, it is a discrete category, which if recall, it's objects are elements of \\(X\\) and the morphisms are just identity morphisms.  <p>Now consider \\(\\{0, 1\\}^X\\), the category of functors \\(F: X \\to \\{0, 1\\}\\). Then every functor assigns each element of \\(x \\in X\\) to either \\(0\\) or \\(1\\), and assigns the morphism \\(1_x: x \\to x\\) to either \\(1_0: 0\\to 0\\) or \\(1_1: 1 \\to 1\\). </p> <p>One way to view this is to consider \\(\\mathcal{P}(X)\\), and for each \\(S \\in \\mathcal{P}\\), assign \\(x\\) to \\(1\\) if \\(x \\in  S\\) or \\(x\\) to \\(0\\) if \\(x \\not\\in S\\). All of these mappings may be described by elements of \\(\\mathcal{P}\\), but we can also realize that each  of these mappings correspond to the functors in \\(\\{0, 1\\}^X\\). Hence, we see that \\(\\{0, 1\\}^X\\) is isomorphic to \\(\\mathcal{P}(X)\\).  </p> <p> Recall from Example \\ref{group_ring_functors} that, given  a group \\(G\\) and a ring \\(R\\) (with identity), we can create a  group ring \\(R[G]\\) with identity, in a functorial way, establishing  a functor  \\[ R[-]: **Grp** \\to **Ring**. \\] <p>However, we then noticed that the above functor establishes a process  where we send rings \\(R\\) to functors \\(R[-]: **Grp** \\to **Ring**\\).  It turns out that this process is itself a functor, and we now  have the appropriate language to describe it:</p> \\[ F: **Ring** \\to **Ring**^{**Grp**} \\] <p>Specifically, let \\(\\psi: R \\to S\\) be a ring homomorphism.  Now observe that \\(\\psi\\) induces another ring homomorphism </p> \\[ \\psi_G^*: R[G] \\to S[G] \\qquad \\sum_{g \\in G}a_g g \\mapsto \\sum_{g \\in G}\\phi(a_g) g. \\] <p>As a result, we see that such a ring homomorphism induces a natural transformation. To show this, let \\(\\phi: G \\to H\\) be a group homomorphism. Then observe that we get the diagram in the middle.  \\  However, we can follow the elements as in the diagram on the right, which shows  us that the diagram commutes. Hence we see that \\(\\psi^*\\) is a natural transformation  between functors \\(R[-] \\to S[-]\\). Overall, this establishes that we do in fact have  a functor </p> \\[ F: **Ring** \\to **Ring**^{**Grp**}    \\] <p>which we wouldn't be able to describe without otherwise introducing  the notion of a functor category. </p> <p> Let \\(M\\) be a monoid category (one object) and consider the functor category \\(**Set**^M\\). The objects of Set\\(^M\\) are functors \\(F: M \\to **Set**\\), each of which have the following data:  \\[ F(f) : F(M) \\to F(M) \\] <p>where \\(f: M \\to M\\) is an morphism in \\(M\\). Now if we interpret \\(\\circ\\) as the binary relation equipped on \\(M\\), we see that for any \\(g : M \\to M\\),</p> \\[ F(g \\circ f) = F(g) \\circ F(f) \\] <p>by functorial properties. Hence, each functor \\(F\\) maps \\(M\\) to a set \\(X\\) which induces the operation of \\(M\\) on \\(X\\). Therefore the objects of Set\\(^M\\) are other monoids \\(X\\) in Set equipped with the same operation as \\(M\\) and as well as the morphisms between such monoids.  </p>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Graphs%2C%20Quivers%20and%20Free%20Categories/","title":"2.6. Graphs, Quivers and Free Categories","text":"<p>In studying category it is often helpful to imagine the objects and morphisms in action as vertices and edges corresponding to a graph. In fact, such a pictorial representation of a category is not even incorrect; one can pass categories and graphs from one to the other. To speak of this, we first review some terminology.</p> <p> A (small) graph \\(G\\) is a set of vertices \\(V(G)\\) and a set  edges \\(E(G)\\) such that there exists an assignment function \\[ \\partial: E(G) \\to V(G)\\times V(G) \\] <p>which assigns every edge to the ordered pair containing its endpoints.</p> <p>On the other hand, a directed graph is a graph \\(G\\)  where \\(E(G)\\) is now a set of 2-tuples \\((v_1, v_2)\\). This allows  each edge of \\(E(G)\\) to have a specified direction. In this case,  the assignment function has the form \\(\\partial: E(G) \\to V(G)\\).   Now, how do we formulate a morphism between two graphs? </p> <p> A graph homomorphism between two graphs \\(G\\) and \\(H\\) is a  function \\(f: G \\to H\\) which induces maps \\(f_V: V(G) \\to V(H)\\) and \\(f_E: E(G) \\to E(H)\\) where if \\(\\partial(e) = (v_1, v_2)\\), then   \\[ \\partial \\circ f_E(e) = (f_V(v_1), f_V(v_2)). \\] <p></p> <p></p> <p>In some sense, this behaves almost like a functor. This observation  will become important later.  Now since we have a consistent way to speak of graphs and their morphisms, we can form the category Grph where the objects are small graphs and the morphisms are graph morphisms as described above. </p> <p>Finally we introduce the concept of a quiver, which we will  see is basically the skeleton of a category. </p> <p> A quiver is a directed graph \\(G\\) which allows multiple edges  between vertices. Instead of a function \\(\\partial\\), a quiver is equipped with  source and target functions  \\[ s: E(G) \\to V(G) \\qquad t: E(G) \\to V(G). \\] <p>So a quiver is a 4-tuple \\((E(G), V(G), s, t)\\). Now as before, a morphism \\(f: Q \\to Q'\\) between quivers \\((E(Q), V(Q), s, t)\\) and  \\((E(Q'), V(Q'), s', t')\\) is one which preserves edge-vertex relations. Thus, it is a pair of functions \\(f_E: E(Q) \\to E(Q')\\)  and \\(f_V: V(Q) \\to V(Q)'\\) such that </p> \\[ f_V \\circ s = s' \\circ f_E \\qquad f_V \\circ t = t' \\circ f_E. \\] <p></p> <p>\\  \\noindent Now that we have all of those definitions out of the way, what's really going  on here? A quiver can be abstracted as a pair of objects and morphisms. \\  If we let \\(C\\op\\) be the category with two objects, two nontrivial morphisms  and two identity morphisms as below \\  then we see that a quiver is a functor \\(F: \\cc\\op \\to **Set**\\).  With that said,  we can define the category of quivers Quiv, which, based on what we just showed,  is a functor category with objects \\(F: \\cc\\op \\to **Set**\\). This  allows us to interpret quiver homomorphisms as natural transformations.</p> <p>\\textcolor{MidnightBlue}{Now why on earth do we care about these things called quivers?} The reason is because the underlying structure of small categories  take the form of a quiver. For example, the category on the left below  can be turned into a quiver, as on the right, after \"forgetting\" composition and identity  morphisms.  \\  In general, since categories allow multiple arrows between objects,  we can construct a forgetful functor which forgets composition  and identity arrows.</p> \\[ U: **Cat** \\to **Quiv**. \\] <p>Note that if \\(F : \\cc \\to \\cc'\\) is a functor then \\(U(F) : U(\\cc) \\to U(\\cc')\\) is in fact a well-behaved morphism between two quivers. \\textcolor{NavyBlue}{Recall that the construction of a graph homomorphism is basically a functor as we've known to so far}. </p> <p>Not only can we forget categories to generate quivers, we can generate  categories using the skeletal structure of a quiver. This leads to the concept  of a free category; the concept is no different than the concept  of, say, a free group generated by a set \\(X\\). </p> <p> Let \\(Q\\) be a quiver with vertex set \\(V\\) and edge set \\(E\\).  We define the free category generated by \\(Q\\) as  the category with  \\begin{description} \\item[Objects.] The set \\(V\\)  \\item[Morphisms.] The paths of the quiver.  \\end{description} Precisely, a path is any sequence of edges and vertices  \\  with composition of paths defined in the intuitive way:  \\  \\  When we generate the free category, we also remember to add identity arrows to  each vertex.  </p> <p>Since for each quiver \\(Q\\), we can define a free category \\(F_C(Q)\\) on \\(Q\\),  we can realize that this mapping is functorial. That is,  we can define a functor </p> \\[  F_C: **Quiv** \\to **Cat** \\] <p>where it maps on objects and morphisms as \\begin{statement}{Red!10}</p> \\[\\begin{align} Q &amp;\\longmapsto F_C(Q)\\\\ (f: Q \\to Q') &amp;\\longmapsto (F_C(f): F_C(Q') \\to F_C(Q)). \\end{align}\\] <p>\\end{statement} That is, quiver homomorphisms can map to functors \\(F_C(f)\\) between the free categories generated by the respective quivers. </p> <p>Now, what is the relationship between a quiver \\(Q\\) and the quiver \\(U(F_C(Q))\\)?  There must exist an injection \\(i: Q \\to U(F(Q))\\) which sends \\(Q\\) to the skeleton  of \\(U(F_C(Q))\\). It turns out that this morphism is universal from \\(Q\\) to \\(U\\). </p> <p> Let \\(Q\\) be a quiver. Then there is a graph homomorphism \\(i: Q \\to U(F_C(Q))\\)  such that, for any other graph homomorphism \\(\\phi: Q \\to U(\\cc)\\) with \\(\\cc\\) a category, there exists a unique functor \\(F: F_C(Q) \\to \\cc\\) where \\(U(F) \\circ i = \\phi\\). That is, \\   This is an example of a universal arrow; the dotted lines are the morphisms which are forced to exist by the conditions of the diagram, which is the idea of a universal element.</p> <p> Denote each morphism or path in \\(F_C(Q)\\) of length \\(n\\) \\  as \\((v_0, e_0e_1\\cdots e_{n-1}, v_n): v_0 \\to v_n\\). Now define the inclusion  \\(i: Q \\to U(F_C(Q))\\) where each vertex and edge is sent identically. That is, vertices  \\(v\\) map to \\(v\\) in \\(F_C(Q)\\),  and morphisms are sent identically and for each  edge \\(e: v \\to v'\\): \\[ i(e: v \\to v') = (v, e, v'). \\] <p>An important observation to make is the fact that every morphism \\((v_0, e_0e_1\\cdots e_{n-1}, v_n): v_0 \\to v_n)\\) in \\(F_C(Q)\\) is a composition of length 2-morphism:  \\  \\  Therefore, for any graph homomorphism \\(\\phi: Q \\to U(\\cc)\\),  we can create a unique functor \\(F: F_C(Q) \\to \\cc\\) where </p> \\[\\begin{align*} v &amp;\\longmapsto \\phi(v)\\\\ (v_0, e_0e_1\\cdots e_{n-1}, v_n): v_0 \\to v_n &amp;\\longmapsto  \\phi(e_0:  v_0 \\to v_1) \\circ \\phi(e_1: v_1 \\to v_2) \\circ \\cdots \\circ \\phi(e_{n-1}: v_{n-1} \\to v_n) \\end{align*}\\] <p>which then gives us </p> \\[\\begin{align*} U(F) \\circ i = \\phi \\end{align*}\\] <p>as desired.  </p>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Monoids%2C%20Groups%20and%20Groupoids%20in%20Categories/","title":"2.8. Monoids, Groups and Groupoids in Categories","text":"<p>One of the most simplest, useful and yet underrated concepts in mathematics  is the concept of a monoid. The reason why monoids are so useful is because  they capture three main concepts: stacking \"things\" together to create another  \"thing,\" in such a way that our stacking operation is associative,  with the additional assumption of an identity element which doesn't change the  value. Often times in cooking up a mathematical construction, we want to maintain these  three concepts because they are so familiar to our basic human nature. </p> <p>Now recall the definition of a monoid.</p> <p> A monoid \\(M\\) is a set equipped with a binary operation \\(\\cdot: M \\times M \\to M\\)  and an identity element \\(e\\) such that  <ul> <li> <p>[1.] For any \\(x, y, z \\in M\\), we have that \\(x \\cdot (y \\cdot z) = (x \\cdot y) \\cdot z\\) </p> </li> <li> <p>[2.] For any \\(x \\in M\\), \\(x \\cdot e = x = e \\cdot x\\).  </p> </li> </ul> <p>  It turns out that we can abstract the above definition very easily if we just resist the  temptation to explicitly refer to our elements. In order to do this, we need  to find a way to diagrammatically express the above axioms. </p> <p>Towards that goal, rename the binary operation as \\(\\mu: M \\times M \\to M\\) (for notational convenience).  Then to express axiom  (1), we mean that we have 3 elements \\(x, y, z \\in M\\) and there are two ways to compute them, but we want them to be the same. So lets make each different way to compute them one side of a square, which  we'll say it commutes. </p> <p> The result is the diagram on the above left. Since we want this to hold for  all elements in \\(M\\), we construct the diagram more generally on the above right;  this expresses our associativity axiom. Now to express the second axiom diagrammatically, we need a way to discuss the  identity map. So define the map \\(\\eta: \\{\\bullet\\} \\to M\\) where \\(\\eta(\\bullet) = e\\). This  is just a stupid map that picks out the identity. Then axiom (2) can be translated  diagramatically to state that the bottom left diagram commutes.  \\  ince we want this to hold for all \\(m \\in M\\), we generalize this to create a commutative diagram as on the above right. We now have what we need to define a monoid more generally. </p> <p>\\begin{definition} Let \\(\\cc\\) be a category with cartesian products. Denote the terminal  object as \\(T\\). An object \\(M\\) is said to  be a monoid in \\(\\cc\\) if there exist maps </p> \\[\\begin{align*} &amp;\\mu: M \\times M \\to M \\qquad &amp;&amp;**(Multiplication)**\\\\ &amp;\\eta: T \\to M \\qquad &amp;&amp;**(Identity)** \\end{align*}\\] <p>such that the diagrams below commute.  \\  Dually, a comonoid is an object \\(C\\) with maps </p> \\[\\begin{align*} &amp;\\Delta: C \\to C \\times C \\qquad &amp;&amp;**(Comultiplication)**\\\\ &amp;\\epsilon: C \\to T \\qquad &amp;&amp;**(Identity)** \\end{align*}\\] <p>such that the dual diagrams commute.  \\  end{definition} \\textcolor{NavyBlue}{Note that we're being a little sloppy here. For example, the object  \\(M \\times M \\times M\\) doesn't actually exist; we have either \\(M \\times (M \\times M)\\) or \\((M \\times M) \\times M\\).  } However, for any category with cartesian products, we always have that these two objects  are isomorphic. Hence we mean either of the equivalent products when we discuss \\(M \\times M \\times M\\). </p> <p> Let \\(k\\) be a field. Consider the category \\(**Vect**_k\\). Then a monoid  in this category is an object \\(A\\) equipped with maps  </p> <p> Group object in the category of Top is a topological group. </p> <p> Monoid in the category of \\(R\\) modules is an associative algebra. </p> <p>\\chapterimage{chapter3_pic/chapt3head.pdf}</p>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Products%20of%20Categories%2C%20Functors/","title":"2.2. Products of Categories, Functors","text":"<p>As one may expect, the product of categories can be easily defined.</p> <p> Let \\(\\cc\\) and \\(\\dd\\) be categories. Then the \\textbf{product category} \\(\\cc \\times \\dd\\) is the category where  \\begin{description} \\item[Objects.] All pairs  \\((C, D)\\) with \\(C \\in \\ob(\\cc)\\) and \\(D \\in \\ob(\\dd)\\) \\item[Morphisms.] All pairs \\((f, g)\\) where \\(f \\in \\hom(\\cc)\\) and \\(g \\in \\hom(\\dd)\\).  \\end{description} To define composition in this category, suppose we have composable morphisms in \\(\\cc\\)  and \\(\\dd\\) as below.  <p> Then the morphisms \\((f, g)\\) and \\((f', g')\\) in \\(\\cc \\times \\dd\\)  are composable too, and their composition is defined as \\((f', g') \\circ (f , g) = ( f' \\circ f, g' \\circ g)\\). \\ </p> <p>We also define the projection functors \\(\\pi_\\cc: \\cc\\times\\dd \\to \\cc\\) and \\(\\pi_\\dd: \\cc\\times\\dd \\to \\dd\\) where on objects \\((C, D)\\) and morphism \\((f, g)\\), we have that </p> \\[\\begin{align*} &amp;\\pi_\\cc(C, D) = C \\quad &amp;&amp;\\pi_\\dd(C, D) = D\\\\ &amp;\\pi_\\cc(f, g) = f \\quad &amp;&amp;\\pi_\\dd(f, g ) = g \\end{align*}\\] <p></p> <p>These projection functors have the following property. Consider a pair of functors \\(F: \\bb \\to \\cc\\) and \\(G:\\bb \\to \\dd\\). Then \\(F\\) and \\(G\\) determine a unique functor \\(H: \\bb \\to \\cc \\times \\dd\\) where </p> \\[ \\pi_\\cc \\circ H = F \\qquad \\pi_\\dd \\circ H = G. \\] <p>That is, we see that for any morphism \\(f\\) in \\(\\bb\\) we have that \\(H(f) = ( F(f), G(f) )\\). Hence the following diagram commutes \\  and we dash the middle arrow to represent that \\(H\\) is induced, or defined, by this process.</p> <p>We can also take the product of two different functors. </p> <p> Let \\(F: \\cc \\to \\cc'\\) and \\(G: \\dd \\to \\dd'\\) be two functors. Then we define the product functor to be the functor  \\(F \\times G: \\cc \\times \\dd \\to \\cc' \\times \\dd'\\) for which  <ul> <li> <p>[1.] If \\((C, D)\\) is an object of \\(\\cc\\times\\dd\\) then \\((F\\times G)(C, D) = (F(C), G(D))\\) </p> </li> <li> <p>[2.] If \\((f, g)\\) is a morphism of \\(\\cc\\times\\dd\\) then \\((F \\times G)(f,g) = (F(f), G(g))\\) </p> </li> </ul> <p>Additionally, we can compose the product of functors (of course, so long as they have the same number of factors). Thus suppose \\(G,F\\) and \\(G', F'\\) are composable functors. Then observe that </p> \\[ (G \\times G') \\circ (F \\times F') = (G \\circ F) \\times (G' \\circ F'). \\] <p></p> <p>Note that in this formulation we have that </p> \\[ \\pi_{\\cc'}\\circ (F\\times G)  = F \\circ \\pi_\\cc \\quad \\pi_{\\cc'} \\circ (F \\times G) = G \\circ \\pi_{\\dd} \\] <p>Hence, we have the following commutative diagram.</p> <p>\\  Again, the dashed arrow is written to express that \\(F \\times G\\) is the functor defined by this process and makes this diagram commutative.</p> <p> If \\(F\\) is a functor such that \\(F: \\bb \\times \\cc \\to \\dd\\), that is, its domain is a product category, then \\(F\\) is said to be a bifunctor. </p> <p>An example of a bifunctor is the cartesian product \\(\\times\\), which we can apply to sets, groups, and topological spaces. In these instances we know that value of a cartesian product is always determined uniquely by the values of the individual factors, which holds more generally for bifunctors. </p> <p> Let \\(\\bb, \\cc\\) and \\(\\dd\\) be categories. For \\(B \\in \\bb\\) and \\(C \\in \\cc\\), define the functors \\[ H_C: \\bb \\to \\dd \\quad K_B: \\cc \\to \\dd    \\] <p>such that \\(H_C(B) = K_B(C)\\) for all \\(B, C\\). Then there exists a functor \\(F:\\bb \\times \\cc \\to \\dd\\) where \\(F(B, -) = K_B\\) and \\(F(-, C) = H_C\\) for all \\(B, C\\) if and only if  for every pair of morphisms \\(f:B \\to B'\\) and \\(g:C\\to C'\\) we have that </p> \\[ K_{B'}(g) \\circ H_C(f) = H_{C'}(f) \\circ K_B(g). \\] <p>Diagrammatically, this condition is \\  </p> <p>The proof is left as an exercise for the reader.</p> <p> We now introduce what is probably one of the most important examples of  a bifunctor. Note that for any (locally small) category \\(\\cc\\),  we have for each object \\(A\\) a functor. \\[ \\hom(A, -): \\cc \\to **Set** \\] <p>We also have a functor from \\(\\cc\\op\\) (we at the \\(\\op\\) simply for convenience) for each \\(B \\in \\cc\\op\\).</p> \\[ \\hom(-, B): \\cc\\op \\to **Set**     \\] <p>As an application of the proposition, one can  see that that these two functors act as the \\(K_B\\) and \\(H_C\\) functors in the above proposition, and give rise to bifunctor </p> \\[  \\hom: \\cc\\op \\times \\cc \\to **Set**. \\] <p>This is because for any \\(h: A \\to A'\\) and \\(k: B \\to B'\\), the diagram, \\  commutes. Hence the proposition guarantees that \\(\\hom:\\cc\\op\\times\\cc \\to **Set**\\) exists and is unique. </p> <p> Recall that for an integer \\(n\\) and for a ring \\(R\\) with identity \\(1 \\ne 0\\), we can formulate the group \\(\\text{GL}(n, R)\\), consisting of \\(n\\times n\\) matrices  with entry values in \\(R\\). As this takes in arguments, we might guess that we have  a bifunctor  \\[ GL(-, -): \\bm{\\mathbb{N}} \\times **Ring** \\to **Grp** \\] <p>where \\(\\bm{\\mathbb{N}}\\) is a the discrete category with elements as natural numbers. This intuition  is correct: for a fixed ring \\(R\\), we have a functor</p> \\[ GL(-, R): \\bm{\\mathbb{N}} \\to **Grp** \\] <p>while for a fixed natural number \\(n\\) we have a functor </p> \\[ GL(n, -): **Ring** \\to **Grp**.      \\] <p>Below we can visualize the activity of this functor: \\  Above, we start with \\(\\zz\\) since the this is the initial object of the category  Ring.  </p> <p>Now that we understand products of categories a functors, and we have a necessary and sufficient condition for the existence of a bifunctor, we describe necessary and sufficient conditions for the existence of a natural transformation.</p> <p> Suppose \\(F, G: \\bb \\times \\cc \\to \\dd\\) are bifunctors. Suppose that there exists a morphism \\(\\eta\\) which assigns objects of \\(\\bb \\times \\cc\\) to morphisms of \\(\\dd\\). Specifically, \\(\\eta\\) assigns objects \\(B \\in \\bb\\) and \\(C \\in \\cc\\) to the morphism  \\[ \\eta_{(B, C)} : F(B, C) \\to G(B, C). \\] <p>Then \\(\\eta\\) is said to be natural in \\(B\\) if, for all \\(C \\in \\cc\\), </p> \\[ \\eta_{(-, C)} : F(-, C) \\to G(-, C) \\] <p>is a natural transformation of functors from \\(\\bb \\to \\dd\\).   </p> <p>With the previous definition, we can now introduce the necessary condition for a natural transformation to exist between bifunctors.</p> <p> Let \\(F, G: \\bb \\times \\cc \\to \\dd\\) be bifunctors. Then there exists a natural transformation \\(\\eta: F \\to G\\) if and only if \\(\\eta(B, C)\\) is natural in \\(B\\) for each \\(C \\in C\\), and natural in \\(C\\) for each \\(B \\in \\bb\\). </p> <p> \\begin{description} \\item[(\\(\\bm{\\implies}\\))] Suppose that \\(\\eta: F \\to G\\) is a natural transformation. Then every object \\((B, C)\\) is associated with a morphism \\(\\eta_{(B, C)}: F(B, C) \\to G(B, C)\\) in \\(\\dd\\), and this gives rise to the following diagram: \\  Now let \\(C \\in \\cc\\) and observe that  \\[ \\eta_{(-, C)}: F(-, C) \\to G(-,c) \\] <p>is a natural transformation for all \\(B\\). On the other hand, for any \\(B \\in \\bb\\), </p> \\[ \\eta_{(B, -)}: F(B, -) \\to G(B, -) \\] <p>is a natural transformation for all \\(C\\). Therefore, \\(\\eta\\) is both natural in \\(B\\) and \\(C\\) for all objects \\((B, C)\\)</p> <p>\\item[(\\(\\bm{\\impliedby}\\))] Suppose on the other hand that \\(\\eta\\) is a function which assigns objects \\((B, C)\\) to a morphism \\(F(B, C) \\to G(B, C)\\) in \\(\\dd\\). Furthermore, suppose that \\(\\eta(B, C)\\) is natural in \\(B\\) for all \\(C \\in \\cc\\) and natural in \\(C\\) for all \\(B \\in \\bb\\). </p> <p>Consider a morphism \\((f, g) : (B, C) \\to (B', C')\\) in \\(\\bb \\times \\cc\\). Then since \\(\\eta\\) is natural for all \\(B \\in \\bb\\), we know that for all \\(C \\in \\cc\\), </p> \\[ \\textcolor{red}{\\eta}_{(-, C)} : F(-, C) \\to G(-,C)   \\] <p>is a natural transformation. In addition, \\(\\eta\\) is natural for all \\(C \\in \\cc\\) since for all \\(B \\in \\bb\\) </p> \\[ \\textcolor{blue}{\\eta}_{(B, -)} : F(B, -) \\to G(B, -) \\] <p>is a natural transformation. Hence consider the natural transformation \\(\\textcolor{red}{\\eta}_{(-, C)}\\) acting on \\((B, C)\\) and \\(\\textcolor{blue}{\\eta}_{(B', -)}\\) acting on \\((B', C)\\). Then we get the following commutative diagrams.</p> <p>\\  Observe that the bottom row of the first diagram matches the top row of the second.  Also note that \\(f: B \\to B'\\) and \\(g: C \\to C'\\), and that the diagrams imply the equations </p> \\[\\begin{align} G(f, 1_C) \\circ \\textcolor{red}{\\eta}_{(B,C)} &amp;= \\textcolor{red}{\\eta}_{(B', C)} \\circ F(f, 1_C) \\\\ G(1_{B'}, g) \\circ \\textcolor{blue}{\\eta}_{(B', C)} &amp;= \\textcolor{blue}{\\eta}_{(B', C')} \\circ F(1_{B'}, g).  \\end{align}\\] <p>Now suppose we compose equation (\\ref{eq1_prop_2_2}) with \\(G(1_{B'}, g)\\) on the left. Then we get that </p> \\[\\begin{align*} G(1_{B'}, g)\\circ G(f, 1_C) \\circ \\textcolor{red}{\\eta}_{(B, C)} &amp;= \\overbrace{G(1_{B'}, g) \\circ \\textcolor{red}{\\eta}_{(B', C)}}^{\\text{replace via equation (2)}} \\circ F(f, 1_C)\\\\ &amp;= \\textcolor{blue}{\\eta}_{(B', C')} \\circ F(1_{B'}, g) \\circ F(f, 1_C)\\\\ &amp;= \\textcolor{blue}{\\eta}_{(B',C')} \\circ F(1_{B'}\\circ f, g \\circ 1_C)\\\\ &amp;=\\textcolor{blue}{\\eta}_{(B',C')} \\circ F(f, g). \\end{align*}\\] <p>where in the second step we applied equation  (\\ref{eq2_prop_2_2}), and in the third step we composed the morphisms. Also note that we can simplify the left-hand side since </p> <p>[  G(1_{B'}, g)\\circ G(f, 1_C) = G(1_{B'}\\circ f, g \\circ 1_C) = G(f, g). ] Therefore, we have that </p> \\[ G(f, g) \\circ \\textcolor{red}{\\eta}_{(B, C)} = \\textcolor{blue}{\\eta}_{(B', C')} \\circ F(f, g)  \\] <p>which implies that \\(eta\\) itself is a natural transformation. Specifically, it implies the following diagram.  \\  \\end{description}  Note: A way to succinctly prove the reverse implication of the previous proof is as follows. Since we know the diagrams on the left are commutative, just \"\\textcolor{Green}{stack}\" them on top of each other to achieve the diagram in the upper right corner, and then \"\\textcolor{Orange}{squish}\" this diagram down to obtain the third diagram in the bottom right. </p> <p>\\begin{minipage}{0.3\\textwidth} \\  \\end{minipage} \\hspace{1cm} \\begin{minipage}{0.1\\textwidth} \\begin{tikzpicture} \\draw[white] (0,0) -- (1,0); \\draw[thick, Green, -&gt;] (0,-2) -- (2,0); \\draw[thick, Orange, -&gt;] (2.6,-2) to [bend right = 80] (2.6,-4); \\end{tikzpicture}</p> <p>\\end{minipage} \\hfill \\begin{minipage}{0.5\\textwidth} \\  \\end{minipage} \\vspace{1cm}</p> <p>This is essentially what we did in the proof, although this is more crude visualization of what happened, and we were more formal throughout the process. </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li>[1.] Let \\(\\cc\\) and \\(\\dd\\) be categories. Prove that  \\((\\cc \\times \\dd)\\op \\cong \\cc\\op\\times\\dd\\op\\). </li> </ul>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Quotient%20Categories/","title":"2.7. Quotient Categories","text":"<p>The quotient category is a concept that generalizes the ideas of forming quotient groups, rings, modules, and even topological spaces. The core idea of obtaining a quotient \"object\" revolves around the concept of an equivalence class. </p> <p>For example, in constructing the quotient group, one can go about constructing it in two different ways. One is easy, in which you simply form the concept of a coset, and then observe that nice things happen when you make cosets with normal subgroups. The hard way is to construct an equivalence relation, which \\textit{gives rise} to what we recognize as the concept of a coset, and then continuing further to create the quotient groups from normal subgroups. Both ways are equivalent, but one ignores the crucial and powerful idea of equivalence relations. </p> <p> Let \\(\\cc\\) be a locally small category. Suppose \\(R\\) is a function which, for every pair of objects \\(A, B\\), assigns equivalence  relations \\(\\sim_{A, B}\\) on the hom set \\(\\hom_{\\cc}(A, B)\\). Then we may define the  quotient category \\(\\cc/R\\) where  \\begin{description} \\item[Objects.] The same objects of \\(\\cc\\). \\item[Morphisms.] For any objects \\(A,B\\) of \\(\\cc\\), we  set \\(\\hom_{\\cc/R}(A, B) = \\hom_{\\cc}(A, B)/\\sim_{A, B}\\). \\end{description}   Thus we see that morphisms between \\(f: A \\to B\\) in \\(\\cc\\) becomes equivalence classes  \\([f]\\) in \\(\\cc/R\\). </p> <p>With that said, we can naturally define a canonical functor \\(Q: \\cc \\to \\cc/R\\)  where \\(Q\\) acts identically on objects and where  \\(Q(f: A \\to B) = [f] \\in \\hom_{\\cc/R}(A,B)\\). This in fact defines a functor  if we observe that, for a pair of composable morphisms \\(g, f\\).</p> \\[ Q(g) \\circ Q(f) = [g \\circ f] = Q(g \\circ f).  \\] <p>A nice property of this functor is the fact that if \\(f \\sim f'\\), then \\(Q(f) = Q(f')\\). What is even nicer about this functor is that it has the following property. </p> <p> Let \\(\\cc\\) be a locally small category with an equivalence relation \\(\\sim_{A, B}\\) on each set  \\(\\hom_{\\cc}(A, B)\\). Then for any functor \\(F: \\cc \\to \\dd\\) into some category \\(\\dd\\)  such that \\(f \\sim f'\\), \\(F(f) = F(f')\\), there exists a unique functor  \\(H: \\cc/R \\to \\dd\\) such that \\(H \\circ Q = F\\); or, diagrammatically, such that the following diagram commutes. <p> </p> <p> Observe that one functor \\(H: \\cc/R \\to \\dd\\) that we can supply,  which will have the above diagram commute, is one where  \\(H(C) = F(C)\\) on objects and where for any \\([f] \\in \\hom_{\\cc/R}(A, B)\\), \\[ H([f]) = F(f)    \\] <p>where \\(f\\) is an representative of the equivalence class \\(f\\). Note that  this is well defined since \\(F(f) = F(f')\\) if \\(f \\sim_{A,B} f'\\); hence this  will appropriately send equivalent elements to the same morphism. It  is not hard to show that it's unique; one can just suppose such an \\(H\\) exists and then  demonstrate that it behaves like the functor we proposed initially.  </p> <p> <p></p>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Slice%20and%20Comma%20Categories./","title":"2.5. Slice and Comma Categories.","text":"<p>In this section we introduce comma categories, which serve as  a very useful categorical construction. The reason why it is so useful is  because the notion of a comma category has  the potential to simplify an otherwise complicated discussion.  As they can be constructed in any category, and  because they contain a large amount of useful data, they are frequently used as an intermediate step in  more complex categorical constructions. Thus, while the concept  is \"simple,\" they nevertheless appear in all kinds of complicated discussions in category theory.</p> <p> Let \\(\\cc\\) be a category and suppose \\(\\textcolor{Purple}{A}\\) is an object of \\(\\cc\\). We define the **slice category (with \\(\\textcolor{Purple**{A}\\) over \\(\\cc\\))},  denoted \\((\\textcolor{Purple}{A} \\downarrow\\cc)\\), as the category  <p>\\begin{description} \\item[Objects.] All pairs \\((C, f: \\textcolor{Purple}{A} \\to C)\\)  for all \\(C \\in  \\cc\\)  and morphims \\(f:\\textcolor{Purple}{A} \\to C\\). In other words, the objects are all morphisms in \\(\\cc\\) which originate at \\(\\textcolor{Purple}{A}\\). </p> <p>\\item[Morphisms.] For two objects \\((C, f:\\textcolor{Purple}{A}\\to C)\\) and \\((C', f': \\textcolor{Purple}{A} \\to C')\\),  we define </p> \\[ \\textcolor{NavyBlue}{h} : (C, f) \\to (C', f') \\] <p>as a morphism between the objects, where \\(h: C \\to C'\\) is a morphism in  our category such that \\(f' = h \\circ f\\). Alternatively we can describe  the homset more directly:</p> \\[ \\hom_{(A\\downarrow \\cc)}\\Big( (f, C), (f', C')\\Big) = \\{ h: C \\to C' \\in \\cc \\mid f' = h \\circ f \\}. \\] <p>\\end{description}  At this point you may be a bit overloaded with notation if this is the first  time you've seen this before. You need to figure out how this is a category (what's the identity? composition?) and ultimately why you should care about this category. To aid your understanding, a picture might help.</p> <p>We can represent the objects and morphisms of the category \\((\\textcolor{Purple}{A} \\downarrow \\cc)\\) in a visual manner. </p> <p> Now, how does composition work? Composition of two composable morphisms \\(h : (f, C) \\to (f', C')\\) and \\(h' : (f', C') \\to (f'', C'')\\) is given by  \\(h' \\circ h : (f, C) \\to (f'', C'')\\) since clearly </p> \\[ f'' = h' \\circ f' \\hspace{0.5cm}\\text{and}\\hspace{0.5cm} f' = h \\circ f \\implies f'' = h' \\circ (h \\circ f) = (h' \\circ h) \\circ f. \\] <p>We can visually justify composition as well. If we have two commutative diagrams  as on the left, we can just squish them together to get the final commutative diagram on the right.  \\  Hence, we see that \\(h' \\circ h :(f, C) \\to (f'', C'')\\) is defined whenever \\(h'\\) and \\(h\\) are composable as morphisms of \\(\\cc\\).</p> <p>One use of comma categories is to capture and generalize  the notion of a pointed category. Such pointed categories include the category of pointed sets \\(**Set**^*\\) or the category of pointed  topological spaces \\(**Top**^*\\), etc. </p> <p>We've seen, in particular on the discussion of functors, the necessity for  pointed categories. For example, we cannot discuss \"the\" fundamental  group \\(\\pi_1(X)\\) of a topological space \\(X\\) (unless \\(X\\) is path connected, but still only  up to isomorphism). To discuss a fundamental group in a topological space \\(X\\), one needs to select a base point \\(x_0\\).  As we saw in Example \\ref{example:fundamental_group},  \\(\\pi_1\\) is not a functor \\(**Top** \\to **Grp**\\), but is rather a  functor </p> \\[ \\pi_1: **Top**^* \\to **Grp** \\] <p>where \\(**Top**^*\\), which consists of pairs \\((X, x_0)\\) with \\(x_0 \\in X\\), is the category of pointed topological spaces.</p> <p>Similarly, it makes no sense to talk about \"the\" tangent plane of  a smooth manifold. Such an association requires the selection of a point \\(p \\in X\\) to calculate \\(T_p(M)\\).  So, as we saw in Example \\ref{example:manifold_tangent_plane}, this process is not  a functor from \\(**DMan**\\) to \\(**Vect**\\), but is rather a functor </p> \\[ T: **DMan**^* \\to **Vect** \\] <p>where \\(**DMan**^*\\), which consists of pairs \\((M, p)\\) with \\(p \\in M\\), is the category of pointed smooth manifolds. This now motivates the next two examples.</p> <p> Consider the category \\(**Top**^*\\) where  \\begin{description} \\item[Objects.] The objects are pairs \\((X, x_0)\\) with \\(X\\) a topological space  and \\(x_0 \\in X\\). \\item[Morphisms.] A morphism \\(f: (X, x_0) \\to (Y, y_0)\\) is any continuous function  \\(f: X \\to Y\\) such that \\(y_0 = f(x_0)\\). \\end{description} Recall that the one point set \\(\\{\\bullet\\}\\) is trivially a topological space.  Then we can form the category \\((\\{\\bullet\\}\\downarrow **Top**)\\). The claim now is that  \\[ (\\{\\bullet\\}\\downarrow **Top**)  \\cong  **Top**^*. \\] <p>Why? Well, an object of \\((\\{\\bullet\\}\\downarrow **Top**)\\) is simply  a pair \\((X, f: \\{\\bullet\\} \\to X)\\). Observe that </p> \\[ f(\\bullet) = x_0 \\in X, \\] <p>for some \\(x_0 \\in  X\\). So, the pair \\((X, f: \\{\\bullet\\} \\to X)\\) is logically equivalent to a  pair \\((X, x_0)\\) with \\(x_0 \\in X\\). \\textcolor{NavyBlue}{That is, a continuous function from the one point set into a topological space \\(X\\) is equivalent to simply selecting  a single point \\(x_0 \\in X\\)}.  Hence, on objects it is clear why we have an isomorphism.</p> <p>Now, a morphism in this comma category will be of the form  \\(p: (X, f_1: \\{\\bullet\\} \\to X) \\to (Y, f_1: \\{\\bullet\\} \\to Y)\\).  Specifically, it is a continuous function \\(p: X \\to Y\\) such that the diagram below commutes.  \\  In other words, if \\(f_1(\\bullet) = x_0\\) and \\(f_2(\\bullet) = y_0\\),  it is a continuous function \\(p: X \\to Y\\) such that \\(f(x_0) = y_0\\). This is exactly a morphism in \\(**Top**^*\\)! We clearly have a bijection  as claimed. </p> <p>The above example generalizes to many pointed categories, some of which are </p> <ul> <li> <p>\\(**DMan**^* \\cong (\\bullet \\downarrow **DMan**)\\)</p> </li> <li> <p>\\(**Set**^* \\cong (\\bullet \\downarrow **Set**)\\)</p> </li> <li> <p>\\(**Grp**^* \\cong (\\bullet \\downarrow **Grp**)\\)</p> </li> </ul> <p>We now briefly comment for any slice category \\((A \\downarrow \\cc)\\)  built from a category \\(\\cc\\), we can construct a \"projection\" functor </p> \\[ P: (A \\downarrow \\cc) \\to \\cc   \\] <p>where on objects \\(P(C, f: A \\to C) = C\\) and on morphisms \\(P(h:(C, f) \\to (C', f'))= h: C  \\to C'\\). Clearly, this functor is  faithful, but it is generally not full. Such a projection functor is used in technical constructions involving slice categories as  it has nice properties; we will make use of it later when we discuss  limits. </p> <p>Next,we introduce how we can also describe the category of an objects under another category. </p> <p> Let \\(\\cc\\) be a category, and \\(\\textcolor{Purple}{B}\\) an object of \\(\\cc\\). Then we define the category \\(\\textcolor{Purple**{B}\\) under \\(\\cc\\)}, denoted as \\((\\cc \\downarrow \\textcolor{Purple}{B})\\) as follows. \\begin{description} \\item[Objects.] All  pairs \\((C, f)\\) where \\(f: C \\to \\textcolor{Purple}{B}\\) is a morphism in \\(\\cc\\). That is, the objects are morphisms ending  at \\(\\textcolor{Purple}{B}\\).  <p>\\item[Morphisms.] For two objects \\((C, f: C  \\to \\textcolor{Purple}{B})\\) and \\((C', f': C'  \\to \\textcolor{Purple}{B})\\), we define</p> \\[  \\textcolor{NavyBlue}{h}: (C, f) \\to (C', f') \\] <p>to be a morphism between the objects to correspond  to  a morphism \\(\\textcolor{NavyBlue}{h}:C \\to C'\\) in \\(\\cc\\)  such that \\(f = f' \\circ h\\). \\end{description}  Composition of functions \\(\\textcolor{NavyBlue}{h}: (f, C) \\to (f', C')\\) and \\(\\textcolor{NavyBlue}{h'}: (f', C') \\to (f'', C'')\\) exists whenever  \\(\\textcolor{NavyBlue}{h'} \\circ \\textcolor{NavyBlue}{h}\\) is defined as morphisms in \\(\\cc\\). Again, we can represent the elements of the category in a visual manner  \\  The following is a nice example that isn't traditionally seen as an example of a functor. </p> <p> Let \\((G, \\cdot)\\) and \\((H, \\cdot)\\) be two groups, and consider a group  homomorphism \\(\\phi: (G, \\cdot) \\to (H, \\cdot)\\). Abstractly, this  is an element of the comma category \\((**Grp** \\downarrow H)\\). <p>Now for for every group  homomorphism, we may calculate  the kernal of \\(\\ker(\\phi) = \\{g \\in G \\mid \\phi(g) = 0\\}\\). This  is always a subgroup of \\(G\\). What is interesting is that,  from the perspective of slice categories, this  process is functorial:</p> \\[ \\ker(-): (**Grp**\\downarrow H) \\to **Grp**.   \\] <p>To see this, we have to understand what happens on the morphisms. So, suppose we have two objects \\((G, \\phi: G \\to H)\\) and \\((K, \\psi: K \\to H)\\)  of \\((**Grp**\\downarrow H)\\) and a morphism \\(h: G \\to K\\) between the objects.  \\  Then we can define \\(\\ker(h): \\ker(\\phi) \\to \\ker(\\psi)\\), the image  of \\(h\\) under the functor, to  be the restriction \\(h|_{\\text{ker}(\\phi)}: \\ker(\\phi) \\to \\ker(\\psi)\\).  This is a bonafied group homomorphism: by the commutativity of the above  triangle, if \\(g \\in G\\) then \\(\\phi(g) = \\psi(h(g))\\). Hence, if \\(\\phi(g) = 0\\),  i.e., \\(g \\in \\ker(\\phi)\\), then \\(\\psi(h(g)) = 0\\), i.e., \\(h(g) \\in \\ker(\\psi)\\).  So we see that our proposed function makes sense. </p> <p>What this means is that the commutativity of the above triangle forces  a natural relationship between the kernels of \\(\\phi\\) and \\(\\psi\\); not only  as a function of sets, but as a group homomorphism. Therefore, the kernel of a group homomorphism is actually a  functor from a slice category.  </p> <p> In geometry and topology, one often meets the need to define a \\((-)\\)-bundle. By \\((-)\\) we mean vector, group, etc. That is, we often want topological spaces to parameterize  a family of vector spaces or groups in a coherent way.  \\begin{center} \\includegraphics[width = 0.9\\textwidth]{mobius_sphere.png} \\end{center} For example, on the above left we can map the M\u00f6bius strip onto \\(S_1\\) in such a way that the  inverse image of each \\(x \\in S_1\\) is homeomorphic to the interval \\([0, 1]\\). Hence,  each point of \\(x \\in S_1\\) carries the information of a topological space, specifically one of \\([0, 1]\\). <p>On the right, we can recall that \\(S^2\\) is a differentiable manifold, and so each point \\(p\\) has a tangent plane \\(T_p(S^2)\\), which is a vector space. Hence every point on \\(S^2\\), or more generally  for any differentiable manifold, carries the information of a vector space. </p> <p>In general, for a topological space \\(X\\),  we define a bundle over \\(X\\) to be a continuous map \\(p: E \\to X\\) with \\(E\\) being some topological  space of interest. If \\(p: E \\to X\\) an \\(p': E' \\to X\\) are two bundles, a  morphism of bundles \\(q :p  \\to p'\\) is given by a continuous map \\(q: E \\to E'\\) such  that </p> \\[ p = p' \\circ q. \\] <p>Hence we see that a bundle over a topological space \\(X\\) is an element of the  comma category \\(**Top**/X\\), and a morphism of bundles is a morphism in the comma category.  We therefore see that \\(**Top**/X\\) can be interpreted as the \\textbf{category of  bundles of \\(X\\)}.</p> <p>One particular case of interest concerns vector bundles. Let \\(E, X\\) be topological spaces. Recall that a vector bundle  consists of a continuous map \\(\\pi: E \\to X\\) such that</p> <ul> <li> <p>[1.] \\(\\pi^{-1}(x)\\) is a finite-dimensional  vector space over some field \\(k\\)</p> </li> <li> <p>[2.] For each \\(p \\in X\\), there is an open neighborhood \\(U_{\\alpha}\\)  and a homeomorphism </p> </li> </ul> \\[ \\phi_{\\alpha}: U_{\\alpha} \\times \\rr^{n} \\isomarrow \\pi^{-1}(U_{\\alpha}) \\] <p>with \\(n\\) some natural number. We also require that \\(\\pi \\circ \\phi_{\\alpha} = 1_{U_\\alpha}\\).</p> <p>As we might expect, a morphism of vector bundles between \\(\\pi_1: E \\to X\\) and \\(\\pi_2: E' \\to X\\) is given by a continuous map \\(q: E \\to E'\\) such that for each \\(x \\in X\\),  \\(q\\big|_{\\pi_1^{-1}(x)}: \\pi_1^{-1}(x) \\to \\pi_2^{-1}(x)\\) is linear map between vector spaces. </p> <p>To realize this in real mathematics, we can take the classic example of the  tangent bundle on a  smooth manifold \\(M\\) (if you've seen this before, hopefully it is now clear why the word \"bundle\"  is here). In differential geometry this is defined as the set </p> \\[ TM = \\{(p, v) \\mid p \\in M \\text{ and } v \\in T_p(M)\\} \\] <p>where we recall that \\(T_p(M)\\) is the tangent (vector) space at  a point \\(p \\in M\\).  Since \\(M\\) is a smooth manifold there is a differentiable  structure \\((U_\\alpha, \\bm{x}_\\alpha: U_\\alpha \\to M)\\) which allow us to define a map </p> \\[\\begin{gather*} \\bm{y}_\\alpha: U_\\alpha \\times \\rr^n \\to TM \\\\ ((x_1, \\dots, x_n), (u_1, \\dots, u_n)) \\mapsto  \\left(\\bm{x_{\\alpha}}(x_1, \\dots, x_n), \\sum_{i=1}^{n}u_i\\frac{\\partial}{\\partial x_i} \\right). \\end{gather*}\\] <p>This actually provides a differentiable structure on \\(TM\\), demonstrating it  too is a smooth manifold (see Do Carmo). Hence we see that \\(TM\\) is in fact a topological space.  We then see that the mapping \\(\\pi: TM \\to M\\) where </p> \\[ \\pi(p, v) = p \\text{ and } \\pi^{-1}(x) = T_x(M). \\] <p>is a continuous mapping. Hence we've satisfied both (1.) and (2.) in the the definition of a vector bundle.  The other properties can be easily verified so that this provides a nice example of a  vector bundle. </p> <p>We can also formulate categories of objects under and over functors. </p> <p> Let \\(\\cc\\) be a category, \\(C\\) an object of \\(\\cc\\) and \\(F:  \\bb \\to \\cc\\) a functor. Then we define the \\textbf{category \\(C\\) over the functor \\(F\\)}, denoted as \\((C \\downarrow F)\\), as follows.  \\begin{description} \\item[Objects.] All pairs \\((f, B)\\) where \\(B \\in \\text{Obj}(\\bb)\\) such that  \\[ f : C \\to F(B) \\] <p>where \\(f\\) is a morphism in \\(\\cc\\). </p> <p>\\item[Morphisms.] The morphisms \\(h: (f, B) \\to (f', B')\\) of \\((C \\downarrow F)\\) are defined whenever there exists a \\(h:B \\to B'\\) in \\(\\bb\\) such that \\(f' = F(h) \\circ f\\).  \\end{description}  Representing this visually, we have that  \\  Composition of the morphisms in \\((C \\downarrow F)\\) simply requires composition of morphisms in \\(\\bb\\). </p> <p>One can easily construct the \\textbf{category \\(C\\) under the functor \\(F\\)}, \\((F \\downarrow C)\\), in a completely analogous manner as before. But we'll move onto finally defining the concept of the comma category. </p> <p> Let \\(\\bb, \\cc, \\dd\\) be categories and let \\(F: \\bb \\to \\dd\\) and \\(G:\\cc \\to \\dd\\) functors. That is,  \\  Then we define the comma category \\((F \\downarrow G)\\) as follows.  \\begin{description} \\item[Objects.] All pairs \\((B, C, f)\\) where \\(B, C\\) are objects of \\(\\bb, \\cc\\), respectively, such that   \\[ f : F(B) \\to G(C) \\] <p>where \\(f\\) is a morphism in \\(\\dd\\). </p> <p>\\item[Morphisms.] All pairs \\((h, k) : (B, C, f) \\to (B', C', f')\\) where \\(h: B \\to B'\\) and \\(k: C \\to C'\\) are morphisms in \\(\\bb, \\cc\\), respectively, such that \\</p> \\[ f' \\circ F(h) = G(k) \\circ f. \\] <p>\\end{description}  As usual, we can represent this visually via diagrams: \\  where in the above picture we have that \\((h, k):(B, C, f) \\to (B', C', f')\\). Since functors naturally respect composition of functions, one can easily define composition of morphism \\((h, k)\\) and \\((h', k')\\) as \\((h \\circ h', k \\circ k')\\) whenever \\(h \\circ h'\\) and $ k\\circ k'$ are defined as morphisms in \\(\\bb\\) and \\(\\cc\\), respectively.  \\vspace{0.5cm}</p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] Let \\(\\cc\\) be a category with initial and terminal objects \\(I\\) and \\(T\\).  \\begin{itemize}</p> </li> <li> <p>[i.] Show that \\((\\cc \\downarrow T) \\cong \\cc\\).</p> </li> <li> <p>[i.] Also show that \\((I \\downarrow \\cc) \\cong \\cc\\). </p> </li> </ul> <p>\\item[2.] Consider again a group homomorphism \\(\\phi: G \\to H\\), but this time  consider the image \\(\\im(\\phi) = \\{\\phi(g) \\mid g \\in G\\}\\).  Show that this defines a functor </p> \\[ \\im(-): (G \\downarrow **Grp**) \\to **Grp** \\] <p>where on morphisms, a morphism </p> \\[ h: (H, \\phi: G \\to H) \\to (K, \\phi: G \\to K)                 \\] <p>is mapped to the restriction  \\(h|_{\\text{Im}(\\phi)}: \\im(\\phi) \\to \\im(\\psi)\\). </p> <p>In some sense, this is the \"opposite\" construction of the  kernel functor we introduced. Instead of taking the kernel of a group homomorphism,  we can take its image.</p> <p>\\item[3.] Here we prove that the processes of imposing the induced topology  and the coinduced topology are functorial. Moreover, the correct language  to describe this is via slice categories. </p> <ul> <li>[i.] Let \\(X\\) be a set and \\((Y, \\tau)\\) a topological space.  Denote \\(U: **Top** \\to **Set**\\)  to be the forgetful functor. Given any function \\(f: X \\to U(Y)\\), we can use the topology on \\(Y\\) to impose a topology \\(\\tau_X\\) on \\(X\\):</li> </ul> \\[ \\tau_X = \\{U \\subset X \\mid f(U) \\text{ is open in }Y\\}. \\] <p>This is called the induced topology on \\(X\\). So, we see that (by abuse of notation) the function \\(f: X \\to U(Y)\\)  is now a continuous function \\(f: (X, \\tau_X) \\to (Y, \\tau_Y)\\). </p> <p>Prove that this process forms a functor \\(\\text{Ind}: (**Top**\\downarrow U(Y)) \\to (**Top**\\downarrow Y)\\). </p> <ul> <li>[ii.]  This time, let \\((X, \\tau)\\) be a topological space, \\(Y\\) a set,  and consider a function \\(f: U(X) \\to Y\\). We can similarly impose a  topology \\(\\tau_Y\\) on \\(Y\\):</li> </ul> \\[ \\tau_Y= \\{ V \\subset Y \\mid f^{-1}(V) \\text{ is open in }X \\}. \\] <p>This is called  the coinduced topology on \\(Y\\).  Show that this is also a functorial process. </p> <p>\\item[4.]</p> <ul> <li> <p>[i.] Let \\(X\\), \\(Y\\) be topological spaces with \\(\\phi: X \\to Y\\) a continuous function. Show that this induces a functor \\(\\phi_*: (**Top**\\downarrow X) \\to (**Top**\\downarrow Y)\\)  where on objects \\((f: E \\to X) \\mapsto (\\phi \\circ f: E \\to Y)\\). </p> </li> <li> <p>[ii.] Let \\(\\cc\\) be a category. Show that we generalize (i) to define a functor </p> </li> </ul> \\[ (\\cc \\downarrow -): \\cc \\to **Cat** \\] <p>where \\(A \\mapsto (\\cc \\downarrow A)\\).</p> <ul> <li> <p>[ii.] Let \\(**Cat**_*\\) be the pointed category of categories  which we describe as  \\begin{description}</p> </li> <li> <p>[Objects.] All pairs \\((\\cc, A)\\) with \\(\\cc\\) a category and \\(A \\in C\\)</p> </li> <li> <p>[Morphisms.] Functors \\(F\\) which preserve the objects. \\end{description} Can we overall describe the construction of a slice category as a functor</p> </li> </ul> \\[ (-  \\downarrow -): **Cat**_* \\to **Cat**     \\] <p>where \\((\\cc,  A) \\mapsto  (\\cc \\downarrow A)\\)? </p> <p>\\item[5.] In this exercise we'll see that slice categories describe intervals for thin categories.</p> <ul> <li> <p>[i.] Regard \\(\\mathbb{R}\\) as  a thin category, specifically as one with a partial order. For a given  \\(a \\in \\rr\\),  describe the thin category \\((a \\downarrow  \\mathbb{R})\\).</p> </li> <li> <p>[ii.] Suppose \\(P\\) is a partial order (so that \\(p \\le p'\\) and \\(p' \\le p\\) implies  \\(p = p'\\)). Describe in general the categories \\((p \\downarrow P)\\) and \\((P \\downarrow p)\\).</p> </li> </ul> <p>\\end{itemize}</p>"},{"location":"category_theory/Duality%20and%20Categorical%20Constructions/Vertical%2C%20Horizontal%20Composition%3B%20Interchange%20Laws/","title":"2.4. Vertical, Horizontal Composition; Interchange Laws","text":"<p>In the previous section, we considered the idea of forming a composition of natural transformations, and we verified that this formed a valid natural transformation.  That is, if we have three functors \\(F, G, H : \\cc \\to \\dd\\) between two categories \\(\\cc\\) and \\(\\dd\\), and if \\(\\sigma: F \\to G\\) and \\(\\tau:G \\to H\\) are natural transformations, then we can form the natural transformation </p> \\[ (\\tau \\circ \\sigma) : F \\to H. \\] <p>We call such a type of composition as vertical compositions of natural transformations,  since the idea can be captured in the following diagram. </p> <p>\\begin{minipage}{0.8\\textwidth} \\vspace{0.5cm}</p> <p> \\end{minipage} \\vspace{0.3cm}</p> <p>We can also perform a different, but similar type of composition between natural transformations. Suppose \\(F, G: \\bb \\to \\cc\\) and \\(F', G': \\cc \\to \\dd\\) are functors between categories \\(\\bb, \\cc\\), and \\(\\dd\\). Furthermore, suppose we have natural transformations \\(\\eta: F \\to G\\) and \\(\\eta': F' \\to G'\\). Then we have diagram such as the following. </p> <p>\\begin{minipage}{0.7\\textwidth} \\vspace{0.5cm}</p> <p>\\  \\hspace{5cm}  \\end{minipage} \\vspace{-0.5cm} </p> <p>Now let \\(B\\) be an object of \\(\\bb\\). There are two ways we can transfer this object to an object of \\(\\cc\\); namely, via mappings of \\(F\\) and \\(G\\). Thus \\(F(B)\\) and \\(G(B)\\) are two objects of \\(\\cc\\). Since \\(\\eta: F \\to G\\) is a natural transformation between these objects, we see that there's a way of mapping between these two elements in \\(\\cc\\):</p> \\[ \\eta(B): F(B) \\to G(B). \\] <p>Hence, we have two objects in \\(\\cc\\) and a morphism in between them. Hence, we know that the natural transformation \\(\\eta': F' \\to G'\\) implies the following diagram commutes.  \\  Note that in the last diagram, all of the objects and morphisms between them exist in \\(\\dd\\). The easiest way to see why this diagram commutes is to go back directly to the definition of a natural transformation; namely, the pair of objects along with their morphism on the left imply the commutativity of the diagram on the right. </p> <p>This can be done in general for categories \\(\\bb, \\cc\\), and \\(\\dd\\) which have functors \\(F, G:\\bb \\to \\cc\\) and \\(F', G': \\cc \\to \\dd\\) associated with natural transformations \\(\\eta: F \\to G\\) and \\(\\eta': F' \\to G'\\). Furthermore, it holds for all \\(B \\in \\bb\\).</p> <p>Note further that this diagram is similar to a diagram which represents a natural transformation; but between which functors? If we look closely, we see that it is between \\(F\\circ F'\\) and \\(G \\circ G'\\). </p> <p>This leads us to make the following formulaic definition: For natural transformations \\(\\eta: F \\to G\\) and \\(\\eta': F' \\to G'\\) such that \\(F, G: \\bb \\to \\cc\\) and \\(F',G' : \\cc \\to \\dd\\), then for \\(B \\in \\bb\\) we define their \"horizontal\" composition as the diagonal of the above diagram; that is,</p> \\[ (\\eta \\circ \\eta')B = G'(\\eta(B)) \\circ \\eta' F(B) = \\eta'(G(B))\\circ F'(\\eta(B)). \\] <p>The above diagram doesn't quite show that $\\eta \\circ \\eta': F' \\circ F \\to G \\circ G' $ is a natural transformation. In order to do this, we need to start from two objects in \\(\\bb\\) and consider a morphism between them. </p> <p> The function \\(\\eta \\circ \\eta':F\\circ F'  \\to G \\circ G'\\) is a natural transformation between the functors \\(F'\\circ F, G' \\circ G: \\bb \\to \\dd\\).  </p> <p> To show this, we consider a morphism \\(f: B \\to B'\\) between two objects \\(B\\) and \\(B'\\) in \\(\\bb\\). We then claim that the following diagram is commutative: \\  First, observe that the left square is commutative due to the fact that \\(\\eta\\) is a natural transformation from \\(F\\) to \\(G\\). Therefore, it produces a commutative square diagram, and we obtain the above left square diagram by applying \\(F'\\) to the commutative diagram produced by \\(\\eta: F \\to G\\). <p>The right square in the diagram is obtained by the fact that \\(\\eta'\\) is a natural transformation between functors \\(F'\\) and \\(G'\\). Hence the diagram is commutative, and it acts on the objects \\(G(B)\\) and in \\(\\cc\\). Therefore, we see that \\(\\eta \\circ \\eta'\\) is a natural transformation.  </p> <p>\\textcolor{NavyBlue}{Thus we see that we have \"horizontal\" and \"vertical\" notions of composing natural transformations. Let us denote \"horizontal\" transformations as \\(\\circ\\) and \"vertical\" transformations as \\(\\cdot\\) between natural transformations.} </p> <p>It is also notationally convenient to denote functor and natural transformation compositions as </p> \\[ F' \\circ \\tau : F' \\circ F \\to F' \\circ T  \\quad \\eta' \\circ G: F' \\circ G \\to G' \\circ G \\] <p>which are two additional natural transformations. \\textcolor{purple}{(Remember we showed that the left square in the commutative diagram of the previous proof commuted by observing that it was obtained by the commutative diagram produced by the natural transformation \\(\\eta\\) and composing it with \\(F'\\)? What we really showed is that \\(F' \\circ \\eta\\) is a natural transformation, since this natural transformation described that square. Similarly, \\(\\eta' \\circ G\\) is the natural transformation which represents the right square of the commutative diagram in the previous proof.)}</p> <p>With the above notation, we can then write that </p> \\[ \\eta' \\circ \\eta = (G' \\circ \\eta) \\cdot (\\eta' \\circ F) = (\\eta' \\circ G) \\cdot (F' \\circ \\eta).        \\] <p>This idea of ours can be extended to a more general situation. Suppose we have instead three categories \\(\\bb, \\cc\\), and \\(\\dd\\) and where \\(F, G, H: \\bb \\to \\cc\\) and \\(F, G, H: \\cc \\to \\dd\\) are functors associated with natural transformations \\(\\eta: F \\to G, \\sigma : G \\to H\\), and \\(\\eta': F' \\to G', \\sigma': G' \\to H'\\). The following diagram may be more helpful than words: \\</p> <p>\\begin{minipage}{0.65\\textwidth} \\  \\hspace{7cm}  \\end{minipage} \\vspace{-0.5cm} </p> <p>Note we've omitted the label of \\(G\\) and \\(G'\\) on the middle horizontal arrows since they don't exactly fit in there when we include the labels for the natural transformations. </p> <p>Now suppose we have an object \\(B\\) in \\(\\bb\\). Then we can create three objects \\(F(B), G(B)\\) and \\(H(B)\\) in \\(\\cc\\), and we may interchange between these objects via the given natural transformations. Specifically, \\(\\eta(B) : F(B) \\to G(B)\\) and \\(\\sigma(B): G(B) \\to H(B)\\). However, we also know that \\(\\eta', \\sigma'\\) are natural transformations between \\(\\cc\\) and \\(\\dd\\), and hence imply the following commutative diagram. </p> <p>\\  Suppose we start at the upper left corner and want to achieve the value at the bottom right. There are two ways we can do this: We can travel within the interior of the diagram, or we can travel on the outside of the diagram.</p> <p>In traveling on the interior of the diagram,  note that the composition of the arrows of the upper left square is \\(\\eta' \\circ \\eta\\). In addition, composition of the arrows of the bottom right square is \\(\\sigma' \\circ \\sigma\\). </p> <p>In traveling on the exterior of the diagram note that the composition of the top row is  \\(\\eta' \\cdot \\sigma'\\)  and composition of the right most vertical arrows is \\(\\eta \\cdot \\sigma\\). Since both paths achieve the same value, we see that </p> \\[ (\\eta' \\cdot \\sigma') \\circ (\\eta \\cdot \\sigma) =  (\\eta' \\circ \\eta) \\cdot (\\sigma' \\circ \\sigma) \\] <p>which is known as the Interchange Law.</p> <p>This leads us to make the following definition.</p> <p> We define a double category to be a set of arrows which obey two different forms of composition, generally denoted as \\(\\circ\\) and \\(\\cdot\\), which together satisfy the interchange law.  <p>Furthermore, a 2-category is a double category in which \\(\\cdot\\) and \\(\\circ\\) have the same exact identity arrows.</p> <p></p>"},{"location":"category_theory/Filtered%20Colimits%2C%20Coends%2C%20and%20Kan%20Extensions/Filtered%20Categories%20and%20Limits/","title":"6.1. Filtered Categories and Limits","text":"<p>Outside of category theory, the most common types of limits that are taken  in areas such as algebraic geometry and topology are inverse and directed limits.  These are limits which are taken over thin categories (or preorders) which have at  most one morphism between any two morphisms. </p> <p>As we shall see, limits over thin categories do not possess the nice  properties that limits taken over filtered categories have, which we will  see is the categorification  of the notion of a directed set. We will motivate our desire to  work with filtered categories instead of just thin categories by observing an analogous motivation to work with  directed sets instead of \\(\\mathbb{N}\\) in sequences within topology.  The picture in mind should be:</p> <p></p> <p>Let \\(X\\) be a topological space. Recall that a sequence \\(\\{a_n\\}_{n=1}^{\\infty}\\) in \\(X\\) is a function  \\(a: \\mathbb{N}\\to X\\) such that \\(a(n) = a_n\\). We say the sequence converges to a point  \\(x \\in X\\) if for every open set \\(U\\) of \\(x\\) there exists a \\(N \\in \\mathbb{N}\\) such that \\(\\{a_N, a_{N+1}, \\dots, \\} \\subset U\\). </p> <p>Some of the first topological spaces that people worked with were metric spaces \\((X, d)\\),  and the properties of these spaces were worked out over time. People eventually figured out  that </p> <ul> <li> <p>A subset \\(F \\subset X\\) is closed if and only if \\(F\\) contains  the limits of every sequence in \\(F\\).</p> </li> <li> <p>A subset \\(U \\subset X\\) is open if and only if \\(U\\) contains  does not contain the limit of any sequence in \\(X - U\\). </p> </li> </ul> <p>This is a wonderful result! However, it does not generalize to arbitrary topological  spaces. There are weird counterexamples that we will not get into (cite An Introduction  to Topology and Homotopy Theory by Sierdaski).</p> <p>What this means is that sequences over a plain preorder (i.e., \\(\\mathbb{N}\\)) are great, and they  have nice  properties, but they lack the ability to extend their nice properties to arbitrary  topological spaces. We need more if we want it to work over arbitrary spaces. </p> <p>This is where a directed set comes in. </p> <p> A directed set \\(D\\) is a set equipped with a binary relation \\(\\le\\) such that  for all \\(a, b, c \\in D\\), \\begin{description} \\item[1.] \\(a \\le a\\) (Reflexive). \\item[2.] if \\(a \\le b\\) and \\(b \\le c\\), then \\(a \\le c\\) (Transitive) \\item[3.] For all \\(a,b \\in D\\), there exists a \\(c \\in C\\) such that \\(a \\le c\\) and \\(b \\le c\\) (Directed). \\end{description} The first two properties describe a preorder; only the last condition is new to us.  To summarize, the \"directed\" axiom grants us an upper bounded in \\(D\\) for any finite  set of elements of \\(D\\).   <p>Let \\(D\\) be a directed set. Define a net, or Moore-Smtih Sequence,  to be a function \\(\\lambda: D \\to X\\). We say a net \\(\\lambda\\) converges to a point \\(x \\in X\\) if for every open set \\(U\\) containing \\(x\\), there exists a \\(d \\in D\\) such that  \\(\\{\\lambda(c) \\mid c \\ge d\\} \\subset U\\).  </p> <p>Directed sets are then enough to give us the following theorem:</p> <p> Let \\(X\\) be a topological space. <ul> <li> <p>A subset \\(F \\subset X\\) is closed if and only if every convergent  net \\(\\lambda: D \\to X\\) has a limit in \\(F\\)</p> </li> <li> <p>A subset \\(U \\subset X\\) is open if and only if every convergent net  \\(\\lambda: D \\to X - U\\) does not have a limit in \\(U\\).</p> </li> </ul> <p></p> <p>Hence we see that limits taken over preorders have substantial benefits than when  they are simply taken over \\(\\mathbb{N}\\). Similarly, what we will see is that  limits taken over filtered categories enjoy much better properties than limits  simply taken over preorders. First, we introduce filtered categories.</p> <p> We say that a category \\(J\\) is filtered if  <ul> <li> <p>[1.] For any pair of objects \\(j, j'\\),  there exists an object \\(k\\) and morphism \\(u:j \\to k\\) and \\(v: j' \\to k\\). </p> </li> <li> <p>[2.] For any pair of parallel morphism \\(u, v: i \\to j\\),  there exists an object \\(k\\) and a morphism \\(w: j \\to k\\) such that the diagram below  commutes. </p> </li> </ul> <p>We do not say the empty category is filtered; this should be obvious, but  it also needs to be said.</p> <p>\\ </p> <p></p> <p> Let \\(J\\) be a thin category. What does it take for \\(J\\) to be filtered?  Well, in a thin category, there is never  any pair of distinct morphisms. Hence condition (2) is trivial. Therefore,  for \\(J\\) to be filtered, we simply need to satisfy (1). But in the language  of thin categories, condition (1) can be read as ``for any \\(j, j \\in J\\),  there exists a \\(k\\) such that \\(j, j' \\le k\\)''. Such a condition holds  if and only if \\begin{center} every finite subset \\(S \\subset J\\) has an upper bound in \\(J\\). \\end{center}   Thus, a thin category \\(J\\) needs to have the above property in order to  be a filtered category. <p>An example of this concerns the category \\(**Open**(X)\\), where  \\(X\\) is a topological space. The objects are open sets, while morphisms are  inclusions. The maximal element \\(X \\in **Open**(X)\\) always exists, and hence makes this thin category filtered. </p> <p>\\chapterimage{chapter7_pic/chapt7head.pdf} </p>"},{"location":"category_theory/Limits%20and%20Colimits./Adjoints%20on%20Limits/","title":"5.5. Adjoints on Limits","text":"<p>Consider the free monoid functor \\(F\\) and the  forgetful functor \\(U\\), as below. Recall that they form an adjunction.</p> <p> The way that we philosophically  interpret this adjunction is as follows: For a set \\(X\\), a monoid homomorphism \\(\\phi: F(X) \\to M\\) gives rise to a unique set function  \\(f: X \\to U(M)\\). Conversely, a set function \\(g: X \\to U(M)\\)  gives rise to a unique monoid homomorphism \\(\\psi: F(X) \\to M\\). </p> <p>We will now observe that these functors exhibit nice behavior. </p> <ul> <li>Recall that products in \\(**Mon**\\) are simply products of monoids, while  products in Set are cartesian products. One can show that, for two monoids  \\(M\\), \\(N\\), we have the isomorphism </li> </ul> \\[ U(M \\times N) \\cong U(M) \\times U(N). \\] <p>Regarding this functor's behavior,  we say that the forgetful functor \\(U\\) preserves products. </p> <ul> <li>We may ask if the converse holds: Does the free functor preserve products?  The answer is no: Given two sets \\(X, Y\\), it is generally not true that  \\(F(X \\times Y) \\cong F(X) \\times F(Y)\\) (as monoids). </li> </ul> <p>An easy way to see this is to let \\(X = Y = \\{\\bullet\\}\\), the one point set. Then  \\(F(\\{\\bullet\\} \\times \\{\\bullet\\}) \\cong F(\\{\\bullet\\}) \\cong \\zz\\), while  \\(F(\\{\\bullet\\}) \\times F(\\{\\bullet\\}) \\cong \\zz \\times \\zz\\). </p> <ul> <li>What is interesting, however, is that the free functor does preserve  coproducts. Recall that the coproduct in Set is the disjoint union, while the  coproduct in Mon is the free product of monoids. Then it is true that, for two  sets \\(X, Y\\), </li> </ul> \\[ F(X \\amalg Y) \\cong F(X) * F(Y). \\] <p>Thus we see that we have two functors that separately preserve  products and coproducts. This is actually very interesting; after all, a  very useful question to ask about a functor is if it preserves products, coproducts, equalizers,  etc. For example, the fundamental group functor preserves products, and this is an interesting result  one usually proves a topology course. </p> <p>We now explain why we have this nice behavior.</p> <p> Suppose \\(G: \\dd \\to \\cc\\) is a right adjoint and \\(F: \\cc \\to \\dd\\) is its left adjoint. Then \\(G\\) preserves limits and \\(F\\) preserves colimits. </p> <p>Before a proof, we make some comments. </p> <ul> <li> <p>An easy way to remember this is RAPL: \"Right Adjoints Preserve Limits.\"  (Speaking from experience, say it in your head a bunch of times or you'll forget.)  If you can remember RAPL, then you can  remember that, dually, left adjoints preserve colimits. </p> </li> <li> <p>The converse of this theorem does not hold.</p> </li> <li> <p>Typically, this proof is shown in one of two forms: It is \"blackboxed\" with  a slick application of the Yoneda Lemma, which is not illuminating or useful for a new  reader. Or, it is more usefully spelled out by showing that right adjoints preserve limits, and  the second statement is obtained by \"dualizing\". For variety, we will show that left adjoints preserve  colimits. Then, the reader can  try proving themselves that right adjoints preserve  limits.</p> </li> </ul> <p> Let \\((\\Colim H, \\sigma_i: H(i) \\to \\Colim H)\\) be the colimit of the functor \\(H: J \\to \\cc\\). This means that we have the universal diagram below.  \\  <p>Mapping this to \\(\\dd\\) under \\(F: \\cc \\to \\dd\\), we obtain the diagram  \\ </p> <p>We see that \\((F(\\Colim H), F(\\sigma_i): F(H(i)) \\to F(\\Colim H))\\)  is a cone over the functor \\(F \\circ H: J \\to \\dd\\). We must show it is universal. Towards that goal,  let \\((C, \\tau_i: F(H(i)) \\to C)\\)  be a cone over \\(F \\circ H: J \\to \\dd\\).  We must show that </p> <ul> <li> <p>[1.] There exists a \\(\\alpha: F(\\Colim H) \\to C\\) such that  \\(\\alpha \\circ F(\\sigma_i) = \\tau_i\\) for all \\(i \\in J\\)</p> </li> <li> <p>[2.] \\(\\alpha\\) is the unique morphism from \\(F(\\Colim H)\\) to \\(C\\) with  this property.  </p> </li> </ul> <p>We show existence. Observe that each  \\(\\tau_i: F(H(i)) \\to C\\) induces a unique morphism  \\(\\delta_i: H(i) \\to G(C)\\) such that the diagram below commutes.  \\  Hence, we have a family of \\(\\delta_i: H(i) \\to G(C)\\). However, since \\(\\Colim H\\) is the colimit of \\(H\\),  we obtain a unique morphism \\(k: \\Colim H \\to G(C)\\) such that the diagram commutes.    \\ </p> <p>We then map this diagram in \\(\\cc\\) to the diagram below in \\(\\dd\\)  via \\(F\\): \\ </p> <p>Thus we see that \\(\\epsilon_C \\circ F(k): F(\\Colim H) \\to C\\) is a morphism  pointing from \\(F(\\Colim)\\) to \\(C\\) such that the above diagram commutes.  We have proved existence of such a morphism. It is not difficult to show  uniqueness, which is left as an exercise.  Once we have uniqueness, we can then conclude that \\((F(\\Colim H), F(\\sigma_i): F(H(i)) \\to F(\\Colim H)\\)  forms a universal cone  over \\(F \\circ H: J \\to \\dd\\), so that \\(F(\\Colim H)\\) is the colimit, as desired.  </p> <p> Using the above theorem, we now know that the free monoid functor  \\(F: **Set** \\to **Mon**\\) preserves coproducts. Therefore, we can say  that for any sets \\(X, Y\\), we have that  \\[ F(X \\amalg Y) \\cong F(X) * F(Y).    \\] <p>Moreover, the free monoid functor is part of a larger family of free functors: </p> <ul> <li> <p>Free group functor, \\(F: **Set** \\to **Grp**\\)</p> </li> <li> <p>Free abelian group functor, \\(F: **Set** \\to **Ab**\\)</p> </li> <li> <p>Free ring functor, \\(F: **Set** \\to **Ring**\\)</p> </li> <li> <p>Free \\(R\\)-module functor, \\(F: **Set** \\to R**-Mod**\\)</p> </li> </ul> <p>who are the left adjoints to their respective forgetful functors.  However, the coproduct in some of these  categories is not always the free product. For example, the coproduct of \\(**Grp**\\)  is the free product, but the coproduct in \\(**Ab**\\) is the direct sum. Hence, the above theorem tells us that coproducts are preserved, but to obtain  the correct isomorphism, we need to remember what the coproduct in the codomain category  of our left adjoint is. </p> <p> Let Meas be the category of measure spaces with measure-preserving  morphisms.  More precisely,  \\begin{description} \\item[Objects.] The objects are triples \\((X, \\mathcal{A}, \\mu_X)\\)  where \\(X\\) is a topological space, \\(\\mathcal{A}\\) is a sigma algebra  on \\(X\\), and \\(\\mu_X\\) is a measure on \\(X\\).  <p>\\item[Morphisms.] A morphism between two objects \\((X, \\mathcal{A}, \\mu_X)\\)  and \\((Y, \\mathcal{B}, \\mu_Y)\\) is a function \\(f: X \\to Y\\) such that  \\(f\\) is measurable and preserves measure. That is, is \\(f\\) is measurable  and </p> \\[ \\mu_X(f^{-1}(B)) = \\mu_Y(B) \\] <p>for every \\(B \\in \\mathcal{B}\\).  \\end{description}</p> <p>Let \\(U: **Meas** \\to **Set**\\) be the forgetful functor, forgetting  measure space properties and measurability of the morphisms.  This functor can't have a left-adjoint, since it does not preserve  products. In fact, Meas cannot even have products.  The main issue with this is that we cannot guarantee the projection  morphisms to preserve measure. For example, if we consider the  simple measure space \\((\\mathbb{R}, \\mathcal{B}, \\mu)\\) where \\(\\mathcal{B}\\) consists  of the Borel algebra and \\(\\mu\\) is the Lebesgue measure, then  one reasonable way to try to form a product with itself is to construct the triple</p> \\[ (\\rr \\times \\rr, \\mathcal{B} \\times \\mathcal{B}, \\mu\\times\\mu).  \\] <p>However, observe that the projection \\(\\pi: (\\rr \\times \\rr, \\mathcal{B} \\times \\mathcal{B}, \\mu\\times\\mu) \\to (\\rr, \\mathcal{B}, \\mu)\\) is  not measure preserving:</p> \\[ \\mu \\times \\mu(\\pi^{-1}([0, 1])) = \\mu \\times \\mu([0, 1] \\times \\rr) = \\infty \\] <p>while </p> \\[ \\mu([0, 1]) = 0.             \\] <p>Therefore, we cannot form products. Hence our forgetful functor  has no left adjoint. </p> <p>One could guess that the left adjoint would be the  measure-constructing functor \\(F: **Set** \\to **Meas**\\) where </p> \\[ X \\mapsto (X, \\mathcal{P}, \\mu_0) \\] <p>where \\(\\mathcal{P}\\) is the sigma algebra on the power set, and \\(\\mu_0\\) assigns the measure of each set to zero (i.e. the trivial measure) but this is not the case. In fact, this functor itself also cannot have a left-adjoint  because it doesn't preserve products  (since Meas can't have products).</p> <p></p> <p>{\\large Exercises \\vspace{0.2cm}}</p> <ul> <li> <p>[1.] Denote the free monoid functor as \\(F\\). Prove directly that for two sets \\(X\\), \\(Y\\), we have  the isomorphism of monoids \\(F(X \\amalg Y) \\cong F(X) * F(Y)\\). (Doing this is actually very important;  The proof of Theorem \\ref{theorem:RAPL} will become more intuitive.)</p> </li> <li> <p>[2.] Finish the proof of Theorem \\ref{theorem:RAPL}</p> </li> <li> <p>[3.]  Let \\(\\cc, \\dd\\) be categories with finite products.  \\begin{itemize}</p> </li> <li> <p>[i.] Let \\(F: \\cc \\to \\dd\\) be a functor that preserves products, so that for two objects \\(A\\), \\(B\\) of \\(\\cc\\), there exists an isomorphism </p> </li> </ul> \\[ F(A \\times B) \\cong F(A) \\times F(B). \\] <p>Does this isomorphism have to be natural in \\(A, B\\)?</p> <ul> <li>[ii.] Suppose \\(F: \\cc \\to \\dd\\) is a right adjoint. Is the isomorphism  \\(F(A \\times B) \\cong F(A) \\times F(B)\\) natural now? </li> </ul> <p>\\end{itemize}</p>"},{"location":"category_theory/Limits%20and%20Colimits./Every%20Limit%20in%20Set%3B%20Creation%20of%20Limits/","title":"5.1. Every Limit in Set; Creation of Limits","text":"<p>While we have been discussing limits and colimits of functors, we generally  consider the case in which they exist. However, they sometimes don't exist; after all,  limits and colimits are universal objects.  Categories which do admit  these constructions are  often convenient places to work inside of. This is analogous to complete metric spaces \\(X\\), where  every Cauchy sequence is convergent in \\(X\\). With such an  analogy in  mind, the following definition should make sense.</p> <p> Let \\(\\cc\\) be a category. We say \\(\\cc\\) is complete if all small diagrams in \\(\\cc\\) has limits in \\(\\cc\\); in other words,  if every functor \\(F: J \\to \\cc\\), where \\(J\\) is a small  category, has a limit in \\(\\cc\\).  Similarly, we define:       </p> <p> Let \\(\\cc\\) be a category. We say \\(\\cc\\) is cocomplete if all small diagrams in \\(\\cc\\) has colimits in \\(\\cc\\). In other words,  every functor \\(F: J \\to \\cc\\), where \\(J\\) is a small category,  has a colimit in \\(\\cc\\). </p> <p>Now we show how to construct limits inside of Set, thereby showing that this category is complete. </p> <p> For this example, let \\(J = \\omega\\op\\), where \\(\\omega\\) is the  preorder of natural numbers. Since we are asking for the  opposite category, we reverse the arrows and  get the diagram below. <p> Now suppose \\(F: \\omega\\op \\to **Set**\\) is a functor.  Then if we write \\(F(i) = A_i\\) with \\(A_i \\in **Set**\\),  then we see that the image of \\(F\\) is a family of sets  \\(F_n\\) with functions \\(f_{n}:A_{n+1} \\to A_{n}\\):  \\  One way we could try forming a limit of this diagram is by constructing a cone, using the product of these sets.  \\  However, this isn't exactly what we want. A cone must form a commutative diagram and it's not always true that \\(f_n \\circ \\pi_{n+1} = \\pi_n\\). So let's instead restrict  our attention to a subset \\(\\displaystyle L \\subset \\prod_{i = 0}A_i\\)  where the points \\((a_0, a_1, \\dots, a_n, \\dots)\\) do satisfy this relation.</p> \\[ L = \\big\\{x = (a_0, a_1, a_2, \\dots,)  \\mid f_{n}\\circ \\pi_{n+1}(a_) = \\pi_n(x) \\big\\}. \\] <p>and equip \\(L\\) with the functions \\(\\pi'_{n}\\) where </p> \\[ \\textcolor{NavyBlue}{\\pi'_n} = \\pi_n \\circ i  : L \\to A_n \\] <p>where \\(i: L \\to \\prod\\limits_{i = 1}^{\\infty} F_i\\) is the inclusion function. Then we have \\  so that \\(L\\) forms a cone. We now prove that this cone is universal. </p> <p> The set \\(L\\) is the limit of the functor \\(F: \\omega\\op \\to **Set**\\). </p> <p> Suppose \\(K\\) is another cone over our diagram,  equipped with morphisms \\(\\textcolor{Red}{\\mu_n}: K \\to F_n\\). Since this is another  cone, we have that \\(f_n \\circ \\mu_{n+1} = \\mu_n\\).  Now let \\(k \\in K\\).  Then we can form an element  \\[ x = (\\mu_0(k), \\mu_1(k), \\mu_2(k), \\dots) \\in  \\prod_{i = 1}^{\\infty}F_i \\] <p>since each \\(\\mu_n(k) \\in F_n\\). Now observe that </p> \\[ f_n \\circ \\pi_{n+1}(x) = f_n(\\mu_{n+1}(k)) =  \\mu_n(k) = \\pi_n(x).  \\] <p>Thus we see that \\(f_n \\circ \\pi_{n+1}(x) = \\pi_n(x)\\), so that by definition,  \\(x \\in L\\). Hence we can create a unique function \\(g: K \\to L\\) where for each \\(k \\in K\\),</p> \\[ g(k) = (\\mu_0(k), \\mu_1(k), \\mu_2(k), \\dots) \\] <p>so we then have that </p> \\[ \\pi'_n \\circ g = \\mu_n. \\] <p>Hence, this shows that \\((L, \\pi_n: L \\to F_n)\\) is universal, so that \\(L = \\Lim F\\)!  \\  </p> <p>If we want to view this in terms of the spider diagrams, then we have  \\ </p> <p></p> <p>Here, we've taken a nice, simple diagram  \\(F: \\omega\\op \\to **Set**\\) and shown that there exists a limit \\(L\\) of the diagram  inside of \\(**Set**\\). However, we can do this more generally, so that Set is complete. To  illustrate this we need the notion of a set of cones. </p> <p>Note that in the last example, we can actually think of each \\(x = (x_0, x_1, x_2, \\dots) \\in \\Lim F\\) as a cone. How so? </p> <ul> <li> <p>[1.] For each \\(x = (x_0, x_1, x_2, \\dots) \\in \\Lim F\\),  consider the one-point set \\(\\{*\\}\\).</p> </li> <li> <p>[2.] Associate \\(\\{*\\}\\) with  the family of functions \\(\\pi^*_n: \\{*\\} \\to F_n\\), defined as</p> </li> </ul> \\[ \\pi^x_n(*) = x_n. \\] <p>Now since \\(x \\in \\Lim F\\), we know that \\(f_n(x_{n+1}) = x_{n}\\). But, note that this is equivalent to stating that \\(f_n \\circ \\pi_{n+1}(*) = \\pi_{n-1}(*)\\). Therefore the diagram  \\  commutes for every \\(f_n: F_{n+1} \\to F_n\\), so that's how we can regard every \\(x \\in \\Lim F\\) as a cone. Therefore, if we denote \\(\\cone(*, F)\\) as the set of all cones of \\(\\{*\\}\\) over \\(F\\), we see that \\(\\cone(*, F) = \\Lim F\\). </p> <p> The category Set is complete. That is, if \\(J\\) is a small category, every functor \\(F: J \\to **Set**\\) has a  limit  \\[ \\Lim F = \\cone(*, F) \\] <p>where \\(\\cone(*, F)\\) is the set of all cones of \\(\\{*\\}\\) over \\(F\\). The set \\(\\cone(*, F)\\) forms the limit cone with the morphisms \\(v_i: \\cone(*, F) \\to F_i\\) described as follows. If \\(x \\in \\cone(*, F)\\), then \\(x\\) has a family of  morphisms \\(\\sigma^x_i: \\{*\\} \\to F_i\\). Therefore,</p> \\[ v_i: \\cone(*, F) \\to F_i \\qquad v_i(x) = \\sigma^x_i(*). \\] <p>\\vspace{-0.8cm} </p> <p> First, since \\(J\\) is small, we know that \\(\\cone(*, F)\\) is a  set. For each \\(j \\in J\\), establish the morphism \\(v_j: \\cone(*, F) \\to F_j\\) where \\(v_j(x) = \\sigma^x_j(x)\\), and \\(\\sigma^x_j: \\{*\\} \\to F_j\\) is the morphism associated with \\(x\\) as a cone over \\(F\\).  <p>We now show that it is a cone.  Suppose \\(f: i \\to j\\) is a morphism in \\(J\\). Then observe that  \\(F(f) \\circ v_i(x) = F(f) \\circ \\sigma^x_i(x) = \\sigma^x_j(x) = v_j(x)\\). Hence the diagram  \\  commutes, so \\(\\cone(*, F)\\) really does form a cone over \\(F\\).  To show this is universal, and hence our limit, suppose that  \\(A\\) in Set also forms a cone over \\(F\\) with morphisms  \\(\\tau_j: X \\to F_j\\). Note that for each \\(a \\in A\\), we can form a cone from \\(\\{*\\}\\) to \\(F\\), if we define \\(\\sigma^a_j: \\{*\\}  \\to F_j\\) as \\(\\sigma^a_j(*) = \\tau_j(a)\\). Then the diagram  \\  must also commutes since it commutes for each \\(\\tau_j\\).  Thus we can define a unique  function \\(g: A \\to \\cone(*, F)\\), where each point \\(a\\) is sent to the cone which it forms from \\(\\{*\\}\\) over \\(F\\). Therefore, \\(\\cone(*, F)\\) is universal, so that </p> \\[ \\Lim F = \\cone(*, F) \\] <p>as desired.  </p> <p>The above proof can be repeated to show that others categories are complete, like Grp or Rng. </p> <p>In attempting to find the limit \\(F: J \\to \\cc\\) in some category \\(\\cc\\), one strategy is to to compose this functor with another one \\(G: \\cc \\to \\dd\\), with the prior knowledge that \\(\\dd\\) is complete. If one knows \\(\\dd\\) is complete, one then use this information to find  the limit of \\(F: J \\to \\cc\\). </p> <p> Let \\(F: J \\to \\cc\\) be a functor. A functor \\(G: \\cc \\to \\dd\\) creates limits for \\(F\\) if whenever \\((\\Lim G \\circ F, \\tau: \\Delta(\\Lim G \\circ F) \\to G \\circ F)\\) exists,  the limit \\((\\Lim F, \\sigma: \\Delta(\\Lim F) \\to F)\\) such that  \\[ G(\\Lim F) = \\Lim G \\circ F \\qquad G(\\sigma) = \\tau. \\] <p>Similarly, a functor \\(G: \\cc \\to \\dd\\) creates colimits for \\(F\\) if whenever \\((\\Colim G \\circ F, \\tau: G \\circ F \\to \\Delta(\\Lim G \\circ F)\\)  exists, the colimit \\((\\Colim F, \\sigma: F \\to \\Delta(\\Colim F)\\) exists  and </p> \\[ G(\\Colim F) = \\Colim G \\circ F \\qquad G(\\sigma) = \\tau. \\] <p></p> <p>The diagram below visually explains this process; the existence of limit  in \\(\\dd\\) on the left implies the existence of the limit in \\(\\cc\\) on the right.  Moreover, the diagram on the left is the image of the diagram on the right  under \\(G\\).  \\ </p> <p> Consider a functor \\(F: J \\to **Grp**\\). We'll show that the forgetful functor \\(U:**Grp** \\to **Set**\\) creates limits for \\(**Grp**\\). <p>By the previous theorem, we know that \\(U \\circ F; J \\to **Set**\\) must have a limit \\(\\cone(*, U \\circ F)\\)  with the family of morphisms \\(v_i: \\cone(*, U\\circ F) \\to U\\circ F_i\\).  Now denote the set \\(\\cone(*,U \\circ F)\\) as \\(L\\).  Then we can endow \\(L\\) with a group structure.</p> <ul> <li> <p>For any \\(\\sigma, \\tau \\in L\\), we define \\(\\sigma \\times \\tau\\) to be the cone where \\((\\sigma \\times \\tau)_i = \\sigma_i \\cdot \\tau_i\\), where \\(\\cdot\\) is the product in \\(F_i\\). </p> </li> <li> <p>For \\(\\sigma \\in L\\), we define the inverse to be  the function \\(\\sigma^{-1}\\) where \\((\\sigma^{-1})_i = \\sigma_i^{-1}\\), with the inverse being taken in \\(F_i\\). </p> </li> </ul> <p>All we're really doing here is taking advantage of the fact  that each \\(\\sigma, \\tau\\) is really just a family of functions  \\(\\sigma_i, \\tau_i: \\{*\\} \\to F_i\\). Thus we're taking advantage of the group structure in each \\(F_i\\). </p> <p>Thus \\(L = \\cone(*, U \\circ F)\\) is a group, which then makes the family of morphisms \\(v_i: \\cone(*, U \\circ F)\\) into a family of group homomorphisms. To show this, simply observe that </p> \\[ v_i(\\sigma \\times \\tau) = (\\sigma \\times \\tau)_i = \\sigma_i \\cdot \\tau_i = v_i(\\sigma)\\cdot v_i(\\tau).            \\] <p>Now we claim that the cone \\(\\cone(*, U \\circ F)\\) with the morphisms \\(v_i: \\cone(*, U \\circ F) \\to F_i\\) is universal. To show this, let \\(G\\) be a group and suppose \\(G\\) forms a cone over \\(F\\)  with morphisms \\(\\phi_i: G \\to F_i\\). Then \\(U(G)\\) forms a cone over \\(\\cone(*, U \\circ F)\\) in Set with morphisms  \\(U(\\phi_i): U(G) \\to U(F_i)\\). </p> <p>Since we know \\(\\cone(*, U \\circ F)\\) is a universal cone in Set,  there exists a \\(h: U(G) \\to L\\) such that \\(U(\\phi_i) = U(v_i)\\circ h_i\\). However, note that \\(h\\) can be thought of as a  group homomorphism. For any \\(g, g' \\in G\\), we have</p> \\[\\begin{align*} h_i(g \\cdot g') = \\phi_i(g \\cdot g')  = \\phi_i(g) \\times \\phi_i(g') &amp;= h_i(g) \\times h_i(g')\\\\ &amp;= (h(g) \\cdot h(g'))_i. \\end{align*}\\] <p>Therefore, \\(h:U(G) \\to L\\) can be realized back into Grp as a group homomorphism \\(h: G \\to L\\), thereby showing \\(\\cone(*, U \\circ F)\\)  is a universal cone in Grp. This is one way in showing  Grp is complete.  What we really did in the last example was nothing special. Using the fact that  Set is complete, we transferred \\(F: J \\to **Grp**\\) over to Set via the forgetful functor \\(U: **Grp**  \\to **Set**\\). We calculated the limit, and showed that this can be realized as a limit in Grp. In this sense, \\(U: **Grp** \\to  **Set**\\) creates limits in Grp. A similar strategy can be carried out for other forgetful functors. </p> <p> Let \\(\\cc\\) be a category and \\(A\\) an object of \\(\\cc\\). Recall that  with the comma category \\((A \\downarrow \\cc)\\), we have a projection  functor \\(P: (A \\downarrow \\cc) \\to \\cc\\) where on objects  \\((C, f: A \\to C)\\) and morphisms \\(h: (C, f: A \\to C) \\to (C', f: A \\to C')\\)  we have that  \\[ P(C, f: A \\to C) = C \\qquad P(h) = h: C \\to C'.             \\] <p>Now for any functor \\(F:J \\to (A \\downarrow \\cc)\\), the functor  \\(P: (A \\downarrow \\cc) \\to \\cc\\) creates limits. To see this, we first interpret  a functor \\(F: J \\to (A \\downarrow \\cc)\\). For each \\(j\\), we have that  </p> \\[ F(j) = (C_j , f_j: A \\to C_j)  \\] <p>for some \\(C_j \\in \\cc\\) and \\(f_j: A \\to C_j\\). If \\(u: j \\to k\\) is a morphism in \\(J\\), then \\(F(u) : C_j \\to C_k\\) is a morphism in \\(\\cc\\)  such that the diagram below commutes (as, that's what morphisms do in comma categories).  \\  Note that this is a cone over \\(F\\) in \\(\\cc\\).  Now suppose we have a limit \\(\\Lim P \\circ F\\) in \\(\\cc\\) with  morphisms \\(\\mu_i: \\Lim P \\circ F \\to C_i\\) with \\(i \\in J\\).  Then because \\(\\Lim P \\circ F\\) is a limiting cone, and we must  have a unique \\(v\\) such that the diagram below commutes.  \\  The claim now is that \\((\\Lim P \\circ F, v: A \\to  \\Lim P \\circ F)\\)  is the limit \\(\\Lim F\\) of \\(F: J \\to (A \\downarrow \\cc)\\), which is left  for the reader to show. </p> <p>{\\large Exercises \\vspace{0.2cm}}</p> <ul> <li> <p>[1.]  \\begin{itemize}</p> </li> <li> <p>[i.] Let \\(J = \\omega\\), and let \\(F: J \\to **Set**\\) be a functor  were \\(F(i) = A_i\\). Show that \\(\\Colim F\\) exists and give an expicit  description of it.  \\ Hint: It will be a set endowed with an equivalence relation.</p> </li> <li> <p>[ii.] How does your answer chance when \\(F: J \\to **Set**\\)  is contravariant? </p> </li> </ul> <p>\\end{itemize}</p>"},{"location":"category_theory/Limits%20and%20Colimits./Existence%20of%20Universal%20Morphisms%20and%20Adjoint%20Functors/","title":"5.6. Existence of Universal Morphisms and Adjoint Functors","text":"<p>When we introduced functors, we introduced several if and only  if propositions which gave us criterion on the existence of an adjoint  functor. Notably, we showed that if there exists an adjunction </p> <p> (that is, the classic bijection of homsets which is natural) then there exist universal morphisms </p> \\[ \\eta_C: C \\to G \\circ F(C) \\qquad \\epsilon_D: F\\circ G(D) \\to D  \\] <p>for all objects \\(C, D\\). Furthermore, we only need one of the universal morphisms  to derive an adjunction. Since universal morphisms are simply initial objects  in some comma category, we have the following proposition. </p> <p> Let \\(G: \\dd \\to \\cc\\) be a functor. Then \\(G\\) has a left adjoint  if and only if for each \\(C \\in \\cc\\), the comma category \\(C \\downarrow G\\)  has an initial object. </p> <p> <ul> <li>[\\(\\bm{\\implies}\\)] Suppose \\(G\\) has a left adjoint \\(F: \\cc \\to \\dd\\). Then  for each \\(C \\in \\cc\\), there exists a universal morphism  \\(\\eta_C: C \\to G(F(C))\\).  Now in the comma category, objects will be of  the form </li> </ul> \\[ (D, f: C \\to G(D)) \\] <p>where morphisms between \\((D, f: C \\to G(D))\\) and \\((D', f': C \\to G(D'))\\)  will be induced by morphisms \\(h: D \\to D'\\) such that  \\  commutes. First, observe that \\((F(C), \\eta_C: C \\to G(F(C)))\\)  is an object of the comma category. Second, observe that the  bijection of homsets </p> \\[ \\hom_{\\dd}(F(C), D) \\cong \\hom_{\\cc}(C, G(D))    \\] <p>(natural in \\(C, D\\))  guarantees that every object \\((D, f: C \\to G(D))\\) in the comma category corresponds uniquely to a morphism \\(h: F(C) \\to D\\). Moreover, uniqueness guarantees that the diagram  \\  must commute. Hence, \\((F(C), \\eta_C: C \\to G(F(C)) )\\) is an  initial object \\(C \\downarrow G\\). </p> <ul> <li>[\\(\\impliedby\\)] Now suppose that \\(C \\downarrow G\\) has an  initial object \\((D, \\eta_C: C \\to G(D))\\). Actually, denote  the object \\(D\\) as \\(F(C)\\). When we write \\(F(C)\\), we're not denoting  a functor, because we'll show this is a functor. Anyways, our initial  object can be written as </li> </ul> \\[ (F(C), \\eta_C: C \\to G(F(C))). \\] <p>This defines a mapping on objects \\(C \\mapsto F(C)\\). To show that this  is a functor, suppose we have a morphism \\(f: C \\to C'\\) in \\(\\cc\\). Then  we have square  \\  Adding the final leg to this diagram would show that \\(F\\) is a functor. But  since \\((F(C), \\eta_C: C \\to G(F(C)))\\) is an initial object in \\((C \\downarrow G)\\),  and \\((F(C'), \\eta_{C'}: C' \\to G(F(C)))\\) is an object in this category,  there must be a unique morphism \\(F(f):F(C) \\to F(C')\\). Uniqueness of  this morphism forces commutativity of the square  \\  and therefore \\(F\\) is a functor. Simultaneously, this shows \\(F\\) is left adjoint  to \\(G\\), as desired. </p> <p> We can repeat the proof to achieve the following result as well.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor. Then \\(F\\) has a right adjoint if and only  if for each \\(D \\in D\\), the comma category \\(D \\downarrow F\\) has a terminal  object.   Thus we see that initial and terminal objects are key to figuring out  when a functor has a left or right adjoint, and hence when they preserve limits. We can investigate a little deeper into this. </p> <p>(Initial Object Existence.) If \\(\\cc\\) is a complete category with small homsets, then \\(\\cc\\)  has an initial object if and only if it satisfies the \\textbf{Solution  Set Condition}:  \\begin{center} \\begin{minipage}{0.9\\textwidth} There exists objects \\((C_i)_{i \\in I} \\in \\cc\\) such that for every \\(C \\in \\cc\\), there is a  a morphism \\(f_i: C_i \\to C\\) for at least one \\(i \\in I\\).  \\end{minipage} \\end{center}    </p> <p> <ul> <li> <p>[\\(\\implies\\)]  Suppose \\(\\cc\\) has an initial object \\(C'\\). Then \\(I\\) is the one-point  set since for each \\(C \\in C\\) there exists one unique morphism  \\(f: C' \\to C\\). </p> </li> <li> <p>[\\(\\impliedby\\)]  On the other hand, assume the solution set condition.  Since \\(\\cc\\) is complete, it must have products, so we may  take the product</p> </li> </ul> \\[ W = \\prod_{i \\in J}C_i. \\] <p>This product has associated projection morphisms  \\(\\displaystyle \\pi_k: \\prod_{i \\in J}C_i \\to C_k\\). Therefore,  for each object \\(C \\in \\cc\\), there exists at least one  morphism between \\(W\\) and \\(C\\) by composition:</p> \\[ f_k \\circ \\pi_k: W \\to C. \\] <p>By hypothesis, the collection of endomorphisms \\(\\hom_{\\cc}(W, W)\\)  is a set. Therefore, we may form an equalizer \\(e: V \\to W\\)  of this set. Observe that for each \\(C \\in \\cc\\), there exists  at least one morphism between \\(V\\) and \\(C\\) by composition:</p> \\[ f_k \\circ \\pi_k\\circ e: V \\to C. \\] <p>We'll now show that all morphisms are equal. Suppose the contrary;  that there are two distinct morphisms \\(f, g: V \\to C\\). Denote the equalizer  of this pair as \\(e_1: u \\to v\\). Then we have that  \\  commutes. The morphism \\(s\\) is induced via the universality of  both \\(U\\) and \\(V\\). Since \\(e \\circ e_1 \\circ s : W \\to W\\), and  \\(e\\) is the equalizer of endomorphisms of \\(W\\), we have that </p> \\[ (e \\circ e_1 \\circ s)\\circ e = e.   \\] <p>Since equalizers are monic, we can cancel on the left side to conclude  that </p> \\[ e_1 \\circ s \\circ e = 1_V.  \\] <p>However, this implies that the right inverse of \\(e_1\\) is \\(s \\circ e\\).  Since \\(e_1\\) is already monic, it must be an isomorphism. Hence \\(f = g\\), so that  \\(V\\) is an initial object as desired. </p> <p></p> <p>We can now combine all of our propositions and theorems into the following  one, which is the main adjoint functor theorem of interest. </p> <p>[ (General Adjoint Functor Theorem.)] Let \\(\\dd\\) be complete with small homsets. A functor  \\(G: \\dd \\to \\cc\\) has a left adjoint if and only if it preserves all small  limits and satisfies the solution set condition:  \\begin{center} \\begin{minipage}{0.9\\textwidth} For each \\(C \\in \\cc\\),  there exists a set of objects \\((D_i)_{i \\in I}\\) \\(\\dd\\) and a family  of arrows \\[ f_i: C \\to G(D_i) \\] <p>such that for every morphism \\(h: C \\to G(D)\\), there  exists a \\(j \\in I\\) and a morphism \\(t: D_j \\to D\\) such that</p> \\[ h = G(t) \\circ f_i. \\] <p>\\end{minipage} \\end{center}   </p> <p>\\textcolor{Plum}{The above theorem helps us find out when we can get a  left adjoint. Prior to this theorem, we already know what happened if  we were given a functor who has a left adjoint. Namely, it must preserve  limits. This natural question one would then ask is if the converse holds. The above theorem tells us no, the converse doesn't hold  and in fact we need to make sure the functor satisfies the \\textbf{solution  set condition}. In the next section, we'll give an example of a functor  which preserves limits from a complete category, but still has no left adjoint.}</p> <p>As a converse to the above theorem, we have the following. </p> <p>[ (Representability Theorem.)] Let \\(\\cc\\) be a small, complete category. A functor \\(K: \\cc \\to  **Set**\\) is representable if and only if \\(K\\) preserves limits  and satisfies the following solution set condition:  \\begin{center} \\begin{minipage}{0.9\\textwidth} There exists a set \\(S \\subset \\ob(\\cc)\\) such that for any  \\(C \\in \\cc\\) and any \\(x \\in K(C)\\), there exists an \\(s \\in S\\),  an element \\(y \\in K(s)\\) and an arrow  \\[ f: s \\to C \\text{ such that } K(f)(y) = x. \\] <p>\\end{minipage} \\end{center}   </p>"},{"location":"category_theory/Limits%20and%20Colimits./Inverse%20and%20Direct%20Limits./","title":"5.2. Inverse and Direct Limits.","text":"<p>In the previous example, we calculated the limit of the diagram indexed by \\(\\omega\\op\\). It turns out that in general, we can construct  a lot of mathematical ideas by first modeling them as the limit  of a functor \\(F: J \\to \\cc\\), where \\(J\\) is a partially ordered set. Thus we give a special name to this concept.</p> <p> Let \\(\\cc\\) be a category, and suppose the \\(F: J\\op \\to \\cc\\) has  a limit object \\(\\Lim F\\) in \\(\\cc\\), where \\(J\\) is a partially ordered set (where, if \\(i \\le j\\), then there exists \\(f: i \\to j\\)).  Then \\(\\Lim F\\) is said to be a inverse limit or projective limit.  <p>Dually, we define the colimit of a functor \\(F: J \\to F\\)  to be direct limit.  </p> <p>There are many famous examples of these limits, with the following example probably being the most familiar. </p> <p> Consider the functor \\(F: \\omega\\op \\to **Rng**\\) where  we define \\(F(n) = F_n = \\zz/p^n\\zz\\) with \\(p\\) being a prime.  Then we have a diagram  <p> where the maps \\(f_n: \\zz/p^{n+1}\\zz \\to \\zz/p^n\\zz\\) are the  projection maps. The limit of this diagram turns out to be the **\\(\\bm{p**\\)-adic integers} \\(\\zz_p\\), and this is one way of defining them. The most popular way to define them it to work in ring theory, establish \\(p\\)-adic valuations, and realize that the  valuations turn \\(\\zz\\) into a metric space; one which can be  completed with respect to the metric to give rise to \\(\\zz_p\\).</p> <p>First, observe that they form a cone. Define the map </p> \\[ \\pi_{n}: \\zz_p \\to \\zz/p^{n}\\zz  \\qquad \\pi\\left( \\sum_{k = 0}^{\\infty} a_kp^k \\right) =  \\sum_{k =0}^{n-1}a_kp^k + p^{n}\\zz. \\] <p>Now observe that </p> \\[\\begin{align*} f_n \\circ \\pi_{n+1} \\left( \\sum_{k = 0}^{\\infty}a_kp^k \\right) = f_n\\left( \\sum_{k =0}^{n}a_kp^k + p^{n+1}\\zz \\right) &amp;= \\sum_{k =0}^{n-1}a_kp^k + p^{n}\\zz\\\\ &amp;= \\pi_n\\left(  \\sum_{k = 0}^{\\infty}a_kp^k \\right) \\end{align*}\\] <p>so we may conclude that \\(f_n \\circ \\pi{n+1} = \\pi_n\\). Therefore, \\(\\zz_p\\) does in fact form a cone with the  morphisms \\(\\pi_n\\), so the following diagram commutes. \\  Showing this is universal is simple once we realize that each element of \\(\\zz_p\\) may be thought of as a cone, in the same fashion as we did with Set. That is, we can just  apply the previous theorem to Rng.  This then shows that it's the universal object which we desire. </p> <p>What about direct limits? A less-talked about idea , although definitely not less interesting, is the  dual of the above construction.</p> <p> Consider the functor \\(F: \\omega \\to **Grp**\\) where we have  \\(F(n) = F_n = \\zz/p^n\\zz\\), with \\(p\\) being a prime. This time however  we have the diagram  \\  where we define each \\(f_n: \\zz/p^n\\zz \\to \\zz/p^{n+1}\\zz\\)  as the homomorphism  \\[ f_n\\left( \\sum_{k =0}^{n-1}a_kp^k + p^{n}\\zz \\right) =  \\sum_{k =0}^{n}a_kp^{k+1} + p^{n+1}\\zz. \\] <p>That is, we simply multiply the sum by a power of \\(p\\). It turn outs  that the direct limit is the **Pr\u00fcfer \\(\\bm{p**\\)-Group} \\(\\zz(p^\\infty)\\). The Pr\u00fcfer 2-Group is pictured below. \\begin{center} \\includegraphics{../pictures/chapter5_pic/prufer_2_group/prufer_2_group.pdf} \\end{center}      The Pr\u00fcfer \\(p\\)-group is the set of all \\(p^n\\) roots of unity, as \\(n\\) ranges over all positive integers. Hence the points lie on the complex unit circle. Specifically, it is the group  </p> \\[ \\zz(p^{\\infty}) = \\left\\{\\text{exp}\\left({\\dfrac{2\\pi i m}{p^n}}\\right) \\mid 0 \\le m &lt; p^n, n \\in \\mathbb{Z}^{+}  \\right\\}   \\] <p>which forms a group under complex multiplication.  How does this form a limit for our diagram? </p> <p> </p> <p>Inverse limits are also used in Galois Theory. In Galois Theory, one can define a field extension \\(L/F\\) to be a finite, normal, separable extension.  However, it turns out that one can remove the requirement for the extension to be finite. We then obtain infinite Galois groups, which are constructed as follows. </p> <p> Let \\(F\\) be a field, and suppose \\(L/F\\) is normal, separable extension (\\textcolor{Red}{not necessarily finite!}). Then we can define \\(L/F\\) to be a Galois extension, and we may speak of a Galois group \\(\\gal(L/F)\\), as follows. <p>Let \\(\\mathcal{F}(L/F)\\) be the category of all finite, normal extensions \\(K\\) of \\(F\\) such that \\(F \\subset K \\subset L\\), and \\(\\mathcal{G}(L/F)\\) is the category of all their Galois groups. Note that both \\(\\mathcal{F}(L/F)\\) and  \\(\\mathcal{G}(L/F)\\) are partially ordered sets, ordered by subset inclusion.  To be precise, if \\(K_i \\subset K_j\\) are in \\(\\mathcal{F}(L/F)\\), then </p> \\[ \\gal(K_j/F) \\subset \\gal(K_i/F) \\] <p>and because \\(\\mathcal{G}(L/F)\\) is a preorder on subset inclusion, this implies the existence of some arrow \\(f: \\gal(K_j/F) \\to \\gal(K_i/F)\\). We can describe  \\(f = \\proj_{K_j/K_i}\\) where </p> \\[ \\proj_{K_j/K_i}: \\gal(K_j/F) \\to \\gal(K_i/F) \\qquad \\proj_{K_j/K_i}(\\sigma) = \\sigma\\big|_{K_i}.   \\] <p>That is, we take each permutation \\(\\sigma \\in \\gal(K_j/F)\\) and restrict its action to \\(K_i\\), thereby making it a permutation of \\(K_i\\) which fixes \\(F\\),  and therefore a member of \\(\\gal(K_i/F)\\). </p> <p>Now consider the product with the associated morphisms</p> \\[   \\prod_{K \\in\\mathcal{F}(L/F)}\\gal(K/F) \\qquad  \\pi_{K_i}: \\prod_{K \\in\\mathcal{F}(L/F)}\\gal(K/F)  \\to \\gal(K_i/F) \\] <p>Then we define</p> \\[ \\gal(L/F) = \\left\\{ x = (\\cdots, \\sigma_k, \\cdots) \\in \\prod_{K \\in \\mathcal{F}(L/F)}\\gal(K/F)  \\mid \\proj_{K_i/K_j} \\circ \\pi_{K_i}(x) = \\pi_{K_j}(x) \\right\\}. \\] <p>So \\(\\gal(L/F)\\) forms a cone with morphisms \\(\\pi_{K_i}\\): \\  We then have to work to show that this cone is universal. However, the faster route is to simply recognize that we can  index \\(\\mathcal{G}(L/F)\\) in a monotonic way, since it is a partially order set. Thus there exists a partially ordered set \\(J\\) such that  if \\(f: i \\to j\\) exists in \\(J\\), then </p> \\[ F(i)= \\gal(K_i/F) \\quad F(j) = \\gal(K_j/F)  \\implies  F(f): \\gal(K_i/F) \\to \\gal(K_j/F)   \\] <p>Thus we have a functor \\(F: J \\to \\mathcal{G}(L/F)\\) which hits every  Galois group \\(\\gal(K/F)\\) in such a way that it preserves the order in \\(\\mathcal{G}(L/F)\\). Since the limit of every small diagram exists in Grp, we can define \\(\\gal(L/F)\\) to be the inverse limit of this functor, and we already know that the limit will have the form</p> \\[ \\gal(L/F) = \\left\\{ (\\cdots, \\sigma_k, \\cdots) \\in \\prod_{K \\in \\mathcal{F}(L/F)}\\gal(K/F) \\mid \\proj_{K_i/K_j} \\circ \\pi_{K_i} = \\pi_{K_j} \\right\\}. \\] <p>and that it will be universal. So, this is how we extend the definition of Galois group from a finite, normal, separable extension to simple a normal, separable extension. </p> <p>\\noindent This construction can be done more generally on a partially ordered system of groups, to create these things called  profinite groups. </p> <p> Suppose we are given a partially ordered set of finite groups \\(G_i\\), indexed by some set \\(I\\), equipped with morphisms \\(\\{f^j_i: G_j \\to G_i \\mid i, j \\in I \\quad i \\le j\\}\\) such that  <ul> <li> <p>[1.] \\(f_i^i: G_i \\to G_i\\) is the identity \\(\\id_{G_i}\\) </p> </li> <li> <p>[2.] \\(f_i^j \\circ f_j^k = f_i^k\\). </p> </li> </ul> <p>Then we define the profinite group \\(G\\)  of this system to be the inverse limit:</p> \\[ G = \\left\\{(g_i)_{i \\in I} \\in \\prod_{i \\in I} G_i \\ \\mid f_i^j(g_i) = g_j \\right\\}. \\] <p>Note that requiring \\(f_i^j(g_i) = g_j\\) is the same as requiring \\(f_i^j\\circ \\pi_i(x) = \\pi_j(x)\\), where \\(x \\in G\\),  which is how we defined \\(\\gal(L/F)\\).   \\textcolor{MidnightBlue}{Thus in the previous example, we have that not only can we actually define \\(\\gal(L/F)\\), but our construction  leads to it to becoming a profinite group. Profinite groups are actually very  special, in that they can be interpreted topologically.}</p>"},{"location":"category_theory/Limits%20and%20Colimits./Limits%20from%20Products%2C%20Equalizers%2C%20and%20Pullbacks./","title":"5.3. Limits from Products, Equalizers, and Pullbacks.","text":"<p>In our construction of limits for Sets, we basically forced  the existence of a cone, because we could. This is usually the general strategy  when it comes to calculating the limit of a diagram in a given category;  one uses available, useful constructions which are already present  inside of a category. For example; in Set, we used  the fact that it is cartesian closed to formulate infinite products.</p> <p>Since the general strategy for showing Set is complete can  be extended to other categories, one may wonder \"well, why? And when  will I no longer be able to apply this strategy?\" The theorem  below answers this question. </p> <p> Let \\(\\cc\\) be a category and \\(J\\) a small category.  Suppose \\(\\cc\\) has equalizers for every pair of morphisms in \\(\\cc\\), and all products indexed by objects of \\(J\\) and morphisms of \\(J\\). Then every functor  \\(F: J \\to \\cc\\) has a limit in \\(\\cc\\).  </p> <p>\\textcolor{purple}{What do we mean by all products \"indexed by  objects of \\(J\\) and morphisms of \\(J\\)\"?} What we want to do is be able  to create products of the form </p> \\[ \\prod_{j \\in J}F_j \\qquad\\qquad \\prod_{u:i \\to k} F_\\text{\\text{cod}(u)} = \\prod_{u:j \\to k}F_k.    \\] <p>and know that they're in \\(\\cc\\).  The product on the far left is indexed by objects of \\(J\\), while the equal ones on the right are indexed by morphisms \\(u: i \\to k\\) in \\(J\\). It's a bit weird to think of a product \"indexed by morphisms,\" but it's  exactly what it sounds like: we index over all the morphisms, and  take the product of the domain or codomain (in the above, we did codomain).</p> <p>\\textcolor{purple}{Why do we need this weird concept?} To  answer this, let's go over the construction of limits in Set in a bit different way. </p> <p>When we had a diagram \\(F: J \\to \\cc\\) in \\(\\cc\\), our first guess  in constructing the limit was designing the \\(\\displaystyle \\prod_{j}F_j\\) with morphisms \\(\\displaystyle \\pi_i : \\prod_{j}F_j \\to F_i\\). However,  this doesn't actually form a cone, since for each \\(u: j \\to k\\), we can't guarantee </p> \\[ F(u) \\circ \\pi_j = \\pi_k \\] <p>That is, we can't guarantee the diagram </p> <p> will commute, which is what we need for a cone. Since we needed  \\(F(u) \\circ \\pi_j = \\pi_k\\), we forced it. But this forcing is  simply realizing that, all \\(\\displaystyle x \\in \\prod_{j \\in J}F_j\\)  which satisfy \\(F(u) \\circ \\pi_j = \\pi_k\\), are simply  members of the equalizer of \\(F(u) \\circ \\pi_j\\) and \\(\\pi_k\\). </p> <p> Consider the products \\(\\displaystyle \\prod_{j \\in J}F_j\\) and  \\(\\displaystyle \\prod_{u: i \\to k}F_k\\) where in the last product we  index over all morphisms in \\(J\\).  With both products, consider the projection morphisms \\[\\begin{align*} &amp;\\pi'_j: \\prod_{u:i \\to k}F_k \\to F_j\\\\ &amp;\\pi_j: \\prod_{i \\in J}F_i \\to F_j. \\end{align*}\\] <p>Note that because we have products, we have universal properties which we can take advantage of. That is,  the following diagrams must commute for some \\(f\\) and \\(g\\).  \\  Note however that we can stack these diagrams on top of each other, to obtain \\  Since we have equalizers for every pair of arrows, we can form the equalizer \\(\\displaystyle e:D \\to \\prod_{i \\in J}F_i\\)  of both \\(f\\) and \\(g\\) for some object \\(D\\). \\ </p> <p>Now that we have a morphism  \\(\\displaystyle e: D \\to \\prod_{i \\in J}F_i\\), we can compose this with projections \\(\\displaystyle  \\prod_{i \\in J}F_i \\to F_i\\) to produce a family of morphisms \\(\\pi_i \\circ e: D \\to F_i\\). If we like, we can even  add this to our diagram above to get the following: \\  (\\textcolor{Red}{It looks like a boat!}) Denote \\(\\mu_i = \\pi_i \\circ e: D \\to F_i\\). Then what the above boat diagram  tells us is that </p> \\[ \\pi'_k \\circ g = \\pi_k \\qquad F(u)\\circ \\pi_i = \\pi'_k \\circ f. \\] <p>Composing both equations with \\(e\\), we get </p> \\[ \\pi'_k \\circ g \\circ e = \\pi_k \\circ e \\qquad F(u)\\circ \\pi_i \\circ e= \\pi'_k \\circ f\\circ e. \\] <p>but since \\(g \\circ e = f \\circ e\\), what this really tells us is that</p> \\[ F(u) \\circ \\pi_i \\circ e = \\pi_k \\circ e \\implies F(u) \\circ \\mu_i = \\mu_k. \\] <p>for every \\(u: i \\to k\\) in \\(J\\).  Therefore, we see that we have that  \\  commutes, so that \\(D\\) equipped with the morphisms \\(\\mu_i: D \\to F_i\\) forms a cone. We now show that this is universal, so that \\(D\\) is our limit. We do this  by taking advantage of the universal property which equalizers posses. </p> <p>Suppose \\(C\\) is another object which forms a cone with  morphisms \\(\\tau_i: C \\to F_i\\). Then there exists a map \\(\\displaystyle e': C \\to \\prod_{i \\in J}F_i\\) such that  \\(\\pi \\circ e' = \\tau_i\\). Moreover, this implies that  \\(f \\circ e = g \\circ e\\). But the universal property of  the equalizer \\(e\\) states that for any subject object, there exists a morphism  \\(h: D \\to C\\) such that the diagram below commutes.  \\  Since \\(h: D \\to C\\) is unique, this shows that \\(D\\)  equipped with the morphisms  \\(\\displaystyle \\mu_i: D \\to F_i\\) forms a limit of the diagram, so that \\(D = \\Lim F\\).  We actually proved much more than what was stated in the theorem,  since we literally found the explicit form the limit. </p> <p>As a corollary, we have the following result which is due to the above theorem. The only difference is we strengthen our hypothesis, which makes it less general. </p> <p> Let \\(\\cc\\) be a category. If \\(\\cc\\) has all equalizers (coequalizers) and finite products (coproducts), then  \\(\\cc\\) has all finite limits (colimits).  </p> <p>By Proposition \\ref{prop_category_finite_products}, one can obtain finite products  by simply demanding the existence of binary products and a terminal object. Hence  we can restate the above corollary:</p> <p> Let \\(\\cc\\) be a category. If \\(\\cc\\) has all equalizers (coequalizers), binary products (coproducts) and a terminal object, then \\(\\cc\\) has  all finite limits. </p> <p>Not what is even more interesting is that we can construct equalizers and  finite products from pullbacks. </p> <p>Specifically, suppose our category \\(\\cc\\) has pullbacks and a terminal object \\(T\\).  For any pair of objects \\(A, B\\) in \\(\\cc\\), suppose we take the pull back on  the morphisms \\(t_A: A \\to T\\) and \\(t_B: B \\to T\\). This then give  rise to an object \\(P\\) equipped with two morphisms \\(p_1: P \\to A\\)  and \\(p_2: P \\to B\\), universal in the sense demonstrated below.  \\  Now on the top left we have our pull back. However, on the top right, we've  unraveled the pullback and ignored the terminal object to observe that \\(P\\)  has the universal property of what a product would demand. Hence we may denote  \\(P = A \\times B\\) as the product. Thus by Proposition \\ref{prop_category_finite_products}  \\(\\cc\\) has all finite products. \\textcolor{NavyBlue}{Note that we wouldn't have been able  to construct this if we didn't have a terminal object; For example, if \\(\\cc\\)  was a discrete category, we wouldn't even have any morphisms to take a pullback on!}</p> <p>Now to derive equalizers, consider a pair of parallel morphisms  \\(f, g: A \\to B\\). Then we may simply take their pullback to obtain the diagram below.  \\  If \\(p: A \\times A \\to A\\) is the natural projection map, then  because we have a trivial mapping \\(1_A: A \\to A\\), there exists a canonical  canonical map \\(i: A \\to A \\times A\\) such that \\(p \\circ i = 1_A\\).  Similarly, because we have mappings \\(p_1, p_2: P \\to B\\), we must have a  mapping \\(h: P \\to A \\times A\\).  \\  Now we can take the pullback on the morphism \\(h: P \\to A \\times A\\)  and \\(i: A \\to A \\times A\\) to obtain the equalizer. \\  Hence we see that for finite limits, we can reduce our assumptions to pullbacks and  a terminal object, giving rise to the final corollary. </p> <p> If a category has pullbacks and a terminal object, then it has all finite limits.  </p>"},{"location":"category_theory/Limits%20and%20Colimits./Preservation%20of%20Limits/","title":"5.4. Preservation of Limits","text":"<p> Let \\(F: J \\to C\\) be a diagram and suppose  \\(G: \\cc \\to \\dd\\) is a functor. If for every limit \\(\\Lim F\\) exists in \\(\\cc\\) with morphisms \\(u_i: C \\to F_i\\), we say \\(G\\) preserves limits if  \\(G(\\Lim F)\\) is  a limit with morphisms \\(G(u_i): G(C) \\to G(F_i)\\). Moreover,  we call such a functor a continuous functor. </p> <p>As an immediate consequence of the definition, it should be noted  that a composition of continuous functors is continuous. </p> <p>Below we see a visual definition of a continuous functor. </p> <p> There's one particular and important functor which is always continuous  in any category. </p> <p> Let \\(\\cc\\) be a small category. Then for each \\(C \\in \\cc\\),  the functor  \\[ \\hom_{\\cc}(C, -): \\cc \\to **Set** \\] <p>preserves limits. (Dually, the functor  \\(\\hom_{\\cc}(-, C) = \\hom_{\\cc}(C, -): \\cc\\op \\to **Set**\\) takes colimits to limits.) </p> <p> Let \\(F: J \\to \\mathcal{C}\\) be a diagram with a limiting object \\(\\text{Lim } F\\) equipped with the morphisms \\(\\sigma_i: \\text{Lim } F \\to F_i\\). Then applying the \\(\\text{Hom}_{\\mathcal{C}}(C, -)\\) functor to \\(\\text{Lim } F\\) and to  each \\(u_i\\), we realize it forms a cone in \\(**Set**\\).  \\  Now we show that \\(\\text{Hom}_{\\mathcal{C}}(C, \\text{Lim } F)\\), equipped with the morphisms  \\(\\sigma_{i*}\\), is a universal cone; that is, it is a limit.  Suppose that \\(X\\) is a set which forms a cone with the morphisms \\(\\tau_i: X \\to \\text{Hom}_{\\mathcal{C}}(C, F_i)\\).  \\  <p>Then for each \\(x \\in X\\),  we see that \\(\\tau_i(x) : C \\to F_i\\). The diagram above tells us that \\(u \\circ \\tau_i(x) = \\tau_j(x)\\) for each \\(x\\). Hence each \\(x \\in X\\) induces a  cone with apex \\(C\\) with morphisms \\(\\tau_i(x): C \\to F_i\\).  \\  However, \\(\\text{Lim } F\\) is the limit of \\(F: J \\to \\mathcal{C}\\). Therefore, there  exists a unique arrow \\(h_x: C \\to \\text{Lim } F\\) such that  \\(h_x \\circ \\sigma_i = \\tau_i(x)\\). Now we can uniquely define a function \\(: X \\to \\text{Hom}_{\\mathcal{C}}(C, \\text{Lim } F)\\) where  \\(h(x) = h_x: C \\to \\text{Lim } F\\), in such a way that the diagram below commutes. \\  Therefore, \\(\\text{Hom}_{\\mathcal{C}}(C, \\text{Lim } F)\\) is a limit in Set.  At this point, you may be wondering: What is the difference between  a functor which \"creates limits\" and one which preserves them?  We'll see that their definitions are different, but creating limits  is the same as preserving them </p> <p> Suppose \\(G: \\cc \\to \\dd\\) creates limits for \\(F: J \\to \\cc\\).  If \\(G \\circ F: J \\to \\dd\\) has a limit in \\(\\dd\\), then  \\(G\\) is continuous.  </p> <p> Suppose \\(F: J \\to \\cc\\) has limit \\(\\Lim F\\) in \\(\\cc\\) with morphisms  \\(v_i: \\Lim F \\to F_i\\) for each \\(i \\in J\\).  Further, suppose \\(G \\circ F: J \\to \\dd\\) has a limit  \\(\\Lim G \\circ F\\) with morphisms \\(u_i: \\Lim G \\circ F \\to G\\circ F_i\\).  <p>Since \\(G: \\cc \\to \\dd\\) creates limits, this implies  the existence of a limiting object \\(X\\) with morphisms  \\(\\sigma_i: X \\to F_i\\) for \\(F: J \\to C\\)  where \\(G(X) = \\Lim G\\circ F\\) and \\(G(\\sigma_i) = u_i\\).  However, limiting objects are unique (by their universal properties). As they must be isomorphic, there exists an isomorphism  \\(\\phi: X \\to \\Lim F\\) for which \\(v_i \\circ \\phi= \\sigma_i\\).  Thus we see that </p> \\[ G(\\Lim F) \\cong G(X) = \\Lim G \\circ F \\qquad  G(v_i \\circ \\phi) = G(\\sigma_i) = u_i. \\] <p>Therefore, \\(G\\) preserves limits and so is continuous.  </p> <p>We have the following as a corollary. </p> <p> Suppose \\(G: \\cc \\to \\dd\\) creates limits and \\(\\cc\\) is complete.  Then \\(\\dd\\) is complete and \\(G\\) preserves limits.  </p>"},{"location":"category_theory/Limits%20and%20Colimits./Subobjects%20and%20Quotient%20Objects/","title":"5.7. Subobjects and Quotient Objects","text":"<p>The entire point of category theory, contrary to its name, is to unify  mathematics. Mathematicians saw the same stories over and over again in algebra  and topology, and one day they got sick of it and decided to start naming the patterns  they were seeing. Mathematicians achieved a level of abstraction where we no longer really care about the objects,  but we want to study the morphisms between them.  However, in many categories, the objects are often things like groups,  rings, or topological spaces; hence there are subgroups, subrings,  and spaces with subset topologies which also exist inside categories we study.  This presents a challenge for category theory: how do we generalize the notion of  subgroups or subspaces if we always avoid explicit reference to the elements?</p> <p>It turns out that the correct way to go about this is to consider the philosophy of  sub-\"things\": whenever \\(S\\) is a sub-\"thing\" of \\(X\\), there usually exists a monomorphism</p> \\[ m: S \\to X. \\] <p>For example, in Set, \\(S \\subset X\\) implies that there's an injection \\(i: S \\to X\\); a monomorphism  is injective in Set, so this makes sense. In Top, if \\(S \\subset X\\) where \\(S\\) is given the subspace  topology, then the inclusion function \\(i: S \\to X\\) is continuous, so there does exist  a monomorphism \\(m: S \\to X\\) in Top. </p> <p>Thus we see that these monomorphisms give us sub-\"things,\" and so we might naively say  the set of all \"subobjects\" of an object \\(X\\) in a category \\(\\cc\\) is the set</p> \\[ \\text{Sub}_{\\cc}(X) = \\{S \\in \\text{Ob}(\\cc) \\mid \\exists f: S \\to X \\text{ with } f \\text{ monic }\\}. \\] <p>However, the space of all of  these monomorphisms is huge, and also repetitive. For example, in Set, if we have  \\(X = \\{1, 2, 3, 4, 5\\}\\), then there are all kinds of monomorphisms into \\(X\\):</p> <p> Each arrow is basically saying the same thing. How do we deal with this? Well, we can impose an equivalence relation on this space to obtain something smaller and more manageable.</p> <p>Let \\(A\\) an object of our category \\(\\cc\\). Consider monomorphisms \\(f: C \\to A\\) and  \\(g: D \\to A\\). Define the relation \\(\\le\\) on monomorphisms of this form where  \\begin{statement}{ProcessBlue!10} \\begin{minipage}{0.6\\textwidth}</p> \\[ f \\le g \\text{ if there exists an } h \\text{ where } f = g \\circ h. \\] <p>\\end{minipage}\u00a0 \\begin{minipage}{0.4\\textwidth} \\begin{tikzcd}[column sep = 1.6cm, row sep = 0.3cm] C \\arrow[dr, hookrightarrow, \"f\"] &amp; \\ &amp; A  \\ D \\arrow[ur,hookrightarrow, swap, \"g\"] \\arrow[uu, dashed, \"h\"] \\end{tikzcd} \\end{minipage} \\end{statement} for some monomorphism \\(h: D' \\to D\\). Note that if \\(f \\le g\\) and \\(f \\ge g\\),  then \\(C\\) and \\(D\\) are isomorphic (this is not true in general; this only true here  because \\(f, g\\) are monomorphisms). So we now have our equivalence relation: we say \\(f \\sim g\\) if  there exists an isomorphism \\(\\phi: D \\to C\\) which makes the above diagram commute. </p> <p> Let \\(\\cc\\) be a category and let \\(A\\) be an object. We say a subobject  of \\(A\\) is an equivalence class of monomorphisms \\(f: S \\to A\\) under the equivalence relation  \\(\\sim\\). We denote this space of equivalence classes as  \\[ \\text{Sub}_{\\cc}(A) = \\Big\\{[f] \\mid f:C \\to A \\text{ is a monomorphism} \\Big\\}. \\] <p></p> <p> Let \\(\\cc\\) be a category. An interesting application of subobjects occurs in functor categories. To illustrate this  we consider the functor category \\(**Set**^{\\cc}\\); that is, the category with functors  \\(F: \\cc \\to **Set**\\) whose morphisms are natural transformation \\(\\eta: F \\to G\\) between  such functors.  <p>If we play around with these functors long enough, we may ask the question: What happens when, for a functor \\(F: \\cc \\to **Set**\\), there is another  functor \\(G: \\cc \\to **Set**\\) such that </p> \\[ G(A) \\subset F(A)? \\] <p>Could we logically call \\(G\\) a \"subfunctor\" of \\(F\\)? We could with a little more work.  Because \\(G(A) \\subset F(A)\\),  we know that there exists a monomorphism (just an injection here) \\(i_A: G(A) \\to F(A)\\). Now a natural question to ask here is if this translates to a natural transformation. That is, does the diagram below commute? \\  The answer is no. This is because \\(G(f)\\) and \\(F(f)\\) could be two entirely different functions which do  two entirely different things to the same elements in different domains; however,  one way for this diagram to commute is if \\(G(f)\\) is \\(F(f)\\) restricted to  the set \\(G(A)\\). That is, if </p> \\[ G(f) = F(f)\\big|_{G(A)}. \\] <p>The diagram then commutes. But is this the only way to make it commute? Suppose with no assumption of \\(G(f)\\) that the diagram did commute. Then we can still make a morphism \\(F(f)\\big|_{G(A)}: G(A) \\to G(B)\\) to  get the commutative diagram  \\ </p> <p>Then we see that \\(i_B \\circ G(f) = i_B \\circ F(f)\\big|_{G(A)}\\). However,  \\(i_B\\) is a monomorphism,  so \\(G(f) = F(f)\\big|_{G(A)}\\). Hence the only way to  make the diagram commute is if \\(G(f)\\) is a restriction of \\(F(f)\\). </p> <p>Thus we could define \\(G: \\cc \\to **Set**\\) to be a subfunctor of \\(F: \\cc \\to **Set**\\)  if \\(G(A) \\subset F(A)\\) and \\(G(f: A \\to B) = F(f)\\big|_{G(A)}\\). Or, equivalently,  if \\(G(A) \\subset F(A)\\) and that this relation is natural. </p> <p>However, we can recover the same concept by applying subobjects to this functor category.  In this case, we can (with laziness) say a \\(G: \\cc \\to **Set**\\) is a subobject  of the functor \\(F: \\cc \\to **Set**\\)  in \\(**Set**^{\\cc}\\) if there exists a monic natural transformation  \\(\\eta: G \\to F\\). </p> <p>Unwrapping this definition, we see that a monic natural transformation in this  case is just one where each morphism \\(\\eta_A: G(A) \\to F(A)\\) is a monomorphism, which, in our case,  just means an inclusion function, such that the necessary square commutes. However, we  already showed that we get the commutativity of the necessary square if and only if  \\(G(f: A \\to B) = F(f)\\big|_{G(A)}\\). </p> <p>Hence we have recovered the same concept of a subfunctor in two different ones;  one in which we followed our intuition, and one in which we blinded applied the concept of a  subobject in the functor category \\(**Set**^{\\cc}\\). </p> <p>The previous example allows us to make the definition:</p> <p> Let \\(\\cc, \\dd\\) be categories. Then a functor \\(G: \\cc \\to \\dd\\) is a subfunctor of \\(F: \\dd \\to \\cc\\) if \\(G\\) is a subobject of \\(F\\)  in the functor category \\(\\dd^{\\cc}\\). </p> <p>Now, perhaps unsurprisingly, the entire process above can be dualized. When we dualize,  however, we obtain a generalization of the concept of quotient objects. Instead of just dualizing and  being boring, we'll motivate why we'd even care for such a dual concept.  \\</p> <p>In interesting categories such as Ab or Top, we not only have  subgroups and subspaces, but we also have quotient groups and quotient spaces.  For the case of abelian groups, we can, for any such group \\(G\\), consider any  subgroup \\(H \\le G\\) and construct the quotient group \\(G/H\\). This comes with a  a nice epimorphism \\(\\pi: G \\to G/H\\) where \\(g \\mapsto g + H\\). </p> <p>For topological spaces \\((X, \\tau)\\) in Top, we can define an equivalence  relation \\(\\sim\\) on \\(X\\) and consider the topological space \\((X/\\sim, \\tau')\\)  such that \\(\\tau'\\) is the topology where a set \\(U\\) is open if \\(\\{x \\mid [x] \\in U\\}\\) is open in \\(\\tau\\). We can then equip ourselves with a continuous projection map  \\(\\pi: X \\to X/\\sim\\), which is also an epimorphism. </p> <p>With these few examples, we see that it is worthwhile to generalize the concept  of quotient objects; to do this however requires no explicit mention of the elements of  the objects of the category. However, we can maintain the philosophy seen in the previous  two examples to generalize the concept.</p> <p>For an object \\(A\\) in a category \\(\\cc\\), we consider all epimorphisms</p> \\[ e: A \\to Q \\] <p>and call objects such objects \\(Q\\) as quotient objects. Again, the space of these  objects is too large, so we instead consider ordering relation </p> <p>\\begin{statement}{ProcessBlue!10} \\begin{minipage}{0.6\\textwidth}</p> \\[ f \\le g \\text{ if there exists an } h \\text{ where } f = h \\circ g. \\] <p>\\end{minipage}\u00a0 \\begin{minipage}{0.4\\textwidth} \\begin{tikzcd}[column sep = 1.6cm, row sep = 0.3cm] &amp; C  \\ A  \\arrow[ur, -&gt;&gt;, \"f\"] \\arrow[dr, -&gt;&gt;, swap, \"g\"] &amp; \\ &amp; D \\arrow[uu, dashed, swap, \"h\"] \\end{tikzcd} \\end{minipage} \\end{statement} Observing that \\(f \\le g\\) and \\(g \\le f\\) together imply that \\(C \\cong D\\), we see that  we may construct an equivalence relation \\(\\sim\\) where $f \\sim g $ if there exists an isomorphism  \\(\\phi: D \\to C\\) such that \\(f = \\phi \\circ g\\). We can now outline a clear definition.</p> <p> Let \\(\\cc\\) be a category and let \\(A\\) be an object. We say a quotient object of  \\(A\\) is an equivalence class of morphisms \\(f: A \\to Q\\). We then denote  \\[ \\text{Quot}_\\cc(A) = \\Big\\{ [f] \\mid f: A \\to Q \\text{ is an epimorphism }\\Big\\}. \\] <p></p> <p> A quotient object in Cat is a quotient category (from chapter 2) </p>"},{"location":"category_theory/Monoidal%20Categories/Braided%20and%20Symmetric%20Monoidal%20Categories/","title":"7.5. Braided and Symmetric Monoidal Categories","text":"<p>Braided and symmetric monoidal categories serve as some of the most fruitful and  most studied environments of monoidal categories. The formulation of these categories  may seem mysterious and random, but they have been recognized as important  in their applications to physics. Specifically, braided monoidal categories were  first defined by Joyal-Street in an attempt to abstract the solutions to the  Yang-Baxter equation, an important equation of matrices in statistical mechanics.  It turns out that braided monoidal categories are exactly the categorical environment  one needs to describe the category of representations of a Hopf algebra \\(**Rep**(H)\\).  This then allows us a machine which produces solutions to the Yang-Baxter equation, ultimately  letting us describe families of such solutions. But it gets even more interesting:  the Yang-Baxter equation turns out to be the necessary criteria to establish a representation  of the Braid group; such a representation is a knot invariant, so this is something of interest  to both mathematicians and physicists. </p> <p>Before we dive into what exactly braided monoidal categories are, we'll introduce  the concept of braids. </p> <p> The \\(n\\)-th braid group \\(B_n\\) consists of braids on \\(n\\)-strands whose  group product is braid composition. More rigorously,  \\[ B_n =  \\left&lt; \\sigma_1, \\dots, \\sigma_n, \\sigma_1^{-1}, \\dots, \\sigma_n^{-1} \\mid (1), (2)\\right&gt; \\] <p>where \\((1), (2)\\) are generator relations described below. </p> <ul> <li> <p>[1.] \\(\\sigma_i\\sigma_j = \\sigma_j\\sigma_i\\) whenever \\(|i - j|&gt;1\\)</p> </li> <li> <p>[2.] \\(\\sigma_{i+1}\\sigma_i\\sigma_{i+1} = \\sigma_{i}\\sigma_{i+1}\\sigma_i\\).</p> </li> </ul> <p> Relations (1) and (2) are imposed in order to reflect physical reality. Below the relations are pictured on a three strands.</p> <p> The first two braids represent \\(\\sigma_3\\sigma_1\\) and \\(\\sigma_1\\sigma_3\\).  Clearly, these are physically equal. Note however this would not work if they were adjacent, i.e.,  \\(\\sigma_2\\sigma_1\\ne\\sigma_2\\sigma_1\\). Hence we set \\(\\sigma_i\\sigma_j=\\sigma_j\\sigma_i\\)  for \\(|i - j| &gt; 1\\). For the second pair of braids, it may take some staring to see that they are physically equal.  As we shall see, the relation \\(\\sigma_{i+1}\\sigma_i\\sigma_{i+1} = \\sigma_{i}\\sigma_{i+1}\\sigma_i\\), called the braid relation, is of deep importance.</p> <p> A Braided Monoidal Category \\(\\cc\\) is a monoidal  category \\((\\cc, \\otimes, I)\\) equipped additionally with a natural transformation,  know as the \"twist\" morphism \\begin{statement}{ProcessBlue!10} \\[ \\sigma_{A,B}: A\\otimes B \\to B\\otimes A\\qquad **(Twist Morphism)** \\] <p>\\end{statement}  such that the following diagrams commute for all objects \\(A, B, C\\) of \\(\\cc\\).  \\begin{statement}{ProcessBlue!10} \\  end{statement} </p> <p>Note that just because we have a twist morphism, it is not necessarily the case  that \\(\\sigma_{B,A}\\circ\\sigma_{A,B} = 1_{A\\otimes B}\\). That is, applying the twist morphism  twice is not guaranteed to give you back the identity. This case is treated separately.  </p> <p> The canonical example of a braided monoidal category is the braid category \\(\\mathbb{B}\\). This is the category where: \\begin{description} \\item[Objects.] All integers \\(n \\ge 0\\). \\item[Morphisms.] For any two integers \\(m, n\\), we have that  \\[ \\hom_{\\mathbb{B}}(n,m) = \\begin{cases} B_n &amp; \\text{if } n = m\\\\ \\varnothing &amp; \\text{if } n \\ne m \\end{cases} \\] <p>\\end{description}  Composition in this category is simply braid composition. We can introduce a tensor  product \\(\\otimes\\) on \\(\\mathbb{B}\\) where on objects \\(n \\otimes m = n + m\\)  while on morphisms \\(\\alpha \\otimes \\beta\\) is the direct sum braid. The direct  sum braid is simply the braid obtained by placing two braids side-by-side. \\  With an identity object being the empty braid, we see that \\(\\mathbb{B}\\)  is a strict monoidal category. The associators and unitors are simply identity morphisms. However, this category also have a natural braiding structure. For any two objects  \\(n,m\\), introduce the braiding </p> \\[ \\sigma_{n,m}: n + m \\isomarrow m + n \\] <p>where on objects the addition is simply permuted; on morphisms, however,  \\(\\sigma_{n,m}\\) corresponds to the braid of length \\(n + m\\) as below. \\begin{figure}[h!] \\centering \\def\\height{5} \\def\\ptsep{0.7} \\def\\sep{5.333} \\begin{tikzpicture} \\tikzset{ position label/.style={ below = 3pt, text height = 2ex, text depth = 1ex } }</p> <p>\\draw[thick, decoration={brace}, decorate] (-0.2,0.3) -- (3,0.3); \\draw[thick, decoration={brace}, decorate] (3.6,0.3) -- (6.7,0.3); \\node at (1.4, 0.6) {\\(m\\)}; \\node at (5.15, 0.6) {\\(n\\)};</p> <p>\\foreach \\x in {5.333, 6.333, 7.333}{ \\filldraw (\\x\\ptsep, 0) circle (0.07cm);  \\draw (\\x\\ptsep,0) -- (\\x\\ptsep - 5.333\\ptsep, -\\height); }</p> <p>\\node at (8.333\\ptsep, 0) {\\(\\cdots\\)}; \\filldraw (9.333\\ptsep,0) circle (0.07cm);  \\draw (9.333\\ptsep,0) -- (9.333\\ptsep - \\sep*\\ptsep, -\\height);</p> <p>\\foreach \\x in {0, 1, 2}{ \\filldraw (\\x\\ptsep, 0) circle (0.07cm);  \\draw[line width = 2mm, white] (\\x\\ptsep,0) -- (\\x\\ptsep +\\sep\\ptsep, -\\height);  \\draw (\\x\\ptsep,0) -- (\\x\\ptsep +\\sep*\\ptsep, -\\height);  }</p> <p>\\node at (3\\ptsep, 0) {\\(\\cdots\\)}; \\draw (4\\ptsep,0) -- (4\\ptsep + \\sep\\ptsep, -\\height); \\draw[line width = 2mm, white] (4\\ptsep,0) -- (4\\ptsep + \\sep\\ptsep, -\\height); \\draw (4\\ptsep,0) -- (4\\ptsep + \\sep\\ptsep, -\\height); \\filldraw (4*\\ptsep,0) circle (0.07cm); </p> <p>\\foreach \\x in {0, 1, 2}{ \\filldraw (\\x*\\ptsep, 0) circle (0.07cm);  }</p> <p>\\foreach \\x in {0, 1, 2}{ \\filldraw (\\x*\\ptsep, -\\height) circle (0.07cm);  }</p> <p>\\node at (3\\ptsep, -\\height) {\\(\\cdots\\)}; \\filldraw (4\\ptsep,-\\height) circle (0.07cm); </p> <p>\\foreach \\x in {0, 1, 2}{ \\filldraw (\\x*\\ptsep, -\\height) circle (0.07cm);  }</p> <p>\\foreach \\x in {5.333, 6.333, 7.333}{ \\filldraw (\\x*\\ptsep, -\\height) circle (0.07cm);  }</p> <p>\\node at (8.333\\ptsep, -\\height) {\\(\\cdots\\)}; \\filldraw (9.333\\ptsep,-\\height) circle (0.07cm); </p> <p>\\node at (5,-1.2) {\\rotatebox{-45}{\\(\\cdots\\)}}; \\node at (2.5,-4.3) {\\rotatebox{-45}{\\(\\cdots\\)}}; \\node at (2.6,-0.7) {\\rotatebox{45}{\\(\\cdots\\)}}; \\node at (5.1,-4) {\\rotatebox{45}{\\(\\cdots\\)}}; \\end{tikzpicture} \\end{figure}</p> <p>It is a simple exercise to show that this satisfies the hexagon axioms;  the task is simplified due to the fact that the associators are identities.  While this category may seem like a boring example, it plays a critical role  in demonstrating coherence for braided monoidal categories, something we will do later. </p> <p> Let \\(**GrMod**_R\\) be the category of graded \\(R\\)-modules  \\(M = \\{M_n\\}_{n = 1}^{\\infty}\\). Recall from  \\ref{example_graded_R_modules} That \\(**GrMod**_R\\) forms  a monoidal category. The tensor product  of two graded \\(R\\) modules \\(M =\\{M_n\\}_{n=1}^{\\infty}\\) and \\(P = \\{P_n\\}_{n= 1}^{\\infty}\\) is the graded \\(R\\)-module \\(M\\otimes P\\) whose \\(n\\)-th level is given by \\[ (M \\otimes P)_n = \\bigoplus_{i + j = n}M_i \\otimes P_j. \\] <p>We can additionally introduce a braiding on this category for each invertible elements \\(k \\in R\\); specifically, we define the braiding  \\(\\sigma_{M,P}: M \\otimes P \\to P \\otimes M\\) to be the graded module homomorphism whose \\(n\\)-th degree is </p> \\[\\begin{align*}  (\\sigma_{M, P})_n: &amp;\\bigoplus_{i + j = n}M_i \\otimes P_j \\to  \\bigoplus_{i + j = n} P_j \\otimes M_i\\\\ &amp;(m \\otimes p) \\longmapsto k^{ij}p \\otimes m \\end{align*}\\] <p>whenever \\(m \\in M_i\\) and \\(p \\in P_j\\).  Observe that with this braiding we get that \\  which clearly commutes. The second hexagon axiom is also easily seen to be satisfied: \\  Thus we see that \\(**GrMod**_R\\) is more than just a monoidal category; each invertible  element of \\(R\\) induces a braiding, making it a braided monoidal category as well. </p> <p> If \\(M\\) is monoidal, we can recall from  Example \\ref{example_monoidal_functor_category} that the functor category  \\(\\cc^M\\) is also monoidal. If additionally we have that \\(M\\) is braided with  a braiding \\(\\sigma_{A,B}: A\\otimes B \\to B \\otimes A\\), then we can extend this to a braiding on the functor category of \\(\\cc^M\\)  by defining, for two functors \\(F,G: \\cc \\to M\\), the natural transformation \\[ \\beta_{F,G}: F \\otimes G \\to G \\otimes F  \\] <p>defined pointwise for each \\(A \\in \\cc\\) as the morphism</p> \\[ (\\beta_{F,G})_A = \\sigma_{F(A), G(A)}: F(A)\\otimes G(A) \\isomarrow G(A) \\otimes F(A).  \\] <p>One can then check that this natural transformation satisfies the braided hexagon  axioms since the braiding \\(\\sigma\\) in \\(M\\) does, so that \\(\\cc^M\\) is additionally braided  if \\(M\\) is additionally braided.  </p> <p> A Symmetric Monoidal Category \\(\\cc\\) is a braided monoidal category  such that, for the twist morphism,  \\[ \\sigma_{B,A}\\circ\\sigma_{A,B} = 1_{A\\otimes B}. \\] <p></p> <p>Symmetric monoidal categories are basically monoidal categories which collapse the  information which braided monoidal categories have the potential to encode.  Their environment is much simpler, but at the cost of information.</p> <p> Recall from the previous examples that \\(**GrMod**_R\\) can be treated  as a braided monoidal category. A braiding is given an invertible element  \\(r \\in R\\). However, consider the idempotent elements of this ring, i.e., the  elements \\(r \\in R\\) such that \\(r^2 = 1\\). Then we see that these elements not only give  rise to braidings \\[\\begin{align*}  (\\sigma_{M, P})_n: &amp;\\bigoplus_{i + j = n}M_i \\otimes P_j \\to  \\bigoplus_{i + j = n} P_j \\otimes M_i\\\\ &amp;(m \\otimes p) \\longmapsto k^{ij}p \\otimes m \\end{align*}\\] <p>but these braidings have the property that \\(\\sigma_{M,P}\\circ \\sigma_{P,M} = 1_{M\\otimes P}\\),  since \\(r=1\\). Hence the category of graded modules may be specially treated as symmetric monoidal categories  whenever there is an idempotent element of the ring \\(R\\). </p> <p> Recall from \\ref{example:permutation_category} that the permutation category  \\(\\mathbb{P}\\) forms a monoidal category where objects are nonnegative integers and  homsets are given by the symmetric groups. The monoidal product \\(\\otimes\\)  simply sums the object, while two permutations \\(\\tau \\in S_n\\) and \\(\\rho\\in S_m\\) are sent to the direct sum permutation \\(\\tau\\otimes \\rho \\in S_{n+m}\\) (this permutation  simply horizontally stacks).  <p>In this category, we can introduce a symmetric braiding \\(\\sigma_{n,m}: n+m \\to m+n\\) to be the unique permutation \\(\\sigma_{n,m} \\in S_{n+m}\\) pictured below.  \\  One thing to notice is that this is the underlying permutation of braid given in  Figure \\ref{figure:braiding_on_braid_category}. With the existence of this element of \\(S_{n+m}\\) for every pair of objects  \\(n,m\\) in \\(\\mathbb{P}\\), we see that the permutation category is actually symmetric monoidal. </p> <p> A PROP, an acronym coined by Mac Lane  for \"Product and Permutation Category\",  is a symmetric monoidal category \\(\\mathbb{P}\\) containing  the category \\((\\mathbb{N}, 0, +)\\).  </p> <p> Consider the category \\(**FinSet**\\), where the objects  are natural numbers \\(n\\) and a morphism \\(f: n \\to m\\) is a function  from a set of size \\(n\\) to one of size \\(m\\).  <p>Here, we necessarily include \\(0\\) as an object; this denotes the empty set.  First we demonstrate that this is monoidal.  Let \\(n, m\\) be any integers. Then we'll show that \\(+: **FinSet**\\times**Finset** \\to **FinSet**\\)  is a bifunctor. First, we acknowledge that \\(n + m \\in **FinSet**\\). </p> <p>Next, consider the set of morphisms</p> \\[\\begin{align*} h&amp;: k \\to n \\qquad f: n \\to n'\\\\ j&amp;: l \\to m \\qquad g: m \\to m'. \\end{align*}\\] <p>Let \\(S_k\\) be the set of \\(k\\) elements. Now since \\(f, g\\) are functions,  we know that \\(f: S_n \\to S_{n'}\\) and \\(g: S_m \\to S_{m'}\\) for some sets in  Set.Then we can define \\(f + g: (n + n') \\to (m + m')\\) to be the function  in Set where </p> \\[ f + g: S_n \\amalg S_{n'} \\to S_m \\amalg S_{m'}. \\] <p>where </p> \\[ (f + g)(x, i) =  \\begin{cases} (f(x), 0) \\quad \\text{ if } i = 0\\\\ (g(x), 1) \\quad \\text{ if } i = 1. \\end{cases} \\] <p>Hence \\(f + g\\) makes sense in FinSet as morphism \\(f + g: (n + n') \\to (m + m')\\). </p> <p>Now consider the morphisms \\(f \\circ h\\) and \\(g \\circ j\\). Observe that  \\(f\\circ h + g \\circ j : k + l \\to n' + m'\\). This is then the function </p> \\[ f\\circ h + g \\circ j: S_k \\amalg S_{l} \\to S_{n'} \\amalg S_{m'} \\] <p>but note that </p> \\[\\begin{align*} f\\circ h + g \\circ j: S_k \\amalg S_{l} \\to S_{n'} \\amalg S_{m'} =    (f + g)\\circ(h + j) \\end{align*}\\] <p>Hence we must have that \\((f + g)\\circ(h + j) = f\\circ h + g \\circ j\\), so that  we have that \\(+\\) is a bifunctor. </p> <p>Now we show that this is a monoidal category. Define the natural isomorphisms </p> \\[\\begin{align*} \\alpha_{n,m,p} &amp;: n + (m + p) \\isomarrow (n + m) + p\\\\ \\lambda_n &amp;: 0 + n \\isomarrow n\\\\ \\rho_n &amp;: n + 0 \\isomarrow n. \\end{align*}\\] <p>We can describe these functions in further detail.  Observe that \\(\\alpha_{n, m, p}\\) can be realized to be a function where </p> \\[ \\alpha_{n,m,p}: S_n\\amalg(S_m \\amalg S_p) \\isomarrow (S_n \\amalg S_m)\\amalg S_p. \\] <p>Elements of \\(S_n \\amalg(S_m \\amalg S_p)\\) will be either \\((x, 0)\\)  where \\(x \\in S_n\\), or \\((x, 1)\\) where \\(x \\in S_m\\amalg S_p\\). In turn, the elements  of this set are of the form \\((y, 0)\\) where \\(y \\in S_m\\) and \\((y, 1)\\) where  \\(y \\in S_p\\). </p> <p>On the other hand, elements of \\((S_n \\amalg S_m)\\amalg S_p\\) are of the form  \\((x', 0)\\) if \\(x' \\in S_n \\amalg S_m\\) or are of the form \\((x', 1)\\) if \\(x' \\in S_p\\).  Furthermore, elements of \\(S_n\\amalg S_m\\) are of the form \\((y', 0)\\) if \\(y' \\in S_n\\) \\ and \\((y', 1)\\) if \\(y' \\in S_m\\).</p> <p>Now we can explicitly define \\(\\alpha_{n,m,p}\\) as </p> \\[\\begin{align} \\alpha_{n,m,p}(x, i) =  \\begin{cases} ((x, 0), 0) \\qquad &amp;\\text{ if } i = 0\\\\ ((y, 1), 0) \\qquad &amp;\\text{ if } i = 1 \\text{ and } x = (y, 0)\\\\ (y, 1) \\qquad &amp;\\text{ if } i = 1 \\text{ and } x = (y, 1) \\end{cases} \\end{align}\\] <p>and \\(\\lambda\\) as </p> \\[\\begin{align*} \\lambda_n(x, 1) = x \\end{align*}\\] <p>and \\(\\rho\\) as </p> \\[\\begin{align*} \\rho_n(x, 0) = x. \\end{align*}\\] <p>Note for both \\(\\lambda\\) and \\(\\rho\\), there is only one case for \\((x ,i)\\) since  for \\(\\lambda\\), \\(i\\) is never \\(0\\) and for \\(\\rho\\), \\(i\\) is never 1.  </p> <p>All of these establish a bijection, and hence an isomorphism.  Now to demonstrate that they are natural, consider \\(f: n \\to n'\\),  \\(g: m \\to m'\\) and \\(h: p \\to p'\\). First, we'll want to show that the diagram  \\  commutes, which we can do by a case-by-case basis. First we follow the path</p> \\[\\begin{align*} [(f + g) + h] \\circ \\alpha_{n, m, p}:  S_n\\amalg(S_m \\amalg S_p) \\to (S_{n'}\\amalg S_{m'})\\amalg S_{p'}. \\end{align*}\\] <p>and then show it is equivalent to the other path. \\begin{description} \\item[\\(\\bm{i = 0}\\)] If the input is \\((x, 0)\\), we see that \\(\\alpha_{n,m,p}(x, i) = ((x,0),0)\\). If this is fed into \\((f + g) + h\\), the output will be \\((f + g)(x, 0)\\), whose output  will be \\(((f(x), 0), 0)\\). </p> <p>However, suppose we first put \\((x, 0)\\) into \\(f+ (g + h)\\). Then  we would have directly obtain \\((f(x), 0)\\). Feeding this into \\(\\alpha_{n', m', p'}\\), we would  get \\(((f(x), 0), 0)\\). Hence we obtain naturality in this case. </p> <p>\\item[\\(\\bm{i = 1}\\).] Suppose now the input is \\((x, 1)\\). Then either \\(x = (y, 0)\\) with \\(y \\in S_m\\)  or \\((y, 1)\\) where \\(y \\in S_p\\). </p> <p>\\begin{description} \\item[\\(\\bm{y \\in S_m}\\).] Suppose \\(x = (y, 0)\\). Then we see that  \\(\\alpha_{n,m,p}(x, 1) = ((y, 1), 0)\\). Plugging this into \\(( f+ g) + h\\), we  get </p> \\[ [ ( f+ g) + h]((y, 1), 0) = ([f + g](y, 1), 0) = ((g(y), 1), 0). \\] <p>However, we also could have obtained this value by first starting with \\(f + (g + h)\\). In this case, </p> \\[ [f + (g + h)]((y, 0), 1) = ([g + h](y, 0), 1) = ((g(y), 0), 1).  \\] <p>Plugging this into \\(\\alpha_{n',m',p'}\\), we then get that </p> \\[ \\alpha_{n',m',p'}((g(y), 0), 1) = ((g(y), 1), 0). \\] <p>Hence the two paths are equivalent. </p> <p>\\item[\\(\\bm{y \\in S_p}\\).] Suppose \\(x = (y, 1)\\), Then we have that  \\(\\alpha_{n, m, p}((y,1), 1) = (y, 1)\\). Sending this into \\((f + g)+ h\\), we get </p> \\[ [(f + g) + h](y, 1) = (h(y), 1). \\] <p>However, we could have achieved this value by first plugging \\(((y, 1),1)\\) into  \\(f + (g + h)\\):</p> \\[ [f + (g + h)]((y, 1), 1) = ([g + h](y, 1), 1) = ((h(y), 1), 1). \\] <p>Then sending this into \\(\\alpha_{n',m',p'}\\), we get </p> \\[ \\alpha_{n',m',p'}((h(y), 1), 1) = (h(y), 1). \\] <p>Thus the two paths are equivalent. \\end{description} Hence we see that this diagram does commute, so that \\(\\alpha\\) is natural.  \\end{description} [Show naturality works for \\(\\lambda\\) and \\(\\rho\\).]</p> <p>Now we show that these natural isomorphisms satisfy the monoidal properties.  Specifically, we'll show that the diagram  \\  must commute. To do this, we consider how these functions are realized in Set.  If we consider \\((x, i) \\in S_n\\amalg(\\varnothing \\amalg S_m)\\), we see that  we have two cases to consider.  \\begin{description} \\item[\\(\\bm{i = 0}\\).] If \\(i = 0\\), then we see that \\(\\alpha_{n, 0, m}(x, 0) = ((x, 0), 0)\\). Sending  this into \\(\\rho_n + 1_m\\), we get that \\([\\rho_m + 1_m]((x, 0), 0) = (\\rho(x, 0), 0) = (x, 0)\\). </p> <p>On the other hand, we could obtain this value by directly sending \\((x, 0)\\) into  \\(1_n + \\lambda_m\\). Observe that \\([1_n + \\lambda_m](x, 0) = (1_n(x), 0) = (x, 0)\\).  Hence the diagram commutes for this case. </p> <p>\\item[\\(\\bm{i = 1}\\).] If \\(i = 1\\), then our element is of the form  \\((x, 1)\\). However, we know that \\(x = ((x, 1), 0)\\), since \\((x, 1) \\in 0 + m\\).  Thus observe that \\(\\alpha_{n, 0, m}((x, 1), 1) = (x, 1)\\). Consequently,  we get that \\([\\rho_n + 1_m](x, 1) = (1_m(x), 1) = (x, 1)\\). </p> <p>On the other hand, we can start instead be evaluating  \\([1_n + \\lambda_m]((x, 1), 1) = (\\lambda(x, 1), 1) = (x, 1)\\). Hence the diagram commutes  in this case. \\end{description} Thus we see that this diagram holds for all naturals \\(n, m\\). </p>"},{"location":"category_theory/Monoidal%20Categories/Coherence%20for%20Braided%20Monoidal%20Categories/","title":"7.6. Coherence for Braided Monoidal Categories","text":"<p>We saw with monoidal categories that ultimately everything we were saying made sense.  That is, we saw that our definition does not give us an contradictions, and that we  can obtain a significant coherence result which ultimately allows us to not worry  about the particular parenthesization of a monoidal product. Further, we saw that  diagrams freely built from associators and unitors were all commutative. </p> <p>With braided monoidal categories we can get a similar statement. This time, however,  it is a bit weaker, although it is nevertheless extremely useful. It was Joyal and Street  in the 1993 paper who both first proved the coherence for braided monoidal categories.  Their work heavily relies on the work of G.M. Kelly, and they use very slick,  higher categorical tricks.</p> <p>In this section, we spell out those tricks. </p> <p>[Joyal-Street] Let \\(\\aa\\) be a category with \\(\\vv\\) a monoidal category. Suppose \\(T: \\aa \\to \\vv\\) is a functor. We define a Yang-Baxter operator to be a family of isomorphisms \\[ y_{A,B}: T(A)\\otimes T(B) \\isomarrow T(B) \\otimes T(A). \\] <p>for each \\(A, B \\in \\aa\\) such that the diagram below commutes. such that the diagram below commutes. </p> <p></p> <p>Note that here we omit the associators although they are implicitly included in the diagram.  Note also that, for any functor  \\(T: \\aa \\to \\vv\\) with \\(\\vv\\) a braided monoidal category, \\(T\\) trivially has a Yang-Baxter operator \\(y\\) where we set </p> \\[ y_{A,B} = \\sigma_{T(A),T(B)}. \\] <p>Before we move forward we introduce a notion that can be found in \\cite{Joyal1993BraidedTC}, originally from \\cite{kelly_clubs}. For our purposes, we will denote the category obtained via disjoint unions of the symmetric groups \\(S_n\\) as \\(\\mathbb{P}\\). That is, the objects of \\(\\mathbb{P}\\) are natural numbers and </p> \\[ \\hom_{\\mathbb{P}}(n,m) = \\begin{cases} S_n &amp; \\text{if } n = m\\\\ \\varnothing &amp; \\text{if } n \\ne m  \\end{cases} \\] <p> Let \\(\\aa\\) be a category and suppose suppose \\(\\dd \\in **Cat**/\\mathbb{P}\\). That is, \\(\\dd\\) is a category with an associated functor \\(\\Gamma: \\dd \\to \\mathbb{P}\\). Then we define the category \\(\\dd\\int\\aa\\) where  \\begin{description} \\item[Objects.] Finite strings \\([A_1, A_2, \\dots, A_n]\\) with \\(A_i \\in \\aa\\) \\item[Morphisms.] For two strings \\([A_1, \\dots, A_n]\\) and \\([B_1, \\dots, B_n]\\), denoted as \\([A_i]\\) and \\([B_i]\\), \\[ \\hom_{\\dd\\int\\aa}\\Big([A_i],[B_i]\\Big) = \\Big\\{(\\alpha, f_1, \\dots, f_n) \\mid f_i \\in \\hom_{\\aa}(A_i, B_{\\sigma(i)})  \\Big\\} \\] <p>Here \\(\\alpha\\) is a morphism of \\(\\dd\\) such that \\(\\Gamma(\\alpha) = \\sigma \\in S_n\\). Finally, we allow no morphisms between two different strings of different length. \\end{description}  For any category \\(\\aa\\), there exists a natural inclusion functor </p> \\[\\begin{gather*} i_{\\aa}: \\aa \\to \\dd\\int\\aa\\\\ i_{\\aa}(A) = [A] \\qquad i_{\\aa}(f: A \\to B) = (e_1,f) : [A] \\to [B] \\end{gather*}\\] <p>where \\(e_1\\) is the sole element of \\(S_1\\). This functor will be useful for us later. Next we formalize the following category which can be thought of as a generalized functor category. </p> <p> Let \\(\\aa, \\bb\\) be categories. Denote the category \\(\\{\\aa, \\bb\\}\\) as the category with objects \\((n, F: \\aa^n \\to \\bb)\\)  whose morphisms are  \\[ \\hom_{\\{\\aa, \\bb\\}}\\Big((n, T), (m,S)\\Big) = \\begin{cases}    \\{(\\sigma, \\eta: \\sigma\\cdot T \\to S) \\} &amp; \\text{if } n = m\\\\ \\varnothing &amp; \\text{if } n \\ne m. \\end{cases} \\] <p>Here \\(\\sigma \\in S_n\\), and \\(\\eta: \\sigma \\cdot T \\to S\\) is a natural transformation from the functor \\(\\sigma \\cdot T\\) defined pointwise as </p> \\[ \\sigma \\cdot T(A_1, A_2, \\dots, A_n) = T(A_{\\sigma(1)}, \\dots, A_{\\sigma(n)}) \\] <p>to the functor \\(S\\).  </p> <p>There are two things we need to say about this category. First, for any generalized functor category \\(\\{\\aa, \\vv\\}\\), there exists a a projection functor  \\(\\Gamma: \\{\\aa, \\bb\\} \\to \\mathbb{P}\\) defined on objects and morphisms as</p> \\[\\begin{gather*} \\Gamma(n, T: \\aa^n \\to \\bb) = n  \\qquad  \\Gamma(\\sigma, \\eta: \\sigma\\cdot T \\to S) =  \\sigma. \\end{gather*}\\] <p>Hence we see that each category \\(\\{\\aa, \\bb\\}\\) is actually a member of \\(**Cat**/\\mathbb{P}\\), because it always comes equipped with a functor into \\(\\mathbb{P}\\).</p> <p>Second, if \\(\\vv\\) is a strict monoidal category, then so is \\(\\{\\aa, \\vv\\}\\). One can see this by defining for  two functors \\(T: \\aa^n \\to \\vv\\) and \\(S: \\aa^m \\to \\vv\\) the functor \\(T \\otimes S: \\aa^{n+m} \\to \\vv\\) which is a functor that can be defined pointwise as </p> \\[ (T \\otimes S)(A_1, \\dots, A_{n+m}) = T(A_1, \\dots, A_n)\\otimes S(A_{n+1}, \\dots, A_{n+m}). \\] <p>Thus if \\(\\vv\\) is strict, then so it \\(\\{\\aa, \\vv\\}\\).</p> <p>What is useful about this construction is that  Kelly showed that the functors </p> \\[ (-)\\int A : **Cat**/\\mathbb{P} \\to **Cat**  \\qquad  \\{A, (-) \\}: **Cat**\\to **Cat**/\\mathbb{P} \\] <p>form an adjunction.  We use this in the next proposition, which is also aided by the following lemma. </p> <p> Let \\(\\vv\\) be a strict monoidal category.  Suppose \\(T: **1** \\to \\vv\\) has a Yang-Baxter operator \\(y\\). Then there exists a unique strict monoidal functor \\(T': \\mathbb{B} \\to \\vv\\) such that the diagram below commutes.  \\  Further, we have that \\(T'(\\sigma) = y\\). </p> <p>\\begin{proof} Denote the element of \\(\\mathbf{1}\\) as \\(\\bullet\\). Then \\(T(\\bullet) = X\\) for some \\(X \\in \\vv\\). Towards a definition of \\(T'\\), let  \\(T': \\mathbb{B} \\to \\vv\\)  be defined on objects as \\(T'(1) = X\\). If we force \\(T'\\) to be strict,  this will define its value on all objects of  \\(\\mathbb{B}\\). On morphisms, first observe that  each \\(\\beta \\in B_n\\) can be expressed in terms of its  generators \\(\\sigma_i\\). Hence it suffices to define the action of  \\(T'\\) on a generator \\(\\sigma_i\\), and we do this naturally as:</p> \\[\\begin{align*} T'(\\sigma_i) &amp;= 1_X^{\\otimes(i-1)}\\otimes y_{X,X}\\otimes 1_X^{\\otimes(n-i-1)}: X^{\\otimes n} \\to X^{\\otimes n} \\end{align*}\\] <p>We then define \\(T'(\\beta)\\) as the iterative composite over the generators.  We are then left to check that the relations of \\(\\mathbb{B}\\) are preserved (which they are).  This then allows us to define \\(T': \\mathbb{B} \\to \\vv\\) to be a unique, well defined strict  monoidal functor which allows the diagram to commute. \\end{proof}</p> <p> Let \\(\\vv\\) be a strict monoidal category, and suppose we have a functor \\(T: \\aa \\to \\vv\\) with associated Yang-Baxter operator \\(y\\). Let \\(z\\) be the Yang-Baxter operator on \\(i_{\\aa}: \\aa \\to \\mathbb{B}\\int\\aa\\). Then there exists a unique strict monoidal functor \\(T': \\mathbb{B}\\int\\aa \\to \\vv\\) such that the diagram \\  commutes and that \\(T'(y') = y\\).  </p> <p>\\begin{proof} Recall that \\(\\{\\aa, \\vv\\}\\) is a strict monoidal category if \\(\\vv\\) is. Consider again the one point category \\(\\mathbf{1}\\) and construct functors \\(F_S: \\mathbf{1} \\to \\{\\aa, \\vv\\}\\)  and \\(j: \\mathbf{1} \\to  \\mathbb{B}\\) where \\(F_T(\\bullet) = T: \\aa \\to \\vv\\) and \\(i(\\bullet) = 1\\). Then by the previous work, there exists a map \\(T^{\\#}: \\mathbb{B} \\to \\{\\aa, \\vv\\}\\) such that the diagram below commutes. \\ </p> <p>Now construct the maps \\(\\{F_S\\}: \\{*\\} \\to \\hom(\\mathbf{1}, \\{\\aa, \\vv\\})\\) and \\(\\{S\\}: \\{*\\} \\to \\hom (\\aa, \\vv)\\) where \\(\\{F_S\\}(*) = F_S\\) and \\(\\{S\\}(*) = S\\). Consider the pullback squares below.  \\ </p> <p>\\  First, \\(P\\) corresponds to the set of functors \\(T: \\mathbb{B} \\to \\{\\aa, \\vv\\}\\) such that precomposition with \\(i\\) is equal to \\(F\\). Meanwhile, the set \\(Q\\) consists of functors \\(T': \\mathbb{B}\\int\\aa \\to \\vv\\) where precomposition with \\(i_{\\aa}\\) is equal to \\(S\\). However, these sets are in bijection due to the adjoint relation we have. In other words, the diagrams  \\  are in bijection. Hence we see that \\(T^{\\#}\\) corresponds uniquely with a functor \\(T'\\) such that  the diagram  \\  commutes and preserves the Yang-Baxter operators as desired.  \\end{proof}</p> <p> Let \\(\\vv\\) be an \\(B\\)-category and suppose we have a functor \\(F: \\aa \\to \\vv\\). Then there is an equivalence of categories  \\[ \\mathbb{B}\\text{*Fun*}(\\mathbb{B}{\\textstyle\\int}\\aa, \\vv) \\simeq \\text{*Fun*}(\\aa, \\vv). \\] <p>given by precomposition of each \\(F: \\mathbb{B}\\int\\aa \\to \\vv\\) with \\(i_{\\aa}: \\aa \\to \\mathbb{B}\\int\\aa\\). </p> <p>\\begin{proof} We follow the same argument as Joyal and Street. By the previous lemma, every \\(SB\\)-monoidal category is strongly equivalent to a strict \\(SB\\)-monoidal category \\(\\vv'\\) via a pair of functors \\(E: \\vv \\to \\vv'\\) and \\(E': \\vv' \\to \\vv\\). Hence observe that if we have an equivalence of categories \\((-) \\circ i_{\\aa}: \\mathbb{B}\\fun(\\mathbb{B}\\int\\aa, \\vv') \\to \\fun(\\aa, \\vv')\\), then the diagram below commutes \\  and the top dashed arrow is an equivalence as well. So it suffices to prove this for the strict case.  Now, the proposed functor \\(F\\) behaves as </p> \\[ F(S: \\mathbb{B}\\int\\aa \\to \\vv) = S \\circ i_{\\aa}: \\aa \\to \\vv. \\] <p>We must demonstrate that this is fully faithful and essentially surjective. \\begin{description} \\item[Fully faithful.] Let \\(F, G: \\mathbb{B}\\int\\aa \\to \\vv\\) be strong \\(SB\\)-monoidal functors. Then define the function</p> \\[ \\phi: \\hom_{\\mathbb{B}\\text{Fun}(\\mathbb{B}{\\textstyle\\int}\\aa, \\vv)}(F,G) \\to  \\hom_{\\text{Fun}(\\aa, \\vv)}(F \\circ i_{\\aa},  G\\circ_{i_{\\aa}}). \\] <p>where, given a natural transformation \\(\\eta: F \\to G\\), we have that \\(\\phi(\\eta): F\\circ i_{\\aa} \\to G \\circ i_{\\aa}\\) is a natural transformation defined as </p> \\[ \\phi(\\eta)_A = \\eta_{[A]}. \\] <p>We show that this is injective. Suppose \\(\\phi(\\eta) = \\phi(\\eta')\\) for two natural transformations \\(\\eta, \\eta': F \\to G\\) with \\(F, G \\in \\mathbb{B}\\text{Fun}(\\mathbb{B}\\int\\aa, \\vv)\\). The fact that \\(\\phi(\\eta) = \\phi(\\eta')\\) implies that </p> \\[   \\eta_{[A]} = \\eta_{[A']}. \\] <p>As these are natural transformations between monoidal functors, we have that the diagram below commutes.  \\  The morphisms \\(P_1\\) and \\(P_2\\) are the isomorphisms built inductively from </p> \\[ F_2: F([A]) \\otimes F([B] \\isomarrow F([A, B]) \\] <p>which comes equipped with the data of a strong monoidal functor [see Mac Lane, p. 256]. Moreover, the diagram commutes by Mac Lane's coherence theorem. </p> <p>The above diagram similarly holds with \\(\\eta\\) replaced as \\(\\eta'\\), since \\(\\eta'\\) is also a natural transformation of monoidal functors. Hence what we see is that </p> \\[\\begin{align*} \\eta_{[A_1, \\dots, A_n]} \\circ P_1 &amp;=  P_2 \\circ \\eta_{[A_1]}\\otimes \\cdots \\otimes \\eta_{[A_n]}\\\\ &amp;= P_2 \\circ \\eta'_{[A_1]}\\otimes \\cdots \\otimes \\eta'_{[A_n]}\\\\ &amp;= \\eta'_{[A_1, \\dots, A_n]} \\circ P_1. \\end{align*}\\] <p>As \\(P_1\\) is an isomorphism, we have that \\(\\eta_{[A_1, \\dots, A_n]} = \\eta'_{[A_1, \\dots, A_n]}\\), so that  \\(\\phi(\\eta) = \\phi(\\eta')\\) implies that \\(\\eta = \\eta'\\). Hence the functor is faithful. The functor is clearly full, since  by the above process we can always take a natural transformation \\(\\eta: F\\circ i_{\\aa} \\to G\\circ i_{\\aa}\\) and build it into a natural transformation \\(\\eta: F \\to G\\). </p> <p>\\item[Essentially Surjective.] Consider a functor \\(F: \\aa \\to \\vv\\).  By Proposition \\ref{proposition:triangle_commutes}, we know there exists  a unique \\(S: \\mathbb{B}\\int\\aa \\to \\vv\\) such  that \\(S \\circ i_{\\aa} = F\\).  Hence we have essential surjectivity; in fact, we have  a stronger version in the strict case. \\end{description} \\end{proof}</p>"},{"location":"category_theory/Monoidal%20Categories/Enriched%20Categories/","title":"7.8. Enriched Categories","text":"<p>When we originally defined categories, we sought a degree of large generality  that was able to capture a huge amount of mathematical phenomenon. However, this  was not out a mere desired for generality; as Mac Lane puts it, \"good general theory does not search for the maximum generality, but for the right generality\" (108).  But it does turn out that in defining categories so widely we lose some of their internal  structure; for example, in many categories, every homset might have a underlying  abelian group structure. These are called preadditive categories and are extremely  useful, in that they give us a first step towards a general framework (but not to general)  that allows one to do homological algebra in. </p> <p>Now if we've lost some original framework, how do we recover it? First, recall that  in categories, objects are basically dummies. It doesn't matter how I denote my objects  in my category \\(\\cc\\); you and are I talking about the same category if our morphisms  act the same exact way. For example, the categories </p> <p> and  \\  here the above objects are \\(n\\) words describing how politicians suck, are the same preorders. Thus, because categorical  structure is primarily found within the morphisms, i.e. the homsets, we only need to  fix these to take back our original structure. </p> <p>\\begin{definition} Let \\((\\mathcal{V}, \\otimes, I)\\) be a monoidal category. A small category \\(\\cc\\) is a \\(\\mathcal{V}\\)-category  or an enriched category over \\(\\mathcal{V}\\) if </p> <ul> <li> <p>[1.] For each \\(A, B \\in \\cc\\), we have that \\(\\hom_{\\cc}(A, B) \\in \\mathcal{V}\\) </p> </li> <li> <p>[2.] There exists a \"composition\" operator </p> </li> </ul> \\[ \\circ_{A, B, C} : \\hom_{\\cc}(A, B) \\times \\hom_{\\cc}(B, C) \\to \\hom_{\\cc}(A, C) \\] <ul> <li>[3.] For each object \\(A \\in \\cc\\), we have a \"identity object\" </li> </ul> \\[ i_A: I \\to \\hom_{\\cc}(A, A)   \\] <p>such that our composition operator is associative:  \\  and such that our unital elements in each homset behave morally like an identity  element should: \\  end{definition}</p> <p> The following is a classic example due to F.W. Lawvere. A Lawvere metric space is a set \\(X\\)  equipped with a distance function \\(d: X\\times X \\to \\rr\\) such that  <ul> <li> <p>[1.] \\(d(x, x) = 0\\) for all \\(x \\in X\\)</p> </li> <li> <p>[2.] \\(d(x, z) \\le d(x, y) + d(y, z)\\) for all \\(x, y, z \\in X\\).  </p> </li> </ul> <p>It turns out that, we may equivalently define such a space as a category enriched  over \\(([0, \\infty), +, 0)\\).</p> <p>Recall that \\(([0, \\infty), +, 0)\\) where \\(+\\) is addition forms a symmetric monoidal  category. Here we treat \\([0, \\infty]\\) as a poset where for a pair of objects \\(a, b\\)  there exists exactly one morphism </p> \\[ a \\to b \\text{ iff } b \\le a.    \\] <p>Now what does it look like for a category \\(\\cc\\) to be \\([0, \\infty]\\)-category?  It means that for any pair of objects \\(A, B\\), we have that \\(\\hom_{\\cc}(A, B) \\in [0, \\infty)\\). If we denote \\(d(A, B) = \\hom_{\\cc}(A, B)\\), this then implies that we have a function </p> \\[ d: \\ob(\\cc)\\times\\ob(\\cc) \\to [0, \\infty]. \\] <p>Enriched categories also grant us a composition morphism</p> \\[  \\hom_{\\cc}(A, B) \\times \\hom_{\\cc}(B, C) \\to \\hom_{\\cc}(A, C) \\] <p>for all objects \\(A, B, C\\). But in \\([0, \\infty)\\), morphisms are just size relations, so what this really means is that</p> \\[ d(A,C) \\le d(A, B) + d(B, C) \\] <p>for all \\(A, B, C \\in \\cc\\) Finally, we see the identity criterion states that for each object \\(A\\),  we have a morphism \\(i_{A}: 0 \\to \\hom_{\\cc}(A, A)\\) which translates to</p> \\[ d(A, A) \\le 0 \\implies d(A, A) = 0 \\] <p>since \\(d(A, A) \\in [0, \\infty]\\). This should feel very familiar; what we've just  come up with is nearly a metric space structure on the objects of our category!  We are only missing the symmetry relation. For that, this special construction is known  as a Lawvere metric space.  </p> <p> Recall that a (strict) 2-category is a category \\(\\cc\\) such that, in addition  to the morphisms \\(f: A \\to B\\) between objects \\(A, B \\in \\cc\\), there  exists 2-morphisms \\(\\alpha: f \\to g\\) between parallel morphisms  \\(f, g: A \\to B\\).  \\  These two morphisms have access to two different forms of composition. On one hand, there  is \"vertical\" composition  \\  while on the other, there is \"horizontal\" composition. \\  Moreover, we require that the interchange law be satisfied and that  the morphisms form a category under the vertical composition given by \\(\\circ\\). However, we can rephrase this as saying a category \\(\\cc\\) is a 2-category if  <ul> <li> <p>[1.] For each \\(A, B \\in \\cc\\) we have that \\((\\hom_{\\cc}(A, B), \\circ)\\) is a category</p> </li> <li> <p>[2.] There exist a composition operator \\(\\circ : \\hom(A, B) \\times \\hom(B, C) \\to \\hom(A, C)\\) \\ </p> </li> <li> <p>[3.] For each object \\(A\\), we have a functor \\(i_A: 1 \\to \\hom(A, A)\\), where \\(1\\)  is the one object category with one morphism that is sent to \\(1_A\\).  </p> </li> </ul> <p>Above, (3) is stupidly simple; but the reason we're framing it this way is to demonstrate  that a strict 2-category \\(\\cc\\) is the same thing as a category \\(\\cc\\) enriched over the monoidal  category \\((**Cat**, \\times, 1)\\); the category of small categories whose  monoidal product is the cartesian product and whose identity is the one-object-one-morphism category \\(1\\). </p> <p></p>"},{"location":"category_theory/Monoidal%20Categories/Mac%20Lane%27s%20Coherence%20Theorem/","title":"7.4. Mac Lane's Coherence Theorem","text":"<p>\\subsection*{Step One: Category of Binary Words} To begin the proof of the coherence theorem, we need to first state  the theorem itself. This task itself is quite laborious, although it is a worthwhile  investment to establish clear terminology and notation, especially in writing the  proof itself. Our primary tool will be the abstract concept of a binary  word. </p> <p> Let \\(x_0, x_1\\) be two distinct symbols. A binary word \\(w\\)  is an element defined recursively as follows. <ul> <li> <p>\\(x_0\\) and \\(x_1\\) are binary words. </p> </li> <li> <p>If \\(u, v\\) are binary words, then \\((u) \\otimes (v)\\) is a binary word.</p> </li> </ul> <p>More precisely, a binary word is any element in the  free magma \\(M = F(\\{x_0,x_1\\})\\) generated by \\(x_0,x_1\\), but we will see  that the first definition we offered is more useful and transparent. </p> <p> Since \\(x_0, x_1\\) are binary words so is the expression \\((x_0)\\otimes (x_1)\\). Similarly, the expressions  \\[ (x_0)\\otimes((x_0)\\otimes(x_1)) \\quad ((x_0)\\otimes(x_1))\\otimes x_1    \\] <p>are binary words. </p> <p>From the previous example, we see that the notation is a bit clunky.  On one hand, our definition, which states that \\((u)\\otimes(v)\\) is a binary word if \\(u,v\\) are, is required so that we can logically  manage our parentheses. On the other, it makes notation clunky. </p> <p>To remedy this, we will often omit parentheses. Given an expression of  a binary word, we will always omit the parentheses around individual symbols in the  expression. With this rule, we have that:</p> \\[\\begin{gather*} (x_0)\\otimes (x_1) = x_0 \\otimes x_1\\\\ (x_0)\\otimes((x_0)\\otimes(x_1))= x_0\\otimes(x_0 \\otimes x_1)\\\\ ((x_0)\\otimes(x_1))\\otimes (x_1) = ( x_0\\otimes x_1)\\otimes x_1  \\end{gather*}\\] <p>That is, we keep the parentheses which group together individual products,  and throw away the ones which our smart human brains can don't need.</p> <p>Next, we move onto an important quantity that we will often perform induction on.</p> <p> We define the length of a binary word \\(w\\), denoted as \\(\\mathcal{L}(w)\\), recursively as follows.  <ul> <li> <p>sep-0.25em</p> </li> <li> <p>\\(\\mathcal{L}(x_0) = 0\\) and \\(\\mathcal{L}(x_1) = 1\\)</p> </li> <li> <p>If \\(w=  u \\otimes v\\) for two binary words \\(u, v\\), we set \\(\\mathcal{L}(w) = \\mathcal{L}(u) + \\mathcal{L}(v)\\).</p> </li> </ul> <p> </p> <p> The binary words  \\((x_1\\otimes x_0)\\otimes x_1\\), \\((x_1\\otimes x_1)\\otimes x_0\\), \\((x_0 \\otimes (x_1\\otimes x_1))\\otimes x_0\\) all have length \\(2\\).  </p> <p>More informally, the length of binary word is simply the number of \\(x_1\\) symbols that  appear in its expression.  </p> <p> For any binary word \\(w\\), we have that  \\[ \\mathcal{L}(w \\otimes x_0) = \\mathcal{L}(x_0 \\otimes w) =  \\mathcal{L}(w). \\] <p>If additionally \\(u,v\\) are binary words, we also have that </p> \\[\\begin{align*} \\mathcal{L}(u \\otimes(v \\otimes  w))  &amp;=  \\mathcal{L}(u) + \\left(\\mathcal{L}(v) + \\mathcal{L}(w)\\right)\\\\ &amp;= \\left( \\mathcal{L}(u) + \\mathcal{L}(v) \\right) + \\mathcal{L}(w)\\\\ &amp;= \\mathcal{L}((u \\otimes v) \\otimes  w). \\end{align*}\\] <p>We will use the observations made in the previous example later in this section. </p> <p>We now demonstrate that these binary words assemble into a category. </p> <p> The category of binary words is the category \\(\\mathcal{W}\\) where  \\begin{description} \\item[Objects.] All binary words \\(w\\) of length \\(n = 0, 1, 2, \\dots,\\) \\item[Morphisms.] For any two binary words \\(w\\) and \\(v\\), we have that  \\[ \\hom_{\\ww}(v, w) =  \\begin{cases} \\{ \\bullet \\} &amp; \\text{if } v,w \\text{ are the same length}\\\\ \\varnothing &amp; \\text{otherwise}.  \\end{cases}    \\] <p>where \\(\\{\\bullet\\}\\) denotes the one point set. \\end{description} </p> <p>What the above definition tells us is that any two binary words share  a morphism if and only if they are of the same length. Moreover, they will only  ever share exactly one morphism. Since there is always at most one morphism between any two objects  in \\(\\mathcal{W}\\), we see that \\(\\mathcal{W}\\) is a \\hyperref[definition:thin-category]{\\textcolor{Blue}{thin category}}.  Moreover, it is monoidal. To prove that it is monoidal,  we will need the following small lemma. </p> <p> The multiplication of binary words extends to a bifunctor  \\(\\otimes: \\ww \\times \\ww \\to \\ww\\). </p> <p> First, we explain how \\(\\otimes: \\ww \\times \\ww \\to \\ww\\) operates on objects  and morphisms. If \\((u, v)\\) is an object of \\(\\ww \\times \\ww\\), we set \\(\\otimes(u, v) = u \\otimes v\\).  Next, consider two morphisms in \\(\\ww\\). \\[ \\gamma: u \\to u' \\qquad \\beta: v \\to v'. \\] <p>Note that this implies \\(\\ll(u) = \\ll(u')\\) an \\(\\ll(v) = \\ll(v')\\), which  also imply that </p> \\[ \\ll(u \\otimes v) = \\ll(u) + \\ll(v) = \\ll(u') + \\ll(v') = \\ll(u' \\otimes v'). \\] <p>Therefore, we define the image of \\((\\gamma, \\beta)\\) under the functor, \\(\\otimes(\\gamma, \\beta)\\), which we more naturally denote  as \\(\\gamma\\otimes \\beta\\), to be the unique morphism between \\(u \\otimes v \\to u' \\otimes v'\\).</p> <p>We can picture the action of this functor on objects and morphisms more clearly as below.</p> <p> In addition, for any \\((u,v)\\) in \\(\\ww\\times\\ww\\), the identity morphism \\(1_{(u,v)}: (u,v) \\to (u,v)\\)  is mapped to the identity \\(1_{u\\otimes v} :u \\otimes v \\to u \\otimes v\\).  Finally, to demonstrate that this respects composition, suppose that  \\((\\gamma, \\beta)\\) is composable with \\((\\gamma', \\beta')\\) as below. \\  As both \\((\\gamma', \\beta')\\otimes(\\gamma, \\beta)\\) and \\((\\gamma' \\circ \\gamma)\\otimes (\\beta' \\circ \\beta)\\) are parallel morphisms acting as \\((u_1,v_1) \\to (u_3,  v_3)\\), they must be equal  because \\(\\ww\\) is a thin category (and hence parallel morphisms are equal).</p> <p>Therefore, we see that \\(\\otimes: \\ww \\times \\ww \\to \\ww\\) is a bifunctor. </p> <p>We now show that \\(\\ww\\) assembles into a monoidal category. </p> <p> \\((\\ww, \\otimes, x_0)\\) is a monoidal category with monoidal product \\(\\otimes: \\ww \\times \\ww \\to \\ww\\)  and identity object \\(x_0\\). </p> <p> First, we define our product to be given by the bifunctor \\(\\otimes: \\ww\\times\\ww \\to \\ww\\). Second, we define our identity object to be \\(x_0\\).  With these two conditions we now need to find unitors, an associator, and  check that the necessary diagrams commute. <p>Now as any two binary words of the same length share a unique morphism, all morphisms are isomorphisms. Therefore, by Example \\ref{example:binary_word_lengths}, the isomorphisms</p> \\[\\begin{align*} \\alpha_{u,v,w}&amp;: u \\otimes (v \\otimes w) \\isomarrow (u \\otimes v) \\otimes w\\\\ \\lambda_w &amp;: x_0 \\otimes w \\isomarrow w\\\\ \\rho_w &amp;: w \\otimes x_0 \\isomarrow w \\end{align*}\\] <p>are forced to exist. Further, these isomorphisms are natural because all diagrams  commute in a thin category. In addition, since \\(\\ww\\) is a thin category,  all diagrams commute, and so, in particular, the required diagrams  \\  also commute, so that \\((\\ww, \\otimes, x_0)\\) satisfies the axioms of a monoidal category.   We now make a few important comments on how to interpret \\(\\alpha, \\rho\\), and \\(\\lambda\\).</p> <ul> <li> <p>sep-0.25em </p> </li> <li> <p>Each \\(\\alpha_{u,v,w}: u \\otimes (v \\otimes w) \\to (u \\otimes v) \\otimes w\\)  can be thought of as an operator which shifts  the parentheses to the left. Dually, \\(\\alpha_{u,v,w}^{-1}\\) shift them to the right.</p> </li> <li> <p>Each \\(\\lambda_w: x_0 \\otimes w \\isomarrow w\\) can be thought of as an operator  that removes an identity from the left. Dually, \\(\\lambda_w^{-1}\\) adds an identity  to the left.</p> </li> <li> <p>Each \\(\\rho_w: w  \\otimes x_0  \\isomarrow w\\) can be thought of as an operator  that removes an identity from the right. Dually, \\(\\rho^{-1}_w\\) adds  an identity to the right.</p> </li> </ul> <p>Hence, this very primitive monoidal category \\(\\ww\\) encodes some basic and  useful operators on binary words.</p> <p>\\subsection*{Step Two: Pure Binary Words}  In this section we begin discussing a specific subset of binary words, namely  the ones which lack an identity \\(x_0\\). As the theorem is quite complex, this initial restriction  allows us to develop intuition and some tools that simplify the proof later.</p> <p> A pure binary word \\(w\\) of length \\(n\\) is a binary word \\(w\\) of  length \\(n\\) which has no instance the empty word \\(x_0\\).  </p> <p> The only pure binary word of length 1 is \\(x_1\\).  There is also only one pure binary word of length 2, which  is \\(x_1 \\otimes x_1\\). The pure binary words of length 3 are  \\ \\[ x_1\\otimes (x_1 \\otimes x_1) \\qquad  (x_1\\otimes x_1) \\otimes x_1 \\] <p>and the pure binary words of length 4 are as below.</p> \\[\\begin{gather*} x_1\\otimes(x_1\\otimes (x_1 \\otimes x_1)) \\quad  x_1\\otimes((x_1\\otimes x_1) \\otimes x_1) \\quad  ((x_1\\otimes x_1)\\otimes x_1) \\otimes x_1 \\\\ x_1\\otimes((x_1 \\otimes x_1) \\otimes x_1) \\quad  (x_1 \\otimes (x_1 \\otimes x_1))\\otimes x_1 \\end{gather*}\\] <p></p> <p>As a side note, we comment that the number of pure binary words of length \\(n+1\\) is the \\(n\\)-th Catalan number </p> \\[ C_{n}=\\frac{1}{n+1}{2n \\choose n} \\qquad 1,\\, 2,\\, 5,\\, 14,\\, 42,\\, 132,\\, 429,\\, \\cdots \\] <p>However, we make no critical use of this fact in our proofs.   Next, we form a category of pure binary words.</p> <p> The category of pure binary words \\(\\wp\\) is the full subcategory of \\(\\ww\\) constructed by restricting  the objects of \\(\\ww\\) to its pure binary words.  <p>More explicitly, \\(\\wp\\) is the category defined as: \\begin{description}[leftmargin = -0.2cm]</p> <p>\\item[Objects.] All pure binary words \\(w\\) of length \\(n = 0, 1, 2, \\dots,\\)  \\item[Morphisms.] For any two pure binary words \\(u,v\\) of the same length,  we have that \\(\\hom_{\\ww_A}(u,v) = \\{\\bullet\\}\\), the one point set.  No other morphisms are allowed. \\end{description} </p> <p>We now focus on a particular set of morphisms in \\(\\wp\\). Recall that  we may think of each \\(\\alpha_{u,w,v}\\) as a \"shift map\"</p> \\[ \\alpha_{u,w,v}: u \\otimes (v\\otimes  w) \\to (u \\otimes v)\\otimes w \\] <p>which makes a single change in the parenthesis of a binary word.  However, \\(\\alpha\\) itself does not characterize all possible always in which we make a single change of parentheses within a larger, more complex binary word. An example of this is the morphism</p> \\[ 1_{s}\\otimes \\alpha_{u,v,w}: s\\otimes (u \\otimes (v \\otimes w)) \\to s \\otimes ((u \\otimes v)\\otimes w) \\] <p>which makes an internal change of parentheses.  As we will need to focus on these more complicated morphisms,  we rigorously define them below.</p> <p>[\\(\\alpha\\)-arrows] A \\(**forward ** \\bm{\\alpha}**-arrow**\\) of \\(\\wp\\) is a morphism in \\(\\wp\\)  which we recursively define as follows.  <ul> <li>For any triple of pure binary words \\(w_1, w_2, w_3\\) in \\(\\wp\\),  the morphism</li> </ul> \\[ \\alpha_{w_1,w_2,w_3}: w_1\\otimes (w_2 \\otimes w_3) \\to (w_1\\otimes w_2)\\otimes w_3 \\] <p>is a forward \\(\\alpha\\)-arrow. </p> <ul> <li>If \\(\\beta: w \\to w'\\) is a forward  \\(\\alpha\\)-arrow, and \\(u\\) is an arbitrary pure binary word,  then the morphisms </li> </ul> \\[ 1_{u} \\otimes \\beta: u \\otimes w \\to u \\otimes w' \\qquad  \\beta \\otimes 1_{u}:w \\otimes u \\to w' \\otimes u \\] <p>are forward \\(\\alpha\\)-arrows. </p> <p>We also define a **backward \\(\\bm{\\alpha**\\)-arrow} to be the  inverse of a forward \\(\\alpha\\)-arrow. </p> <p> Below are a few simple examples of \\(\\alpha\\)-arrows. The first two are forward,  while the third is backward. \\  We can have even more complicated examples; for example, the morphism below  \\  is an \\(\\alpha\\)-morphism for any pure binary words \\(u, v\\). For example, setting  \\(u = (x_1 \\otimes x_1)\\otimes x_1\\) and \\(v = x_1 \\otimes x_1\\), we obtain the forward \\(\\alpha\\)-arrow  as below. \\  end{example} <p>We emphasize that \\(\\alpha\\)-arrows only ever involve a single  instance of \\(\\alpha\\) or \\(\\alpha^{-1}\\) in their expression.</p> <p>Next, we introduce a particularly important instance of a pure binary word that will  become essential to our proof. </p> <p> We define the terminal word \\(w^{(n)}\\) of length \\(n\\) recursively as follows.  <ul> <li> <p>\\(x_1\\) is the terminal word of length 1.</p> </li> <li> <p>If \\(w^{(k)}\\) is the terminal word of length \\(k\\), then  \\(w^{(k+1)} = w^{k}\\otimes x_1\\) is the terminal word of length \\(k+1\\). </p> </li> </ul> <p>More informally, the terminal word is the unique pure binary  word of length \\(n\\) for which all parentheses begin on the left.  </p> <p>\\begin{example} Below we list the terminal words by length. \\begin{center}</p> <p>|                          Length |\\multicolumn{1}{c|}{Terminal Word}| |---------------------------------|----------------------------------| |                         \\(1\\) |\\(x_1\\)| |            \\(2\\) |\\(x_1 \\otimes x_1 (| |            \\(3\\) |\\)(x_1 \\otimes x_1)\\otimes x_1 (| |            \\(4\\) |\\)((x_1 \\otimes x_1)\\otimes x_1)\\otimes x_1 (| |            \\(5\\) |\\)(((x_1 \\otimes x_1)\\otimes x_1)\\otimes x_1)\\otimes x_1\\)| ||</p> <p>\\end{center}</p> <p>We now introduce a quantity which provides a \"distance-measure\" between a pure binary word of length \\(n\\) and the terminal word \\(w^{(n)}\\).</p> <p> We (recursively) define  the rank of a binary word as follows. <ul> <li> <p>\\(r(x_1) = 0\\).</p> </li> <li> <p>For a pure binary word of the form \\(w = u \\otimes v\\), we set </p> </li> </ul> \\[ r(u\\otimes v) = r(u) + r(v) + \\ll(v) - 1. \\] <p></p> <p> We compute the ranks on the pure binary words of length \\(4\\). \\[\\begin{align*} &amp;r(x_1(x_1(x_1x_1))) =3 \\qquad r(x_1((x_1x_1)x_1)) =2\\\\ &amp;r((x_1x_1)(x_1x_1)) =1 \\qquad r((x_1(x_1x_1))x_1) =1\\\\ &amp;r(((x_1x_1)x_1)x_1) =0 \\end{align*}\\] <p></p> <p>Note that \\(w^{(4)} = ((x_1x_1)x_1)x_1\\) and \\(r(((x_1x_1)x_1)x) = 0\\).  Hence we see that our intuition of the rank being a distance measure  from \\(w^{(n)}\\) so far makes sense. </p> <p>An important property of distance-measuring  functions is nonnegativity, which we will now see is satisfied by  the rank function.</p> <p> Let \\(w\\) be a pure binary word of length \\(n\\). Then \\(r(w) \\ge 0\\).  </p> <p> We prove this by induction on \\(n\\). First observe that this clearly holds for \\(n = 0\\) since \\(r(x_1) = 0\\).  <p>Now let \\(w\\) be a pure binary word of length \\(k\\), and suppose the  statement is true for all pure binary words with length less than \\(k\\). Since \\(k &gt; 1\\), we may write \\(w = u \\otimes v\\) for some pure binary words  \\(u,v\\), in which case </p> \\[ r(w) =  \\overbrace{r(u) + r(v)}^{\\ge 0 \\text{ by induction}} + \\ll(v) -1. \\] <p>Since \\(\\ll(v) \\ge 1\\), we see that \\(r(w) \\ge 0\\) as desired. </p> <p>Keeping with the analogy of the rank being a distance measure, we ought  to verify that it is zero if and only if the input, which is being measured from  \\(w^{(n)}\\), is \\(w^{(n)}\\) itself. We verify that this is the case for the rank function.</p> <p> Let \\(w\\) be a pure binary word of length \\(n\\).  Then \\(r(w) = 0\\) if and only if \\(w = w^{(n)}\\). </p> <p> We proceed by induction. In the simplest case, when \\(n = 1\\), we have that \\(r(x_1) = 0\\) by definition. As \\(x_1 = w^{(1)}\\), we  see that this satisfies the statement. <p>Let \\(w\\) be a pure binary word of length \\(k\\), and suppose  the statement is true for all pure binary words with length  less than \\(k\\). Then we may write our word in the form \\(w = u \\otimes v\\),  and we have that</p> \\[ r(w) = r(u) + r(v) + \\ll(v) - 1.         \\] <p>By Lemma \\ref{lemma:rank_is_positive}  we know that  \\(r(u), r(v) \\ge 0\\). Therefore, if \\(\\ll(v) &gt;  1\\) then  \\(r(w) \\ne 0\\).  Hence, consider the case  for when \\(\\ll(v) = 1\\), so that \\(v = x_1\\). Then </p> \\[ r(u\\otimes v) = r(u) + r(x_1) + \\ll(x_1) - 1 = r(u) \\] <p>Therefore, \\(r(w) = 0\\) if and only if if \\(r(u) = 0\\). But by induction, this holds if and only if \\(u = w^{(k-1)}\\).  So we see that \\(w = w^{(k-1)}\\otimes x_1 = w^{(k)}\\), which proves our result for all \\(n\\). </p> <p> Let \\(\\beta: v \\to w\\) be a forward \\(\\alpha\\)-arrow.  Then \\(r(v) &lt; r(w)\\). In other words, forward \\(\\alpha\\)-arrows decrease rank.  </p> <p> To demonstrate this, we perform induction on the structure of forward \\(\\alpha\\)-arrows.  <p>Our base case is \\(\\beta = \\alpha_{u, v, w}: u\\otimes(v\\otimes w) \\isomarrow (u\\otimes v)\\otimes w\\) for some arbitrary words \\(u, v, w\\). With this case, observe that</p> \\[\\begin{align*} r(u\\otimes(v\\otimes w)) &amp; = r(u) + r(v \\otimes w) + \\ll(v\\otimes w) - 1\\\\ &amp; = r(u) + (r(v) + r(w) + \\ll(w) - 1)\\\\ &amp;+ \\ll(v\\otimes w) - 1 \\end{align*}\\] <p>while </p> \\[\\begin{align*} r((u\\otimes v)\\otimes w) &amp;= r(u\\otimes v) + r(w) + \\ll(w) - 1\\\\ &amp;= r(u) + r(v) + r(w) + \\ll(v) - 1 + r(w) \\\\ &amp;+ \\ll(w)- 1. \\end{align*}\\] <p>If we subtract the quantities, we observe that </p> \\[ r(u\\otimes(v\\otimes w)) - r((u\\otimes v)\\otimes w)  = \\ll(v\\otimes w) - \\ll(w) &gt; 0 \\] <p>since \\(v\\) has at least length 1. Therefore \\(\\alpha_{u,v,w}\\) decreases  length as desired.</p> <p>Next, we reach our inductive step: let \\(\\beta = 1_{u} \\otimes \\gamma : u \\otimes v \\to u \\otimes w\\)  where \\(\\gamma: v \\to w\\) is a forward  \\(\\alpha\\)-arrow for which the statement is already true.</p> <p>In this case we have that </p> \\[\\begin{align*} r(u\\otimes v) = r(u) + r(v) + \\ll(v) - 1. \\end{align*}\\] <p>while </p> \\[\\begin{align*} r(u\\otimes w) = r(u) + r(w) + \\ll(w) - 1. \\end{align*}\\] <p>Since \\(\\ll(v) = \\ll(w)\\) and \\(r(v) &gt; r(w)\\), we see that  \\(r(u \\otimes v) &gt; r(u \\otimes w)\\). Therefore, we see that \\(\\beta = 1_u \\otimes \\gamma\\) decreases rank whenever  \\(\\gamma\\) is a forward \\(\\alpha\\)-arrow that also decreases rank. </p> <p>Finally, let \\(\\beta = \\gamma \\otimes 1_{u}\\) where \\(\\gamma: v \\to w\\)  is a forward \\(\\alpha\\) arrow for which the statement is already true.  Then we may write \\(\\beta: v \\otimes u \\to w \\otimes u\\)  Now observe that </p> \\[ r(v \\otimes  u) = r(v) + r(u) + \\ll(u) - 1 \\] <p>while </p> \\[ r(w \\otimes  u) = r(w) + r(u) + \\ll(u) - 1. \\] <p>Since \\(\\gamma: v \\to w\\) decreases rank, we see that \\(r(v) &gt; r(w)\\) and therefore \\(r(v \\otimes u) &gt; r(w \\otimes u)\\), as desired. </p> <p>This completes the proof by induction, so that the statement is true for all forward \\(\\alpha\\)-arrows.  </p> <p>Thus what we have on our hands is the following. We know that the rank of word \\(w\\) is zero if and only if \\(w = w^{(n)}\\). Further, we know that applying \\(\\alpha\\)-arrows to a pure binary word will decrease its rank. In other words, shifting the parentheses of a pure binary  word \\(w\\) brings \\(w\\) \"closer\" to \\(w^{(n)}\\) (whose parentheses are all on the left).  Therefore, the rank of a pure binary word gives us a measure for how far  a binary word \\(w\\) is away from \\(w^{(n)}\\). </p> <p>The following lemma demonstrates our interest in the word \\(w^{(n)}\\).</p> <p> Let \\(w\\) be a pure binary word of length \\(n\\). If \\(w \\ne w^{(n)}\\),  then there exists a finite sequence of forward \\(\\alpha\\)-arrows from \\(w\\) to \\(w^{(n)}\\).  <p></p> <p> We first show that for every pure binary  word \\(w \\ne w^{(n)}\\) there exists a forward \\(\\alpha\\)-arrow  \\(\\beta\\) with domain \\(w\\). We prove this statement by induction on length. <p>Observe the result is immediate for \\(n = 1, 2\\). Suppose the result is true for  binary words with length less than \\(n \\ge 3\\).  Let \\(w\\) be a pure binary word with length \\(n\\).  Then \\(w = u \\otimes v\\), with \\(u,v\\) other pure binary words.  We now consider two cases for \\(u\\) and \\(v\\).</p> <ul> <li>[(1)] The first case is when  \\(\\ll(v) = 1\\), so that \\(v = x_1\\).  As \\(w \\ne w^{(n)}\\) we know that \\(u \\ne w^{(n-1)}\\), and since \\(u\\) has  length less than \\(w\\), we see that by induction there exists a forward \\(\\alpha\\)-arrow  \\(\\beta: u \\to u'\\). Using \\(\\beta\\), we can construct the  forward \\(\\alpha\\)-arrow </li> </ul> \\[ \\beta \\otimes 1_{x_1}: u \\otimes x_1 \\to u' \\otimes x_1.   \\] <p>Hence \\(\\beta \\otimes 1_{x_1}\\) is our desired forward \\(\\alpha\\)-arrow with domain \\(w\\).</p> <ul> <li>[(2)] The second case is when \\(\\ll(v) &gt; 1\\). In this case we may write  \\(w = u \\otimes (r \\otimes s)\\). A natural choice for a forward  \\(\\alpha\\)-arrow in this case is simply </li> </ul> \\[ \\alpha_{u,v,s}:  u \\otimes (r \\otimes s) \\to (u \\otimes r) \\otimes s \\] <p>so that this case is also satisfied. </p> <p>As we see, in all cases for \\(w \\ne w^{(n)}\\), we can find a forward \\(\\alpha\\)-arrow  with domain \\(w\\). As \\(\\alpha\\)-arrows decrease rank, and \\(r(w) = 0\\) if and only if \\(w^{(n)}\\), this guarantees a sequence of \\(\\alpha\\)-arrows from \\(w\\) to \\(w^{(n)}\\), which is what  we set out to show. </p> <p>The previous proposition has an immediate, useful corollary.  It will be used as one of the building blocks for the next section. </p> <p> Every morphism in \\(\\wp\\) can be expressed as a finite composition  of \\(\\alpha\\)-arrows. </p> <p> Let \\(v, w\\) be arbitrary pure binary words. Denote \\(\\phi_{v,w}: v \\to w\\) to be  the unique morphism from \\(v\\) to \\(w\\). By Proposition \\ref{proposition_existence_of_w_to_wn} there exists chains of forward \\(\\alpha\\)-arrows whose composite  we denote as \\(\\Gamma_1: v \\to w^{(n)}, \\Gamma_2: w \\to w^{(n)}\\). Our situation is pictured below. \\  However, \\(\\wp\\) is a thin category, so parallel morphisms must be equal. Therefore \\[ \\phi_{v,w} = \\Gamma_2^{-1} \\circ \\Gamma_1. \\] <p>Hence \\(\\phi_{v,w}\\) is a composition of \\(\\alpha\\)-arrows. As \\(\\phi_{v,w}\\)  was arbitrary, we see that every morphism in \\(\\wp\\)  is a finite composition of \\(\\alpha\\)-arrows.  What this corollary says is that every morphism in \\(\\wp\\) can be expressed as a composite of forward and backward \\(\\alpha\\)-arrows. However, we emphasize that  there can be many different ways to represent a morphism in \\(\\wp\\) via  \\(\\alpha\\)-arrows. This will be an issue which we discuss later in the next section.</p> <p>\\subsection*{Step Three: Coherence for \\(A^{\\otimes n}\\) in \\(\\alpha\\)} Using our results from the previous section, we are almost ready to  take our first major step in the proof of Mac Lane's Coherence  Theorem. Before we do so, we need to introduce terminology to even state the theorem which we will prove in this section.  Towards that goal we introduce a few more definitions. </p> <p> Let \\((\\mathcal{M}, \\alpha, \\lambda, \\rho, I, \\otimes)\\) be a monoidal  category. For an object \\(A\\) of \\(\\mm\\), we  define the proxy map of \\(A\\) to be a partial functor \\[ (-)_A: \\wp \\to \\mm \\] <p>as follows. Note by partial functor, we mean a functor defined on all objects  of \\(\\wp\\), but only a subset of all morphisms of \\(\\wp\\). </p> <p>\\begin{description} \\item[Objects.]  We define the action on objects recursively as follows.</p> <ul> <li> <p>We set \\((x_1)_A = A\\).</p> </li> <li> <p>For a binary word \\(w = u \\otimes v\\), we define </p> </li> </ul> \\[ (w)_A = (u\\otimes  v)_A =  (u)_A \\otimes (v)_A \\] <p>\\item[Morphisms.] We define the partial functor only on  \\(\\alpha\\)-arrows. We do this recursively as follows.</p> <ul> <li>For \\(\\alpha_{u,v,w}\\) with \\(u,v,w\\) as pure binary words, we set:</li> </ul> \\[\\begin{align*} (\\alpha_{u,v,w})_A &amp;= \\alpha_{(u)_A,(v)_A,(w)_A}\\\\ (\\alpha^{-1}_{u,v,w})_A &amp;= \\alpha^{-1}_{(u)_A,(v)_A,(w)_A} \\end{align*}\\] <ul> <li>For \\(1_{u} \\otimes \\beta\\)  and \\(\\beta \\otimes 1_{u}\\) with \\(\\beta\\) an \\(\\alpha\\)-arrow, we set:</li> </ul> \\[\\begin{align*} &amp;(1_{u} \\otimes \\beta)_A = 1_{(u)_A} \\otimes (\\beta)_A\\\\  &amp;(\\beta \\otimes 1_{u})_A = (\\beta)_A\\otimes 1_{(u)_A} \\end{align*}\\] <p>\\end{description} </p> <p>We now introduce the theorem of the section. This theorem is the first major  step in the proof of the coherence theorem, and the  rest of this section will  be dedicated to proving it. </p> <p>[Coherence in \\(\\alpha\\).] Let \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category.  For every object \\(A\\), there exists a unique functor  \\(\\Phi_A: \\ww_P \\to \\mm\\) which restricts to the proxy map  \\((-)_A\\) on objects and \\(\\alpha\\)-arrows of \\(\\wp\\).  </p> <p>We address the question the reader most likely has in mind right now:  Why did we only define the proxy map on \\(\\alpha\\)-arrows? Why  not  define it on all of the morphisms of \\(\\wp\\) to get a  functor to begin with?  We did this to avoid a potential well-definedness issue, which we  now elaborate on.</p> <p>Let us attempt to naturally extend the proxy map to a functor. With Corollary \\ref{corollary:morphisms_of_wp}, it is clear how to proceed on  defining \\((-)_A\\) on general morphisms.  Let  \\(\\gamma: v \\to w\\) be any morphism in \\(\\wp\\).  By Corollary \\ref{corollary:morphisms_of_wp},  there exist forward and backward \\(\\alpha\\)-arrows  \\(\\gamma_1, \\dots, \\gamma_n\\) such  that  </p> \\[ \\gamma = \\gamma_n \\circ \\cdots \\circ \\gamma_1. \\] <p>Since the proxy map is in fact defined on \\(\\alpha\\)-arrows, and since  functors preserve composition, we are required to define</p> \\[ (\\gamma)_A = (\\gamma_n)_A\\circ \\cdots \\circ (\\gamma_1)_A. \\] <p>However, we need to be careful. Suppose that we can also  express \\(\\gamma\\) as the finite composition of \\(\\alpha\\)-morphisms  \\(\\delta_1, \\dots , \\delta_m\\).</p> \\[ \\gamma = \\delta_m \\circ \\cdots \\circ \\delta_1. \\] <p>While \\(\\gamma_n \\circ \\cdots \\circ \\gamma_1 = \\delta_m \\circ \\cdots \\circ \\delta_1\\)  because \\(\\wp\\) is a thin category, and therefore parallel morphisms are equal,  we have no idea if </p> \\[ (\\gamma_n)_A\\circ \\cdots \\circ (\\gamma_1)_A = (\\delta_m)_A\\circ \\cdots \\circ (\\delta_1)_A \\] <p>is true in \\(\\mm\\). That is, we do not know if equivalent morphisms  in \\(\\wp\\) are mapped to equal morphisms under the proxy map. Our issue is one of well-definedness. </p> <p>This issue is similar to one which arises in group theory. When one attempts to define a group homomorphism on a  quotient group, they must understand that there are different, equivalent ways to represent an element. In this situation  they must make sure that the equivalent elements are mapped to the same target  in the codomain.</p> <p> To illustrate our point, we include a concrete example of our  problem which also demonstrates its nontriviality.  For notational convenience, we suppress the instances of the monoidal product \\(\\otimes\\). Let \\[ \\gamma: x_1 ((x_1  x_1)  (x_1  x_1)) \\to  ((x_1 (x_1  x_1)) x_1)  x_1. \\] <p>Then we have many possible ways of expressing \\(\\gamma\\) in terms of  our \\(\\alpha\\)-arrows. Some potential ways we could express \\(\\gamma\\) are displayed  below in \\textcolor{Purple}{purple}, \\textcolor{NavyBlue}{blue}, or \\textcolor{Orange}{orange}. \\  s this is a thin category, we know that the composition of these paths  are equal in \\(\\wp\\).  However, we now have many ways to define \\(\\gamma\\) under the proxy map \\((-)_A\\).  We could write</p> \\[\\begin{align*}   (\\gamma)_A &amp;=  (\\textcolor{Purple}{(\\alpha^{-1}_{x_1,x_1,x_1}\\otimes  1_{x_1}) \\otimes 1_{x_1}})_A \\circ \\cdots \\circ  (\\textcolor{Purple}{1_{x_1}\\otimes \\alpha_{x_1x_1, x_1, x_1}})_A \\\\ &amp;=\\textcolor{Purple}{(\\alpha^{-1}_{A,A,A}\\otimes  1_{A})\\otimes 1_{A}} \\circ \\cdots \\circ \\textcolor{Purple}{1_{A}\\otimes \\alpha_{AA, A, A}} \\end{align*}\\] <p>or </p> \\[\\begin{align*} (\\gamma)_A &amp;=  (\\textcolor{NavyBlue}{\\alpha_{x_1, x_1x_1, x_1}\\otimes 1_{x_1}})_A \\circ \\cdots \\circ  (\\textcolor{NavyBlue}{1_{x_1}\\otimes \\alpha_{x_1x_1, x_1, x_1}})_A\\\\ &amp;=  \\textcolor{NavyBlue}{\\alpha_{A, AA, A}\\otimes 1_{A}} \\circ \\cdots \\circ \\textcolor{NavyBlue}{1_{A}\\otimes \\alpha_{AA, A, A}} \\end{align*}\\] <p>or</p> \\[\\begin{align*} (\\gamma)_A &amp;=  (\\textcolor{Orange}{\\alpha_{x_1(x_1x_1),x_1, x_1}})_A\\circ (\\textcolor{Orange}{\\alpha_{x_1, x_1x_1, x_1x_1}})_A\\\\ &amp;= \\textcolor{Orange}{\\alpha_{A(AA),A, A}} \\circ \\textcolor{Orange}{\\alpha_{A, AA, AA}} \\end{align*}\\] <p>But as morphisms in \\(\\mm\\), we don't know if  these compositions in \\(\\mm\\), displayed below, are all equal. \\  ence we need to show that the \\textcolor{Purple}{purple}, \\textcolor{NavyBlue}{blue}, and \\textcolor{Orange}{orange} compositions are equal in \\(\\mm\\). While we could perform tedious diagram chases to show that they  are  equal in \\(\\mm\\), that would only address three of the many possible ways to express  \\(\\gamma\\). It also would not take care of the case for much larger binary words!  Hence, this problem is very nontrivial in general; we need higher level techniques to get  what we want.  </p> <p>Therefore, to define a functor in the first place, we need to prove the following  fact. </p> <p> Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category,  and let \\(A\\) be an object of \\(\\mm\\). Let \\(v,w\\) be binary words of the same length. If \\(\\beta_1, \\dots , \\beta_k\\) and \\(\\gamma_1, \\dots , \\gamma_{\\ell}\\) are  \\(\\alpha\\)-arrows with  \\[ \\beta_k \\circ \\cdots \\circ \\beta_1, \\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1: v \\to w \\] <p>then \\((\\beta_k)_A  \\circ \\cdots \\circ (\\beta_1)_A = (\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_1)_A\\) in \\(\\mm\\). </p> <p>To prove this proposition, we will see that it actually suffices to prove  the the special case with \\(w = w^{(n)}\\) and with \\(\\beta_1, \\dots , \\beta_{k}\\) and \\(\\gamma_1, \\dots , \\gamma_{\\ell}\\) all forward \\(\\alpha\\)-arrows. That is, it suffices to prove the following proposition. </p> <p> Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category,  and let \\(A\\) be an object of \\(\\mm\\). Let \\(w\\) be a pure binary word of length \\(n\\). If \\(\\beta_1, \\dots , \\beta_k\\) and \\(\\gamma_1, \\dots , \\gamma_{\\ell}\\) are  forward \\(\\alpha\\)-arrows with  \\[ \\beta_k \\circ \\cdots \\circ \\beta_1, \\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1: w \\to w^{(n)} \\] <p>in \\(\\wp\\), then \\((\\beta_k)_A  \\circ \\cdots \\circ (\\beta_1)_A = (\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_1)_A\\) in \\(\\mm\\). </p> <p>To prove this it will suffice to prove the Diamond Lemma (stated below). It will turn out the bulk of the overall proof toward our theorem will be  spent on the Diamond Lemma. At the risk of downplaying its importance,  we leave the proof of the Diamond Lemma to the end since  it is very tedious and involved, and we do not want to  disrupt the flow of the current discussion.</p> <p>We summarize our plan on how to prove Theorem \\ref{theorem:coherence_in_alpha}. The uncolored boxes, and the implications between them, are what is left to do.</p> <p>\\ </p> <p>[Diamond Lemma] Let \\(w\\) be a pure binary word and suppose \\(\\beta_1,\\beta_2\\) are two  forward \\(\\alpha\\)-arrows as below. \\  There exists a pure binary word \\(z\\) and two \\(\\gamma_1: w_1 \\to z, \\gamma_2: w_2 \\to z\\), with \\(\\gamma_1, \\gamma_2\\) a composition of forward \\(\\alpha\\)-arrows, such that for any monoidal category  \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) the diagram below is  commutative in \\(\\mm\\). \\  </p> <p>Since the above lemma is an existence result,  we emphasize this fact by coloring  the arrows, which we are asserting to exist, Green.  This is a practice we will continue.</p> <p>As promised, we now prove Proposition \\ref{proposition:parallel_w_to_wn_equal_in_M}  using the Diamond lemma. We restate the statement of the proposition for the reader's  convenience.</p> <p>\\begin{customprop}{\\ref{proposition:parallel_w_to_wn_equal_in_M}} Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category,  and let \\(A\\) be an object of \\(\\mm\\). Let \\(w\\) be a pure binary word of length \\(n\\). If \\(\\beta_1, \\dots , \\beta_k\\) and \\(\\gamma_1, \\dots , \\gamma_{\\ell}\\) are  forward \\(\\alpha\\)-arrows with </p> \\[ \\beta_k \\circ \\cdots \\circ \\beta_1, \\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1: w \\to w^{(n)} \\] <p>in \\(\\wp\\), then \\((\\beta_k)_A  \\circ \\cdots \\circ (\\beta_1)_A = (\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_1)_A\\) in \\(\\mm\\). \\end{customprop}</p> <p> To prove the desired statement, we proceed by induction  on the rank of a pure binary word \\(w\\).  In what follows we write we will write \\(w = u \\otimes v\\) since \\(\\ll(w) \\ge 3\\).  <p>For our base case let \\(w\\) be a word of rank 0. Then by Proposition \\ref{proposition:rank_is_zero}  we see that \\(w = w^{(n)}\\) so that this statement is trivial. </p> <p>Next suppose the statement is true for all words with rank at  most \\(k\\) where \\(k \\ge 0\\). Let \\(w\\) be a pure binary word of rank \\(k+1\\). We want to show that the diagram in \\(\\mm\\) \\  is commutative. By the Diamond Lemma \\ref{lemma:diamond_lemma},  there exists exist a pure binary word \\(z\\) and two composites of  forward \\(\\alpha\\)-arrows \\(\\beta'\\) and \\(\\gamma'\\) such that the diagram below  is commutative in \\(\\mm\\).  \\  Let \\(\\Gamma_z: z \\to w^{(n)}\\) by any composition of forward \\(\\alpha\\)-arrows from \\(z\\) to \\(w^{(n)}\\);  at least one must exist by Proposition \\ref{proposition_existence_of_w_to_wn}.  We can now combine our two diagrams in \\(\\mm\\) to obtain the diagram below.  \\  By Lemma \\ref{lemma:directed_decreases_rank}, we know  that forward \\(\\alpha\\)-arrows  decrease rank, so that \\(r(u_1) &lt; r(w)\\) and  \\(r(v_1) &gt; r(w)\\). Hence we invoke our induction hypothesis to conclude that  both the lower left and lower right triangles commute in \\(\\mm\\). As the original upper diamond  already commutes via the Diamond Lemma, we see that the entire diagram is commutative.  Therefore we have that </p> \\[ (\\beta_k)_A  \\circ \\cdots \\circ (\\beta_1)_A = (\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_{1})_A \\] <p>in \\(\\mm\\). This completes our induction and hence the proof. </p> <p>As promised, we use the above proposition to prove  Proposition \\ref{proposition:parallel_in_M_are_equal}.</p> <p>\\begin{customprop}{\\ref{proposition:parallel_in_M_are_equal}} Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category,  and let \\(A\\) be an object of \\(\\mm\\). Let \\(v,w\\) be binary words of the same length. If \\(\\beta_1, \\dots , \\beta_k\\) and \\(\\gamma_1, \\dots , \\gamma_{\\ell}\\) are  \\(\\alpha\\)-arrows with </p> \\[ \\beta_k \\circ \\cdots \\circ \\beta_1, \\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1: v \\to w \\] <p>then \\((\\beta_k)_A  \\circ \\cdots \\circ (\\beta_1)_A = (\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_1)_A\\) in \\(\\mm\\). \\end{customprop}</p> <p> We begin by denoting the domain and codomain of the \\(\\alpha\\)-arrows to make our discussion clear. Let \\(u_0, \\dots, u_k, t_0, \\dots, t_{\\ell}\\) be the pure binary words  such that \\(u_0 = t_0 = v\\), \\(v_k = u_{\\ell} = w\\) and \\[\\begin{align*} &amp;\\beta_i: u_{i-1} \\to  u_i, \\quad i = 1, 2, \\dots, k\\\\ &amp;\\gamma_j: t_{j-1} \\to  t_j, \\quad j = 1, 2, \\dots, \\ell \\end{align*}\\] <p>Note that each morphism may either be  forward or backward.  With this notation we can picture our parallel \\(\\alpha\\)-arrows in \\(\\wp\\)  as below. \\  Now consider the image of this diagram in \\(\\mm\\),  which we do not yet know to be commutative.  \\  Our goal is to show that this diagram in \\(\\mm\\) is in fact commutative.  This will then show our desired equality.</p> <p>By Proposition \\ref{proposition_existence_of_w_to_wn}, we can  connect each pure binary word \\(u_i\\) and \\(t_i\\) to the terminal word \\(w^{(n)}\\) with forward \\(\\alpha\\)-arrows \\(\\Gamma_{u_i}: u_i \\to w^{(n)}\\) and \\(\\Gamma_{t_i}: t_i \\to w^{(n)}\\). If we add these to our diagram (and suppress the notation on the \\(\\Gamma\\)'s),  it becomes \\  whose image under the proxy map in \\(\\mm\\) is \\  Thus the diagram has become a cone, with apex \\(w^{(n)}\\), which  is sliced by the triangles. The base of this cone is the original diagram. We now show that each triangle is commutative.</p> <p>Note that each triangle is of two possible forms: it either  consists of \\(\\beta_i\\) or \\(\\gamma_i\\). Without loss of generality,  consider a triangle with an instance of \\(\\beta_i\\), as below.  \\  Now if \\(\\beta_i\\) is a forward \\(\\alpha\\)-arrow,  observe that by Proposition \\ref{proposition:parallel_w_to_wn_equal_in_M}  it is a commutative diagram in \\(\\mm\\). </p> <p>On the other hand, suppose \\(\\beta_i\\) is a backward \\(\\alpha\\)-arrow.  Then \\(\\beta_i^{-1}\\) is a forward \\(\\alpha\\)-arrow. Then we may rewrite the triangle as  \\  so that it now consists entirely of forward \\(\\alpha\\)-arrows. This then  allows us to apply Proposition \\ref{proposition:parallel_w_to_wn_equal_in_M} to guarantee that it is a commutative diagram in \\(\\mm\\).  Thus, what we have shown is that  each triangle in the  above diagram is commutative in \\(\\mm\\). This literally means that  for each \\(i\\), </p> \\[\\begin{align*} (\\Gamma_{u_i})_A\\circ (\\beta_i)_A = (\\Gamma_{u_{i-1}})_A &amp;\\implies  (\\beta_i)_A = \\textcolor{NavyBlue}{(\\Gamma_{u_i})_A^{-1} \\circ (\\Gamma_{u_{i-1}})_A}  \\\\ (\\Gamma_{t_i})_A\\circ (\\gamma_i)_A = (\\Gamma_{t_{i-1}})_A &amp;\\implies  (\\gamma_i)_A = \\textcolor{Purple}{(\\Gamma_{t_i})_A^{-1} \\circ (\\Gamma_{t_{i-1}})_A} \\end{align*}\\] <p>Therefore, we see that \\((\\beta_k)_A \\circ \\cdots \\circ (\\beta_1)_A\\) can be written as </p> \\[\\begin{align*} \\Big(\\textcolor{NavyBlue}{(\\Gamma_{u_k})_A^{-1} \\circ (\\Gamma_{u_{k-1}})_A}\\/) \\circ \\Big( \\textcolor{NavyBlue}{(\\Gamma_{u_{k-1}})_A^{-1} \\circ (\\Gamma_{u_{k-2}})_A}\\Big) \\circ \\cdots \\circ  \\Big(\\textcolor{NavyBlue}{(\\Gamma_{u_1})_A^{-1} \\circ (\\Gamma_{u_{0}})_A} \\Big) \\end{align*}\\] <p>which is a \"telescoping\" composition that reduces to </p> \\[\\begin{align*} (\\Gamma_{u_k})_A^{-1} \\circ (\\Gamma_{u_{0}})_A. \\end{align*}\\] <p>Similarly, we can expression \\((\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_1)_A\\) as  </p> \\[\\begin{align*} \\Big(\\textcolor{Purple}{(\\Gamma_{t_{\\ell}})_A^{-1} \\circ (\\Gamma_{t_{\\ell-1}})_A}\\Big) \\circ  \\Big(\\textcolor{Purple}{(\\Gamma_{t_{\\ell-1}})_A^{-1} \\circ (\\Gamma_{t_{\\ell-2}})_A}\\Big) \\circ \\cdots \\circ  \\Big(\\textcolor{Purple}{(\\Gamma_{t_1})_A^{-1} \\circ (\\Gamma_{t_{0}})_A} \\Big) \\end{align*}\\] <p>which also reduces to </p> \\[ (\\Gamma_{t_{\\ell}})_A^{-1} \\circ (\\Gamma_{t_{0}})_A. \\] <p>However, \\(u_k = t_{\\ell}\\) and \\(u_0 = t_0\\), so that </p> \\[ (\\Gamma_{u_k})_A^{-1} \\circ (\\Gamma_{u_{0}})_A = (\\Gamma_{t_{\\ell}})_A^{-1} \\circ (\\Gamma_{t_{0}})_A \\implies (\\beta_k)_A \\circ \\cdots \\circ (\\beta_1)_A = (\\beta_k)_A \\circ \\cdots \\circ (\\beta_1)_A \\] <p>Thus we have that our original diagram in \\(\\mm\\) \\  is commutative. Therefore we have that  parallel sequences of \\(\\alpha\\)-arrows are equal in \\(\\mm\\), as desired. </p> <p>Finally, we use all of our previous work to prove Theorem \\ref{theorem:coherence_in_alpha}. In this case, the proof is simply the definition of our desired functor. We state the theorem here for the reader's convenience.</p> \\[\\begin{customthm}{\\ref{theorem:coherence_in_alpha}}[Associator Coherence.] Let $(\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)$ be a monoidal category.  For every object $A$, there exists a unique functor  $\\Phi_A: \\ww_P \\to \\mm$ which agrees with the proxy map $(-)_A$ on the objects and $\\alpha$-arrows. \\end{customthm}\\] <p>To define this functor, we will (in this order) define the functor on  (1) object, (2) \\(\\alpha\\)-arrows, (3) general morphisms of \\(\\wp\\), and then  finally show that our definition preserves composition.</p> <p>\\begin{description} \\item[Objects.]  For a pure binary word \\(w\\), we define \\(\\Phi_A(w) = (w)_A\\).</p> <p>\\item[Morphisms.] </p> <ul> <li> <p>[(1)] If \\(\\beta\\) is an \\(\\alpha\\)-arrow, we define  \\(\\Phi_A(\\beta) = (\\beta)_A\\). </p> </li> <li> <p>[(2)]  Now we define our functor on a general morphism \\(v \\to w\\) in \\(\\wp\\). For convenience  denote this as \\(\\phi_{v,w}: v \\to w\\). </p> </li> </ul> <p>We know by  Corollary \\ref{corollary:morphisms_of_wp} that there exist finitely many  forward and backward \\(\\alpha\\)-arrows \\(\\gamma_1, \\dots, \\gamma_k\\) such  that </p> \\[ \\phi_{v,w} = \\gamma_k \\circ \\cdots \\circ \\gamma_1.  \\] <p>Therefore, define </p> \\[ \\Phi_A(\\phi_{v,w}) = \\Phi(\\gamma_k \\circ \\cdots \\circ \\gamma_1)= (\\gamma_k)_A \\circ \\cdots \\circ (\\gamma_1)_A. \\] <p>By Proposition \\ref{proposition:parallel_in_M_are_equal}, we see that  this definition is well-defined.  </p> <p>Note that this definition allows the functor to also be well-defined  on identities, i.e., in all instances, \\(\\Phi_A(1_u) = 1_{u_A}\\).</p> <p>We now show that this definition of our functor behaves under composition. Let \\(\\phi_{u,v}: u \\to v\\) and \\(\\phi_{v,w}: v \\to w\\) be morphisms in  \\(\\wp\\). Then there exist sequences of \\(\\alpha\\)-arrows  \\(\\beta_1, \\dots, \\beta_k\\) and \\(\\gamma_1, \\dots, \\gamma_{\\ell}\\)  such that </p> \\[\\begin{gather*} \\phi_{u,v}= \\beta_k \\circ \\cdots \\circ \\beta_1 \\qquad \\phi_{v,w} = \\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1. \\end{gather*}\\] <p>Then we can write </p> \\[\\begin{align*} \\Phi(\\phi_{v,w} \\circ \\phi_{u,v}) &amp;= \\Phi(\\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1 \\circ \\beta_k \\circ \\cdots \\circ \\beta_1)\\\\ &amp;= (\\gamma_{\\ell})_A \\circ \\cdots \\circ (\\gamma_1)_A \\circ (\\beta_k)_A \\circ \\cdots \\circ (\\beta_1)_A\\\\ &amp;= \\Phi(\\gamma_{\\ell} \\circ \\cdots \\circ \\gamma_1) \\circ \\Phi(\\beta_k \\circ \\cdots \\circ \\beta_1)\\\\ &amp;= \\Phi(\\phi_{v,w}) \\circ \\Phi(\\phi_{u,v}) \\end{align*}\\] <p>Hence we see that our definition on morphisms behaves appropriately on composition,  so that \\(\\Phi\\) is in fact a functor.  \\end{description}</p> <p>We conclude this section by proving the Diamond Lemma, which we have now  seen to play a critical role in this proof.  \\begin{customlemma}{\\ref{lemma:diamond_lemma}}[Diamond Lemma] Let \\(w\\) be a pure binary word and suppose \\(\\beta_1,\\beta_2\\) are two  forward \\(\\alpha\\)-arrows as below. \\  There exists a pure binary word \\(z\\) and two \\(\\gamma_1: w_1 \\to z, \\gamma_2: w_2 \\to z\\), with \\(\\gamma_1, \\gamma_2\\) a composition of forward \\(\\alpha\\)-arrows, such that for any monoidal category  \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) the diagram below is  commutative in \\(\\mm\\). \\  is commutative.  \\end{customlemma}</p> <p>As we said before, the above lemma is an existence result,  so we emphasize this fact by coloring  the arrows, which we are asserting to exist, Green. </p> <p> We will prove this using induction on the length of \\(w = u \\otimes v\\). Therefore, throughout the proof, suppose the result is already true for all words of length less than that  of \\(w\\).  <p>We proceed in a case-by-case basis, exhausting the possible forms of \\(\\beta_1\\) and \\(\\beta_2\\). For our purposes, we will express \\(w = u \\otimes v\\).  Whenever \\(\\ll(v) &gt; 1\\), we write \\(v= s \\otimes t\\). </p> <p>Let \\(\\beta_1, \\beta_2\\) be forward \\(\\alpha\\)-arrows.  Then \\(\\beta_1\\) could be of the forms</p> \\[ \\alpha_{u,s,t}\\qquad 1_u \\otimes \\gamma_1 \\qquad \\gamma_1\\otimes 1_v \\] <p>and \\(\\beta_2\\) could be of the forms </p> \\[ \\alpha_{u,s,t}\\qquad 1_u\\otimes\\gamma_2 \\qquad \\gamma_2\\otimes 1_v. \\] <p>with \\(\\gamma_1, \\gamma_2\\) already forward \\(\\alpha\\)-arrows. Therefore, our cases for \\(\\beta_1,\\beta_2\\),  displayed in tuples, are listed in the table below. \\begin{center}</p> \\((\\beta_1,\\beta_2)\\) \\(\\alpha_{u,s,t}\\) \\(1_u\\otimes \\gamma_2\\) \\(\\gamma_2 \\otimes 1_v\\)                 \\ [0.2cm]                                  \\(\\alpha_{u,s,t}\\) \\(\\textcolor{Red}{(\\alpha_{u,s,t}, \\alpha_{u,s,t})}\\) \\(\\textcolor{NavyBlue}{(\\alpha_{u,s,t},1_u \\otimes \\gamma_2)}\\) \\(\\textcolor{Orange}{(\\alpha_{u,s,t}, \\gamma_2\\otimes 1_v)}\\) 0.2cm]                 \\(1_u \\otimes \\gamma_1\\) \\(\\textcolor{NavyBlue}{(1_u \\otimes \\gamma_1, \\alpha_{u,s,t})}\\) \\(\\textcolor{Magenta}{(1_u \\otimes \\gamma_1 ,1_u \\otimes \\gamma_2)}\\) \\(\\textcolor{Purple}{(1_u \\otimes \\gamma_1, \\gamma_2\\otimes 1_v)}\\) 0.2cm]                 \\(\\gamma_1\\otimes 1_v\\) \\(\\textcolor{Orange}{(\\gamma_1\\otimes 1_v, \\alpha_{u,s,t})}\\) \\(\\textcolor{Purple}{(\\gamma_1\\otimes 1_v ,1_u \\otimes \\gamma_2)}\\) \\(\\textcolor{ProcessBlue}{(\\gamma_1\\otimes 1_v, \\gamma_2\\otimes 1_v)}\\) <p>\\end{center} While there are 9 cases displayed above, we have pointed out via color the pairs of cases which are logically  equivalent to each other due to the symmetry of our problem. Therefore, we actually have 6 cases to check  We now proceed to the proof.</p> <p>\\noindentCase 1: \\(\\textcolor{Red}{(\\alpha_{u,s,t}, \\alpha_{u,s,t})}.\\)\\ In this case, we have that \\(\\beta_1 =  \\beta_2\\), for which the statement is trivially true. \\</p> <p>\\noindentCase 2: \\(\\textcolor{Purple}{(\\gamma_1\\otimes 1_v ,1_u \\otimes \\gamma_2)}\\)\\ Suppose \\(\\beta_1 = \\gamma_1 \\otimes 1_v\\) and \\(\\beta_2 = 1_u\\otimes \\gamma_2\\).  Here, \\(\\gamma_1: u \\to u'\\) and \\(\\gamma_2: v \\to  v'\\) for some pure binary  words $ u',v'$. Then we get the diagram  \\  which commutes by the bifunctoriality of \\(\\otimes\\).  \\ \\ Case 3: \\(\\textcolor{ProcessBlue}{(\\gamma_1\\otimes 1_v, \\gamma_2\\otimes 1_v)}\\)\\ Suppose \\(\\beta_1 = \\gamma_1 \\otimes 1_v\\) and \\(\\beta_2 = \\gamma_2 \\otimes 1_v\\) with \\(\\gamma_1: u \\to u_1\\) and \\(\\gamma_2: u \\to u_2\\) both forward \\(\\alpha\\)-arrows.</p> <p>Then in this case we have the triangle below in \\(\\mm\\). \\  Note that the above diagram  is the image of diagram  \\  under the functor \\((-)\\otimes (v)_A\\). As \\(\\ll(u) &lt; \\ll(u\\otimes v)\\),  we know by our induction hypothesis that  there exists a pure binary word \\(z\\) and a pair of composite, forward \\(\\alpha\\)-arrows \\(\\sigma_1: u_1 \\to z\\) and  \\(\\sigma_2: u_2 \\to z\\) such that the diagram below commutes in \\(\\mm\\). \\  Therefore we can apply the functor  \\((-)\\otimes (v)_A\\) on the above diagram  to obtain the commutative diagram below \\  which proves this case. \\ \\ Case 4: \\(\\textcolor{Magenta}{(1_u \\otimes \\gamma_1 ,1_u \\otimes \\gamma_2)}\\)\\ The next case is when \\(\\beta_1 = 1_u \\otimes \\gamma_1\\) and \\(\\beta_2 = 1_u\\otimes\\gamma_2\\) with \\(\\gamma_1: v \\to v_1\\) and \\(\\gamma_2: v \\to v_2\\). However, this can be proved  in a similar manner as the previous case using  the induction hypothesis and the functor \\((u)_A \\otimes (-)\\). \\ \\noindentCase 5: \\(\\textcolor{Orange}{(\\alpha_{u,s,t}, \\gamma_2\\otimes 1_v)}\\)\\ Let \\(\\beta_1 = \\alpha_{u,s,t}\\),  so that \\(w = u \\otimes (s \\otimes t)\\).  Let \\(\\beta_2 = \\gamma_2 \\otimes 1_{v} = \\gamma_2 \\otimes 1_{s\\otimes t}\\)  with \\(\\gamma_2: u \\to u'\\) a forward \\(\\alpha\\)-arrow. Then we will have the diagram in \\(\\mm\\) \\  which commutes in \\(\\mm\\) by naturality of \\(\\alpha\\).  \\ \\ Case 6: \\(\\textcolor{NavyBlue}{(\\alpha_{u,s,t},1_u \\otimes \\gamma_2)}\\)\\ Let \\(\\beta_1 = \\alpha_{u,s,t}\\), \\(\\beta_2 = 1_u \\otimes \\gamma\\) with \\(\\gamma\\) a forward \\(\\alpha\\)-arrow with domain \\(s \\otimes t\\).  By the recursive definition of a forward \\(\\alpha\\)-arrow,  we have three possible cases for \\(\\gamma\\). \\ \\ Case 6.1: \\(\\gamma = 1_s \\otimes \\gamma'\\)\\ With \\(\\gamma = 1_s \\otimes \\gamma'\\) with \\(\\gamma' : t\\to t'\\)  already a forward \\(\\alpha\\)-arrow, we have the diagram in \\(\\mm\\) \\  which commutes in \\(\\mm\\) by naturality of \\(\\alpha\\). \\ \\ Case 6.2: \\(\\gamma = \\gamma' \\otimes 1_t\\)\\ If \\(\\gamma = \\gamma' \\otimes 1_t\\) with \\(\\gamma': s \\to s'\\)  already a forward \\(\\alpha\\)-arrow, we can create the diagram  \\  which also commutes in \\(\\mm\\) by naturality of \\(\\alpha\\). \\ Case 6.3: \\(\\gamma = \\alpha_{s,p,q}\\)\\ The third case for \\(\\gamma\\) is when \\(\\gamma = \\alpha_{s,p,q}\\).  In this case, we  express \\(w = u\\otimes (s\\otimes (p \\otimes q))\\).  We can then construct the diagram \\  which is always commutative in \\(\\mm\\). In this case, the word \\(((u\\otimes s)\\otimes p)\\otimes q\\) acts as our vertex \\(z\\) which completes the diagram. </p> <p>As we have exhausted all possible cases, we see that the statement is true for pure binary words of rank \\(k+1\\) if it is true for all pure binary words with rank at most \\(k\\). By induction, the statement is true for all binary words of any rank,  so that we have proved the theorem. </p> <p>\\subsection*{Step Four: Binary Words}</p> <p>So far we have established a unique functor \\(\\Phi_A: \\wp \\to \\mm\\)  for each object \\(A\\) of any given monoidal category \\(\\mm\\), and this functor grants us coherence in the associators between iterated  monoidal products of a single object. We now consider such monoidal products with the identity \\(I\\) as well,  so that we may say something about coherence with regard to the  unitors \\(\\lambda\\) and \\(\\rho\\) in a general monoidal category.  Towards that goal, we now consider binary words (not just pure binary words) and introduce some definitions.</p> <p>Recall that \\(\\ll\\) calculates the length of a binary word, or more informally,  the number of \\(x_1\\)'s in a binary word. We now introduce a dual quantity which  instead counts the number of \\(x_0\\)</p> <p> Let \\(w\\) be a binary word. Define the identity length of \\(w\\),  denoted \\(\\mathcal{E}\\), recursively as follows.  <ul> <li> <p>\\(\\ee(x_0) = 1\\) and \\(\\ee(x_1) = 0\\).</p> </li> <li> <p>\\(\\ee(u \\otimes v) = \\ee(u) + \\ee(v)\\).</p> </li> </ul> <p></p> <p>Similarly to how \\(\\ll(-)\\) counts the number of \\(x_1\\)'s in a binary word, \\(\\ee(-)\\)  counts the number of \\(x_0\\)'s in a binary word.</p> <p>Next, we introduce the following concept that will later on  be key to our proof of Mac Lane's Coherence Theorem.</p> <p> Let \\(w\\) be a binary word.  We define the clean word derived from \\(w\\), denoted \\(\\overline{w}\\), recursively as follows.  <ul> <li> <p>We set \\(\\overline{x_1} = x_1\\). </p> </li> <li> <p>If \\(\\ll(w) = 0\\) (i.e., it has no instance of \\(x_1\\)) then \\(\\overline{w} = x_0\\). </p> </li> <li> <p>Let \\(u,v\\) be binary words with \\(\\ll(u) = 0\\) and \\(\\ll(v) &gt; 0\\).  Then [ \\overline{u \\otimes v} = \\overline{v \\otimes u} = \\overline{v} ]</p> </li> <li> <p>Let \\(u,v\\) be binary words with \\(\\ll(u), \\ll(v) &gt; 0\\). Then  \\(\\overline{u\\otimes v} =  \\overline{u} \\otimes \\overline{v}\\). </p> </li> </ul> <p></p> <p>Note that for a pure binary word \\(w\\), we have that \\(\\overline{w} =w\\). Informally, the clean word of a binary word of nonzero length is simply the pure  binary word obtained by removing all instances of the identity from its  expression. In the case for a binary word with zero length, we naturally define  the clean word to be \\(x_0\\) .</p> <p> We offer some examples of clean words obtained from binary words. \\begin{center} Word Clean Word \\(x_0\\otimes (x_0 \\otimes x_0)\\) \\(x_0\\) \\(x_0 \\otimes (x_1 \\otimes x_0)\\) \\(x_1\\) \\((x_1 \\otimes x_0) \\otimes x_1\\) \\(x_1 \\otimes x_1\\) \\(((x_1 \\otimes x_0) \\otimes x_0)\\otimes x_1\\) \\(x_1 \\otimes x_1\\) \\((x_1 \\otimes x_0) \\otimes ((x_1 \\otimes x_0) \\otimes x_1)\\) \\(x_1 \\otimes (x_1 \\otimes x_1)\\) <p>\\end{center}   The above example also shows that two different binary words  can have the same clean word.  </p> <p>[Monoidal Arrows] A forward monoidal arrow of \\(\\ww\\) is defined recursively as follows.  <ul> <li>For any triple of binary words \\(u, v, w\\), the morphisms </li> </ul> \\[\\begin{align*} \\alpha_{u, v, w}&amp;: u \\otimes (v \\otimes w) \\isomarrow (u \\otimes v)\\otimes w\\\\ \\lambda_{u}&amp;: x_0 \\otimes u \\isomarrow u\\\\ \\rho_{u}&amp;: u\\otimes x_0 \\isomarrow u \\end{align*}\\] <p>are, respectively, forward \\(\\alpha\\)-, \\(\\lambda\\)-, and \\(\\rho\\)-arrows.  They are collectively  defined to be forward monoidal arrows. </p> <ul> <li>For any binary word \\(u\\) and forward monoidal arrow \\(\\mu\\), the morphisms</li> </ul> \\[ 1_{u} \\otimes \\mu \\qquad \\mu \\otimes 1_{u} \\] <p>are forward monoidal arrows.</p> <p>Finally, we say a backward monoidal arrow  is the inverse of a forward monoidal arrow.  </p> <p>We also establish the following terminology to distinguish our \\(\\alpha\\)-arrows  from our \\(\\lambda\\) and \\(\\rho\\) arrows.</p> <p> A forward unitor arrow is either a forward \\(\\lambda\\)-arrow or a forward \\(\\rho\\)-arrow. Similarly, a  backward unitor arrow is the inverse of a forward unitor arrow. </p> <p>As we have already seen forward \\(\\alpha\\)-arrows,  we provide examples of forward and backward \\(\\lambda, \\rho\\)-arrows.</p> <p>\\begin{example} Below we have a forward and backward \\(\\lambda\\)-arrow. \\  We also have forward and backward \\(\\rho\\)-arrows below. \\  end{example}</p> <p>We now move onto proving some important lemmas regarding  monoidal arrows that we will use for the coherence theorem.</p> <p>The first three are quick, but have particular importance. </p> <p> Let \\(w\\) be a binary word, \\(w \\ne x_0\\).  Then \\(\\ee(w) = 0\\) if and only if \\(w = \\overline{w}\\).  </p> <p>Note that \\(w = x_0\\) is the only case for which the above proposition is not true,  since \\(x_0 = \\overline{x_0}\\) but \\(\\ee(x_0) \\ne 0\\). Hence, our reasoning for excluding it (and it is not a case we will need to concern ourselves with).</p> <p> Suppose \\(\\ee(w) = 0\\), and let us prove the forward direction by induction on  the length of the word.  Let us write \\(w = u \\otimes v\\), suppose that the statement is true for all pure binary words with  length less than \\(w\\). Observe that   \\[ w = u \\otimes v = \\overline{u} \\otimes \\overline{v} = \\overline{u \\otimes v} = \\overline{w}. \\] <p>where we used the induction hypothesis on \\(u, v\\) which have smaller length than  \\(w\\). Thus we see that \\(w = \\overline{w}\\). </p> <p>Conversely, suppose \\(\\overline{w} = w\\), \\(w \\ne x_0\\), and suppose the statement is true for binary words  with length less than \\(w\\). Write \\(w  = u \\otimes v\\).  By the definition  of a clean word, the only way we can have \\(\\overline{w} = w\\) is if \\(u,v\\)  are binary words with nonzero length.  Therefore, if \\(\\overline{w} = w\\) we see that </p> \\[ \\overline{u} \\otimes \\overline{v} = u \\otimes v. \\] <p>Since \\(u, v\\) have smaller length than \\(w\\), we may use the induction  hypothesis to conclude that \\(\\ee(u) = \\ee(v) = 0\\). Hence,  \\(\\ee(w) = 0\\), as desired.  </p> <p> Let \\(w\\) be a binary word. Suppose \\(\\iota: w \\to w'\\) is a forward unitor arrow.  Then \\(\\ee(w') = \\ee(w) - 1\\).  </p> <p>In other words, any unitor arrow always takes away exactly one identity.</p> <p> We prove this by examining the possible cases for \\(\\iota\\).  Write \\(w = u \\otimes v\\). As \\(\\iota\\) is a forward unitor arrow, it has four possible forms.  <ul> <li>[(1)] Suppose \\(\\iota = \\lambda_v: x_0 \\otimes v \\to v\\).  As </li> </ul> \\[ \\ee(v) = \\ee(v) + \\ee(x_0) - 1 = \\ee(v \\otimes x_0) - 1    \\] <p>we see that the statement is satisfied in this case.</p> <ul> <li> <p>[(2)] If \\(\\iota = \\rho_u: u \\otimes x_0 \\to u\\), we can use a similar argument as in (1) to prove the statement.</p> </li> <li> <p>[(3)] Suppose \\(\\iota = 1_{u} \\otimes \\kappa: u \\otimes v \\to u \\otimes v'\\)  where \\(\\kappa: v \\to v'\\) is a forward unitor arrow for which the  statement is already true. Then \\(\\ee(v') = \\ee(v) - 1\\). Hence, </p> </li> </ul> \\[ \\ee(u \\otimes v') = \\ee(u \\otimes v) - 1.  \\] <p>Therefore the statement is satisfied for \\(1_u \\otimes \\kappa\\) if it is  true for \\(\\kappa\\). </p> <ul> <li>[(4)] If \\(\\iota = \\kappa \\otimes 1_v: u \\otimes v \\to u' \\otimes v\\) where \\(\\kappa\\) is a forward unitor for which the statement is already true,  then we may prove this case by following a similar argument as in (3). </li> </ul> <p>As we have examined all cases, we may conclude that for every  forward unitor \\(\\iota: w \\to w'\\), we have that \\(\\ee(w') = \\ee(w) - 1\\)  as desired.  </p> <p> Let \\(\\iota: w \\to w'\\) be a forward unitor arrow. Then  \\(\\overline{w} = \\overline{w}'\\).  </p> <p>In other words, unitor arrows do not alter the particular format of a clean word.</p> <p> First, observe that the result is trivial if \\(\\ll(w) = \\ll(w') = 0\\).  Therefore, let \\(w = u \\otimes v\\) be such a binary  word with \\(\\ee(w) &gt; 0\\). Suppose the statement is true for  binary words \\(v\\) such that \\(\\ee(v) &lt; \\ee(w)\\).  Let \\(\\iota:w \\to w'\\) be a forward unitor arrow.  By the recursive definition of \\(\\iota\\), our forward unitor arrow has  four possible forms.  <ul> <li> <p>[(1)] Suppose \\(\\iota = \\lambda_v: x_0 \\otimes v \\to v\\).  However, note that \\(\\overline{x_0 \\otimes v} = \\overline{v}\\),  so that this case is true. </p> </li> <li> <p>[(2)] If \\(\\iota = \\rho_u: u \\otimes x_0 \\to u\\),  then this case may be proven in a similar manner as case (1).</p> </li> <li> <p>[(3)] Suppose  \\(\\iota = 1_u \\otimes \\kappa: u \\otimes v \\to u \\otimes v'\\) where  \\(\\kappa\\) is a forward unitor arrow for which the result is already true.  Since \\(\\ll(u \\otimes v) &lt; 0\\), we have a few subcases. </p> </li> </ul> <p>Suppose \\(\\ll(v) &gt; 0\\).  Then by our assumption on \\(\\kappa\\),  \\(\\overline{v} = \\overline{v}'\\). Therefore, if  \\(\\ll(u) = 0\\), we see that </p> \\[ \\overline{u \\otimes v} = \\overline{v}  = \\overline{v}' = \\overline{u \\otimes v}' \\] <p>which satisfies this case.  If instead \\(\\ll(u) &gt; 0\\), then </p> \\[ \\overline{u \\otimes v} = \\overline{u}\\otimes\\overline{v} =  \\overline{u}\\otimes\\overline{v}' = \\overline{u \\otimes v}' \\] <p>which again satisfies the case. </p> <p>Finally, suppose \\(\\ll(v) = 0\\).  Then \\(\\overline{u \\otimes v} =  \\overline{u} = \\overline{u \\otimes v}'\\). </p> <p>In all cases we see that \\(\\overline{u \\otimes v} = \\overline{u \\otimes v}'\\)  as desired. </p> <ul> <li>[(3)] Our third case if when \\(\\iota = \\kappa \\otimes 1_v: u \\otimes v \\to u' \\otimes v\\)  with \\(\\kappa\\) a forward  unitor for which the result is already true. However, this  case can be proved similarly as in case (2).</li> </ul> <p>In all instances, we see that for a forward unitor arrow \\(\\iota: w \\to w'\\),  we have that \\(\\overline{w} = \\overline{w}'\\), as desired.  </p> <p>The following lemma is an important existence result that will be used in the  next proposition.</p> <p> Let \\(w\\) be a binary word with \\(\\ee(w) &gt; 0\\). Then there exists a forward unitor with domain \\(w\\). </p> <p> We prove this by induction on the total length of a binary word  \\(\\ll(w) + \\ee(w)\\).  Thus, let \\(w = u \\otimes v\\) be a binary word with \\(\\ee(w) &gt; 0\\)  and suppose the statement is true  for all binary words \\(z\\) with \\[ \\ll(z) + \\ee(z) &lt; \\ll(w) + \\ee(w). \\] <p>Then we have a few cases for \\(w\\). </p> <ul> <li> <p>[(1)] Suppose \\(u = x_0\\). Then we take the forward unitor  \\(\\lambda_v: x_0 \\otimes v \\to v\\).</p> </li> <li> <p>[(2)] Suppose \\(v = x_0\\). We may similarly take \\(\\rho_u: u \\otimes x_0 \\to u\\),  so that this case is satisfied. </p> </li> <li> <p>[(3)] Suppose \\(u, v \\ne x_0\\). Since \\(\\ee(w) &gt; 1\\),  either \\(\\ee(u)\\) or \\(\\ee(v) &gt; 0\\). Without loss of generality, suppose  \\(\\ee(u) &gt; 0\\). Since </p> </li> </ul> \\[ \\ll(u) + \\ee(u) = \\ll(u) + \\ee(u) \\] <p>we may apply our induction hypothesis to conclude that there exists  a forward unitor \\(\\iota: u \\to u'\\) with domain \\(u\\).  Hence, the morphism </p> \\[ \\iota \\otimes 1_v : u \\otimes v \\to u' \\otimes v \\] <p>is a forward unitor with domain \\(u \\otimes v = w\\). </p> <p>As we have evaluated all cases, we see that the statement is true for all binary words as desired.  </p> <p>The previous four lemmas now give rise to the following proposition. </p> <p> Let \\(w\\) be a binary word with \\(\\ee(w) = \\ell\\). Then there exists a  composable sequence of \\({\\ell}\\)-many forward unitor arrows \\(\\iota_{\\ell}, \\cdots, \\iota_1\\) as below: \\[ \\iota_{\\ell} \\circ \\cdots \\circ \\iota_1: w \\to w'. \\] <p>Moreover, for every such chain, we have that \\(w' = \\overline{w}\\).  </p> <p> To prove existence of such a chain for every binary word with nonzero identity  length, we may proceed by induction. Let \\(w\\) be a binary word with \\(\\ee(w) &gt; 0\\), and  suppose that such a chain exists for binary words \\(v\\) with  \\(\\ee(v) &lt; \\ee(w)\\). Then by Lemma 2.5.10, there exists a forward unitor  \\(\\iota: w \\to w'\\). By Lemma 2.5.8, \\(\\ee(w') = \\ee(w) - 1\\), so by our induction  hypothesis, there exists a chain of forward unitor arrows  \\[ \\iota_{{\\ell}-1}\\circ \\cdots \\circ \\iota_1: w' \\to \\overline{w}'. \\] <p>Hence, \\(\\iota \\circ \\iota_{{\\ell}-1}\\circ \\cdots \\circ \\iota_1: w \\to \\overline{w}\\) is a forward chain of unitors with initial domain \\(w\\), which proves existence.</p> <p>To prove that \\(w' = \\overline{w}\\),  denote the domain and codomain of our unitors  \\(\\iota_i: w_{i-1} \\to w_{i}\\), so that \\(w_0 = w\\).  By Lemma 2.5.9, for each \\(i\\) we have that \\(\\overline{w_{i-1}} = \\overline{w_{i}}\\).  Hence \\(\\overline{w} = \\overline{w_{{\\ell}}}\\).  By Lemma 2.5.8, we have that \\(\\ee(w_i) = \\ee(w_{i-1}) - 1\\).  Therefore, </p> \\[ \\ee(w_{\\ell}) = \\ee(w) - {\\ell} = 0.   \\] <p>However, by Lemma 2.5.7, we see that this implies \\(w_{\\ell} = \\overline{w_{\\ell}} = \\overline{w}\\).  Hence we see that </p> \\[ \\iota_{\\ell} \\circ \\cdots \\circ \\iota: w \\to \\overline{w} \\] <p>as desired. </p> <p>The previous proposition immediately implies the next. </p> <p> Let \\(w\\) be a binary word with \\(\\ll(w) &gt; 0\\). Then there exists a sequence  of forward monoidal arrows from \\(w\\) to \\(w^{(n)}\\).  </p> <p> By Lemma \\ref{lemma:existence_w_to_clean_w}, we have a  sequence of forward unitor arrows from \\(w\\) to \\(\\overline{w}\\). \\[ \\mu_k \\circ \\cdots \\circ \\mu_1: w \\to \\overline{w} \\] <p>Since \\(\\overline{w}\\) is a  pure binary word, we can then use Proposition \\ref{proposition_existence_of_w_to_wn} to guarantee a sequence of forward \\(\\alpha\\)-arrows  from \\(\\overline{w}\\) to \\(w^{(n)}\\). </p> \\[ \\beta_{\\ell} \\circ \\cdots \\circ \\beta_1: \\overline{w} \\to w^{(n)}   \\] <p>Composing these morphisms then gives us our desired monoidal arrow:</p> \\[ \\beta_{\\ell} \\circ \\cdots \\circ \\beta_1 \\circ \\mu_k \\circ \\cdots \\circ \\mu_1: w \\to w^{(n)} \\] <p>so that such a sequence of forward monoidal arrows exists. </p> <p>And the previous proposition gives us the following corollary.</p> <p> Every morphism in \\(\\ww\\) can be expressed as a composition of  a sequence of forward and backward monoidal arrows. </p> <p> The proof is the same exact proof as that of Corollary \\ref{corollary:morphisms_of_wp}.  We use the previous proposition with the fact that \\(\\ww\\) is a thin category to conclude this.  </p> <p>\\subsection*{Step Five: Coherence for \\(A^{\\otimes n}\\) for \\(\\rho, \\lambda\\)}</p> <p>In this section, we extend the work we've completed with  the associators to now include the unitors. We will obtain a theorem  similar to Theorem \\ref{theorem:coherence_in_alpha}. To even state the  theorem, we need to introduce a new definition. </p> <p> Let \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category.  For each object \\(A\\) in \\(\\mm\\), we define the general proxy map of \\(A\\) to be the partial functor \\((-)_A: \\ww \\to \\mm\\) defined as follows.  <p>\\begin{description} \\item[Objects]  We define the general proxy map on objects recursively. </p> <ul> <li> <p>We set \\((x_0)_A = I\\) and \\((x_1)_A = A\\)</p> </li> <li> <p>For a binary word \\(w = u \\otimes v\\) we set:</p> </li> </ul> \\[ (w)_A = (u \\otimes v)_A = (u)_A\\otimes (v)_A \\] <p>\\item[Morphisms] We define the partial functor only on \\(\\alpha\\)-, \\(\\lambda\\)-, and \\(\\rho\\)-arrows.  This is also done recursively. </p> <ul> <li>For binary words \\(u,v,w\\), we set:</li> </ul> \\[\\begin{align*} (\\alpha_{u,v,w})_A &amp;= \\alpha_{(u_A, v_A, w_A)} : u_A \\otimes (v_A \\otimes w_A) \\isomarrow (u_A \\otimes v_A) \\otimes w_A \\\\ (\\lambda_{u})_A &amp;= \\lambda_{u_A}: I \\otimes u_A \\isomarrow u_A \\\\ (\\rho_{u})_A&amp;= \\rho_{u_A} : u_A \\otimes I \\isomarrow u_A \\end{align*}\\] <ul> <li>For a more general \\(\\alpha, \\lambda\\), or \\(\\rho\\)-arrow of  the form \\(1_{u}\\otimes \\beta\\) or \\(\\beta\\otimes 1_{u}\\) we set:</li> </ul> \\[\\begin{align*} &amp;(1_{u} \\otimes \\beta)_A = 1_{u_A} \\otimes (\\beta)_A\\\\  &amp;(\\beta \\otimes 1_{u})_A = (\\beta)_A\\otimes 1_{u_A} \\end{align*}\\] <p>\\end{description} Before concluding this definition, we note that there is some potential  ambiguity in our definition on the unitors. This is because sometimes  a forward unitor arrow in \\(\\ww\\) can be expressed in two ways. </p> <p>The reader may check that all possible cases for ambiguity are the three cases below. \\  As parallel morphisms in \\(\\ww\\), they are equal. Therefore, in order for our definition to be well-defined, we need  that the corresponding pairs of morphisms \\  to be equal in \\(\\mm\\).  One can show that these morphisms are equal in \\(\\mm\\) using the unitor diagrams \\ref{mon_definition_diag_2}, \\ref{mon_definition_diag_3}, and \\ref{mon_definition_diag_4}.   Regarding our notation, note that we are recycling the same notation from the proxy map  to the general proxy map. This is because the only difference between the  two is that the general proxy map is simply an extension of the proxy map which  is now defined on identity elements \\(x_0\\) and unitors. </p> <p>The goal of this section is to prove the following theorem, which can  be thought of as an extension of Theorem \\ref{theorem:coherence_in_alpha}.</p> <p>[Coherence in Unitors] Let \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category. For each object \\(A\\), there exists a unique strict monoidal functor \\(\\Delta_A: \\ww \\to \\mm\\)  which agrees with the general proxy map on objects and monoidal morphisms.  </p> <p>The above theorem is implied by Proposition \\ref{proposition:full_parallel_in_M_are_equal} (stated below), in the same way that Theorem \\ref{theorem:coherence_in_alpha} followed  from Proposition \\ref{proposition:parallel_in_M_are_equal}.</p> <p> Let \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category, and consider  two binary words \\(v,w\\). Let \\(\\mu_1, \\dots, \\mu_k\\) and \\(\\eta_1, \\dots,\\eta_{\\ell}\\) be monoidal arrows with: \\[ \\mu_k \\circ \\cdots \\circ \\mu_1, \\; \\eta_{\\ell} \\circ \\cdots \\circ \\eta_1: v \\to w \\] <p>Then \\((\\mu_k)_A \\circ \\cdots \\circ (\\mu_1)_A = (\\eta_{\\ell})_A\\circ\\cdots\\circ (\\eta_1)_A\\)  in \\(\\mm\\). </p> <p>The above proposition is implied by Proposition \\ref{proposition:full_parallel_w_to_wn} (stated below),  in the same way that Proposition \\ref{proposition:parallel_in_M_are_equal}  followed from Proposition \\ref{proposition:parallel_w_to_wn_equal_in_M}</p> <p> Let \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category, and consider  a binary word \\(w\\). Let \\(\\mu_1, \\dots, \\mu_k\\) and \\(\\eta_1, \\dots,\\eta_{\\ell}\\) be forward monoidal arrows with: \\[ \\mu_k \\circ \\cdots \\circ \\mu_1,\\;  \\eta_{\\ell} \\circ \\cdots \\circ \\eta_1: w \\to w^{(n)} \\] <p>Then \\((\\mu_k)_A \\circ \\cdots \\circ (\\mu_1)_A = (\\eta_{\\ell})_A\\circ\\cdots\\circ (\\eta_1)_A\\)  in \\(\\mm\\). </p> <p>Once we have the above proposition, we can prove Proposition \\ref{proposition:full_parallel_in_M_are_equal}, and hence our desired theorem, using  the same technique as in in the Proof of Proposition \\ref{proposition:parallel_in_M_are_equal}. </p> <p>We briefly recall such techniques: We consider two parallel chains of monoidal arrows. We then  connect each object in the chain to \\(w^{(n)}\\) with a chain of forward monoidal arrow (recall that  a chain must exist for each object). We then have a bunch of adjacent triangles with apex \\(w^{(n)}\\) and  we can conclude via the Proposition \\ref{proposition:full_parallel_w_to_wn} that each such triangle commutes. We then conclude that  the original two parallel chains form  a commutative diagram in \\(\\mm\\). Thus, our two chains have the same composite in \\(\\mm\\). This then proves Proposition \\ref{proposition:full_parallel_in_M_are_equal}, which then grants us Theorem \\ref{theorem:coherence_in_unitors}. </p> <p>As our goal has been reduced to proving Proposition \\ref{proposition:full_parallel_w_to_wn}, we prove this proposition using the following two results. </p> <p>The first result is the following proposition.</p> <p>[Arrow Reorganization] Let \\(\\mu_1, \\dots, \\mu_k\\) be composable forward monoidal arrows with  \\(\\ell\\)-many unitor arrows. Then there exist composable forward unitor arrows  \\(\\eta_1, \\dots, \\eta_{\\ell}\\) and forward \\(\\alpha\\)-arrows \\(\\eta_{\\ell + 1}, \\dots \\eta_m\\)  such that, for any monoidal category \\(\\mm\\) with object \\(A\\), we have that \\[ (\\mu_k)_A \\circ \\cdots \\circ (\\mu_1)_A = \\overbrace{(\\eta_m)_A \\circ \\cdots \\circ (\\eta_{\\ell + 1})_A}^{\\text{Forward } \\alpha's}\\circ \\overbrace{(\\eta_{\\ell})_A\\circ \\cdots \\circ (\\eta_1)_A}^{\\text{Unitors in front}} \\] <p>in \\(\\mm\\).   </p> <p>The above proposition basically states that monoidal arrows can be reorganized in a particular way  with all of the unitors in the front. The second result that we need in order to prove Proposition  \\ref{proposition:full_parallel_w_to_wn} is the following proposition. </p> <p>[Unitor-Chain Equivalence] Let \\(w\\) be a binary word with nonzero length and with \\(\\ee(w) = k\\).  Suppose \\(\\mu_1, \\dots, \\mu_k\\) and \\(\\eta_1, \\dots, \\eta_k\\) are a  composable sequence of forward  unitor arrows: \\[ \\mu_k \\circ \\cdots \\circ \\mu_1, \\; \\eta_k \\circ \\cdots \\circ \\eta_1: w \\to \\overline{w} \\] <p>Then \\((\\mu_k)_A \\circ \\cdots \\circ (\\mu_1)_A =  (\\eta_k)_A \\circ \\cdots \\circ (\\eta_1)_A\\) in \\(\\mm\\). </p> <p>For the sake of organization, we will assume the validity of these two  results now so that we may prove \\ref{proposition:full_parallel_w_to_wn}  We will then prove these two results in the next section.</p> <p>\\begin{varprf}[Proof of Proposition \\ref{proposition:full_parallel_w_to_wn}] \\textcolor{white}{Hello!} \\ Let </p> \\[ \\mu_{n_1} \\circ \\cdots \\circ \\mu_1, \\; \\eta_{n_2} \\circ \\cdots \\circ \\eta_1: w \\to w^{(n)} \\] <p>be any two composites of forward monoidal arrows from \\(w\\) to \\(w^{(n)}\\). Since \\(\\ee(w) = k\\) and \\(\\ee( w^{(n)}) = 0\\), we know  by Lemma \\ref{lemma:unitors_decrease_unit_length} that there are exactly \\(k\\)-many forward unitors in each expression.  We can then use Proposition  \\ref{proposition:arrow_reorganization} to find forward unitor arrows \\(\\gamma_1, \\dots \\gamma_k, \\delta_1, \\dots, \\delta_k\\) and forward \\(\\alpha\\)-arrows  \\(\\gamma_{k+1}, \\dots, \\gamma_{m_1}\\), \\(\\delta_{k+1}, \\dots, \\delta_{m_2}\\) such that:</p> \\[\\begin{align*} &amp;(\\mu_{n_1})_A \\circ \\cdots \\circ (\\mu_1)_A = \\overbrace{(\\gamma_{m_1})_A \\circ \\cdots \\circ (\\gamma_{k+1})_A}^{\\text{Forward } \\alpha's}\\circ \\overbrace{(\\gamma_{k})_A\\circ \\cdots \\circ (\\gamma_1)_A}^{\\text{Unitors in front}}\\\\ &amp;(\\eta_{n_2})_A \\circ \\cdots \\circ (\\eta_1)_A = \\overbrace{(\\delta_{m_2})_A \\circ \\cdots \\circ (\\delta_{k+1})_A}^{\\text{Forward } \\alpha's}\\circ \\overbrace{(\\delta_{k})_A\\circ \\cdots \\circ (\\delta_1)_A}^{\\text{Unitors in front}} \\end{align*}\\] <p>By Proposition \\ref{proposition:exists_w_to_clean_w},  we know that the domain of the composition of our unitors is \\(\\overline{w}\\):</p> \\[ \\gamma_{k} \\circ \\cdots \\circ \\gamma_1,\\, \\delta_{k} \\circ \\cdots \\circ \\delta_1: w \\to \\overline{w} \\] <p>Diagramatically, our situation is displayed below. \\  By Proposition \\ref{proposition:unitor_chain_equivalence}, the upper half of this diagram (above \\((\\overline{w})_A\\))  must commute. By  Proposition \\ref{proposition:parallel_in_M_are_equal},  the bottom half of this diagram (below \\((\\overline{w})_A\\)), which  consists entirely of forward \\(\\alpha\\)-arrows, must commute. Therefore, the entire diagram commutes, and this completes the proof. \\end{varprf}</p> <p>\\subsection*{Step Six: Arrow Reorganization and Unitor Chain Equivalence} We now discuss what it takes to prove the  Arrow Reorganization and Unitor-Chain Equivalence results. </p> <p>To prove the Arrow Reorganization result, it suffices to prove a special case which is precisely stated in the following lemma.</p> <p>[Associator-Unitor Swap.] Let \\(\\mu: w \\to w_1\\) be a forward \\(\\alpha\\)-arrow and let \\(\\iota: w_1 \\to w_2\\) be a forward unitor arrow. Then either one of the following two situations must occur. <ul> <li> <p>There exists a binary word \\(z\\),  a forward unitor arrow \\(\\iota': w \\to z\\)  and a forward \\(\\alpha\\)-arrow \\(\\mu': z \\to w_2\\) such that, for any monoidal  category \\(\\mm\\), the diagram below commutes. \\ </p> </li> <li> <p>There exists a forward unitor arrow \\(\\iota': w \\to w_2\\) such that, for any monoidal  category \\(\\mm\\), the diagram below commutes. \\  end{itemize} </p> </li> </ul> <p>As before, the above lemma is an existence result,  so we emphasize this fact by coloring  the arrows that we are asserting to exist Green. </p> <p>Assuming the above lemma, we prove the Arrow Reorganization Proposition.  \\begin{varprf}[Proof of Arrow Reorganization (Proposition \\ref{proposition:arrow_reorganization}).] We summarize rather than introducing too much notation, since the proof strategy  is rather simple. Consider a sequence of monoidal arrows  \\(\\mu_1, \\dots, \\mu_k\\). Suppose \\(\\mu_{j}\\) is a unitor arrow. If \\(\\mu_{j-1}\\)  is an \\(\\alpha\\)-arrow, we perform an associator-unitor swap, obtaining a new chain  whose composite is the same in \\(\\mm\\). If not, we leave it alone  and check the other unitor arrows. </p> <p>We perform this reorganization, swapping associator arrows and unitor arrows one at a time,  until we have a sequence of morphisms in which no unitor arrow is preceded by  an \\(\\alpha\\)-arrow (and hence all unitors begin at the front of our chain). The repeated application of the Associator-Unitor swap guarantees that the composite of this new chain is equal to the composite of our original chain. \\end{varprf}</p> <p>We now understand how to prove the Arrow Reorganization Proposition: it  relies critically on the Associator-Unitor Swap. As we now understand how the  Associator-Unitor swap is used, we offer its proof.</p> <p>\\begin{varprf}[Proof of Associator-Unitor Swap (Lemma \\ref{lemma:associator_unitor_swap}).] We prove this using a case-by-case basis. For our proof, we write  \\(w = u \\otimes v\\). Whenever \\(\\ll(v) &gt; 1\\), we write \\(w = u\\otimes (s \\otimes t)\\).  If \\(\\ll(t) &gt; 1\\), we will write \\(w = u\\otimes(s \\otimes (p \\otimes q))\\). </p> <p>Since \\(\\mu\\) is a forward \\(\\alpha\\)-arrow, it could be of the forms </p> \\[ \\alpha \\quad 1_u \\otimes \\eta_1 \\quad \\eta_1\\otimes 1_v \\] <p>with \\(\\eta_1\\) a forward \\(\\alpha\\)-arrow. Since \\(\\iota\\)  is a forward unitor arrow, it could be of the forms </p> \\[ \\lambda_v \\quad \\rho_u \\quad 1_u \\otimes \\eta_2 \\quad \\eta_2\\otimes 1_v \\] <p>with \\(\\eta_2\\) either a forward unitor arrow.  We display our table below, this time coloring the entries  in order to group together similar cases.</p> <p>\\begin{center}</p> \\((\\mu,\\iota)\\) \\(1_u\\otimes \\eta_2\\) \\(\\eta_2 \\otimes 1_v\\) \\(\\lambda_v\\) \\(\\rho_u\\)             \\ [0.2cm]                          \\(\\alpha\\) \\(\\textcolor{NavyBlue}{(\\alpha_{u,s,t},1_u \\otimes \\eta_2)}\\) \\(\\textcolor{NavyBlue}{(\\alpha_{u,s,t}, \\eta_2\\otimes 1_v)}\\) \\(\\textcolor{Orange}{(\\alpha_{u,s,t}, \\lambda_v)}\\) \\(\\textcolor{Orange}{(\\alpha_{u,s,t}, \\rho_u)}\\) 0.2cm]             \\(1_u \\otimes \\eta_1\\) \\(\\textcolor{Purple}{(1_u \\otimes \\eta_1 ,1_u \\otimes \\eta_2)}\\) \\(\\textcolor{Purple}{(1_u \\otimes \\eta_1, \\eta_2\\otimes 1_v)}\\) \\(\\textcolor{ProcessBlue}{(1_u\\otimes\\eta_1 ,\\lambda_v)}\\) \\(\\textcolor{ProcessBlue}{(1_u\\otimes\\eta_1 ,\\rho_u)}\\) 0.2cm]             \\(\\eta_1\\otimes 1_v\\) \\(\\textcolor{Purple}{(\\eta_1\\otimes 1_v ,1_u \\otimes \\eta_2)}\\) \\(\\textcolor{Purple}{(\\eta_1\\otimes 1_v, \\eta_2\\otimes 1_v)}\\) $ \\textcolor{ProcessBlue}{(\\eta_1\\otimes 1_v, \\lambda_v)}$ <p>\\end{center}    \\noindent </p> <p>Case 1: \\((\\alpha_{u,s,t**, 1_{u\\otimes s} \\otimes \\eta_2)\\)}\\ First consider \\(\\mu = \\alpha_{u,s,t}: u\\otimes(s\\otimes t) \\to (u \\otimes s)\\otimes t\\)  and \\(\\iota = 1_{u\\otimes s} \\otimes \\eta_2\\) with  \\(\\eta_2: t \\to t'\\) either a forward \\(\\lambda\\) or \\(\\rho\\) arrow. We select the forward unitor arrow  \\(1_{u_A}\\otimes(1_{s_A} \\otimes (\\eta_2)_A)\\) and the forward  \\(\\alpha\\)-arrow \\(\\alpha_{u_A, s_A, t'_A}\\) to obtain the diagram  \\  which commutes by naturality of \\(\\alpha\\).  \\ Case 2:** \\((\\alpha_{u,s,t}, \\eta_2 \\otimes 1_t)\\).\\ In this case, \\(\\mu = \\alpha_{u,s,t}: u\\otimes(s\\otimes t) \\to (u\\otimes s)\\otimes t\\),  while \\(\\iota = \\eta_2 \\otimes 1_t\\). Hence, \\(\\eta_2\\) must act on \\((u \\otimes s)\\).  With that said,  \\(\\eta_2\\) must be of the form  [ \\lambda_s \\qquad \\rho_u \\qquad \\tau \\otimes 1_s \\qquad 1_u \\otimes \\sigma  ]</p> <p>with \\(\\tau: u \\to u'\\) and \\(\\sigma: s \\to s'\\) either forward \\(\\lambda\\) or \\(\\rho\\) arrows.  Thus we check each of these cases are satisfied.  \\ Case 2.1: \\(\\eta_2 = \\lambda_{s_A}\\)\\ In this case, \\(u = I\\). We can construct a triangular diagram  by appending \\(\\lambda_{s_A \\otimes t_A}: I \\otimes (s_A \\otimes t_A) \\to s_A \\otimes t_A\\) as  below.  \\  which commutes in \\(\\mm\\) by Proposition \\ref{proposition:unitor_diagrams}. \\ Case 2.2: \\(\\eta_2 = \\rho_u\\)\\ In this case, \\(s_A = I\\). We can append the morphism  \\(1_{u_A} \\otimes \\lambda_{t_A}: u_A \\otimes (I \\otimes t_A) \\to u_A \\otimes t_A\\)  to create a triangular diagram as below.  \\  The above diagram is guaranteed to commute by unitor-axiom (Diagram \\ref{mon_definition_diag_2}) in any monoidal category \\(\\mm\\). \\ Case 2.3: \\(\\eta_2 = \\tau \\otimes 1_s\\)\\ In this case, \\(\\eta_2 = \\tau \\otimes 1_s\\) with \\(\\tau\\) a forward \\(\\lambda\\)  or \\(\\rho\\)-arrow. We can first apply the forward arrow \\(\\tau \\otimes (1_{s_A} \\otimes 1_{t_A})\\) followed by \\(\\alpha_{u'_A, s_A, t_A}\\) to obtain the diagram  \\ </p> <p>which commutes by naturality of \\(\\alpha\\).  \\ Case 2.4: \\(\\eta_2 = 1_u \\otimes \\sigma\\). This case  is nearly identical to the previous, creating a desired diagram  which commutes by naturality of \\(\\alpha\\). </p> <p>This proves all of our cases for when \\(\\mu = \\alpha_{u_A,s_A,t_A}\\)  and \\(\\iota = (\\eta_2)_A \\otimes 1_{t_A}\\), and so we move onto our other  cases.  \\ Case 3: \\((\\alpha_{u,s,t}, \\lambda_{t})\\)\\ This case cannot happen, since we cannot apply \\(\\lambda: x_0 \\otimes t \\to x_0\\) after \\(\\alpha_{u,s,t}: u\\otimes (s\\otimes t)  \\to (u \\otimes s)\\otimes t\\) as \\(u \\otimes s \\ne x_0\\) for any binary words \\(u, s\\).  \\ Case 4: \\((\\alpha_{u,s,t}, \\rho_{u\\otimes s})\\)\\ In this case, we'll have that \\(\\mu = \\alpha_{u_A,s_A,t_A}\\) and  \\(\\iota = \\rho_{u_A \\otimes s_A}\\). This implies that  \\(t_A = I\\).  We can then append the forward \\(\\rho\\)-arrow \\(1_{u_A}\\otimes \\rho_{s_A}\\)  to obtain the diagram  \\ </p> <p>which we know commutes due to Proposition \\ref{proposition:unitor_diagrams}. \\ Case 5: \\((1_u \\otimes \\eta_1, 1_u \\otimes \\eta_2)\\). In this case \\(\\mu = 1_{u_A}\\otimes (\\eta_1)_A\\) and  \\(\\iota = 1_{u_A}\\otimes (\\eta_2)_A\\) with \\(\\eta_1\\) a forward \\(\\alpha\\)-arrow  and \\(\\eta_2\\) either a forward \\(\\lambda\\) or \\(\\rho\\)-arrow.  We can prove this case by induction. </p> <p>Suppose the statement  is true for word of length less than \\(n\\), and let \\(w = u\\otimes v\\) be a binary word of length \\(n\\). Then we have the diagram on the left \\  which is the image of the diagram on the right under  the functor \\(u_A \\otimes (-)\\). By induction, there exists either a binary  word \\(z\\), and a  forward \\(\\lambda\\) or \\(\\rho\\) arrow \\(\\eta': v_A \\to z\\) and  a forward \\(\\alpha\\)-arrow \\(\\eta'': z \\to v_A''\\) such that the diagram below commutes in  \\(\\mm\\).  \\  We can then take the image of this under the functor \\(u_A\\otimes (-)\\) to obtain the  commutative diagram below. \\  As \\(1_{u_A} \\otimes (\\eta')_A\\) is a forward \\(\\lambda\\) or \\(\\rho\\) arrow  since \\((\\eta')_A\\) is, and since \\(1_{u_A} \\otimes (\\eta'')_A\\) is a forward  \\(\\alpha\\)-arrow since \\((\\eta'')_A\\) is, we have that the case must be true  for all words by induction.  \\ Case 6: \\((1_u \\otimes \\eta_1, \\eta_2 \\otimes 1_{v'})\\)\\ In this case, \\(\\mu = 1_{u_A} \\otimes (\\eta_1)_A\\) with  \\(\\eta_1: v \\to v'\\) a forward \\(\\alpha\\)-arrow, and  \\(\\iota = (\\eta_2)_A \\otimes 1_{v'}\\) with \\(\\eta_2: u \\to u'\\)  either a forward \\(\\lambda\\) or \\(\\rho\\) arrow.  We can use the forward \\(\\lambda\\) or \\(\\rho\\) arrow  \\((\\eta_2)_A \\otimes 1_{v_A}\\) followed by the \\(\\alpha\\)-arrow  \\(1_{u'_A}\\otimes (\\eta_1)_A\\) to obtain the diagram below. \\  The above diagram commutes by functoriality of \\(\\otimes\\), completing this case. \\ Case 7: \\((1_u \\otimes \\eta_1, \\lambda_{v'})\\)\\ In this case we'll have \\(\\mu = 1_u \\otimes \\eta_1\\) with  \\(\\eta_1\\) a forward \\(\\alpha\\)-arrow and \\(\\iota = \\lambda_{v'}\\).  This then implies that \\(u = I\\).  We can then append the \\(\\lambda\\)-arrow \\(\\lambda_{v_A}\\) followed  by the \\(\\alpha\\)-arrow \\((\\eta_1)_A: v_A \\to v'_A\\) to obtain the diagram \\  which commutes by naturality of \\(\\lambda\\).  \\ Case 8: \\((1_u \\otimes \\eta_1, \\rho_u)\\)\\ This case cannot happen, since to apply \\(\\rho_u\\) after \\(1_u \\otimes \\eta_1\\) implies that the codomain of \\(\\eta_1\\) is \\(x_0\\), which is not possible if \\(\\eta_1\\)  is an \\(\\alpha\\)-morphism.  \\ Case 9: \\((\\eta_1 \\otimes 1_v, 1_u \\otimes \\eta_2)\\)\\ Equivalent to Case 5. \\ Case 10: \\((\\eta_1\\otimes 1_v, \\eta_2\\otimes 1_v)\\)\\ Equivalent to Case 6. \\ Case 11:\\((\\eta_1 \\otimes 1_v, \\lambda_v)\\)\\ This case cannot happen, since to apply \\(\\lambda_v\\) after  \\(\\eta_1 \\otimes 1_v\\) implies that the codomain of \\(\\eta_1\\) is  \\(x_0\\), which is not possible for an \\(\\alpha\\)-arrow.  \\ Case 12: \\((\\eta_1 \\otimes 1_v, \\rho_u)\\)\\ In this case, we have that \\(\\mu = (\\eta_1)_A \\otimes 1_{v_A}\\)  and \\(\\eta_2 = \\rho_{u_A}\\). This implies that  \\(v_A = I\\).  We can then append the forward \\(\\rho\\) arrow \\(\\rho_{u_A}\\) followed  by the forward \\(\\alpha\\)-arrow \\((\\eta_1)_A\\) to the diagram to obtain  \\  which commutes by naturality of \\(\\rho\\). </p> <p>This proves all the cases, which completes the proof. \\end{varprf}</p> <p>Thus we have proven the Associator-Unitor Swap. Our final task is to  prove the Unitor-Chain Equivalence. To do so, it suffices to prove the following lemma. </p> <p>(Unitor Diamond Lemma.) Let \\(w\\) be a binary word, and \\(\\mu_1, \\mu_2\\) a pair of forward unitor arrows  as below.  \\  There there exists a binary word \\(z\\) and a pair of forward unitor arrows  \\(\\eta_1: w_1 \\to z\\), \\(\\eta_2: w_2 \\to z\\) such that for any  monoidal category \\((\\mm, \\otimes, I, \\alpha, \\lambda, \\rho)\\),  the diagram below is commutative in \\(\\mm\\). \\  end{lemma} <p>As before, we color the arrows which we are asserting to exist Green.</p> <p> To prove this, we do a case-by-case basis again. In general,  we will  write \\(w = u\\otimes v\\), and if \\(\\ll(v) &gt; 1\\), we write \\(w = u\\otimes (s \\otimes t)\\).  <p>Now since \\(\\mu_1, \\mu_2\\) are forward unitor arrows, \\(\\mu_1\\) could be of the form </p> \\[ 1_u \\otimes \\eta_1 \\qquad \\eta_1 \\otimes 1_v \\qquad \\lambda_v \\qquad \\rho_u  \\] <p>while \\(\\mu_2\\) could be of the form </p> \\[ 1_u \\otimes \\eta_2 \\qquad \\eta_2 \\otimes 1_v \\qquad \\lambda_v \\qquad \\rho_u  \\] <p>with \\(\\eta_1, \\eta_2\\) both forward unitor arrows. Therefore, our possible cases are as follows. We could have \\(\\mu_1 = \\mu_2\\). Or, we could have any of the cases below.  The paired-coloring indicates logically equivalent cases due to the symmetry of our problem.  \\begin{center}</p> \\((\\beta_1,\\beta_2)\\) \\(1_u\\otimes \\eta_2\\) \\(\\eta_2 \\otimes 1_v\\) \\(\\lambda_v\\) \\(\\rho_u\\)         \\ [0.2cm]                  \\(1_u \\otimes \\eta_1\\) \\(\\textcolor{Orange}{(1_u \\otimes \\eta_1 ,1_u \\otimes \\eta_2)}\\) \\(\\textcolor{ForestGreen}{(1_u \\otimes \\eta_1, \\eta_2\\otimes 1_v)}\\) \\(\\textcolor{Purple}{(1_u\\otimes\\eta_1 ,\\lambda_v)}\\) \\(\\textcolor{red!80!White}{(1_u\\otimes\\eta_1 ,\\rho_u)}\\) 0.2cm]         \\(\\eta_1\\otimes 1_v\\) \\(\\textcolor{ForestGreen}{(\\eta_1\\otimes 1_v ,1_u \\otimes \\eta_2)}\\) \\(\\textcolor{Orange}{(\\eta_1\\otimes 1_v, \\eta_2\\otimes 1_v)}\\) $ \\textcolor{Magenta}{(\\eta_1\\otimes 1_v, \\lambda_v)}$ \\(\\textcolor{ProcessBlue}{(\\eta_1\\otimes 1_v, \\rho_u)}\\) 0.2cm]         \\(\\lambda_v\\) \\(\\textcolor{Purple}{(\\lambda_v ,1_u \\otimes \\eta_2)}\\) \\(\\textcolor{Magenta}{(\\lambda_v, \\eta_2\\otimes 1_v)}\\) \\(\\textcolor{Blue}{(\\lambda_v, \\lambda_v)}\\) \\(\\textcolor{Blue}{(\\lambda_v, \\rho_u)}\\) 0.2cm]         \\(\\rho_u\\) \\(\\textcolor{red!80!White}{(\\rho_u ,1_u \\otimes \\eta_2)}\\) \\(\\textcolor{ProcessBlue}{(\\rho_u, \\eta_2\\otimes 1_v)}\\) \\(\\textcolor{Blue}{(\\rho_u, \\lambda_v)}\\) \\(\\textcolor{Blue}{(\\rho_u, \\rho_u)}\\) <p>\\end{center}</p> <p>Since we've already implemented this case-by-case proof strategy several times,  we will point out the cases which we've seen before, and take care of the cases  that are new.  \\ Case 1: \\(\\textcolor{Orange}{(1_u \\otimes \\eta_1 ,1_u \\otimes \\eta_2)}\\) This case can be proven by induction on total length \\(\\ll(w) + \\ee(w)\\), using  a similar argument as in Case 3 of Lemma \\ref{lemma:diamond_lemma}.</p> <p>Case 2: \\(\\textcolor{ForestGreen}{(1_u \\otimes \\eta_1, \\eta_2\\otimes 1_v)}\\)  This case can be proven via functoriality, in a similar manner as Case 2 of Lemma \\ref{lemma:diamond_lemma}.</p> <p>Case 3: \\(\\textcolor{Purple}{(1_u\\otimes \\eta_1, \\lambda_v)}\\).\\ With \\(\\mu_1 = 1_u\\otimes \\eta_1\\) and \\(\\mu_2 = \\lambda_v\\), denote \\(\\eta_1: v \\to v'\\). In this case, we can use the morphisms \\(\\lambda_{(v')_A}\\) and \\(\\eta_1\\) to obtain the  diagram \\  hich commutes by naturality of \\(\\lambda\\).  \\ Case 5: \\(\\textcolor{red!80!White}{(1_u\\otimes\\eta_1 ,\\rho_u)}\\).\\ With \\(\\mu_1 = 1_u\\otimes \\eta_1, \\mu_2 = \\rho_u\\), note that the only  choice for  \\(\\eta_1\\) is \\(\\eta_1 = 1_{x_0}\\). However, there is no unitor  arrow with domain \\(x_0\\), so this does not result in a valid case for us to consider. \\ Case 6: \\(\\textcolor{Magenta}{(\\eta_1\\otimes 1_v, \\lambda_v)}\\). \\ With \\(\\mu_1 = \\eta_1\\otimes  1_v, \\mu_2 = \\lambda_v\\), note that the  only  choice for \\(\\eta_1\\) is again \\(1_{x_0}\\). Once again, there is no unitor arrow  with domain \\(x_0\\), so this is also not a valid case that we need to consider. \\ Case 7: \\(\\textcolor{ProcessBlue}{(\\eta_1 \\otimes 1_v, \\rho_u)}\\). \\ With \\(\\mu_1 = \\eta_1\\otimes 1_v, \\mu_2 = \\rho_u\\), we can use the morphisms  \\(\\rho_{(u')_A}\\) and \\(\\eta_1\\) to obtain  \\  hich commutes by naturality of \\(\\rho\\). </p> <p>Case 8: \\(\\textcolor{Blue}{(\\lambda_v, \\lambda_v)}\\).  In this case, we see that \\(\\mu_1 = \\mu_2\\), so that the statement is trivially  satisfied in this case. </p> <p>With all cases verified, we see that the statement must be true for all  binary words, as desired.  </p> <p>We now show how this proves the Unitor-Chain Equivalence, which we restate  for the readers convenience. </p> <p>[Unitor-Chain Equivalence] Let \\(w\\) be a binary word with nonzero length and with \\(\\ee(w) = k\\).  Suppose \\(\\mu_1, \\dots, \\mu_k\\) and \\(\\eta_1, \\dots, \\eta_k\\) are forward  unitors and that: [ \\mu_k \\circ \\cdots \\circ \\mu_1, \\; \\eta_k \\circ \\cdots \\circ \\eta_1: w \\to \\overline{w} ] <p>Then \\((\\mu_k)_A \\circ \\cdots \\circ (\\mu_1)_A =  (\\eta_k)_A \\circ \\cdots \\circ (\\eta_1)_A\\) in \\(\\mm\\). </p> <p> We prove this by induction on \\(\\ee(w)\\). Suppose the result is true for  binary words \\(v\\) with \\(\\ee(v) &lt; \\ee(w)\\), and consider two composable  chains of forward unitors \\(\\mu_1, \\dots, \\mu_k, \\eta_1, \\dots, \\eta_k\\)  as described above. We seek to show that the diagram  \\  is commutative in \\(\\mm\\). By the Unitor Diamond Lemma, there exists a binary  word \\(z\\) and two forward unitors \\(\\iota_1: u \\to z\\) and \\(\\iota_2: v \\to z\\)  such that  \\  is commutative in \\(\\mm\\). Now, by Lemma \\ref{lemma:unitors_preserve_clean_word},  we have that  \\(\\overline{z} = \\overline{w}\\). By Lemma \\ref{lemma:unitors_decrease_unit_length},  \\(\\ee(z) = k - 2\\). Hence, by Proposition \\ref{proposition:exists_w_to_clean_w},  there exists a chain of forward unitors \\(\\nu_1, \\dots, \\nu_{k-2}\\)  such that \\(\\nu_{k-2} \\circ \\cdots \\circ \\nu_1: z \\to \\overline{w}\\). Our situation is displayed below. For clarity, we suppress \\(\\nu_{k-2} \\circ \\cdots \\circ \\nu_1: z \\to \\overline{w}\\)  in the diagram below. \\  By Lemma \\ref{lemma:unitors_decrease_unit_length}, we know that  \\(\\ee(u_1), \\ee(v_1) &lt; \\ee(w)\\). Therefore, we may apply our induction  hypothesis to conclude that the lower left and lower right triangles must  commute. As the original upper square commutes by the Unitor Diamond Lemma,  this implies that  \\[ (\\mu_k)_A \\circ \\cdots \\circ (\\mu_1)_A  = (\\eta_k)_A \\circ \\cdots \\circ (\\eta_2)_A \\] <p>as desired.  </p> <p>At this point, we have formally filled in all of the potential gaps in the proof  of Theorem \\ref{theorem:coherence_in_unitors}. We have completed the hard work required to prove Mac Lane's Coherence Theorem. We will use the next section  to see how our previous results immediately apply our desired coherence result.</p> <p>\\subsection*{Step Seven: Proving the Main Theorem}</p> <p>At this point we have proven coherence in associators and unitors, but  only when considering iterated monoidal products of a single object.  We have not yet achieved our desired result, which should say something  about more general monoidal products with different objects in the expression.  However, our previous work quickly implies our desired theorem. We first introduce  a definition and perform a clever trick. </p> <p>In what follows, we let \\(**1**\\) denote the terminal category  whose sole object is denoted \\(\\bullet\\).</p> <p> Let \\((\\mm, \\otimes, I)\\) be a monoidal category.  Define the \\textbf{iterated functor category\\footnote{The notation of this category  is due to Mac Lane, but he did not supply a name for this category. So I made one up. Today, this construction is known as an endomorphism  operad.} of \\(\\mm\\)}, denoted as \\(**It**(\\mathcal{M})\\), to be the category where: \\begin{description} <ul> <li> <p>[Objects.] Functors \\(F: \\mathcal{M}^n \\to \\mathcal{M}\\) for all \\(n = 0, 1, 2, \\dots\\) When \\(n = 0\\), we let \\(\\mm^0 = **1**\\).</p> </li> <li> <p>[Morphisms.] Natural transformations \\(\\eta: F \\to G\\) between such functors. \\end{description} </p> </li> </ul> <p>We will give this category a monoidal structure. Towards that goal, we introduce the following bifunctor</p> \\[ \\odot : **It**(\\mathcal{M})\\times **It**(\\mathcal{M}) \\to  **It**(\\mathcal{M}) \\] <p>whose behavior we describe on objects and morphisms as follows.  \\begin{description}</p> <ul> <li>[On objects.] For two functors \\(F: \\mm^n \\to \\mm\\), \\(G: \\mm^m \\to \\mm\\),  we define the functor \\(F \\odot G: \\mm^{n+m} \\to \\mm\\) pointwise as </li> </ul> \\[ (F \\odot G)(A_1, \\dots, A_{n+m}) =  F(A_1, \\dots, A_n) \\otimes G(A_{n+1}, \\dots, A_{n+m}) \\] <p>where \\(\\otimes\\) is the monoidal product of \\(\\mm\\).</p> <ul> <li>[On morphisms.]  Let \\(F_1, G_1: \\mm^n \\to \\mm\\) and \\(F_2, G_2: \\mm^m \\to \\mm\\).  Given natural transformations </li> </ul> \\[ \\eta: F_1 \\to G_1 \\qquad \\mu: F_2 \\to G_2 \\] <p>we define the natural transformation \\(\\eta \\odot \\mu: F_1 \\odot G_1 \\to F_2 \\odot G_2\\)  pointwise as</p> \\[ (\\eta \\odot \\mu)_{(A_1, \\dots, A_{n+m})} = (\\eta)_{(A_1, \\dots, A_n)} \\otimes (\\mu)_{(A_{n+1}, \\dots, A_{n+m})} \\] <p>\\end{description}</p> <p>The above bifunctor is what allows us to regard \\(**It**(\\mm)\\) as a monoidal  category. This is more precisely stated in the following lemma.</p> <p>\\begin{lemma} Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\lambda, \\rho)\\) be a monoidal category.  Then </p> \\[ \\left(**It**(\\mm), \\odot, c, \\bm{\\alpha}, \\bm{\\lambda}, \\bm{\\rho}\\right) \\] <p>is a monoidal category where  \\begin{itemize}</p> <ul> <li> <p>The monoidal product is the bifunctor \\(\\odot: **It**(\\mathcal{M})\\times **It**(\\mathcal{M}) \\to  **It**(\\mathcal{M})\\)</p> </li> <li> <p>The identity object is the functor \\(c: **1** \\to \\mm\\), where \\(c(\\bullet) = I\\) </p> </li> <li> <p>For functors \\(F_j: \\mm^{i_j} \\to \\mm\\), \\(j = 1, 2, 3\\), the associator </p> </li> </ul> \\[ \\bm{\\alpha}_{F_1, F_2, F_3}: F_1 \\odot (F_2 \\odot F_3) \\to (F_1 \\odot F_2) \\odot F_3 \\] <p>is the natural transformation defined pointwise for each \\((A_1, \\dots, A_{i_1 + i_2 + i_3}) \\in  \\mm^{(i_1 + i_2 + i_3)}\\) as </p> \\[ (\\bm{\\alpha}_{F_1, F_2, F_3})_{(A_1, \\dots, A_{i_1 + i_2 + i_3})} = \\alpha_{(F(A_1, \\dots, A_{i_1}), F(A_{i_1 + 1}, \\dots A_{i_1 + i_2}), F(A_{i_1 + i_2 + 1}, \\dots, A_{i_1 + i_2 + i_3}))} \\] <ul> <li>For a functor \\(F: \\mm^n \\to \\mm\\), the left unitor \\(\\bm{\\lambda}: c \\odot F \\to F\\)  is the natural transformation defined pointwise for \\((\\bullet, A_1, \\dots, A_n) \\in **1**\\times \\mm^{n}\\) as </li> </ul> \\[ (\\bm{\\lambda}_F)_{(\\bullet, A_1, \\dots, A_n)} = \\lambda_{F(A_1, \\dots, A_n)} \\] <p>while the right unitor \\(\\bm{\\rho}: F \\odot c \\to F\\)  is the natural transformation defined similarly as</p> \\[ (\\bm{\\rho}_F)_{(A_1, \\dots, A_n, \\bullet)} = \\rho_{F(A_1, \\dots, A_n)} \\] <p></p> <p>It is simple to check that these satisfy the axioms of a monoidal category.  We now reach the final theorem.</p> <p>[Coherence Theorem for Monoidal Categories.] For every monoidal category \\(\\mm\\), there exists a unique,  strict monoidal functor  \\[ \\Phi_{\\text{\\small id}}: \\ww \\to **It**(\\mm) \\] <p>where \\(\\Phi_{\\text{\\small id}}(x_1) = \\id: \\mm \\to \\mm\\).</p> <p></p> <p> As \\((**It**(\\mm), \\odot, c)\\) is a monoidal category by Lemma  \\ref{lemma:IT_is_monoidal}, the theorem follows by a simple application of Theorem \\ref{theorem:coherence_in_unitors} to this  monoidal category. </p> <p>A reader might be wondering: How does the above theorem grant us coherence?  Let us first investigate the behavior of this functor. </p> <p>Under the functor, the morphism in \\(\\ww\\)  \\  s mapped by \\(\\Phi_{\\id}\\) to the natural transformation between the functors in \\(**It**(\\mm)\\) \\  nd, as functors from \\(\\mm^3 \\to \\mm\\),  we may substitute any \\(A,B,C\\) to obtain a natural isomorphism </p> \\[ \\alpha_{A,B,C}: A \\otimes (B \\otimes C) \\to (A \\otimes B)\\otimes C \\] <p>in \\(\\mm\\). Next, we know that functors preserve diagrams. Therefore, our commutative  pentagon diagram in \\(\\ww\\) \\  s mapped by \\(\\Phi_{\\id}\\) to a commutative  diagram of natural transformations in \\(**It**(\\mm)\\)  between the functors below \\  nd as the above functors are of the form \\(\\mm^4 \\to \\mm\\), we may substitute any \\(A,B,C,D \\in \\mm\\) to obtain the commutative diagram  \\  n \\(\\mm\\). </p> <p>So far, our functor makes sense. Moreover, we already knew that the above pentagon commutes  for all \\(A,B,C,D \\in \\mm\\). Thus, what about diagram \\ref{diagram:assoc_5}?</p> <p>Again, functors preserve diagrams. Therefore, the commutative  diagram in \\(\\ww\\) (see next page) is mapped by \\(\\Phi_{\\id}\\) to the commutative  diagram of natural transformations in \\(**It**(\\mm)\\) between functors (see second page) and as functors from \\(\\mm^5 \\to \\mm\\), we may substitute any \\(A,B,C,D,E\\) to obtain the  commutative diagram in \\(\\mm\\) (on the third page).</p> <p>\\  \\  \\  his process continues for every possible diagram in \\(\\ww\\).  Each diagram in \\(\\ww\\) is mapped to a corresponding diagram  in \\(**It**(\\mm)\\) made up of identity functors, and  with the identity functor, we are free to substitute whatever instance  of \\(A, B, C, \\dots \\in M\\) in it. The arrows between the identity functors  are natural transformations which reduce to instances of \\(\\alpha, \\rho, \\lambda\\) in \\(\\mm\\) upon substituting objects in the identity functor.  What matters here is the functoriality of \\(\\Phi_I\\). It guarantees that all the diagrams obtained as the image of \\(\\Phi_{\\text{\\small id}}\\)  will commute. </p> <p>This completes our work towards proving Mac Lane's Coherence Theorem.</p>"},{"location":"category_theory/Monoidal%20Categories/Monoidal%20Categories/","title":"7.1. Monoidal Categories","text":"<p>The concept of a monoidal category is motivated by the very simple observation that  some categories are canonically equipped with their own algebraic data which allows us  to multiply objects of the category to get new objects. This is similar to how in a group \\(G\\),  we multiply two group elements \\(g\\), \\(h\\) to get another group element \\(gh \\in G\\). These types of categories appear frequently enough in many  settings that it has been necessary to really understand what the core ingredients of these categories are.  The task of defining these categories, however, takes a bit of work. Before we  offer the definition and discuss such work we motivate monoidal categories with two key  examples. </p> <p> Consider the category Set. Then for two sets \\(A\\), \\(B\\), we can take  their cartesian product to create a third set  \\[ A \\times B = \\{(a, b) \\mid a \\in A, b \\in B\\}. \\] <p>We also know that given three sets \\(A, B, C\\), we have an isomorphism  \\(A \\times (B \\times C) \\cong (A \\times B) \\times C\\). The bijection is given  by the function </p> \\[ \\alpha_{A, B, C}: A \\times (B \\times C) \\isomarrow (A \\times B)\\times C \\qquad  (a, (b,c)) \\mapsto ((a, b), c). \\] <p>In addition, there is a particularly special set \\(\\{\\bullet\\}\\), the one element set.  For this set, we know that \\(\\{\\bullet\\} \\times A \\cong A \\times \\{\\bullet\\}  \\cong A\\).  The bijections are given by </p> \\[\\begin{align*} &amp;\\lambda_A: \\{\\bullet\\} \\times A \\isomarrow A \\qquad (\\bullet, a) \\mapsto a\\\\ &amp;\\rho_A: A \\times \\{\\bullet\\} \\isomarrow A \\qquad (a, \\bullet) \\mapsto a \\end{align*}\\] <p>A final observation that is easy to check is that our morphisms \\(\\alpha_{A, B, C}\\), \\(\\lambda_A\\), and \\(\\rho_A\\) are natural.  Naturality for \\(\\alpha\\) means that  for any three functions \\(f: A \\to A'\\), \\(g: B \\to B'\\), \\(h: C \\to C'\\), the  diagram below commutes </p> <p> while naturality for \\(\\lambda\\) and \\(\\rho\\) means that for any function \\(f: A \\to A'\\), the diagrams  below commute.  \\  (Here, 1 denotes the identity \\(1: \\{\\bullet\\} \\to \\{\\bullet\\}\\)). While being able to find  the functions \\(\\alpha, \\rho, \\lambda\\) and observing that they are natural  may not be surprising in Set, what is surpising is that this behavior continues in  many other categories.  </p> <p> Let \\(k\\) be a field, and consider the category \\(**Vect**_k\\) of vector spaces over \\(k\\). For two vector spaces \\(U\\), \\(V\\), we may take their tensor product to create a third  vector space over \\(k\\). There are many ways to describe \\(U \\otimes V\\); here is  one of them: \\[ U \\otimes V = \\left\\{ u \\otimes v \\;\\middle|\\; u \\in U ,\\, v \\in V \\quad \\text{ such that } \\quad \\begin{aligned} &amp; 1.\\; (u_1 + u_2) \\otimes v = u_1 \\otimes v + u_2 \\otimes v\\\\ &amp; 2.\\; u \\otimes (v_1 + v_2) = u \\otimes v_1 + u \\otimes v_2\\\\ &amp; 3.\\; c (u \\otimes v) = (c u) \\otimes v = u \\otimes (cv), c \\in k \\end{aligned} \\right\\} \\] <p>Moreover, if \\(U\\), \\(V\\) have bases \\(\\{e_i\\}_{i \\in I}\\), \\(\\{f_j\\}_{j \\in J}\\), then the  basis of \\(U \\otimes V\\) is \\(\\{e_i \\otimes f_j\\}_{(i,j) \\in I \\times J}\\). </p> <p>From linear algebra, we know that \\(U \\otimes (V \\otimes W) \\cong (U \\otimes V) \\otimes W\\).  To show this, we will define an isomorphic linear transformation  \\(U \\otimes (V \\otimes W) \\to (U \\otimes V) \\otimes W\\). However, recall that to define  such a linear transformation, it suffices to define it on the basis.  Thus, let \\(W\\) have basis \\(\\{g_{\\ell} \\}_{\\ell \\in L}\\). Then we define </p> \\[ \\alpha_{U, V, W}: U \\otimes (V \\otimes W) \\isomarrow (U \\otimes V) \\otimes W  \\] <p>where on the basis elements </p> \\[ \\alpha_{U,V,W}(e_i \\otimes (f_j \\otimes g_{\\ell})) = (e_i \\otimes f_j) \\otimes g_{\\ell}. \\] <p>This establishes our desired isomorphism. </p> <p>In addition, the field \\(k\\) is trivially a vector space over itself; its basis is the  multiplicative identity \\(1\\). Moreover,  we have the isomorphisms \\(k \\otimes V \\cong V \\otimes k \\cong V\\). The isomorphisms  are given by the linear transformations</p> \\[\\begin{align*} \\lambda_V: k \\otimes V \\isomarrow V \\qquad 1 \\otimes e_i \\mapsto e_i\\\\ \\rho_V: V \\otimes k \\isomarrow V \\qquad e_i \\otimes 1 \\mapsto e_i \\end{align*}\\] <p>Here, we've defined the two transformations on the bases. </p> <p>Similarly to our last example, we comment that \\(\\alpha, \\lambda, \\rho\\) defined here  are natural. This means that for any three linear transformations  \\(f: U \\to U'\\), \\(g: V \\to V'\\), and \\(h: W \\to W'\\), the diagram below commutes \\  and we additionally have that the diagrams below commute.  \\  end{example}</p> <p>The observations we have made here continue to be true upon investigating many other categories \\(\\cc\\) in which we have some known, natural way of combining elements. In each case, the  story is the same. The key ingredients are:</p> <ul> <li> <p>There is some product \\(\\otimes: \\cc \\times \\cc \\to \\cc\\) (specifically, it is a bifunctor)</p> </li> <li> <p>For all \\(A, B, C \\in \\cc\\), there is a natural isomorphism </p> </li> </ul> \\[ \\alpha_{A,B,C}: A \\otimes (B \\otimes C) \\isomarrow (A \\otimes B) \\otimes C \\] <ul> <li>There is a special object \\(I\\) of \\(\\cc\\) such that, for any object \\(A\\), we have  the natural isomorphisms </li> </ul> \\[ \\lambda_{A}: I \\otimes A \\isomarrow A \\qquad \\rho_{A}: A \\otimes I \\isomarrow A      \\] <p>The fact that we keep seeing these patterns in many categories is what motivates the following definition.</p> <p> A monoidal category \\(\\cc = (\\cc, \\otimes, I)\\)  is a category \\(\\cc\\) equipped with a bifunctor  \\(\\otimes: \\cc \\times \\cc \\to \\cc\\), a (special) object \\(I\\), and three natural isomorphisms \\begin{statement}{ProcessBlue!10} \\[\\begin{align} \\alpha_{A, B, C}&amp;: A\\otimes(B\\otimes C) \\isomarrow (A \\otimes B)\\otimes C \\quad &amp;&amp;**(Associator)**\\\\ \\lambda_A&amp;: I \\otimes A \\isomarrow A \\quad &amp;&amp;**(Left Unit)**\\\\ \\rho_A&amp;: A \\otimes I \\isomarrow A \\quad &amp;&amp;**(Right Unit)** \\end{align}\\] <p>\\end{statement} such that the following coherence conditions hold. \\begin{statement}{ProcessBlue!10} \\begin{equation} \\begin{tikzcd}[column sep = 0.1cm, row sep = 1cm] A \\otimes(I \\otimes B)  \\arrow[rr, \"\\alpha_{A, I, B}\"]  \\arrow[dr, swap, \"1_A\\otimes \\lambda_{B}\"]  &amp; &amp; (A \\otimes I)\\otimes B \\arrow[dl, \"\\rho_{A} \\otimes 1_B\"] \\ &amp; A \\otimes B &amp; \\end{tikzcd} \\end{equation} \\begin{equation} \\begin{tikzcd}[column sep = 1.8cm, row sep = 1.6cm] A\\otimes (B \\otimes (C \\otimes D))  \\arrow[r, \"\\alpha_{A,B,C\\otimes D}\"]  \\arrow[d, swap, \"1_A\\otimes \\alpha_{B, C, D}\"] &amp; (A \\otimes B)\\otimes(C\\otimes D)  \\arrow[r, \"\\alpha_{A\\otimes B, C, D}\"] &amp; ((A \\otimes B)\\otimes C)\\otimes D\\ A\\otimes((B\\otimes C)\\otimes D) \\arrow[rr, swap, \"\\alpha_{A, B\\otimes C, D}\"] &amp;&amp; (A \\otimes(B\\otimes C))\\otimes D  \\arrow[u, swap, \"\\alpha_{A, B, C}\\otimes 1_D\"] \\end{tikzcd} \\end{equation} \\end{statement}   We also define some terminology within this definition.</p> <ul> <li> <p>We call the bifunctor \\(\\otimes\\) the monoidal product </p> </li> <li> <p>We refer to \\(I\\) as the identity object</p> </li> <li> <p>We refer to diagram \\ref{triangle_diag_1} as the unit diagram and  diagram \\ref{pentagonal_diag} as the pentagon diagram. </p> </li> </ul> <p>Further, we say a strict monoidal category is one in which the  associator, left unit and right unit are all identities. </p> <p>The reader should be wondering: What are those \"coherence conditions\"? The short answer is that we need the coherence conditions in order for our ideas to  make any logical sense. While that answer is very vague and unsatisfying,  we are not quite yet ready to fully explain why those two diagrams are necessary. We will however say</p> <ul> <li> <p>The reader is definitely not expected at this moment to  understand why those diagrams are necessary.</p> </li> <li> <p>We will eventually explain why those diagrams are necessary. </p> </li> </ul> <p>Before we explain why the diagrams are necessary, we develop further intuition regarding  monoidal categories with some more examples. </p> <p>\\begin{example} As one might expect, \\((**Set**, \\times, \\{\\bullet\\})\\) is a monoidal category. We have verified most of the details except the coherence  conditions, but it is not too difficult to show that the unit and pentagon diagram  commute in Set.</p> <p>However, we can put another monoidal category structure on Set with the  following data:</p> <ul> <li> <p>We let the disjoint union bifunctor \\((-)\\coprod (-): **Set**\\times **Set** \\to **Set**\\) be our monoidal product. </p> </li> <li> <p>We let the empty set \\(\\varnothing\\) be our identity object.</p> </li> </ul> <p>With these settings, we can define natural isomorphisms for any three sets \\(X, Y, Z\\) </p> <ul> <li> <p>\\(\\alpha_{X, Y, Z}: X \\amalg (Y \\amalg Z) \\isomarrow (X \\amalg Y) \\amalg Z\\) </p> </li> <li> <p>\\(\\lambda_X: \\varnothing \\amalg X \\isomarrow X\\)</p> </li> <li> <p>\\(\\rho_X: X \\amalg \\varnothing \\isomarrow X\\)</p> </li> </ul> <p>in the obvious way, and check that the required diagrams commute. In this way, we have that  \\((**Set**, \\amalg, \\varnothing)\\) is also a monoidal category.  </p> <p>The previous example demonstrates that a given category can have more than one  monoidal structure on it. This is analagous to the fact that sometimes one can put two  different group structures on an underlying set. </p> <p>The previous example may also make us wonder if we can generalize our logic to consider  other categories in which finite products and coproducts exist. The answer is yes,  and this gives us many examples of monoidal categories: </p> <ul> <li> <p>\\((**Top**, \\times, \\{\\bullet\\})\\)</p> </li> <li> <p>\\((**Ab**, \\oplus, \\{e\\})\\)</p> </li> <li> <p>\\((\\text{R}**-Mod**, \\times, \\{0\\})\\) </p> </li> </ul> <p> If \\(\\cc\\) is a category with finite products and a terminal object \\(T\\), then  \\((\\cc, \\times, T)\\) is a monoiodal category. We refer to this type of monoidal  category as a cartesian monoidal category.  <p>Dually, if it has finite coproducts and an initial object  \\(I\\), then \\((\\cc, \\amalg, I)\\) is also a monoidal category. We call this type of  monoidal category a cocartesian monoidal category. </p> <p>We now introduce less obvious, but useful examples of monoidal categories.</p> <p> Let \\(R\\) be a commutative ring. Then the category of all \\(R\\)-modules, (\\(R\\)-Mod, \\(\\otimes\\), \\(\\{0\\}\\)),  forms a monoidal category under the tensor product. Recall that the  tensor product between two \\(R\\)-modules \\(N \\otimes M\\) is an initial object  in the comma category \\((N \\times M \\downarrow R**-Mod**\\)  where the morphisms are bilinear. Alternatively in diagrams, we have  that  \\  Now consider a third \\(R\\)-module \\(P\\); then we have two ways of constructing  the tensor product. To demonstrate that we may identify these objects up  to isomorphism, construct the maps \\[ f: (M \\otimes N)\\times P \\to M \\otimes (N \\otimes P) \\qquad  \\left(\\sum_{i}m_i\\otimes n_i, p\\right) \\mapsto  \\sum_{i}m_i \\otimes (n_i \\otimes p) \\] <p>and </p> \\[ f': M \\times (N \\otimes P) \\to (M \\otimes M) \\otimes P \\qquad  \\left(m, \\sum_{j}n_j\\otimes p_j  \\right) \\mapsto  \\sum_{j}(m \\otimes n_j) \\otimes p_j. \\] <p>These maps are bilinear due to the bilinearity of \\(\\otimes\\). Hence we see  that the universal property of the tensor product gives us unique map \\(\\alpha\\) and  \\(\\alpha'\\) such that the diagrams below commute.  \\  Based on how we defined \\(f\\) and \\(f'\\), and since we know that \\(\\phi\\) and \\(\\phi'\\)  is, we can determine that \\(\\alpha\\) and \\(\\alpha'\\) are \"shift maps\", i.e, </p> \\[\\begin{align*} \\alpha\\left( \\sum_{i}(m_i \\otimes n_i)\\otimes p_i \\right) = \\sum_{i}m_i\\otimes(n_i \\otimes p_i) \\qquad \\alpha'\\left( \\sum_{i}m_i \\otimes (n_i \\otimes p_i) \\right) = \\sum_{i}(m_i\\otimes n_i )\\otimes p_i. \\end{align*}\\] <p>Hence we see that \\(\\alpha\\) and \\(\\alpha'\\) are inverses, so what we have is an associator: </p> \\[ \\alpha_{M,N,P} : (M\\otimes N)\\otimes P \\isomarrow M\\otimes(N\\otimes P). \\] <p>Now consider the trivial \\(R\\)-module, denoted \\(I = \\{0\\}\\). For any \\(R\\)-module \\(M\\) we have evident maps</p> \\[ \\sum_{i} 0\\otimes m_i \\mapsto m_i \\qquad \\sum_{i}m_i\\otimes 0 \\mapsto 0     \\] <p>which provide isomorphisms, so that we have left and right associators </p> \\[ \\lambda_M: I \\otimes M \\isomarrow M \\qquad  \\rho_M: M\\otimes I \\isomarrow M. \\] <p>Finally, the triangular and pentagonal diagrams are commutative since shifting the tensor product on individual elements does not change (up to isomorphism) the  value of the overall elements.  </p> <p> Consider the category \\(**GrMod**_R\\) which consist of graded \\(R\\)-modules \\(M = \\{M_n\\}_{n =1}^{\\infty}\\)  Then this forms a monoidal category \\((**GrMod**_R, \\otimes, I)\\)  where \\(I = \\{(0)_n\\}_{n=1}^{\\infty}\\) is the trivial graded \\(R\\)-module and where we define the monoidal product as \\(M \\otimes N = \\{(M \\otimes N)_n\\}_{n=1}^{\\infty}\\) where   \\[ (M \\otimes N)_n = \\bigoplus_{i + j = n}M_i \\otimes N_j. \\] <p>To show this monoidal, the first thing we must check is that we have an associator.  Towards this goal, consider three graded \\(R\\)-modules \\(M = \\{M_n\\}_{n = 1}^{\\infty}\\), \\(N = \\{N_n\\}_{n = 1}^{\\infty}\\) and \\(P = \\{P_n\\}_{n = 1}^{\\infty}\\). Then  the \\(m\\)-th graded module of \\(M \\otimes (N \\otimes P)\\) is </p> \\[\\begin{align*} [M \\otimes (N \\otimes P)]_m =  \\bigoplus_{i + j = m} M_i \\otimes (N \\otimes P)_j &amp;= \\bigoplus_{i + j = m} M_i \\otimes \\left( \\bigoplus_{h + k = j} N_h \\otimes P_k \\right)\\\\ &amp;= \\bigoplus_{i + h + k = m} M_i \\otimes (N_h \\otimes P_k)\\\\ &amp;\\cong \\bigoplus_{i + h + k = m} (M_i \\otimes N_h) \\otimes P_k\\\\ &amp;= \\bigoplus_{l + k = m} \\left( \\bigoplus_{i + h = l} M_i \\otimes N_h \\right) \\otimes P_k\\\\ &amp;=  \\bigoplus_{l + k = m} (M \\otimes N)_l \\otimes P_k\\\\ &amp;= [M \\otimes (N \\otimes P)]_m \\end{align*}\\] <p>where in the third step we used the fact that the tensor product commutes  with direct sums and in the fourth step we used the canonical associator  regarding the tensor products of three element.  Thus we see that we have an associator </p> \\[ \\alpha: M \\otimes (N \\otimes P) \\isomarrow (M \\otimes N)\\otimes P \\] <p>which as a graded module homomorphism, acts on each level as </p> \\[ \\alpha_m: [M \\otimes (N \\otimes P)]_m \\isomarrow [(M \\otimes N) \\otimes P]_m \\] <p>where in each coordinate of the direct sums we apply an instance of the associator  \\(\\alpha'\\) between the tensor product of three \\(R\\)-modules. The naturality of this  associator is inherited from \\(\\alpha'\\). In addition, we have natural left and right unitors </p> \\[ \\lambda_M: I \\otimes M \\isomarrow M \\qquad \\rho_M: M \\otimes I \\isomarrow M \\] <p>where on each level we utilize the natural left and right unitors for non-graded  \\(R\\)-modules. </p> <p> Let \\((M, \\otimes, I, \\alpha, \\rho, \\lambda)\\) be a monoidal category, \\(\\cc\\) any other category. Then  the functor category \\(\\cc^M\\) is a monoidal category. We treat the constant functor \\(I: \\cc \\to M\\) where  \\[ I(A) = I \\text{ for all } A \\] <p>as the identity element, and we can define a tensor product on this  category as follows: on objects \\(F, G: \\cc \\to M\\), we define \\(F\\boxtimes G\\)  as the composite</p> \\[ \\begin{tikzcd} F \\boxtimes G: \\cc \\to  \\arrow[r, \"\\Delta\"] &amp; \\cc \\times \\cc  \\arrow[r, \"(F \\times G)\"] &amp; M \\times M  \\arrow[r, \"\\otimes\"] &amp; M \\end{tikzcd} \\] <p>which can be stated pointwise as \\((F\\boxtimes G)(C) = F(C)\\otimes G(C)\\). On morphisms,  we have that if \\(\\eta: F_1 \\to F_2\\) and \\(\\eta': G_1 \\to G_2\\) are natural  transformations, then we say \\(\\eta \\boxtimes \\eta': F_1\\boxtimes G_1 \\to F_2 \\boxtimes G_2\\)  is a natural transformation, where we define </p> \\[ (\\eta \\boxtimes \\eta')_A = \\eta_A \\otimes \\eta'_A: F_1(A)\\otimes G_1(A) \\to F_2(A)\\otimes G_2(A). \\] <p>Note that such a natural transformation is well-defined  as the diagram below commutes</p> \\[ \\begin{tikzcd}[column sep = 1.4cm, row sep = 1.4cm] A \\arrow[d, \"f\"] \\\\ B \\end{tikzcd} \\hspace{1cm} \\begin{tikzcd}[column sep = 1.4cm, row sep = 1.4cm] F_1(A)\\otimes G_1(A) \\arrow[r, \"\\eta_A\\otimes \\eta_A'\"] \\arrow[d, swap, \"F_1(f)\\otimes G_1(f)\"] &amp; F_2(A)\\otimes G_2(A) \\arrow[d, \"F_2(f)\\otimes G_2(f)\"] \\\\ F_1(B)\\otimes G_1(B) \\arrow[r, swap, \"\\eta_B\\otimes \\eta_B'\"] &amp; F_2(A)\\otimes G_2(A) \\end{tikzcd} \\] <p>since \\(\\otimes: M \\times M \\to M\\) is a bifunctor. Finally, for functors \\(F, G, H: \\cc \\to M\\) define the associator  \\(\\alpha'_{F, G, H}: F\\boxtimes(G\\boxtimes H) \\isomarrow (F\\boxtimes (G \\boxtimes H))\\)  as the natural transformation where for each object \\(A\\)</p> \\[ (\\alpha'_{F, G, H})_A = \\alpha_{F(A), G(A), H(A)}: F(A)\\otimes (G(A)\\otimes H(A)) \\to (F(A)\\otimes G(A))\\otimes H(A) \\] <p>and the unitors \\(\\lambda'_F: I\\boxtimes F \\to F\\) and \\(\\rho'_F: F \\boxtimes I \\to F\\)  as the natural transformations where for each object \\(A\\) </p> \\[ (\\lambda'_F)_A  = \\lambda_A: I \\otimes F(A) \\to F(A) \\qquad  (\\rho'_F)_A = \\rho_A: F(A)\\otimes I \\to F(A). \\] <p>One can then show that these together satisfy the pentagon and unit axioms.  </p> <p> Consider the category \\(\\mathbb{P}\\) whose objects are the natural numbers  (with 0 included) and whose morphisms are the symmetric groups \\(S_n\\). That is,  \\begin{description} \\item[Objects.] The objects are \\(n = 0, 1,2, \\dots\\). \\item[Morphisms.] For any objects \\(n,m\\) we have that  \\[ \\hom_{\\mathbb{P}}(n,m)= \\begin{cases} S_n &amp; \\text{if } n = m\\\\ \\varnothing &amp; \\text{if} n \\ne m. \\end{cases} \\] <p>\\end{description} Note that there are many ways of constructing this category; we just present the  simplest. In general terms this is the countable disjoint union of the symmetric groups.  Even more generally, this can be done for any family of groups (or rings, monoids, semigroups). </p> <p>What is interesting about this category is that it intuitively forms a strict monoidal  category. That is, we can formulate a bifunctor \\(+:\\mathbb{P}\\times \\mathbb{P} \\to \\mathbb{P}\\) on objects as addition of natural numbers and on morphisms as </p> \\[ \\sigma \\otimes \\tau \\in S_{n+m}  \\] <p>where \\(\\sigma \\in S_n\\) and \\(\\tau \\in S_m\\) and where \\(\\sigma \\otimes \\tau\\) denotes the  direct sum permutation. I could tell you in esoteric language and  notation what that is, or I could just show you: \\(\\sigma\\) and \\(\\tau\\), displayed as below  \\  become \\(\\sigma \\otimes \\tau\\) which is displayed as below. \\  To make this monoidal, we specify that \\(0\\) is our identity element whose associated  identity morphism is the empty permutation. Now clearly this operation is strict on objects. On morphisms, it is also strict in  the same way that stacking three Lego pieces together in the two possible different  ways are equivalent. Hence the associators and unitors are all identities and  this forms a strict monoidal category.  </p> <p> Let \\(\\cc\\) be a category equipped with natural isomorphisms \\[\\begin{align*} \\alpha_{A, B, C}&amp;: A\\otimes(B\\otimes C) \\isomarrow (A \\otimes B)\\otimes C \\quad \\\\ \\lambda_A&amp;: I \\otimes A \\isomarrow A \\quad \\\\ \\rho_A&amp;: A \\otimes I \\isomarrow A \\quad  \\end{align*}\\] <p>for all objects \\(A, B, C \\in \\cc\\) and some identity object \\(I\\).  Suppose the pentagonal diagram \\ref{pentagonal_diag} holds for  all objects of \\(\\cc\\). Then for all \\(A, B \\in \\cc\\), the diagram  \\  commutes if and only if the diagram  \\  commutes, which commutes if and only if the diagram  \\  commutes. </p> <p>\\textcolor{NavyBlue}{This ultimately tells us that the definition of a monoidal category  is not unique.} That is, there are two different yet exactly equivalent ways in which  we could have defined a monoidal category. </p> <p>Thus what we see is that the definition of a monoidal category is very vast,  and asks for a lot. Moreover, we've shown that there are different coherence conditions  we could have imposed (the pentagonal diagram being the same),  but they amount to stating the same thing. </p>"},{"location":"category_theory/Monoidal%20Categories/Monoidal%20Functors/","title":"7.2. Monoidal Functors","text":"<p>\\begin{definition} Let \\((\\cc, \\otimes, I)\\) and \\((\\dd, \\odot, J)\\) be monoidal categories.  A (lax) monoidal functor is a functor \\(F: \\cc \\to \\dd\\) equipped with the following data.</p> <ul> <li>For each pair \\(A\\), \\(B\\) in \\(\\cc\\), we have  a natural morphism </li> </ul> \\[ \\phi_{A,B}: F(A)\\odot F(B) \\to F(A\\otimes B) \\] <p>such that for any third object \\(C\\), the diagram below commutes. (Note that  we suppress the subscripts for clarity.) </p> <p></p> <ul> <li>There exists a unique morphism \\(\\epsilon: J \\to F(I)\\) such that, for any object \\(A\\) of \\(\\cc\\),  the diagrams below commute. (Again,  we suppress the subscripts for clarity.) \\ </li> </ul> <p>We say the \\(F\\) is  strong if \\(\\phi\\) and \\(\\epsilon\\) are isomorphisms and strict if \\(\\phi\\) and \\(\\epsilon\\) are identities.</p> <p>We also define a monoidal natural transformation between two monoidal functors \\(\\eta: F \\to G\\) to be a natural transformation between the functors  such that, for every \\(A, B\\), the diagram below commutes.  \\  end{definition}</p> <p> Consider the power set functor \\(\\mathcal{P}: **Set** \\to **Set**\\) which associates each set \\(X\\) with its power set \\(\\pp(X)\\). We may  ask if this yields a monoidal functor  \\[ \\mathcal{P}: (**Set**, \\times, \\{\\bullet\\}) \\to (**Set**, \\times, \\{\\bullet\\}) \\] <p>in any sense of lax, strong, or strict. It turns out that we may define a lax monoidal  functor, but not a strong or strict. </p> <p>Towards defining a lax monoidal functor, let \\(A, B\\) two sets. Define  \\(\\phi_{A,B}: \\pp(A)\\times\\pp(B) \\to \\pp(A \\times B)\\) to be a function where if  \\(U, V\\) are subsets of \\(A, B\\) respectively, then</p> \\[ \\phi_{A,B}(U, V) = U \\times V.      \\] <p>In addition, we define the function \\(\\epsilon: \\{\\bullet\\} \\to P(\\{\\bullet\\})\\) where </p> \\[ \\epsilon(\\bullet) = \\{\\bullet\\}. \\] <p>Observe that with this data we have that for any sets \\(A,B,C\\), the diagram below commutes \\  and that for any set \\(A\\) the diagrams below commute. \\  Note that our choice that \\(\\epsilon(\\bullet) = \\{\\bullet\\}\\) was necessary in  order for the above two diagrams to commute. </p> <p>We now show that this cannot be a strong or strict monoidal functor. To see this, let \\(A, B\\)  be two sets. If \\(|X|\\) denotes the cardinality of a set \\(X\\), then observe that</p> \\[ |\\pp(A) \\times \\pp(B)| = 2^{|A|}\\cdot 2^{|B|} = 2^{|A| + |B|} \\] <p>while </p> \\[ |\\pp(A \\times B)| = 2^{|A\\times B|}.    \\] <p>We see that in general these two sets are not of the same cardinality,  and therefore one cannot establish an isomorphism between these two sets for all \\(A, B\\),  which we would need to do to at least construct a strong monoidal functor. Hence, we cannot regard this functor as strong or strict monoidal. </p> <p> The category of pointed topological spaces \\(**Top**^*\\) is the category  where  \\begin{description} \\item[Objects.] Pairs \\((X, x_0)\\) with \\(X\\) a topological space and \\(x_0 \\in X\\)  \\item[Morphisms.] A morphism \\(f:(X, x_0) \\to (Y, y_0)\\) is given by a continuous function  \\(f: X \\to Y\\) such that \\(f(x_0) = y_0\\). \\end{description} This category is what allows us to characterize the fundamental group of a topological  space as a functor  \\[ \\pi_1: **Top**^* \\to **Grp**   \\] <p>which sends a pointed space \\((X, x_0)\\) to its fundamental group \\(\\pi_1(X, x_0)\\) with \\(x_0\\) as the selected  basepoint. We demonstrate that this can be regarded as a monoidal functor </p> \\[ \\pi_1: \\Big(**Top**^*, \\times, (\\{\\bullet\\}, \\bullet)\\,\\Big) \\to (**Grp**, \\times, \\{e\\}) \\] <p>where \\(\\{e\\}\\) is the trivial group. The reader may be wondering how we are putting  a cartesian product structure on the \\(**Top**^*\\), so we explain:  For two topological spaces \\(X, Y\\), we define </p> \\[ (X, x_0) \\times (Y, y_0) = (X \\times Y, (x_0, y_0)) \\] <p>where \\(X \\times Y\\) is given the product topology. The identity object \\((\\{\\bullet\\}, \\bullet)\\)  is the trivial topological space with basepoint \\(\\bullet\\).  </p> <p>For any two pointed topological spaces \\((X, x_0), (Y, y_0)\\), define  the function \\(\\phi_{X,Y}: \\pi_1(X, x_0) \\times \\pi_1(Y, y_0) \\to \\pi(X \\times Y, (x_0, y_0))\\)  where for two loops \\(\\beta, \\gamma\\) based as \\(x_0, y_0\\) respectively, then </p> \\[ \\phi_{X, Y}(\\beta, \\gamma) = \\beta \\times \\gamma: [0, 1] \\to X \\times Y \\] <p>which is in fact a loop in \\(X \\times Y\\) based at \\((x_0, y_0)\\). The above function is bijective; an inverse  can be constructed by sending a loop \\(\\delta\\) in \\(X \\times Y\\)  based at \\((x_0, y_0)\\) to the tuple \\((p\\circ \\delta, q \\circ \\delta)\\)  where </p> \\[ p: X \\times Y \\to X \\qquad q: X \\times Y \\to Y   \\] <p>are the continuous projection maps. It is not difficult to see that this preserves group  products, so that \\(\\phi_{X, Y}\\) establishes the isomorphism</p> \\[ \\pi_1(X \\times Y, (x_0, y_0)) \\cong \\pi_1(X, x_0) \\times \\pi_1(Y, y_0) \\] <p>a fact usually proved in a topological course. In addition, this isomorphism to be natural: for two pointed topological spaces  \\((X, x_0)\\) and \\((Y, y_0)\\), and for a pair of base-point preserving continuous  functions \\(f: (X, x_0) \\to (W, w_0)\\) and \\(g: (Y, y_0) \\to (Z, z_0)\\),  the following diagram commutes.  \\  Thus \\(\\phi_{X,Y}\\) is our desired natural isomorphism. </p> <p>Next, define \\(\\epsilon: \\{e\\} \\to \\pi_1(\\{\\bullet\\}, \\bullet)\\) to be the group homomorphism that takes \\(e\\) to the trivial loop at \\(\\bullet\\).  As in the previous example, we are actually forced to define \\(\\epsilon\\) in this  way since \\(\\{e\\}\\) is initial in Grp. </p> <p>With this data, one can easily check that the necessary diagrams are commutative,  so that the fundamental group functor \\(\\pi_1\\) is strong monoidal.  </p> <p> Recall that a Lie algebra is a vector space \\(\\mathfrak{g}\\) over a field \\(k\\)  with a bilinear function \\([-,-]: \\mathfrak{g} \\times \\mathfrak{g} \\to \\mathfrak{g}\\) such that  \\begin{description} \\item[Antisymmetry.] For all \\(x, y \\in \\mathfrak{g}\\), \\([x, y] = -[y, x]\\) \\item[Jacobi Identity.] For all \\(x, y, z \\in \\mathfrak{g}\\) we have that  \\[ [x, [y, z]] + [y, [z, x]] + [z, [x,y]] = 0. \\] <p>\\end{description} For every Lie algebra \\(\\mathfrak{g}\\), we may create the universal enveloping algebra \\(U(\\mathfrak{g})\\). This is the algebra constructed as  follows: If \\(T(\\mathfrak{g})\\) is the tensor algebra of \\(\\mathfrak{g}\\), i.e., </p> \\[ T(\\mathfrak{g}) = k \\oplus (\\mathfrak{g} \\otimes \\mathfrak{g}) \\oplus (\\mathfrak{g} \\otimes \\mathfrak{g} \\otimes \\mathfrak{g}) \\oplus \\cdots  \\] <p>and \\(I(\\mathfrak{g})\\) is the ideal generated by elements of the form \\(x\\otimes y - y \\otimes x - [x,y]\\),  then </p> \\[ U(\\mathfrak{g}) = T(\\mathfrak{g})/I(\\mathfrak{g}). \\] <p>By Corollary V.2.2(b) of \\cite{quantumgroups}, this construction is actually a functor </p> \\[ U: **LieAlg** \\to k**-Alg**. \\] <p>Both categories can be regarded monoidal: \\((**LieAlg**, \\oplus, \\{\\bullet\\})\\)  is the monoidal category where we apply the cartesian product between Lie algebras,  and \\((k**-Alg**, \\otimes, k)\\) is the monoidal category  where we apply tensor products between \\(k\\)-algebras over the field \\(k\\).  The associators and unitors are the same that we have encountered in previous examples of monoidal categories with  cartesian and tensor products.</p> <p>We demonstrate that the universal enveloping algebra functor  is strong monoidal:</p> \\[ U: (**LieAlg**, \\oplus, \\{\\bullet\\})   \\to  (k**-Alg**, \\otimes, k) \\] <ul> <li>By Corollary V.2.3 of \\cite{quantumgroups}, we have that if  \\(\\mathfrak{g_1}\\) and \\(\\mathfrak{g_2}\\) are two Lie algebras  then \\(U(\\mathfrak{g}_1 \\oplus \\mathfrak{g}_2) \\cong U(\\mathfrak{g}_1) \\otimes U(\\mathfrak{g}_2)\\). One can use Corollary V.2.3(a) to show that this isomorphism is natural in both \\(\\mathfrak{g}_1\\) and  \\(\\mathfrak{g}_2\\). We let this morphism be our required isomorphism</li> </ul> \\[ \\phi_{\\mathfrak{g}_1, \\mathfrak{g}_2}: U(\\mathfrak{g}_1\\oplus \\mathfrak{g}_2)   \\to U(\\mathfrak{g}_1)\\otimes U(\\mathfrak{g}_2). \\] <ul> <li>Note that \\(U(\\{\\bullet\\}) = k\\). Therefore, we let  \\(\\epsilon: k \\to k\\) be the identity. </li> </ul> <p>As the associators and unitors are simple for monoidal categories with cartesian  and tensor products, it is not difficult to show that the required diagrams commute.  In this case, what is more difficult is obtaining naturality in \\(\\phi\\), although this is  taken care of (in a long proof) in Kassel's text.  </p>"},{"location":"category_theory/Monoidal%20Categories/Monoids%2C%20Groups%2C%20in%20Symmetric%20Monoidal%20Categories/","title":"7.7. Monoids, Groups, in Symmetric Monoidal Categories","text":"<p>Recall from section ? that we were able to construct monoid and groups which  were internal to some category \\(\\cc\\). The philosophy behind the construction  is one we've seen before: we of course think of monoids and groups by their elements,  but we resist the temptation and instead present an object-free, diagrammatic  set of axioms for monoids and rings. We utilized the cartesian product in the  category \\(\\cc\\) to demonstrate this. However, we now know that the cartesian  product in any category is a small example of a category with a symmetric monoidal structure.  Hence we revisit the concepts of a monoid and group, and expand their  generality by demonstrating that they can be defined in a symmetric monoidal category. </p> <p> Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\rho, \\lambda)\\)  be a monoidal category and let \\(M\\) be an object of \\(\\mathcal{M}\\). We say \\(M\\) is if there exist maps  \\[\\begin{align*} \\mu&amp;: M\\otimes M \\to M \\\\ \\eta&amp;: I \\to M \\end{align*}\\] <p>referred to as the multiplication and identity maps, such that the diagrams below  commute. </p> <p> </p> <p> One of the most useful examples of this concept arises from the notion of  an algebra \\(A\\) over some field \\(k\\), where \\(A\\) is a vector space over the field  \\(k\\).  </p>"},{"location":"category_theory/Monoidal%20Categories/What%20are%20those%20Coherence%20Conditions%3F/","title":"7.3. What are those Coherence Conditions?","text":"<p>We are now going to address the elephant in the room: we have not explained why  we have included diagrams \\ref{triangle_diag_1} and \\ref{pentagonal_diag} in our definition.  To explain this, we are going to discuss the general structure of a monoidal category  and answer the natural questions that arise. </p> <p>Let \\((\\mathcal{M}, \\otimes, I, \\alpha, \\rho, \\lambda)\\) be a monoidal category.  For objects \\(A,B,C, \\dots\\) of \\(\\mathcal{M}\\), we can use the monoidal product \\(\\otimes\\) to generate various new expressions such as \\(A\\otimes B\\) which represent different objects in \\(\\mm\\). Observe that using three objects, there are two different  ways to combine the 3 objects:</p> \\[ A\\otimes (B \\otimes C) \\qquad  (A\\otimes B) \\otimes C. \\] <p>There are five ways to combine 4 objects:</p> \\[\\begin{gather*} A\\otimes(B\\otimes (C \\otimes D)) \\quad  A\\otimes((B\\otimes C) \\otimes D) \\quad  ((A\\otimes B)\\otimes C) \\otimes D \\\\ A\\otimes((B \\otimes C) \\otimes D) \\quad  (A \\otimes (B \\otimes C))\\otimes D. \\end{gather*}\\] <p>And there are 14 ways to combine 5 objects. We will not list them here.  </p> <p>Initially, we don't really know what the relationship  is between the various expressions we are generating. For example, we may  naturally wonder if</p> \\[ A \\otimes(B \\otimes C) \\text{ and } (A \\otimes B)\\otimes C \\] <p>or</p> \\[ A\\otimes(B\\otimes (C \\otimes D))  \\text{ and }A\\otimes((B \\otimes C) \\otimes D) \\] <p>have any relation with each other. This is because  in practice when \\(A,B,C,D\\) are sets, vector spaces, groups, or whatnot, the above expressions  do have something to do with each other. As we have seen,  that relationship is usually an isomorphism. Therefore, if we are to develop some kind of  theory of monoidal categories which we can apply to real mathematics,  we ought to make sure that these objects are isomorphic in some way. </p> <p>Monoidal categories by definition do in fact provide isomorphisms  between different choices of multiplying together  a set of objects. For example, from the axioms of a monoidal category, we know that the objects \\(A\\otimes(B\\otimes C)\\) and \\((A \\otimes B) \\otimes C\\)  are related via the natural isomorphism \\(\\alpha_{A,B,C}\\).</p> <p></p> <p>We also know from the axioms of a monoidal category that  the 5 products of 4 objects are related via the diagram consisting  of natural isomorphisms as below. \\  oreover, this diagram is guaranteed to be commutative for all \\(A\\),\\(B\\),\\(C\\),\\(D\\) in \\(\\mm\\) (we  will elaborate why this is a profound, useful fact).</p> <p>Finally, repeatedly using instances of \\(\\alpha\\),  the 14 ways to multiply 5 objects are related via the 3 dimensional  diagram as below.  \\  owever, it is not an axiom of monoidal categories that this last diagram is commutative  (with a ton of work, one could prove it to be commutative).</p> <p>To understand what's going on, let us  first understand why commutativity is important.  The axioms of a monoidal category grant us the commutativity of the pentagon, which  connects the five different ways of multiplying four objects \\(A,B,C,D\\).  This tells us the following principle: while there are 5 different ways we can multiply four objects  \\(A,B,C,D\\), each such choice is canonically isomorphic to any other choice. </p> <p>To see this, suppose you and I want to multiply objects \\(A,B,C,D\\) together.  Suppose  my favorite way to do it is \\((A\\otimes B)\\otimes(C \\otimes D)\\),  while you choose \\((A\\otimes(B\\otimes C))\\otimes D\\). Then we might be in trouble:  I have two possible ways, displayed below in \\textcolor{NavyBlue}{blue}  and \\textcolor{Orange}{orange}, to \"reparenthesize\" my product to get your object.  \\  ortunately, the commutativity of the pentagonal diagram enures that  the two paths are equal. That is, </p> \\[ \\textcolor{NavyBlue}{\\alpha}\\circ ((\\textcolor{NavyBlue}{1\\otimes \\alpha})\\circ \\textcolor{NavyBlue}{\\alpha^{-1}}) = (\\textcolor{Orange}{\\alpha^{-1}\\otimes 1})\\circ \\textcolor{Orange}{\\alpha}. \\] <p>so that, in reality, I actually have one unique isomorphism (i.e., a canonical isomorphism) from my object to yours, and you can also canonically get from your object to mine by inverting  the unique isomorphism. </p> <p>However, our choice of two different parenthesizations was arbitrary. The  commutativity of the entire diagram therefore tells us  that any choice of \"parenthesizing\" \\(A \\otimes B \\otimes C \\otimes D\\), the  product of 4 objects in \\(\\mm\\), is canonically isomorphic  to any other possible choice. This brings up a few questions. </p> <ul> <li> <p>What do we mean by \"parenthesizing?\"</p> </li> <li> <p>What about a product with \\(n\\)-many objects \\(A\\) for \\(n &gt; 4\\)?</p> </li> </ul> <p>We will rigorously specify what we mean by parenthesizing in a bit. To answer  the second question, we state that this result holds for \\(n &gt; 4\\); this is one version of  the Coherence Theorem. </p>"},{"location":"category_theory/Operads/General%20Operads%20in%20Symmetric%20Monoidal%20Categories/","title":"9.2. General Operads in Symmetric Monoidal Categories","text":"<p>Every time we find ourselves working in Set, we should feel a great  deal of shame and embarrassment. Before anyone catches us, we can atone for our sins by drawing diagrams that avoid specific reference  to the element of the sets, thereby transitioning our work to an arbitrary category. Given our previous work, we can do this; but what were the main ingredients?  Note that we basically only needed the properties of Set and  its cartesian product. Given this, and the fact that Set is symmetric  monoidal given the cartesian product, we can largely generalize  our previous work to arbitrary symmetric monoidal categories.</p> <p> Let \\((\\cc, \\otimes, I)\\) be a symmetric monoidal category. A (symmetric) operad \\(X\\) over \\(\\cc\\) is a family of objects \\(\\{X_n\\}_{n \\in \\mathbb{N}}\\),  in \\(\\cc\\), where each \\(X_n\\) has a group action by \\(S_n\\) and with <ul> <li> <p>[1.] A unit morphism \\(\\eta: I \\to X_1\\)</p> </li> <li> <p>[2.] For each \\(n \\in \\mathbb{N}\\) and \\(a_i \\in \\mathbb{N}\\) where  \\(i = 1, 2, \\dots, n\\), a composition morphism </p> </li> </ul> \\[ \\mu: X_n \\otimes X_{a_1} \\otimes \\cdots \\otimes X_{a_n} \\to X_{a_1 + \\cdots + a_n} \\] <p>subject to the associativity, identity, and equivariance axioms outlined below. \\begin{description} \\item[(OP1) Associativity.] Let \\(n \\ge 0\\) and choose \\(a_i \\ge 0\\) for \\(i = 1,2, \\dots, n\\).  Further, for each \\(a_i\\), choose \\(k_{i,j} \\ge 0\\) for \\(j = 1, 2, \\dots, a_i\\).  Let \\(\\gamma\\) be the isomorphism which rearranges the factors of the tensor product  as below:</p> \\[\\begin{gather*} \\gamma: (X_n \\otimes X_{a_1} \\otimes \\cdots \\otimes X_{a_n}) \\otimes X_{k_{1,1}} \\otimes \\cdots \\otimes X_{k_{1, a_1}} \\otimes \\cdots \\otimes X_{k_{n,1}} \\otimes \\cdots \\otimes X_{k_{n, a_n}} \\\\ \\isomarrow\\\\ X_n\\otimes(X_{a_1}\\otimes X_{k_{1,1}} \\otimes \\cdots  \\otimes X_{k_{1,a_1}}) \\otimes \\cdots \\otimes (X_{a_n}\\otimes X_{k_{n, 1}} \\otimes \\cdots  \\otimes X_{k_{n, a_n}}) \\end{gather*}\\] <p>Then we demand that the diagram below commutes. </p> <p></p> <p>\\item[(OP2) Identity.] Letting \\(A\\) be an arbitrary object of \\(\\cc\\), let \\(\\lambda: I \\otimes A \\isomarrow A\\) and \\(\\rho: A \\otimes I \\isomarrow A\\)  as the left and right unitors in our symmetric monoidal category. Then  the diagrams below must hold for all \\(n \\ge 0\\).  \\ </p> <p>\\item[(OP3) Equivariance 1.] Let \\(\\tau \\in S_n\\),  and let \\(\\tau^*\\) be the isomorphism \\(\\tau^*: X_{a_1} \\otimes \\cdots \\otimes X_{a_n} \\isomarrow X_{\\tau(a_1)} \\otimes \\cdots \\otimes X_{\\tau(a_n)}\\) and by abuse of notation  denote \\(\\tau\\) as the morphism \\(\\tau: X_n \\to X_n\\) which is given by the group action.  Then the diagram below must commute. \\  Here, \\(\\tau'\\) is the block permutation described below:</p> \\[\\begin{gather*} (\\overbrace{\\textcolor{Red}{1, 2, \\dots, a_1}}^{\\text{1st block}}, \\dots,  \\overbrace{\\textcolor{Green}{a_1 + \\cdots + a_i+1, \\dots a_1 + \\cdots + a_{i+1}}}^{i\\text{-th block}}, \\dots \\overbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots + a_{n-1}+ 1, \\dots, a_1 + \\cdots + a_n}}^{n\\text{-th block}})\\\\ \\mapsto\\\\ (\\dots, \\overbrace{\\textcolor{Red}{1, 2, \\dots, a_1}}^{\\tau(1)\\text{-th block}}, \\dots  ,  \\overbrace{\\textcolor{Green}{a_1 + \\cdots + a_i+1, \\dots, a_1 + \\cdots + a_{i+1}}}^{\\tau(i)\\text{-th block}}, \\dots ,  \\overbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots +  a_{n-1}+1, \\dots, a_1 + \\cdots + a_{n}}}^{\\tau(n)\\text{-th block}}, \\dots ). \\end{gather*}\\] <p>\\item[(OP4) Equivariance 2.] Let \\(\\sigma_i \\in S_{a_i}\\) for \\(i = 1, 2, \\dots, n\\).  By abuse of notation, denote \\(\\sigma_i: X_{a_i} \\to X_{a_i}\\) to  be the map given by the group action. Then we have that  \\  where \\((\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\\) is the permutation in \\(S_{a_1 + \\cdots + a_n}\\) defined as below.</p> <p>\\begin{gather*} (\\overbrace{\\textcolor{Red}{1, 2, \\dots, a_1}}^{\\text{1st block}} ,\\dots, \\overbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots + a_{n-1}+ 1,, \\dots, a_1 + \\cdots + a_{n-1}a_n}}^{n\\text{-th block}} ) \\ \\mapsto\\ (\\underbrace{\\sigma_1(\\textcolor{Red}{1}), \\sigma_1(\\textcolor{Red}{2}), \\dots, \\sigma_1(\\textcolor{Red}{a_1})}_{\\text{1st block}},  \\dots, </p> <p>\\underbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots +  a_{n-1}+} \\sigma_n(\\textcolor{RoyalBlue}{1}),  \\dots, \\textcolor{RoyalBlue}{a_1 + \\cdots +  a_{n-1}+}\\sigma_n(\\textcolor{RoyalBlue}{a_n}) }_{n\\text{-th block}} ) \\end{gather*}</p> <p>\\end{description} </p> <p> As before, we can create an endomorphism operad. That is, if we let \\(\\cc\\)  be a symmetric monoidal category, then we can let \\(\\aend_A(n) = \\hom_{\\cc}(A^{\\otimes n}, A)\\).  Then \\(u: I \\to \\hom_{\\cc}(X, X)\\) is defined to be the unique map to the identity. Given \\(f \\in \\aend_A(n)\\) and \\(g_i \\in \\aend_A(a_i)\\) where \\(a_i \\in \\mathbb{N}\\) for \\(i = 1,2, \\dots, n\\),  then we define our composition pointwise:  \\[ f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n) = f \\circ (g_1 \\otimes \\cdots \\otimes g_n). \\] <p>Finally, given \\(\\sigma \\in S_n\\), we can define a group action by assigning  \\(f \\cdot \\sigma\\) to the morphism which rearranges the positioning of  \\(A^{\\otimes n}\\) according to the permutation \\(\\sigma\\). With these hypotheses  one can check that the axioms of an operad are satisfied as we did in the previous section  when \\(\\cc = **Set**\\).  </p>"},{"location":"category_theory/Operads/Operads%20on%20Sets/","title":"9.1. Operads on Sets","text":"<p>Let \\(Y, Z\\) be sets. Consider a function \\(g: Y \\to Z\\). The way we've been  taught to think about this function is as a process where we're  sending an element \\(y \\mapsto g(y)\\)  in a well-defined manner. </p> <p></p> <p>Furthermore, if we have another function \\(f: X \\to Y\\),  then we can set up a pipeline \\(x \\mapsto f(x) \\mapsto g(f(x))\\). This then  establishes an obvious function \\(g \\circ f: X \\to Z\\).  \\  But the way that we've thought about functions, and more generally morphisms, is actually over-simplistic. Here we will demonstrate that we can \\emph{generalize  the concept of morphism composition}.</p> <p>Denote \\(\\aend_n(X)\\) to be the set of all functions  \\(f:X^n \\to X\\). Then for such a function, if we stick with our simplistic concept of plugging things in, we  imagine something like \\ </p> <p>However, a more natural way is to imagine that we're taking values  \\(n\\)-many values \\(x_i \\in X\\) and plugging them into the function \\(f: X^n \\to X\\).  That is, we don't have to just think of one \\(g: Y \\to X^n\\) to form a concept of  composition. We can instead imagine that each of these \\(x_i\\) values came  from functions \\(g_1: Y_1 \\to X\\), \\(g_2: Y_2 \\to X, \\cdots, g_n: Y_n \\to X\\).  \\  his is in its own right a function; a function from \\(Y_1 \\times Y_2 \\times Y_n \\to X\\). It's a generalization of function composition; when we only have one \\(g_1\\) we just  get back our original notion of function composition. We've been restricting ourselves this whole time. Now to make this even more interesting, suppose \\(Y_1 = X^{a_1}, Y_2 = X^{a_2}, \\dots, Y_n = X^{a_n}\\) where \\(a_1, a_2, \\dots, a_n\\) are positive integers. That is, suppose we have that \\(g_i \\in \\aend_{a_i}(X)\\).  \\  he above composition can be expressed as \\(f(g_1, g_2, \\dots, g_n)\\) which we may denote as</p> \\[ f \\circ_{a_1, a_2, \\dots, a_n}(g_1, g_2, \\dots, g_n): X^{a_1}\\times X^{a_2}\\times \\cdots \\times X^{a_n} \\to X. \\] <p>and note that we've construction a function in \\(\\aend_{a_1 + a_2 + \\cdots + a_n}(X)\\) using one \\(f \\in \\aend_n(X)\\) and \\(n\\)-many \\(g_i \\in \\aend_i(X)\\).  Then what we see is that our composition map is really a function that can be written formally as </p> \\[ \\circ_{a_1, a_2, \\dots, a_n}: X^n \\times (X^{a_1}\\times X^{a_2}\\times \\cdots \\times X^{a_n}) \\to  X^{a_1 + a_2 + \\cdots + a_n}  \\] <p>Then we can make this even more interesting. Each \\(g_i: X^{a_i} \\to X\\) is just like \\(f: X^n \\to X\\). Hence we can repeat the same process on each \\(g_i\\), and plug a family of functions \\(h_{i, j}: X^{k_{i,j}}\\to X\\) where \\(j = 1, 2, \\dots, a_i\\).  \\  ow there are two ways to think about this function. There is </p> \\[ [f \\circ_{a_1, a_2, \\dots, a_n} (g_1, g_2, \\dots, g_n)]\\circ_{k_{1,1}, \\dots, k_{1, a_1}, \\dots, k_{n, a_1}, \\dots, k_{n, a_n}}(h_{1,1}, \\dots, h_{n, a_n}) \\] <p>which first composes \\(f\\) with the \\(g\\)-family, and then composes with the \\(h\\)-family, and then there is </p> \\[ f \\circ_{(k_{1,1}+ \\cdots + k_{1, a_1}), \\dots, (k_{n, 1}+ \\cdots + k_{n, a_n})} \\big(g_1 \\circ_{k_{1,1}, \\dots, k_{1, a_1}}(h_{1,1}, \\dots, h_{1, a_1}), \\dots, g_n \\circ_{k_{n, 1}, \\dots, k_{n, a_n}}(h_{n,1}, \\dots, h_{n, a_n})\\big) \\] <p>which first composes each \\(g\\) with its respective \\(h\\)-family, and then composing the resulting structure with \\(f\\). Since these are just functions, and individual composition is associative,  the above two ways are the same. This construction which we have demonstrated is an example of an operad; specifically, a symmetric operad. The previous example can now be seen as motivation for the following two definitions (which will definitely need repeated read-overs).</p> <p> A nonsymmetric operad \\(X\\) in Set consists of  a family of sets \\(\\{X_n\\}_{n=1}^{\\infty}\\), an identity element \\(I \\in X_1\\) (whose purpose will soon be elaborated), and a composition map  \\[\\begin{align*} \\circ_{n, a_1, a_2, \\dots, a_n}: X_n \\times (X_{a_1} \\times X_{a_2}\\times \\cdots \\times X_{a_n}) \\to X_{a_1 + a_2 + \\cdots + a_n}\\\\ (f, g_1, g_2, \\dots, g_n) \\mapsto f\\circ_{a_1, a_2, \\dots, a_n} (g_1, g_2, \\dots, g_n) \\end{align*}\\] <p>which must exist for each \\(n = 1, 2, \\dots\\), and any \\(a_1, a_2, \\dots, a_n \\in \\mathbb{N}\\),  such that \\begin{description} \\item[(NS-OP1: Associativity.) ] Let \\(n \\in \\mathbb{N}\\) and consider \\(f \\in X_n\\).  Let \\(a_1, a_2 \\dots, a_n \\in \\mathbb{N}\\). Then  </p> \\[\\begin{gather*} [f \\circ_{a_1, a_2, \\dots, a_n} (g_1, g_2, \\dots, g_n)]\\circ_{k_{1,1}, \\dots, k_{1, a_1}, \\dots, k_{n, a_1}, \\dots, k_{n, a_n}}(h_{1,1}, \\dots, h_{n, a_n})\\\\ =\\\\ f \\circ_{(k_{1,1}+ \\cdots + k_{1, a_1}), \\dots, (k_{n, 1}+ \\cdots + k_{n, a_n})} \\big(g_1 \\circ_{k_{1,1}, \\dots, k_{1, a_1}}(h_{1,1}, \\dots, h_{1, a_1}), \\dots, g_n \\circ_{k_{n, 1}, \\dots, k_{n, a_n}}(h_{n,1}, \\dots, h_{n, a_n})\\big) \\end{gather*}\\] <p>\\item[(NS-OP2): Identity.] For every \\(f \\in X_n\\) we have that </p> \\[ f \\circ_{1, 1, \\dots, 1} (I, I, \\dots, I) = f = I \\circ_n (f). \\] <p>\\end{description} </p> <p> A symmetric operad is a nonsymmetric operad \\(X\\) with a  right group action \\(\\cdot_n: X_n \\times S_n \\to X_n\\) by the symmetric group \\(S_n\\) for each \\(n = 1, 2, \\dots\\), subject to the following axioms.  \\begin{description} \\item[(S-OP1: Equivariance 1)] Let \\(f \\in X_n\\) and pick \\(g_{1} \\in X_{a_1}, \\dots, g_n \\in X_{a_n}\\) for  some \\(a_1, a_2, \\dots, a_n \\in \\mathbb{N}\\). Then for a \\(\\tau \\in S_n\\), we must  have  \\[ (f \\cdot \\tau)\\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n) = (f \\circ_{a_{\\tau^{-1}(1)}, \\dots, a_{\\tau^{-1}(n)}}(g_{\\tau^{-1}(1)}, \\dots, g_{\\tau^{-1}(n)}))\\cdot \\tau' \\] <p>where \\(\\tau' \\in S_{a_1 + \\cdots + a_n}\\). Here, \\(\\tau'\\) is a block permutation  that swaps the \\(i\\)-th block with the \\(\\tau(i)\\)-th block. That is, if  \\(\\tau \\in S^n\\) as a permutation acts as </p> \\[ (1, 2, \\dots, n) \\mapsto (\\tau(1), \\tau(2), \\dots, \\tau(n)) \\] <p>then \\(\\tau' \\in S_{a_1 + a_2 + \\cdots + a_n}\\) acts as </p> \\[\\begin{gather*} (\\overbrace{\\textcolor{Red}{1, 2, \\dots, a_1}}^{\\text{1st block}}, \\dots,  \\overbrace{\\textcolor{Green}{a_1 + \\cdots + a_i+1, \\dots a_1 + \\cdots + a_{i+1}}}^{i\\text{-th block}}, \\dots \\overbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots + a_{n-1}+ 1, \\dots, a_1 + \\cdots + a_n}}^{n\\text{-th block}})\\\\ \\mapsto\\\\ (\\dots, \\overbrace{\\textcolor{Red}{1, 2, \\dots, a_1}}^{\\tau(1)\\text{-th block}}, \\dots  ,  \\overbrace{\\textcolor{Green}{a_1 + \\cdots + a_i+1, \\dots, a_1 + \\cdots + a_{i+1}}}^{\\tau(i)\\text{-th block}}, \\dots ,  \\overbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots +  a_{n-1}+1, \\dots, a_1 + \\cdots + a_{n}}}^{\\tau(n)\\text{-th block}}, \\dots ). \\end{gather*}\\] <p>\\item[(S-OP2: Equivariance 2)] Let \\(f, g_i\\) is as above, and choose \\(\\sigma_1 \\in S_1, \\dots, \\sigma_{n} \\in S_n\\).  Then we have that </p> \\[ f \\circ_{a_1, \\dots, a_n}(g_1 \\cdot \\sigma_1, \\dots, g_n \\cdot \\sigma_n) = (f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n))\\cdot (\\sigma_1, \\dots,\\sigma_n) \\] <p>where \\((\\sigma_1, \\sigma_2, \\dots, \\sigma_n) \\in S_{a_1 + a_2 + \\cdots + a_n}\\)  is the permutation described as below. </p> <p>\\begin{gather*} (\\overbrace{\\textcolor{Red}{1, 2, \\dots, a_1}}^{\\text{1st block}} ,\\dots, \\overbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots + a_{n-1}+ 1,, \\dots, a_1 + \\cdots + a_{n-1}a_n}}^{n\\text{-th block}} ) \\ \\mapsto\\ (\\underbrace{\\sigma_1(\\textcolor{Red}{1}), \\sigma_1(\\textcolor{Red}{2}), \\dots, \\sigma_1(\\textcolor{Red}{a_1})}_{\\text{1st block}},  \\dots, </p> <p>\\underbrace{\\textcolor{RoyalBlue}{a_1 + \\cdots +  a_{n-1}+} \\sigma_n(\\textcolor{RoyalBlue}{1}),  \\dots, \\textcolor{RoyalBlue}{a_1 + \\cdots +  a_{n-1}+}\\sigma_n(\\textcolor{RoyalBlue}{a_n}) }_{n\\text{-th block}} ) \\end{gather*}</p> <p>\\end{description} </p> <p> We can continue with our previous construction concerning  the family of sets  \\[ \\aend_n(X) = \\{f: X^n \\to X \\mid f \\in **Set**\\} \\] <p>to demonstrate that it  forms a symmetric operad. As we already established associativity NS-OP1, we need  to verify the identity axiom NS-OP2. Such an identity element can be chosen if  we select \\(I = 1_X: X \\to X\\). On one hand we have for any \\(f \\in X^n\\) that</p> \\[ f \\circ_{1, 1, \\dots, 1}(I, I, \\dots, I) = f(1_x, 1_x, \\dots, 1_x) = f \\] <p>while on the other we have that \\(I \\circ_n f = 1_X \\circ f = f\\).  Next, define a group action of \\(S_n\\) on \\(\\aend_n(X)\\) as </p> \\[ (f \\cdot \\sigma)(x_1, x_2, \\dots, x_n) = f(x_{\\sigma(1)}, x_{\\sigma(2)}, \\dots, x_{\\sigma(n)}). \\] <p>We now verify S-OP1 with this group action.  Let \\(f \\in \\aend_n(X)\\) and \\(g_i \\in \\aend_i(X)\\) for \\(i = 1, 2, \\dots, n\\). For a given \\(\\tau \\in S_n\\), consider the points \\((x_1, \\dots, a_1) \\in X^{a_1}, \\dots, (x_1, \\dots, a_n) \\in X^{a_n}\\). Observe that \\((f \\cdot \\tau) \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)\\) first plugs in  the each \\((x_{a_{i}-1}, \\dots, x_{a_i})\\) into \\(g_i\\), which is then  plugged into \\(f\\). However, the action of \\(\\tau\\) swaps these resulting coordinates.  Thus we get that </p> \\[ (f \\cdot \\tau) \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)(x_1, \\dots, a_1, \\dots, x_{a_{n-1}+1}, \\dots, x_{a_n}) =  (\\dots, \\overbrace{g_i(x_{a_{i-1}+1}, \\dots, x_{a_i})}^{\\tau(i)-\\text{th entry}}, \\dots ) \\] <p>How do we write this more formally? Well, to answer that, we need to know the answer  to the following question: which \\(g_i(x_{a_{i-1}+1}, \\dots, x_{a_i})\\) maps to, say, the 1st coordinate? This is equivalently to asking: what is \\(\\tau^{-1}(1)\\)?  Hence we see that </p> \\[\\begin{gather*} (f \\cdot \\tau) \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)(x_1, \\dots, a_1, \\dots, x_{a_{n-1}+1}, \\dots, x_{a_n})\\\\ = f(g_{\\tau^{-1}(1)}(x_{a_{\\tau^{-1}(1)-1}+1}, \\dots, x_{a_{\\tau^{-1}(1)}}), \\dots, g_{\\tau^{-1}(n)}(x_{a_{\\tau^{-1}(n)-1}+1}, \\dots, x_{a_{\\tau^{-1}(n)}}))\\\\ =  f \\circ_{\\tau^{-1}(1), \\tau^{-1}(2), \\dots, \\tau^{-1}(n)}(g_{\\tau^{-1}(1)}, g_{\\tau^{-1}(2)}, \\dots, g_{\\tau^{-1}(n)}) (x_{a_{\\tau^{-1}(1)-1}+1}, \\dots, x_{a_{\\tau^{-1}(1)}}, \\dots, x_{a_{\\tau^{-1}(n)-1}+1}, \\dots, x_{a_{\\tau^{-1}(n)}})\\\\ = \\Big(f \\circ_{\\tau^{-1}(1), \\tau^{-1}(2), \\dots, \\tau^{-1}(n)}(g_{\\tau^{-1}(1)}, g_{\\tau^{-1}(2)}, \\dots, g_{\\tau^{-1}(n)}) \\cdot \\tau'\\Big)(x_1, \\dots, x_{a_1}, \\dots, x_{a_{n-1}+1}, \\dots, x_{a_n}). \\end{gather*}\\] <p>where \\(\\tau'\\) is the block permutation described in the definition.  Thus we see that </p> \\[ (f \\cdot \\tau) \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n) = f \\circ_{\\tau^{-1}(1), \\tau^{-1}(2), \\dots, \\tau^{-1}(n)}(g_{\\tau^{-1}(1)}, g_{\\tau^{-1}(2)}, \\dots, g_{\\tau^{-1}(n)}) \\cdot \\tau' \\] <p>as desired. Thus we have S-OP1. Finally, we show S-OP2, which  is a bit easier to demonstrate. As before, let \\(f, a_i\\) and \\(g_i\\) be as described  before. Let \\(\\sigma_1 \\in S_1, \\dots, \\sigma_n \\in S_n\\).  Then </p> \\[\\begin{gather*} f \\circ_{a_1, \\dots, a_n}(g_1 \\cdot \\sigma_1, \\dots, g_n \\cdot \\sigma_n)(x_1, \\dots, x_{a_1}, \\dots, x_{a_{n-1}+1}, \\dots, x_{a_n})\\\\ = f\\Big( (g_1 \\cdot \\sigma_1)(x_1, \\dots, x_{a_1}), \\dots, (g_n \\cdot \\sigma_n)(x_{a_{n-1}+1}, \\dots, x_{a_n})  \\Big)\\\\ = f\\Big( g_1(x_{\\sigma_1(1)}, \\dots, x_{\\sigma_1(a_1)}), \\dots, g_n(x_{\\sigma_n(1)}, \\dots, x_{\\sigma_n(a_n)}) \\Big)\\\\ =  \\Big( f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n) \\Big)(x_{\\sigma_1(1)}, \\dots, x_{\\sigma_1(a_1)}, \\dots, x_{\\sigma_n(1)}, \\dots, x_{\\sigma_n(a_n)})\\\\ =  \\Big( f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n) \\Big) \\cdot (\\sigma_1, \\dots, \\sigma_n)(x_1, \\dots, x_{a_1}, \\dots, x_{a_{n-1}+1}, \\dots, x_{a_n}) \\end{gather*}\\] <p>Thus we see that </p> \\[ f \\circ_{a_1, \\dots, a_n}(g_1 \\cdot \\sigma_1, \\dots, g_n \\cdot \\sigma_n) =(f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)) \\cdot (\\sigma_1, \\dots, \\sigma_n) \\] <p>so that S-OP2 is satisfied.  All together, we have that for any set \\(X\\), the family of sets \\(\\aend_{n}(X)\\)  forms a symmetric operad. </p> <p> Consider the family of sets \\(\\text{Assoc}_n = S_n\\) where each level  is the \\(n\\)-th symmetric group. Suppose that \\(\\tau \\in S_n\\) and  that \\(\\sigma_1 \\in S_{a_1}, \\sigma_2 \\in S_{a_2}, \\dots, \\sigma_n \\in S_{a_n}\\) for \\(a_1, a_2, \\dots, a_n \\in \\mathbb{N}\\). Then we define  \\[ \\tau \\circ_{a_1, \\dots, a_n}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n) \\in S_{a_1 + a_2 + \\cdots + a_n}    \\] <p>as a permutation of \\(a_1 + a_2 + \\cdots + a_n\\) letters. Before we describe the permutation, we'll introduce some notation. Consider the (ordered) tuple of the first \\(a_1 + \\cdots + a_n\\) integers. </p> \\[ (\\textcolor{Red}{1, 2, \\dots, a_1},  \\textcolor{Green}{a_1 + 1, a_1 + 2, \\dots, a_1 + a_2}, \\dots \\textcolor{RoyalBlue}{(a_{1} + \\cdots + a_{n-1})+1, \\dots, (a_{1} + \\cdots + a_{n-1}) + a_n}) \\] <p>We can more compactly denote this tuple as </p> \\[ (\\textcolor{Red}{1, 2, \\dots, a_1}, \\textcolor{Green}{1', 2', \\dots, a'_2},  \\dots,  \\textcolor{RoyalBlue}{1', 2', \\dots, a'_n})  \\] <p>where from either context or coloring it will be clear what each \\(1', 2',\\dots\\)  indicates. For example, above we'll have that  \\(\\textcolor{Green}{1' = a_1 + 1}\\) and \\(\\textcolor{Green}{2' = a_1+ 2}\\)  wheres  \\(\\textcolor{RoyalBlue}{1' = (a_1 + \\cdots + a_{n-1}) + 1}\\) and \\(\\textcolor{RoyalBlue}{2' = (a_1 + \\cdots + a_{n-1}) + 2}\\). With that said, we can define \\(\\tau \\circ_{a_1, \\dots, a_n}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n) \\in S_{a_1 + a_2 + \\cdots + a_n}\\) by its action on such a tuple, pictured below.  \\  which can be rewritten more formally as </p> \\[ (\\overbrace{\\sigma'_{\\tau^{-1}(1)}(1), \\sigma'_{\\tau^{-1}(1)}(2), \\dots, \\sigma'_{\\tau^{-1}(1)}(a_{\\tau^{-1}(1)})}^{1\\text{st block}},  \\dots,  \\overbrace{\\sigma'_{\\tau^{-1}(n)}(1), \\sigma'_{\\tau^{-1}n}(2), \\dots, \\sigma'_{\\tau^{-1}(n)}(a_{\\tau^{-1}(n)})}^{n\\text{-th block}} ) \\in  S_{a_1 + \\cdots + a_n}. \\] <p>Now for each \\(\\sigma_i \\in S_{a_i}\\), let \\(\\rho_{i, j} \\in S_{k_{i,j}}\\)  for \\(j = 1, 2, \\dots, a_i\\) and for \\(k_{i,j} \\in \\mathbb{N}\\).  For notational convenience, denote \\(K = k_{1,1}+ \\cdots + k_{1,a_1}+ \\cdots + k_{n,1} + \\cdots + k_{n, a_n}\\).  By our above definition, we can construct a permutation in \\(S_{K}\\) by composing \\(\\tau\\) with the \\(\\sigma\\)-family and with the \\(\\rho\\)-family. There are two possible ways to construct such a permutation  (and we'll show  that they are equivalent, therefore satisfying NS-OP1).  But before we do that  we must consider the first \\(K\\) integers.  This will be a huge tuple; in full notation this is \\  Using our previous notation we can rewrite this as </p> \\[ ( \\overbrace{\\textcolor{Red}{1, 2, \\dots, k_{1,1}}}^{1\\text{st block}},  \\overbrace{\\textcolor{Orange}{1'},  \\textcolor{Orange}{2'}, \\dots, \\textcolor{Orange}{k_{1,2}}}^{2\\text{nd block}},  \\dots,  \\overbrace{\\textcolor{Green}{1'}, \\textcolor{Green}{2'}, \\dots, \\textcolor{Green}{k_{1, a_1}}}^{a_1\\text{-th block}}, \\dots, \\hspace{-0.5cm} \\overbrace{\\textcolor{Purple}{1'}, \\textcolor{Purple}{2'}, \\dots, \\textcolor{Purple}{k_{n, 1}}}^{(a_1 + \\cdots + a_{n-1}+1)\\text{-th block}} \\hspace{-0.5cm} ,\\dots, \\overbrace{\\textcolor{RoyalBlue}{1'}, \\textcolor{RoyalBlue}{2'}, \\dots, \\textcolor{RoyalBlue}{k_{n, a_n}}}^{(a_1 + \\cdots + a_n)\\text{-th block}} ) \\] <p>where again, for example, \\(\\displaystyle \\textcolor{Orange}{1' = k_{1,1}+1}\\) whereas  \\(\\displaystyle \\textcolor{RoyalBlue}{1' =  \\sum_{i}^{n-1}\\sum_{j=1}^{a_i}k_{i, j} + (k_{n, 1} + \\cdots + k_{n, (a_n-1)}) +1}\\).</p> <p>Now we will first want to calculate </p> \\[ (\\tau \\circ_{a_1, \\dots, a_n} (\\sigma_1, \\sigma_2, \\dots, \\sigma_n))\\circ_{k_{1,1}, \\dots, k_{1, a_1}, \\dots, k_{n, 1}, \\dots, k_{n, a_n}} \\circ(\\rho_{1,1}, \\dots, \\rho_{n, a_n}). \\] <p>The first step to computing this is to note that each \\(\\rho_{i,j}\\) permutes the numbers within its block.  \\begin{tikzcd} ( \\overbrace{\\textcolor{Red}{1, 2, \\dots, k_{1,1}}}^{1\\text{st block}},  \\overbrace{\\textcolor{Orange}{1'},  \\textcolor{Orange}{2'}, \\dots, \\textcolor{Orange}{k_{1,2}}}^{2\\text{nd block}},  \\dots, \\hspace{-0.5cm} \\overbrace{\\textcolor{ProcessBlue}{1'}, \\textcolor{ProcessBlue}{2'}, \\dots, \\textcolor{ProcessBlue}{k_{i, j}}}^{(a_1 + \\cdots + a_{i-1}+j)\\text{-th block}} \\hspace{-0.5cm} ,\\dots, \\overbrace{\\textcolor{RoyalBlue}{1'}, \\textcolor{RoyalBlue}{2'}, \\dots, \\textcolor{RoyalBlue}{k_{n, a_n}}}^{(a_1 + \\cdots + a_n)\\text{-th block}} ) \\arrow[d, \"\\rho_{1,1}\", start anchor = {[xshift = -5.7cm]}, end anchor = {[xshift=-5.7cm]}] \\arrow[d, \"\\rho_{1,1}\", start anchor = {[xshift = -5.7cm]}, end anchor = {[xshift=-5.7cm]}] \\arrow[d,draw = none, start anchor = {[xshift = -3cm]}, end anchor = {[xshift=-3cm]}, \"\\raisebox{+0.2ex}{\\dots}\" description] \\arrow[d, \"\\rho_{i,j}\", start anchor = {[xshift = 0cm]}, end anchor = {[xshift=0cm]}] \\arrow[d,draw = none, start anchor = {[xshift = 1.5cm]}, end anchor = {[xshift=1.5cm]}, \"\\raisebox{+0.2ex}{...}\" description] \\arrow[d, \"\\rho_{n, a_n}\", start anchor = {[xshift = 3.4cm]}, end anchor = {[xshift=3.4cm]}] \\ ( \\underbrace{\\rho_{1,1}(\\textcolor{Red}{1}), \\rho_{1,1}(\\textcolor{Red}{2}), \\dots, \\rho_{1,1}(\\textcolor{Red}{k_{1,1}})}{1\\text{st block}},  \\dots, \\underbrace{\\rho{i,j}'(\\textcolor{ProcessBlue}{1})\\rho'{i,j}(\\textcolor{ProcessBlue}{2}), \\dots, \\rho'{i,j}(\\textcolor{ProcessBlue}{k_{i,j}})}{(a_1 + \\cdots + a{i-1}+j)\\text{-th block}}, \\dots, \\underbrace{\\rho_{n, a_n}'(\\textcolor{RoyalBlue}{1}),  \\rho_{n, a_n}'(\\textcolor{RoyalBlue}{2}), \\dots, \\rho_{n, a_n}'(\\textcolor{RoyalBlue}{k_n, a_n})}{(a_1 + \\cdots + a{n})\\text{-th block}} )) \\end{tikzcd} Now that we've applied the \\(\\rho\\) permutations, we must apply the permutation  \\(\\tau \\circ_{a_1, \\dots, a_n} (\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\\) in \\(S_{a_1 + \\cdots + a_n}\\).  This will instead be a block permutation. Hopefully it is now clear why we were paying  so much attention and to and keeping track of the blocks; we knew ahead of time that we were going to permute our \\(a_1 + \\cdots+ a_n\\) blocks by using our \\(S_{a_1 + \\cdot+ a_n}\\) permutation \\(\\tau \\circ_{a_1, \\dots, a_n} (\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\\) in \\(S_{a_1 + \\cdots + a_n}\\). </p> <p>Recall that for \\(\\rho_{i, j}\\), \\(i\\) ranges from \\(1\\) to \\(n\\)  while \\(j\\) ranges from \\(1\\) to \\(a_i\\). Hence if we permute a block, we can represent it as follows.  \\  hich can be written more formally (that is, more horribly) as </p> \\[ ( \\dots, \\underbrace{\\rho_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}(1),  \\rho_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}(2), \\dots,  \\rho_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}(k_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}) }_{(a_1 + \\cdots + a_{i-1}+j)\\text{-th block}} ,\\dots ) \\] <p>At this point we'll want to see that this is the same as </p> \\[ \\tau \\circ_{(k_{1,1} + \\cdots + k_{1,a_1}), \\dots, (h_{n, 1} + \\cdots + k_{n,a_n})} (\\sigma_1 \\circ_{k_{1,1}, \\dots, k_{1,a_1}}(\\rho_{1,1}, \\dots, \\rho_{1,a_1}), \\dots, \\sigma_n \\circ_{k_{n,1}, \\dots, k_{n,a_n}}(\\rho_{n,1}, \\dots, \\rho_{n,a_n})  ) \\] <p>To do this we need to think about each \\(\\sigma_i \\circ_{k_{i, 1}, \\dots, k_{i, a_i}}(\\rho_{i,1}, \\dots, \\rho_{i, a_i})\\) which isn't too bad. Each is a permutation in \\(S_{k_{i,1} + \\cdots + k_{i, a_i}}\\), and  hence a permutation of the (ordered) tuple below. </p> \\[ ( \\textcolor{Red}{1, 2, \\dots, k_{i, 1}},  \\textcolor{Orange}{k_{i, 1} + 1, k_{i, 1} + 2, \\dots, k_{i, 1} + k_{i, 2}}, \\dots,  \\textcolor{RoyalBlue}{(k_{i, 1} + k_{i, 2} + \\cdots + k_{i, a_{i-1}})+1}, \\dots, \\textcolor{RoyalBlue}{(k_{i, 1} + k_{i, 2} + \\cdots )+ k_{i, a_i}} ) \\] <p>which we again abbreviate as </p> \\[ (\\textcolor{Red}{1, 2, \\dots, k_{i, 1}},  \\textcolor{Orange}{1', 2', k_{i, 2}}, \\dots,  \\textcolor{RoyalBlue}{1'}, \\textcolor{RoyalBlue}{2'}, \\dots, \\textcolor{RoyalBlue}{k_{i, a_i}}). \\] <p>With those notation above each permutation acts as  \\  hich can be more formally understood as the tuple  \\begin{equation} ( \\overbrace{\\rho'{i, \\sigma_i^{-1}(1)}(1), \\rho'{i, \\sigma_i^{-1}(1)}(2), \\dots,  \\rho'{i, \\sigma_i^{-1}(1)}(k{i, \\sigma_1^{-1}(1)})}^{1\\text{st tuple}}, \\dots,  \\overbrace{\\rho'{i, \\sigma_i^{-1}(a_i)}(1), \\rho'{i, \\sigma_i^{-1}(a_i)}(2), \\dots,  \\rho'{i, \\sigma_i^{-1}(a_i)}(k{i, \\sigma_i^{-1}(a_i)})}^{a_i\\text{-th tuple}} ) \\end{equation} Now that we understand what each \\(\\sigma_i \\circ_{k_{i, 1}, \\dots, k_{i, a_i}}(\\rho_{i,1}, \\dots, \\rho_{i, a_i})\\) does for \\(i = 1, 2, \\dots, n\\), and because we know that \\(\\tau \\in S_n\\),  this means we can compose \\(\\tau\\) with this family of \\(n\\)-permutations, which will give rise to a \\(S_{k_{1,1}+ \\cdots + k_{1, a_1} + \\cdots + k_{n, 1} + \\cdots + k_{n, a_n}}\\) permutation. To calculate this  we just now directly apply their composition. This will act on the  \\(k_{1,1} + \\cdots k_{1, a_1} + \\cdots + k_{n, 1} + \\cdots + k_{n, a_n}\\) tuple \\  y rearranging the tuple as below \\  nd using (\\ref{tuples}) we know that this becomes  \\  The above tuple can be (again, horribly) understood as </p> \\[         ( \\dots, \\underbrace{\\rho_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}(1),  \\rho_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}(2), \\dots,  \\rho_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}(k_{\\tau^{-1}(i), \\sigma_{\\tau^{-1}(i)}^{-1}(j)}) }_{(a_1 + \\cdots + a_{i-1}+j)\\text{-th block}} ,\\dots ) \\] <p>Which shows that </p> \\[\\begin{gather*} (\\tau \\circ_{a_1, \\dots, a_n} (\\sigma_1, \\sigma_2, \\dots, \\sigma_n))\\circ_{k_{1,1}, \\dots, k_{1, a_1}, \\dots, k_{n, 1}, \\dots, k_{n, a_n}} \\circ(\\rho_{1,1}, \\dots, \\rho_{n, a_n})\\\\ =\\\\ \\tau \\circ_{(k_{1,1} + \\cdots + k_{1,a_1}), \\dots, (h_{n, 1} + \\cdots + k_{n,a_n})} (\\sigma_1 \\circ_{k_{1,1}, \\dots, k_{1,a_1}}(\\rho_{1,1}, \\dots, \\rho_{1,a_1}), \\dots, \\sigma_n \\circ_{k_{n,1}, \\dots, k_{n,a_n}}(\\rho_{n,1}, \\dots, \\rho_{n,a_n})  ) \\end{gather*}\\] <p>so that NS-OP1 is satisfied. Now verifying NS-OP2 is simple;  note that as \\(S_1\\) has one element, we are forced to identify our identity element  as \\(\\sigma_1\\), the unique permutation of one element that doesn't do anything. Then  for any \\(\\tau \\in S_n\\), we of course have that \\(\\tau \\circ_{1, 1, \\dots, 1}(\\sigma_1, \\sigma_1, \\dots \\sigma_1) = \\tau\\), as each element is unchanged by \\(\\sigma_1\\) before \\(\\tau\\) is applied.  We also know that \\(\\sigma_1 \\circ_n (\\tau) = \\tau\\), since this is just  applying \\(\\tau\\) and then applying the trivial block permutation to the \\(n\\) elements.</p> <p>Now we show S-OP1. As we need a right action of \\(S_n\\) on the \\(n\\)-th level  of our operad, which also happens to be \\(S_n\\), an evident choice would be to  just take the group product. Hence for any \\(\\sigma \\in S_n\\), we say \\(\\tau \\in S_n\\)  acts on \\(\\sigma\\) to give rise to </p> \\[ (\\sigma \\cdot \\tau) = \\sigma \\circ \\tau \\] <p>which is clearly in \\(S_n\\). </p> <p>To demonstrate S-OP1, let \\(\\tau, \\rho \\in S_n\\), and \\(\\sigma_1 \\in S_{a_1}, \\dots, \\sigma_n\\in S_{a_n}\\) for \\(a_i \\in \\mathbb{N}\\). To compute \\((\\tau \\cdot \\rho)\\circ_{a_1, \\dots, a_n}(\\sigma_1, \\dots, \\sigma_n)\\),  denote an (ordered) tuple of the first \\(a_1 + \\cdots + a_n\\) integers as </p> \\[ (\\textcolor{Red}{1, 2, \\dots, a_1}, \\dots, \\textcolor{RoyalBlue}{1', 2', \\dots, a_n}). \\] <p>Then we see that \\((\\tau \\cdot \\rho)\\circ_{a_1, \\dots, a_n}(\\sigma_1, \\dots, \\sigma_n)\\) acts on the tuple to give rise to </p> \\[ (\\sigma'_{\\rho^{-1}(\\tau^{-1}(1))}(1),\\dots, \\sigma'_{\\rho^{-1}(\\tau^{-1}(1))}(a_{\\rho^{-1}(\\tau^{-1}(1))}), \\dots, \\sigma'_{\\rho^{-1}(\\tau^{-1}(n))}(1),\\dots, \\sigma'_{\\rho^{-1}(\\tau^{-1}(n))}(a_{\\rho^{-1}(\\tau^{-1}(n))})) \\] <p>On the other hand we need to also compute \\((\\tau \\circ_{a_{\\rho^{-1}(1)}, \\dots, a_{\\rho^{-1}(n)}}(\\sigma_{\\rho^{-1}(1)}, \\dots, \\sigma_{\\rho^{-1}(n)}))\\cdot \\rho'\\)  where \\(\\rho'\\) is the evident block permutation. However, this is really just  \\((\\tau \\circ_{a_{\\rho^{-1}(1)}, \\dots, a_{\\rho^{-1}(n)}}(\\sigma_{\\rho^{-1}(1)}, \\dots, \\sigma_{\\rho^{-1}(n)}))\\circ \\rho'\\); below we see that its action on an ordered \\(a_1 + \\cdots + a_n\\) tuple is as we would expect. \\  herefore we see that </p> \\[ (\\tau \\cdot \\rho)\\circ_{a_1, \\dots, a_n}(\\sigma_1, \\dots, \\sigma_n) = (\\tau \\circ_{a_{\\rho^{-1}(1)}, \\dots, a_{\\rho^{-1}(n)}}(\\sigma_{\\rho^{-1}(1)}, \\dots, \\sigma_{\\rho^{-1}(n)}))\\cdot \\rho' \\] <p>so that S-OP1 is satisfied. We just now need to show S-OP2 is satisfied,  which is nearly immediate. We will however not pretend we're too good to show  this and demonstrate it anyways.  For each \\(\\sigma_i \\in S_{a_i}\\), pick \\(\\rho_i \\in S_{a_i}\\). Observe that \\(\\tau \\circ_{a_1, \\dots, a_n}(\\sigma_1 \\cdot \\rho_1, \\dots, \\sigma_n \\cdot \\rho_n)\\) \\  eturns the same result as \\((\\tau \\circ_{a_1, \\dots, a_n}(\\sigma_1, \\dots, \\sigma_n))\\cdot(\\rho_1, \\dots, \\rho_n)\\) \\  ince \\((\\tau \\circ_{a_1, \\dots, a_n}(\\sigma_1, \\dots, \\sigma_n))\\cdot(\\rho_1, \\dots, \\rho_n) =  (\\tau \\circ_{a_1, \\dots, a_n}(\\sigma_1, \\dots, \\sigma_n))\\circ(\\rho_1, \\dots, \\rho_n)\\) in our case.  As we have that S-OP2 is satisfied, we have that \\(\\text{Assoc}_n = S_n\\) is a symmetric operad.</p> <p></p> <p> An morphism of operads \\(F: X \\to Y\\) between two  (symmetric) operads \\(X, Y\\) with units \\(I \\in X_1\\) and \\(J \\in Y_1\\)  and \\(S_n\\) group actions \\(\\cdot\\) and \\(*\\) is a family of maps \\(F_n: X_n \\to Y_n\\) such that  \\begin{description} \\item[(M-OP1)] \\(F_1(I) = J\\)  \\item[(M-OP2)] If \\(f \\in X_n\\) and \\(g_1 \\in X_{a_1}, \\dots, g_n \\in X_{a_n}\\)  for \\(a_i \\in \\mathbb{N}\\), then  \\[ F_{a_1 + \\cdots + a_n}(f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)) = F_n(f)\\circ_{a_1, \\dots, a_n}(F_{a_1}(g_1), \\dots, F_{a_n}(g_n)) \\] <p>\\item[(M-OP3)] If \\(f \\in X_n\\) and \\(\\tau \\in S_n\\), then </p> \\[ F_n(f \\cdot \\tau) = F_n(f) * \\tau  \\] <p>\\end{description}  Note: in the case where \\(X, Y\\) are symmetric operads, we define a morphism between  \\(X\\) and \\(Y\\) to be a family of maps \\(F_n: X_n \\to Y_n\\) such that only M-OP1 and M-OP2 hold. </p> <p> A algebra over an Operad \\(X\\) is a morphism of operads \\(F: X \\to \\aend_A\\)  where \\(A\\) is some set. Spelled out, this is a mapping  \\[\\begin{align*} F_n: X_n \\to \\hom_{**Set**}(A^n, A)\\\\ f \\mapsto F_n(f): A^n \\to A  \\end{align*}\\] <p>so that we're mapping elements of our operad to \\(n\\)-ary operations over \\(A\\). This mapping also requires that</p> <ul> <li> <p>[1.] \\(F_1(I) = \\id_A: A \\to A\\)</p> </li> <li> <p>[2.] For \\(f \\in X_n\\), \\(g_i \\in X_{a_i}\\) for \\(i = 1,2, \\dots, n\\), </p> </li> </ul> \\[ F_{a_1 + \\cdots + a_n}(f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)) = F_n(f)\\circ'_{a_1, \\dots, a_n}(F_{a_1}(g_1), \\dots, F_{a_n}(g_n)). \\] <p>Diagrammatically, this means the following diagrams commutes: \\  Or, more visually,  \\ </p> <ul> <li>[3.] Finally, we have that if \\(\\tau \\in S_n\\), then for \\(f \\in X_n\\) and \\((a_1, \\dots, a_n) \\in A^n\\), then</li> </ul> \\[ F_n(f \\cdot \\tau)(a_1, \\dots, a_n)  = (F_n(f) *\\tau) (a_1, \\dots, a_n)  = F_n(f)(a_{\\tau(1)}, \\dots, a_{\\tau(n)}). \\] <p></p> <p> Let \\(X\\) be an operad. A \\textbf{morphism \\(\\Phi: F \\to G\\)  between algebras} \\(F: X \\to \\aend_A\\) and \\(G: X \\to \\aend_B\\) over \\(X\\) is a function  \\(\\phi: A \\to B\\) such that, for \\(f \\in X_n\\) and \\((a_1, \\dots, a_n) \\in A^n\\),  \\[ \\phi(F_n(f)(a_1, \\dots, a_n)) = G(f)(\\phi(a_1), \\dots, \\phi(a_n)) \\] <p>The above relation can be more conveniently expressed as the diagram below  commuting:  \\  which must hold for all \\(f \\in X_n\\) with \\(n \\in \\mathbb{N}\\).  Now suppose that for an operad \\(X\\) we have</p> <p>\\noindent \\begin{minipage}{0.7\\textwidth}       three algebras </p> \\[ F: X \\to \\aend_A \\quad G: X \\to \\aend_B \\quad H: X \\to \\aend_C \\] <p>such that \\(\\Phi: F \\to G\\)  and \\(\\Psi: G \\to H\\) are morphisms of algebras given by functions  \\(\\phi: A \\to B\\) and \\(\\psi: B \\to C\\). A natural question is whether  or not one can define a morphism \\(\\Psi \\circ \\Phi: F \\to G\\). This is however  immediate upon realization that we can stack the diagrams to see  that \\(\\Phi \\circ \\Psi: F \\to H\\) is a morphism of algebras. \\end{minipage} \\begin{minipage}{0.3\\textwidth} \\  \\end{minipage} </p> <p>As a result, if we are given an operad \\(X\\), we can create a category  \\(**Alg**_X\\) whose objects are algebras \\(\\Phi: X \\to \\aend_A\\)  and whose morphisms are morphisms between such algebras. These categories  actually return ordinary categories that we've dealt with in the past. </p> <p> Consider the operad \\(\\text{Assoc}_n = S_n\\). Then we have that  \\[ **Alg**_{\\text{Assoc}_n} \\cong **Mon** \\] <p>where Mon is the category of monoids. (In terms of set theory, we're  being sloppy; but if anyone challenges this we can just pull out a Grothendieck universe  and satisfy their demands.)  To demonstrate this isomorphism we must produce a pair of inverse functors  between these categories. </p> <p>Before we do that,  first consider an object in this category, which  is a family of functions \\(F_n: S_n \\to \\hom_{**Set**}(A^n, A)\\)  for some set \\(A\\). To save some space, denote \\(\\hom_{**Set**}(A^n, A)\\)  as \\([A^n, A]\\). Then the fact that \\(F: **Assoc**_n \\to \\aend_A\\) is an algebra gives us that  the diagram on the left commutes.  \\  As this diagram commutes, we can follow the specific path which is taken  by the identity elements \\(e_2 \\in S_2\\) and \\(e_1 \\in S_1\\). If we denote  \\(F_n(e_n) = \\mu_n: A^n \\to A\\), then we see that \\(\\mu_3 = \\mu_2(\\mu_2, \\id_A)\\).  Note that in particular, \\(\\mu_1 = \\id_A\\) by hypothesis. Hence for \\(a, b, c \\in A\\), we see that \\(\\mu_3 = \\mu_2(\\mu_2(a, b), c)\\).  Conversely, we can repeat the same thing with \\(S_1\\) and \\(S_2\\) swapped, and  obtain a commutative diagram on the left: \\  and following the identity elements again grants us that  \\(\\mu_3 = \\mu_2(\\id_A, \\mu_2)\\). Hence we see that for \\(a, b ,c \\in A\\) \\(\\mu_3(a, b, c) = \\mu_2(a, \\mu_2(b, c))\\). All together we have that </p> \\[ \\mu_2(\\mu_2(a, b), c) = \\mu_2(a, \\mu_2(b, c)). \\] <p>What does this mean? Perhaps this will make it more clear: denote \\(\\mu_2(a,b) = a \\cdot b\\).  Then this means that </p> \\[ (a \\cdot b) \\cdot c = a \\cdot (b \\cdot c). \\] <p>This means that we've proved that \\(A\\) is a set equipped with a binary operator \\(\\mu_2: A \\times A \\to A\\) which is associative! This is almost a monoid; we're just missing an identity element.  However, note that  </p>"},{"location":"category_theory/Operads/Partial%20Composition%3A%20Restructuring%20Operads/","title":"9.3. Partial Composition: Restructuring Operads","text":"<p>After one stares at the definition of an operad for quite some time, they  will realize that the vast and mysterious diagrams and indices are really just  for booking keeping, and that the idea is actually rather quite intuitive.  And of this bookkeeping is what makes operads a bit annoying; we are  constantly having to think about an arbitrarily long tensor products.  However, Freese has pointed out in his text that we can actually rephrase the  language of operads more simply by replacing the arbitrarily long  composition morphism with a partial composition morphism. However, this  itself is not trivial. </p> <p>Let \\(X\\) be a set, and consider the endomorphism operad \\(\\aend_X(n)\\).  For any \\(f \\in \\hom_{**Set**}(X^n, X)\\), we can choose \\(g_i \\in \\hom_{**Set**}(X^{a_i}, X)\\)  for \\(a_i \\in \\mathbb{N}\\) with \\(i = 1, 2, \\dots, n\\). Composition can then be defined pointwise: </p> \\[\\begin{gather*} f \\circ_{a_1, \\dots, a_n}(g_1, \\dots, g_n)(x_1, \\dots, x_{a_1}, \\dots, x_{a_1 + \\cdots + a_{n-1} +1}, \\dots, x_{a_1 + \\cdots + a_{n}})\\\\ = f(g_1(x_1, \\dots x_{a_1}), \\dots, g_n(x_{a_1 + \\cdots + a_n + 1}, \\dots, x_{a_1 + \\cdots + a_n})) \\end{gather*}\\] <p>However, what if we decided to build this function another way; perhaps, handling  one \\(g_i\\) at a time? The way we could do this is by inserting a \\(g_i\\) one at a time:</p> \\[ (f, g_i) \\mapsto f(\\underbrace{x_1, \\dots, x_{k-1}}_{k-1}, \\overbrace{g_i(x'_1, \\dots, x'_{a_i})}^{k\\text{-th spot}}, \\underbrace{x_{k+1}, \\dots, x_n}_{n-(k+1)})   \\] <p>Given that we'd have a total of \\((n + a_i  -1)\\)-many inputs, this then defines a composition  operator </p> \\[ \\circ_k: X^n \\times X^{a_i} \\to X^{n + a_i - 1} \\] <p>for each \\(n, a_i \\ge 0\\). We can then repeatedly apply this composition operator to build  the same function that our operadic composition does. </p> <p> Let \\(X\\) be an operad in a symmetric monoidal category \\(\\cc\\). Then  for each \\(n, m \\ge 0\\), we define the partial composition operator \\(\\circ_k: X_n \\otimes X_m \\to X_{n + m - 1}\\)  as the composition of the arrows pictured below.  <p> In other words, the partial composition operator \\(\\circ_k\\) on \\(X_m \\otimes X_n\\) is the same as our original composition operator \\(\\mu\\) applied to \\(X_m \\otimes X_1 \\otimes \\cdots \\otimes X_n \\otimes \\cdots \\otimes X_1\\). </p> <p>It was Fresse who demonstrated in his gigantic text that the partial composition operator  can equivalently construct operads. The strategy he used is as follows: we first investigate what  properties (i.e. diagrams) that the partial composition operator satisfies. Then, we forget that we ever had  on operad, but we rather consider a sequence of objects which are basically operads, but whose  composition operator has now been replaced by the partial composition operator. Fresse showed that these  objects then form a category, and that this category is isomorphic to the category of operads, thereby  demonstrating an equivalence of operad definitions and paving the way for simpler calculations in demonstrating  that something is an operad. </p> <p>Thus we demonstrate properties of the partial composition operator. Let  \\(X\\) be an operad and recall the associativity pentagon given in OP1.  In the associativity diagram,  replace \\(X_{a_i} = X_1\\) except \\(X_{a_p} = X_r\\) for some \\(p \\le n\\), and set \\(X_{k_{i,j}} = X_1\\)  except for \\(X_{k_{p,q}} = X_s\\) for some \\(q \\le a_p\\). Then we get  the commutative diagram below. </p> <p>\\  ith similar substitutions, we also get that the diagram below commutes.  \\ </p>"},{"location":"category_theory/Operads/The%20Braid%20Groups%20Form%20a%20%28nonsymmetric%29%20Operad/","title":"9.4. The Braid Groups Form a (nonsymmetric) Operad","text":"<p>Recall that the \\(n\\)-th braid group \\(B_n\\) is the collection  of all possible braidings of \\(n\\)-strands, forming a group under composition.  Each braid group has the presentation </p> \\[ B_n = \\left&lt; \\sigma_1, \\dots, \\sigma_{n-1} \\mid \\sigma_i\\sigma_{i+1}\\sigma_{i} = \\sigma_{i+1}\\sigma_{i}\\sigma_{i+1}^{(\\texttt{1})}, \\sigma_i\\sigma_j = \\sigma_j\\sigma_i^{(\\texttt{2})} \\right&gt;    \\] <p>where (\\texttt{1}) holds only when \\(1 \\le i \\le n - 2\\) and (\\texttt{2}) hold only when  \\(|i - j| &gt; 1\\). Below is the braid \\(\\sigma_1 \\sigma_3 \\sigma_2 \\sigma_2 \\sigma_3\\),  where we envision application of the generators starting from the left and going to the right.</p> <p></p> <p>Each braid group has a natural projection mapping \\(\\pi: B_n \\to S_n\\), where  each braid is sent to the underlying permutation. The kernel of this map is the  pure braid group, which doesn't change the permutation. However, recall that \\(S_n\\)  is a symmetric operad, whose composition is given by a block permutation.  That is, given a permutation \\(S_n\\), and \\(n\\)-many other permutations \\(\\sigma_1 \\in S_{a_1}, \\dots,  \\sigma_n \\in S_{a_n}\\), we can form a permutation in \\(S_{a_1 + \\cdots + a_n}\\). \\ </p> <p>This then suggests the idea that there exists an operadic composition for braids; and such  an observation checks out. Given a braid \\(\\beta \\in B_n\\), and \\(n\\)-many other  braids \\(\\alpha_1 \\in B_{a_i}, \\dots, \\alpha_n \\in B_{a_n}\\), we can form a  braid in \\(B_{a_1 + \\cdots + a_n}\\). The operadic composition is analogous to what  we had before with permutations; we're going to stick braids inside of braids. </p> <p> (Topological.) Let \\(\\beta \\in B_n\\) be a braid. We say that the \\(i, (i+1), \\dots (i + k)\\)-th strands form a cable if there exist a cylinder (depends on ambient space; need to decide  one for consistency) which is disjoint from all other strands of \\(\\beta\\). </p> <p> Every cable is obtained from a map \\(\\circ_k: B_n \\times B_m \\to B_{m + n -1}\\).  </p> <p>In general, we can define an \"operadic\" composition where the composition is the cabling of \\(n\\)-braids.</p> \\[ \\circ_{a_1, \\dots, a_n}: B_n \\times B_{a_1} \\times \\cdots \\times B_{a_n} \\to B_{a_1 + \\cdots + a_n} \\] <p>We'll want to show that this does form an operad. But before we do that we'll need to  obtain an algebraic expression, based on the generators of the braids being cabled, which describe the resultant braid. </p> <p>Towards that goal, consider the generator \\(\\sigma_1\\), which simply swaps the first strand over  the second. Suppose we would like to substitute 4 parallel strands in the first strand  of \\(\\sigma_1\\), and just one strand in the second strand of \\(\\sigma_1\\).  How do we calculate this braid?</p> <p>\\  The blue line travels diagonally down, going underneath each red strand once. The blue line crossing underneath the \\(i\\)-th red strand can be represented as  \\(\\sigma_{i}\\). We then multiply all of these together to get the braid.</p> <p>\\  ence we see that the braid is simply \\(\\sigma_4\\sigma_3\\sigma_2\\sigma_1\\). </p> <p>Suppose now that we would like to substitute 2 parallel strands into  the first strand of \\(\\sigma_1\\), and also substitute 3 parallel strands  in the second strand of \\(\\sigma_2\\). Then this produces a braid of 5 strands.  \\  How do we calculate this braid? Observe that the \\(\\textcolor{Red}{i}\\)-th red strand  crossing over the \\(\\textcolor{RoyalBlue}{j}\\)-th strand can be represented  as \\(\\sigma_{i + j - 1}\\). In the previous situation, \\(j\\) was equal to 1, so it  each crossing was just \\(\\sigma_{i}\\).</p> <p>\\  verall, we can simply see that the braid is given by </p> \\[ (\\sigma_2\\sigma_1)(\\sigma_3\\sigma_2)(\\sigma_4\\sigma_3).   \\] <p>Now suppose more generally that we have \\(k_1\\)-many red lines and \\(k_2\\)-many blue lines. Then we can iteratively describe their  crossings one line at time, just like we did above.  The crossings will look somewhat like this:</p> <p>To describe this braid, we note that there will be \\(k_1\\cdot k_2\\)-many crossings,  and hence \\(k_1\\cdot k_2\\)-many generators required to describe  the crossings. If we follow the first blue line, and track  each time it crosses with the red lines, we see that their  crossings will be \\(\\sigma_{k_1}, \\sigma_{k_1-1}, \\dots,  \\sigma_{1}\\). Moving onto the second blue and again traveling down, the crossings  will be \\(\\sigma_{k_1+1}, \\sigma_{k_1}, \\dots,  \\sigma_{2}\\). If we have \\(k_2\\)-many blue lines, this will be done \\(k_2\\) many times.</p> <p>\\  Hence we have that  \\begin{equation} \\sigma_1(k_1, k_2) = \\prod_{m = 1}^{k_2}\\sigma_{(k_1 + \\textcolor{NavyBlue}{m}-1)}\\sigma_{(k_1 + \\textcolor{NavyBlue}{m}-2)}\\cdots\\sigma_{\\textcolor{NavyBlue}{m}} \\end{equation} where starting from \\(m= 1, 2, \\dots, k_2\\) represents us following the \\(m\\)-th blue line and recording  its crossings with the red lines.</p> <p>We get a similar story if we instead consider \\(\\sigma_1^{-1}(k_1, k_2)\\). Here,  we are swapping \\(k_1\\) many strands under \\(k_2\\) many strands,  so, we have to  swap \\(k_1\\) and \\(k_2\\). This then gives us the expression </p> \\[ \\sigma_1^{-1}(k_1, k_2) = \\prod_{m = 1}^{k_1} \\sigma_{(k_1 - \\textcolor{NavyBlue}{m})+1}^{-1} \\sigma_{(k_1 - \\textcolor{NavyBlue}{m})+2}^{-1} \\cdots  \\sigma_{(k_1 - \\textcolor{NavyBlue}{m})+ k_2}^{-1} \\] <p>Now it is easily to generalize this to the other generators; we simply  \\textcolor{NavyBlue}{shift the indices}.</p> \\[\\begin{align*} \\sigma_i(k_1, k_2) &amp;= \\prod_{m = 1}^{k_2}\\sigma_{(k_1 + m-1 + \\textcolor{NavyBlue}{(i-1)} )}\\sigma_{(k_1 + m-2) + \\textcolor{NavyBlue}{(i-1)} }\\cdots\\sigma_{m + \\textcolor{NavyBlue}{(i-1)}} \\\\ &amp;= \\prod_{m' = i}^{(i-1) + k_2}\\sigma_{(k_1 + m'-1)}\\sigma_{(k_1 + m'-2)}\\cdots\\sigma_{m'} \\end{align*}\\] <p>where we set \\(m' = m + (i - 1)\\) to reindex. Note that this returns the original formula  we had once we set \\(i = 1\\).</p> <p>Thus we have that: </p> <p> Let \\(\\sigma_i\\) be a generator. Then the braid obtained by cabling  \\(k_1\\)-many parallel lines into the \\(i\\)-th strand and \\(k_2\\)-many parallel lines  into the \\((i+1)\\)-th strand returns a braid in \\(B_{k_1 + k_2}\\) which may  be expressed as  \\[\\begin{align*} \\sigma_i(k_1, k_2) &amp;= \\prod_{m' = i}^{(i-1) + k_2}\\sigma_{(k_1 + m'-1)}\\sigma_{(k_1 + m'-2)}\\cdots\\sigma_{m'} \\end{align*}\\] <p></p> <p>Now we move onto the more difficult question: suppose we have a general braiding \\(\\beta\\) of \\(n\\) strands,  and suppose we have \\(k_1, \\dots, k_n\\) sets of parallel strands. Suppose that we'd  like to substitute \\(k_1\\)-parallel strands in the first strand of \\(\\beta\\),  \\(k_2\\)-parallel strands in the second, all the way to \\(k_n\\) strands in the \\(n\\)-th strand. This then defines a braid of \\((k_1 + \\cdots + k_n)\\)-many strands which we denote as </p> \\[ \\beta(k_1, k_2, \\dots, k_n). \\] <p>For example, if \\(\\beta = \\sigma_1\\sigma_3\\sigma_2\\sigma_2\\), then we have \\(\\beta\\) below on the bottom left. On the bottom right,  we have \\(\\beta(k_1, k_2, k_3, k_4)\\) where \\(k_1 = 3, k_2 = 2, k_3 = 1, k_4 = 3\\).</p> <p>\\  taring at the diagram, we can see that it may be expressed as</p> \\[\\begin{gather*} (\\sigma_3\\sigma_2\\sigma_1 \\cdot \\sigma_4\\sigma_3\\sigma_2) (\\sigma_6\\sigma_7\\sigma_8)  (\\sigma_5\\sigma_4\\sigma_3 \\cdot \\sigma_6\\sigma_5\\sigma_4 \\cdot \\sigma_7\\sigma_6\\sigma_5)\\\\ (\\sigma_5\\sigma_4\\sigma_3 \\cdot \\sigma_6\\sigma_5\\sigma_4 \\cdot \\sigma_7\\sigma_6\\sigma_5) (\\sigma_8 \\sigma_7 \\sigma_6). \\end{gather*}\\] <p>But how can we do this in general? To explain, first suppose </p> \\[ \\beta = \\sigma_{i_1}\\sigma_{i_2}\\cdots\\sigma_{i_k}. \\] <p>To draw the cabled braid \\(\\beta(k_1, k_2, \\dots, k_n)\\), we see that we have \\(k\\)-crossings to focus on;  these are where the crossings will happen in our cabled braid. For example, in the braid we provided above, we  can highlight the crossings in yellow. \\  t each crossing, we're going to have something like this:  \\  That is, at each crossing, there will be a number of red strands crossing over  blue strands. If we can just describe each of these  crossings using generators \\(\\sigma_j\\) like we did before,  then we can describe the whole braid. </p> <p>We now face the main problem. To describe an arbitrary crossing, we need to know  which generators \\(\\sigma_1, \\sigma_2, \\dots, \\sigma_{k_1 + \\cdots + k_n}\\)  to use, and in general it's not clear which ones to use. For example,  how do we describe the first crossing? We don't know, so we'll write \\(\\sigma_{??}\\).  If, however, we know that the first red strand is, say the \\(k\\)-th strand in \\(\\beta(k_1, \\dots, k_n)\\), then we can write the crossing as \\(\\sigma_k\\). Then we can travel down the blue line,  writing \\(\\sigma_{k-1}, \\sigma_{k - 2}, \\dots\\) until we've hit all the red strands. Then  we can repeat this process for each blue line.</p> <p>So to do this in general, we need to answer three questions:</p> <ul> <li> <p>How far are all of our red strands from the left? </p> </li> <li> <p>How many red strands are there?</p> </li> <li> <p>How many blue strands are there?</p> </li> </ul> <p>If we can answer those three questions, then we can describe exactly what happens  in terms of generators using formula (\\ref{sigma_1_cabling}).</p> <p>We answer the first question:</p> <p> Let \\(\\beta \\in B_n\\) be a braid. Suppose \\(\\beta\\) can be written  as a product of \\(k\\)-many generators \\(\\beta = \\sigma_{i_1}\\sigma_{i_2} \\cdots \\sigma_{i_k}\\) (where any \\(\\sigma\\) is equally possibly an inverse).  Then we define the quantity  \\[ \\phi(\\sigma_{i_1}\\sigma_{i_2}\\dots, \\sigma_{i_j}, s) =  \\begin{cases} \\text{The order which strand }s\\\\ \\text{is from the left after generators}\\\\ \\sigma_{i_1}\\sigma_{i_2}\\dots, \\sigma_{i_j} \\text{ have been applied.} \\end{cases} \\] <p>Of course, \\(\\phi(-, s) = s\\), where \\(-\\) represent empty input, for each strand \\(s\\). This is because each \\(s\\)-th strand is originally the  \\(s\\)-th strand.</p> <p>However, a way to define this is to calculate the underlying permutation  of \\(\\sigma_i^{1}\\sigma_j^{2}\\dots, \\sigma_k^{p}\\) using the natural projection  map \\(\\pi: B_n \\to S_n\\). Hence we see that </p> \\[ \\phi(\\sigma_{i_1}\\sigma_{i_2}\\dots, \\sigma_{i_k}, s) =  \\pi(\\sigma_{i_1}\\sigma_{i_2}\\dots, \\sigma_{i_k})(s). \\] <p></p> <p> Consider the braid \\(\\sigma_1\\sigma_3\\sigma_2\\sigma_2\\sigma_3\\) pictured below. Suppose we've applied \\(\\sigma_1\\sigma_3\\). Then our braids are now reordered from how  they were initially positioned. For instance, after the application of these  generators, the green strand is now the first strand; the red strand is now  the second; the blue strand is the third; and the black strand is now the fourth.  Each color strand is now in a different position than which it started in. \\  However, we can express this observation using our tool. Note that \\(\\pi(\\sigma_1\\sigma_3)\\) is the permutation \\((1, 2, 3, 4) \\mapsto  (2, 1, 4, 3)\\). Hence we see that  \\[ \\phi(\\sigma_1\\sigma_3, 1) = \\textcolor{Green}{2} \\quad \\phi(\\sigma_1\\sigma_3, 2) = \\textcolor{Red}{1} \\quad \\phi(\\sigma_1\\sigma_3, 3) = \\textcolor{RoyalBlue}{4} \\quad \\phi(\\sigma_1\\sigma_3, 4) = \\textcolor{Purple}{3}. \\] <p>What about after the first three generators have been applied? We calculate  again: \\(\\pi(\\sigma_1\\sigma_3\\sigma_2)\\) is the permutation \\((1, 2, 3, 4) \\mapsto (2, 4, 1, 3)\\). Hence we have that </p> \\[ \\phi(\\sigma_1\\sigma_3\\sigma_2, 1) = \\textcolor{Green}{2} \\quad \\phi(\\sigma_1\\sigma_3\\sigma_2, 2) = \\textcolor{NavyBlue}{4} \\quad \\phi(\\sigma_1\\sigma_3\\sigma_2, 3) = \\textcolor{red}{1} \\quad \\phi(\\sigma_1\\sigma_3\\sigma_2, 4) = \\textcolor{Purple}{3}. \\] <p>which matches a simple hand-count that we can perform using the picture below. \\  end{example}</p> <p>This tool allows us to answer our second and third questions. For example, consider again  \\(\\beta(3, 2, 1, 3)\\) where \\(\\beta = \\sigma_1\\sigma_3\\sigma_2\\sigma_2\\sigma_3\\).  How do we calculate,  for example, the crossing \\raisebox{-0.1cm}{\\(\\begin{tikzpicture}\\draw (0,0.2) circle(0.2cm) node {5};\\end{tikzpicture}\\)}, of 3 blue lines over 1 black line, as in the picture below?</p> <p>\\  his crossing is induced by \\(\\sigma_3\\), the fifth generator of \\(\\beta\\).  Hence \\(\\beta\\) tells us to cross the \\(3\\)nd cable over the \\(4\\)rd cable. But what  are these cables? From looking at the diagram, we definitely know. But in general we won't  be able to just look at the diagram. However, our tool can tell us: Since we've applied \\(\\sigma_1\\sigma_3\\sigma_2\\sigma_2\\),  we see that </p> \\[ \\phi(\\sigma_1\\sigma_3\\sigma_2\\sigma_2, 3) = \\textcolor{RoyalBlue}{4} \\qquad \\phi(\\sigma_1\\sigma_3\\sigma_2\\sigma_2, 4) = \\textcolor{Black}{1}. \\] <p>Therefore, we're crossing blue cables over the black cables. We also now know there are \\(k_{\\textcolor{RoyalBlue}{4}} = 3\\) blue cables and \\(k_{\\textcolor{Black}{3}} = 1\\) many black cables. We have almost everything we need except the following: how far  are the blue cables from the left of the diagram? </p> <p>Well, since the blue strands are inside of the third cable, we just need to ask how many  stands are in the first and second cables. But what is the first cable? What's the second? We see that</p> \\[ \\phi(\\sigma_1\\sigma_3\\sigma_2\\sigma_2, 1) =  \\textcolor{Green}{2}. \\quad  \\phi(\\sigma_1\\sigma_3\\sigma_2\\sigma_2, 2) = \\textcolor{Red}{1}. \\] <p>Hence there are </p> \\[ k_{\\textcolor{Green}{2}} + k_{\\textcolor{Red}{1}} = 2 + 3 = 5     \\] <p>strands before the blue strands. We can now calculate the crossings: </p> \\[\\begin{align*} \\sigma_{5 + 3}\\sigma_{5 + 2} \\sigma_{5 + 1} &amp;= \\prod_{m = 1 + 5}^{1 + 5}\\sigma_{3 + (m - 1)}\\sigma_{3 + (m - 2)}\\sigma_m\\\\ &amp;= \\prod_{m = p}^{p + (r - 1)}\\sigma_{q + (m - 1)}\\sigma_{q + (m - 2)}\\sigma_m \\end{align*}\\] <p>where </p> \\[ p = 1 + \\hspace{-1.5cm}\\overbrace{k_2 + k_3}^{\\text{\\# of strands before the red strands}} \\quad  q = \\hspace{-1cm}\\underbrace{k_{\\textcolor{RoyalBlue}{4}}}_{\\text{\\# of strands in the 3rd cable}} r = \\hspace{-1cm}\\overbrace{k_{\\textcolor{Black}{3}}}^{\\text{\\# of strands in the 4th cable}} \\] <p>Therefore we propose the following. </p> <p> Let \\(\\beta \\in B_n\\) be a braid, and suppose it  may be expressed as \\(\\sigma_{i_1}\\sigma_{i_2} \\cdots \\sigma_{i_k}\\) in terms of \\(k\\)-many  generators. Let \\(k_1, \\dots, k_n\\) be positive integers. Then we have that  \\[ \\beta(k_1, k_2, \\dots, k_n)  =  \\psi_1\\psi_2\\dots\\psi_k \\] <p>where, depending on if \\(\\sigma_{i_j}\\) is an instance of an inverse or not,  we have  \\begin{statement}{ProcessBlue!10}</p> \\[ \\prod_{m = p_j}^{p_j + (r_j-1)} \\sigma_{q_j + (m-1)} \\sigma_{q_j + (m-2)} \\cdots  \\sigma_{m} \\quad  \\text{ or } \\quad  \\prod_{m = p_j}^{p_j + (r_j-1)} \\sigma^{-1}_{(q_j + m) - 1} \\sigma^{-1}_{(q_j -m) - 2} \\cdots  \\sigma^{-1}_{m} \\] <p>\\end{statement} where in both cases \\begin{statement}{ProcessBlue!10}</p> \\[ p_j = \\overbrace{ 1 + \\sum_{u = 1}^{i_j-1} k_{\\phi(\\sigma_{i_1}\\cdots\\sigma_{i_{j-1}}, u)} }^{\\text{\\# strands before } i_j\\text{-th cable} } \\qquad \\underbrace{ q_j =  k_{\\phi(\\sigma_{i_1}\\cdots\\sigma_{i_{j-1}}, i_j)} }_{\\text{\\# of strands in the }i_j \\text{-th cable}} \\qquad  \\overbrace{ r_j = k_{\\phi(\\sigma_{i_1}\\cdots\\sigma_{i_{(j-1)}}, (i_j+1))} }^{\\text{\\# of stands in the } (i_j+1)\\text{-th cable} } \\] <p>\\end{statement} </p> <p>The three quantities are the three answers to our original questions: </p> <ul> <li> <p>After applying \\(\\sigma_{i_1}\\dots\\sigma_{i_{j-1}}\\), how many strands come before the cable \\(i_j\\), relative to the left? \\(p_j\\).</p> </li> <li> <p>How many strands are in the \\(i_j\\)-th cable after applying \\(\\sigma_{i_1}\\dots\\sigma_{i_{j-1}}\\)?  \\(q_j\\).</p> </li> <li> <p>How many strands are in the \\((i_j+1)\\)-th after applying \\(\\sigma_{i_1}\\dots\\sigma_{i_{j-1}}\\)?  \\(r_j\\).</p> </li> </ul> <p>\\begin{example} We can apply this to our previous example.  Recall that \\(\\beta = \\sigma_1\\sigma_3\\sigma_2\\sigma_2\\sigma_3\\).  One way to interpret out braid diagram is as a sequence of permutations. In this case we see that we get five permutations because we have five generators. \\  First we compute the table  \\begin{center}</p> \\(j\\) \\(i_{j}\\) \\(p_j\\) \\(q_j\\) \\(r_j\\) [0.5ex]                           1 1 1 k_1 = 3$ \\(k_2 = 2\\) 2 3 \\(1 + k_1 + k_2 = 6\\) \\(k_3 = 1\\) \\(k_4 = 3\\) 3 2 \\(1 + k_2 = 3\\) \\(k_1 = 3\\) \\(k_4 = 3\\) 4 2 \\(1 + k_2 = 3\\) \\(k_4 = 3\\) \\(k_1 = 3\\) 5 3 \\(1 + k_1 + k_2 = 6\\) \\(k_4 = 3\\) \\(k_3 = 1\\) <p>\\end{center}   This then gives us the product </p> \\[\\begin{gather*} \\left(\\prod_{m = p_1}^{p_1 + (r_1-1)} \\sigma_{q_1 + (m-1)} \\sigma_{q_1 + (m-2)} \\cdots  \\sigma_{m}\\right) \\left(\\prod_{m = p_2}^{p_2 + (r_2-1)} \\sigma_{q_2 + (m-1)} \\sigma_{q_2 + (m-2)} \\cdots  \\sigma_{m} \\right) \\\\ \\left(     \\prod_{m = p_3}^{p_3 + (r_3-1)} \\sigma_{q_3 + (m-1)} \\sigma_{q_3 + (m-2)} \\cdots  \\sigma_{m} \\right) \\\\ \\left(    \\prod_{m = p_4}^{p_4 + (r_4-1)} \\sigma_{q_4 + (m-1)} \\sigma_{q_4 + (m-2)} \\cdots  \\sigma_{m} \\right) \\left(     \\prod_{m = p_5}^{p_5 + (r_5-1)} \\sigma_{q_5 + (m-1)} \\sigma_{q_5 + (m-2)} \\cdots  \\sigma_{m} \\right) \\end{gather*}\\] <p>which becomes </p> \\[\\begin{gather*} \\left(\\prod_{m = 1}^{1 + (2-1)} \\sigma_{3 + (m-1)} \\sigma_{3 + (m-2)} \\cdots  \\sigma_{m}\\right) \\left(\\prod_{m = 6}^{6 + (3-1)} \\sigma_{1 + (m-1)} \\sigma_{1 + (m-2)} \\cdots  \\sigma_{m} \\right) \\\\ \\left(     \\prod_{m = 3}^{3 + (3-1)} \\sigma_{3 + (m-1)} \\sigma_{3 + (m-2)} \\cdots  \\sigma_{m} \\right) \\\\ \\left(    \\prod_{m = 3}^{3 + (3-1)} \\sigma_{3 + (m-1)} \\sigma_{3 + (m-2)} \\cdots  \\sigma_{m} \\right) \\left(     \\prod_{m = 6}^{6 + (1-1)} \\sigma_{3 + (m-1)} \\sigma_{3 + (m-2)} \\cdots  \\sigma_{m} \\right) \\end{gather*}\\] <p>which reduces to </p> \\[\\begin{gather*} (\\sigma_3\\sigma_2\\sigma_1 \\cdot \\sigma_4\\sigma_3\\sigma_2) (\\sigma_6 \\sigma_7\\sigma_8)(\\sigma_5\\sigma_4\\sigma_3\\cdot \\sigma_6\\sigma_5\\sigma_4 \\cdot \\sigma_7\\sigma_6\\sigma_5) \\\\ (\\sigma_5\\sigma_4\\sigma_3\\cdot \\sigma_6\\sigma_5\\sigma_4 \\cdot \\sigma_7\\sigma_6\\sigma_5) (\\sigma_8\\sigma_7\\sigma_6) \\end{gather*}\\] <p>which correctly matches what we had before.  </p> <p> We haven't looked at a braid with an under crossing. So,  consider the braid \\(\\beta = \\sigma_1^{-1}\\sigma_2^{-1}\\sigma_3\\sigma_2\\sigma_1 \\in B_4\\),  and let \\(k_1 = 2, k_2 = 3, k_3 = 4, k_4 = 5\\). We'll want to calculate  the braid \\(\\beta(2, 3, 4, 5)\\). Below is \\(\\beta\\) and \\(\\beta(2,3,4,5)\\). \\  To calculate the resulting braid we need to create our table of values. This is more easily done by generating the permutation table on the left; it  tells us how our cables are swapped around.  \\begin{center} Generator Permutation \\(\\varnothing\\) \\((\\textcolor{Red}{1}, \\textcolor{Green}{2}, \\textcolor{Purple}{3}, \\textcolor{RoyalBlue}{4})\\) \\(\\sigma_1^{-1}\\) \\((\\textcolor{Green}{2}, \\textcolor{Red}{1}, \\textcolor{Purple}{3}, \\textcolor{RoyalBlue}{4})\\) \\(\\sigma_1^{-1}\\sigma_2^{-1}\\) \\((\\textcolor{Green}{2}, \\textcolor{Purple}{3}, 1, \\textcolor{RoyalBlue}{4})\\) \\(\\sigma_1^{-1}\\sigma_2^{-1}\\sigma_3\\) \\((\\textcolor{Green}{2}, \\textcolor{Purple}{3}, \\textcolor{RoyalBlue}{4}, 1)\\) \\(\\sigma_1^{-1}\\sigma_2^{-1}\\sigma_3\\sigma_2\\) \\((\\textcolor{Green}{2}, \\textcolor{RoyalBlue}{4}, \\textcolor{Purple}{3}, 1)\\) \\(\\sigma_1^{-1}\\sigma_2^{-1}\\sigma_3\\sigma_2\\sigma_1\\) \\((\\textcolor{RoyalBlue}{4}, \\textcolor{Green}{2}, \\textcolor{Purple}{3}, 1)\\) <p>\\hspace{1cm}</p> \\(j\\) \\(i_{j}\\) \\(p_j\\) \\(q_j\\) \\(r_j\\) [1ex]                           1 1 1 \\(k_1 = 2\\) \\(k_2 = 3\\) [.1ex]                           2 2 \\(1 + k_2 = 4\\) \\(k_1 =2\\) \\(k_3 = 4\\) [.1ex]                           3 3 \\(1 + k_2 + k_3 = 8\\) \\(k_1 = 2\\) \\(k_4 = 5\\) [.1ex]                           4 2 \\(1 + k_2 = 4\\) \\(k_3 =4\\) \\(k_4 = 5\\) .1ex]                           5 1 \\(1\\) \\(k_2 = 3\\) \\(k_4 = 5\\) <p>\\end{center}   This then generates the products </p> \\[\\begin{gather*} \\left(\\prod_{m = 1}^{3}\\sigma_{m + 2}^{-1}\\sigma^{-1}_m\\right) \\left( \\prod_{m = 4}^{7}\\sigma^{-1}_{(m+2)-1}\\sigma_m^{-1} \\right) \\left( \\prod_{m = 8}^{12}\\sigma_{(m+2)-1}\\sigma_m \\right) \\left( \\prod_{m = 4}^{8}\\sigma_{(m+4)-1}\\sigma_{(m+4)-2}\\sigma_{(m+4)-3}\\sigma_{m} \\right) \\\\ \\left( \\prod_{m = 1}^{5}\\sigma_{(m+3)-1}\\sigma_{(m+3)-2}\\sigma_m \\right) \\end{gather*}\\] <p>which becomes </p> \\[\\begin{gather*} (\\sigma^{-1}_2\\sigma^{-1}_1 \\cdot \\sigma^{-1}_3\\sigma^{-1}_2 \\cdot \\sigma^{-1}_4\\sigma^{-1}_3) (\\sigma^{-1}_5\\sigma^{-1}_4\\cdot \\sigma^{-1}_6\\sigma^{-1}_5 \\cdot \\sigma^{-1}_7\\sigma^{-1}_6 \\cdot \\sigma^{-1}_8\\sigma^{-1}_7) \\\\ (\\sigma_9\\sigma_8 \\cdot \\sigma_{10}\\sigma_9 \\cdot \\sigma_{11}\\sigma_{10} \\cdot \\sigma_{12}\\sigma_{11} \\cdot \\sigma_{13}\\sigma_{12}) (\\sigma_7\\sigma_6\\sigma_5\\sigma_4 \\cdot \\sigma_8\\sigma_7\\sigma_6\\sigma_5 \\cdot \\sigma_9\\sigma_8\\sigma_7\\sigma_6 \\cdot  \\sigma_{10}\\sigma_9\\sigma_8\\sigma_7\\cdot \\sigma_{11}\\sigma_{10}\\sigma_9\\sigma_8) \\\\ (\\sigma_3\\sigma_2\\sigma_1 \\cdot \\sigma_4\\sigma_3\\sigma_2 \\cdot \\sigma_5\\sigma_4\\sigma_3 \\cdot \\sigma_6\\sigma_5\\sigma_4 \\cdot \\sigma_7\\sigma_6\\sigma_5) \\end{gather*}\\] <p>which is the correct description of the braid \\(\\beta(2,3,4,5)\\).  </p> <p>Now we can finally answer our desired question:  \\begin{center} Given a braid \\(\\beta \\in B_n\\), and \\(n\\) other braids \\(\\alpha_1 \\in B_{a_1}, \\dots, \\alpha_n \\in B_{a_n}\\),  what is the formula for \\(\\beta(\\alpha_1, \\dots, \\alpha_n)\\)?  \\end{center}To answer this question, we build on our previous work by making the following observation.  Suppose we want to compute \\(\\sigma_1(\\alpha_1, \\alpha_2)\\) where \\(\\sigma_1, \\alpha_1, \\alpha_2\\) appear as below. \\  Then we get the braid diagram as in \\raisebox{-0.1cm}{\\(\\begin{tikzpicture}\\draw (0,0.2) circle(0.2cm) node {1};\\end{tikzpicture}\\)}. \\begin{center} \\includegraphics[scale = 0.1]{braids_cabled.jpg} \\end{center}owever, we can all isotopies to stretch the braid to \\raisebox{-0.1cm}{\\(\\begin{tikzpicture}\\draw (0,0.2) circle(0.2cm) node {2};\\end{tikzpicture}\\)},  then \\raisebox{-0.1cm}{\\(\\begin{tikzpicture}\\draw (0,0.2) circle(0.2cm) node {3};\\end{tikzpicture}\\)},  and then reaching a final stage of \\raisebox{-0.1cm}{\\(\\begin{tikzpicture}\\draw (0,0.2) circle(0.2cm) node {4};\\end{tikzpicture}\\)}.  But note that \\raisebox{-0.1cm}{\\(\\begin{tikzpicture}\\draw (0,0.2) circle(0.2cm) node {4};\\end{tikzpicture}\\)} may be expressed in either of the equivalent ways: \\</p> <p>\\begin{minipage}{0.5\\textwidth} \\  \\  end{minipage} \\begin{minipage}{0.5\\textwidth} \\  \\  end{minipage} \\vspace{1cm}</p> <p>This then gives us the following idea. Suppose we want to calculate  \\(\\beta(\\alpha_1, \\dots, \\alpha_n)\\) where \\(\\alpha_i \\in B_{a_i}\\). Define \\(\\alpha_1\\oplus \\dots \\oplus \\alpha_n\\) as the \\((a_1 + \\cdots + a_n)\\)-braid.  Suppose that \\(\\alpha_j = \\sigma_{j, i_j}, \\dots, \\sigma_{j, i_{k_j}}\\). Then </p> \\[\\begin{gather*} \\alpha_1 \\oplus \\alpha_2 \\oplus \\cdots \\oplus  = (\\sigma_{1, i_1}\\sigma_{1, i_2}, \\dots, \\sigma_{1, i_{k_1}}) (\\sigma_{2, (i_1+a_1)}\\sigma_{2, (i_2+a_2)}, \\dots, \\sigma_{2, (i_{k_1} + a_1) })\\\\ \\cdots  (\\sigma_{n, (i_1+a_1 + \\cdots + a_{n-1})}\\sigma_{2, (i_2+a_2)}, \\dots, \\sigma_{n, (i_{k_1} + a_1 + \\cdots + a_{n-1}) }) \\end{gather*}\\] <p>which concatenates the braid horizontally. Then we see that </p> \\[\\begin{align*} \\beta(\\alpha_1, \\dots, \\alpha_n) = \\beta(a_1, a_2, \\dots, a_n) \\circ \\alpha_1 \\oplus \\alpha_2 \\oplus \\dots \\oplus \\alpha_n. \\end{align*}\\]"},{"location":"category_theory/Persistence%20Modules/General%20Persistence%20Diagrams/","title":"11.4. General Persistence Diagrams","text":"<p>Persistence diagrams (and barcodes) give a visual representation of  how a filtration of a topological space (usually a simplicial complex) evolves. It keeps track of homological dimensions which are \"born\" and \"killed\"  throughout this evolution. </p> <p>Let \\(X\\) be a topological space. We know from algebraic topology that  there exists a \\(n\\)-th singular homology group </p> \\[ H_n(X). \\] <p>Suppose that  \\(f: X \\to \\mathbb{R}\\) is a real-valued function. An example of this is the  height function of a sphere centered at the origin.  Now one thing we can do with these types of functions is take any \\(a \\in \\mathbb{R}\\) and consider</p> \\[ f^{-1}((\\infty,a]) \\subset X. \\] <p>The space \\(f^{-1}((\\infty,a]) \\subset X\\) is a topological space induced by the  subspace topology of \\(X\\). In general, this process can be modeled functorially.  Let \\(\\mathbb{R}\\) be a category with morphisms given by poset structure. Then</p> \\[\\begin{align*} &amp;E: \\mathbb{R} \\to **Top**\\\\ &amp;a \\longmapsto f^{-1}((\\infty, a]) \\end{align*}\\] <p>since if \\(a \\le b\\) then this induces a continuous function </p> \\[ i: f^{-1}((\\infty, a]) \\to f^{-1}((\\infty, b]) \\] <p>namely, the inclusion function. \\textcolor{NavyBlue}{We denote the functor as \\(E\\) for \"evolution,\" as this functor  filters the space \\(X\\). As we send \\(a\\) to infinity, we ultimately obtain the entire  topological space.}</p> <p>Switching focus, consider the homology group of this subspace</p> \\[ H_n(f^{-1}((\\infty, a])). \\] <p>We can also outline this behavior as functorial where we send </p> \\[\\begin{align*} &amp;H:  **Top** \\to **Ab** \\\\ &amp;f^{-1}((\\infty, a]) \\longmapsto H(f^{-1}((\\infty, a])) \\end{align*}\\] <p>since for any \\(a \\le b\\), we have a group homomorphism which we denote as \\(\\phi_a^b\\):</p> \\[ \\phi_a^b: H(f^{-1}(\\infty, a]) \\to H(f^{-1}(\\infty, b]). \\] <p>Now we can outline this overall data pipeline as a functor \\(H \\circ E: \\mathbb{R} \\to **Ab**\\)</p> \\[\\begin{align*} &amp;H \\circ E: \\mathbb{R} \\to **Top** \\to **Ab** \\\\ &amp; a \\longmapsto f^{-1}((\\infty, a]) \\longmapsto H(f^{-1}((\\infty, a])). \\end{align*}\\] <p>What's really happening here? First, \\(E\\) records the evolution of the topological  space under \\(f: X \\to \\mathbb{R}\\). Then \\(H\\) records the homology groups; overall, \\(H \\circ E\\)  records the topological evolution! We are thus interested in the following objects.</p> <p> Let \\(a \\le b\\). Recall that  \\[ H\\circ E(a \\le b) = \\phi_a^b.    \\] <p>Since we are interested in the image of these mappings,  which will be a group, we denote</p> \\[ F([a, b]) = \\im(\\phi_a^b) = \\im\\Big( H(f^{-1}((\\infty, a ])) \\to H(f^{-1}((\\infty, b])) \\Big) \\] <p>to be a persistence homology group from \\(a\\) to \\(b\\).  </p> <p> For a persistence homology group \\(F([a, b])\\), define the Betti number  from \\(a\\) to \\(b\\) as \\[ \\beta_a^{b} = \\text{rank}(F([a, b])). \\] <p></p> <p>In most nice topological spaces, the homology doesn't change much through its  evolution. That is, as we move from \\(a\\) to \\(b\\), the persistence homology groups  \\(F_a^b\\) don't change much.</p> <p>For example, if \\(f: X \\to \\mathbb{R}\\) is the height function  and  \\(X\\) is a sphere, the topology will not change until we get from  one pole to the other. </p> <p></p> <p>\\textcolor{NavyBlue}{What does it mean for  the topology to change in this context}? It means that we were at  some value \\(a\\), but then at \\(a +\\epsilon\\) the homology became different. This means that </p> \\[ H(f^{-1}((\\infty, a])) \\to H(f^{-1})(\\infty, a + \\epsilon]    \\] <p>is not an isomorphism. Finding out when the homology does change is  valuable information, so we keep track of these points.</p> <p> A critical value of \\(f: X \\to \\mathbb{R}\\) is an \\(a \\in \\mathbb{R}\\) such that  there exists an \\(\\epsilon &gt; 0\\) such that  \\[ H_n(f^{-1}((\\infty, a - \\epsilon])) \\to H_n(f^{-1}((\\infty, a +\\epsilon ])) \\] <p>is not an isomorphism. The function \\(f\\) is called tame if  \\(f\\) has finitely many critical values.  \\</p> <p>Let \\(f: X \\to \\mathbb{R}\\) be  a tame function. Then we have finitely many critical values \\(\\{s_1, s_2, \\dots, s_n\\}\\). Let \\(\\{t_0, t_1, \\dots, t_{n}\\}\\) be any interleaved sequence of numbers such that  \\(t_{i-1} &lt; s_i &lt; t_{i}\\). We will see soon why such a choice has much freedom in it.  Now append to this sequence \\(t_{-1} = s_0 = -\\infty\\) an \\(t_{n+1} = s_{n+1} = \\infty\\). </p> <p>We are now ready to define persistence diagrams. \\ \\</p> <p> Let \\(f: X \\to \\mathbb{R}\\) be tame and  \\((s_i, s_j)\\) be a tuple of critical values. Then we define the multiplicity  of \\((s_i, s_j)\\) to be \\[ \\mu_{i}^{j} = \\beta_{t_{i-1}}^{t_i} -\\beta_{b_i}^{b_j} + \\beta_{b_{i}}^{b_{j-1}} - \\beta_{b_i}^{b_j} \\] <p></p> <p> The persistence diagram of the tame function \\(f: X \\to \\mathbb{R}\\) \\(D(f)\\) is the multiset of tuples \\((s_i, s_j)\\) each with multiplicity \\(\\mu_i^j\\). Alternatively, \\[ D(f) = \\bigcup_{i=0}^{n+1}\\bigcup_{j = 0}^{n+1} \\left( \\bigcup_{k = 1}^{\\mu_i^{j}} \\{ (s_i, s_j) \\}\\right) \\] <p></p> <p>\\  Persistence diagrams consist of points in \\(\\rr \\times \\rr \\cup\\{\\infty\\}\\)  above the diagonal \\(y = x\\). Thus let \\(**Dgm**\\) be the category of half open intervals \\([p, q)\\) with \\(p &lt; q\\) and intervals of the form \\([p, \\infty)\\). </p> <p>In what follows, let \\(S = \\{s_1, s_2, \\dots, s_n\\}\\) be a finite set of real numbers, and  let \\((G, +)\\) be an abelian group with identity \\(e\\).</p> <p> A  map \\(X: **Dgm** \\to G\\) is \\(S\\)-constructible if for every \\(I \\subset J\\)  where  \\[ J \\cap S = I \\cap S     \\] <p>we have \\(X(I) = X(J)\\).   The motivation for defining this type of function arises from the  rank function</p> \\[\\begin{align*} \\beta_a^b&amp;:**Dgm** \\to \\mathbb{Z}\\\\ &amp;= \\text{rank}(F([a, b]))\\\\ &amp;= \\text{rank}(\\im\\Big( H(f^{-1}((\\infty, a ])) \\to H(f^{-1}((\\infty, b])) \\Big)) \\end{align*}\\] <p>Suppose that our critical points are \\(S = \\{s_0, s_1, s_2, s_3\\}\\) and that we have two intervals \\(I = [a, b]\\)  and \\(J = [c, d]\\) such that \\(I \\subset J\\) and \\(I \\cap S = J \\cap S\\). </p> <p>\\  Clearly in this case we have that \\(I \\cap S = J \\cap S\\). Now observe that </p> \\[ \\beta_a^b = \\beta_c^d    \\] <p>since these intervals observe the same changes in rank.</p> <p>\\textcolor{NavyBlue}{Therefore, we see that the rank function  for a tame function \\(f: \\mathbb{R} \\to X\\) is \\(S\\)-constructible.}</p> <p> A map \\(Y: **Dgm** \\to G\\) is \\(S\\)-finite if  \\[ Y(I) \\ne e \\implies I = [s_i, s_j) \\text{ or } I = [s_i, \\infty) \\] <p>Alternatively, this states that </p> \\[ I \\ne [s_i, s_j) \\text{ and } I \\ne [s_i, \\infty) \\implies Y(I) = e. \\] <p>which is probably a better way of thinking about this.  </p> <p>This leads to the following definition: </p> <p> A persistence diagram is a finite map \\(Y: **Dgm** \\to G\\).  </p> <p>The motivation for this is due to the persistence diagram. Given a persistence diagram,  we can extend it to a mapping </p> \\[\\begin{align*} &amp;X: **Dgm** \\to \\mathbb{Z}\\\\ &amp;[a, b) \\mapsto \\beta_{a_1}^{b_1} - \\beta_{a_2}^{b_2} + \\beta_{a_2}^{b_1} - \\beta_{a_1}^{b_1} \\end{align*}\\] <p>where \\(a_1 \\le a \\le a_2\\) and \\(b_1 \\le b \\le b_2\\) are values within some sufficiently  small neighborhood of \\(a\\) and \\(b\\). Note that in this extension, if \\([a, b) \\ne [s_i, s_j)\\)  or \\([s_i, \\infty )\\) in, then each \\(\\beta_{a_i}^{b_j}\\) is of full rank, so that </p> \\[ X([a, b)) = 0. \\] <p>Hence we see that the persistence diagram is \\(S\\)-finite where \\(S\\) is the finite set of critical values. </p> <p>We now want to invent a distance between persistence diagrams. To do so, we must  first denote \\(G\\) as not only an abelian group, but one with a  translational invariant partial ordering \\(\\le\\). What we mean by that is if  \\(a \\le b\\) then \\(a + c \\le b + c\\) for any \\(a, b ,c \\in G\\). </p> <p> Consider \\(Y_1, Y_2: **Dgm** \\to G\\) be a pair of persistence diagrams.  We say there exists a morphism \\(\\phi: Y_1 \\to Y_2\\) if  \\[ \\sum_{\\substack{J \\in **Dgm** \\\\ I \\subset J}}Y_1(J) \\le \\sum_{\\substack{J \\in **Dgm** \\\\ I \\subset J}}Y_2(J) \\] <p>for all \\(I \\in **Dgm**\\).  </p> <p>Note the above sums are finite.</p> <p>\\textcolor{NavyBlue}{Observe that if \\(\\phi: Y_1 \\to Y_2\\) and \\(\\phi': Y_2 \\to Y_3\\),  then we can define the unique morphism \\(\\phi' \\circ \\phi : Y_1 \\to Y_3\\). Therefore,  this morphism relation establishes a reflexive, transitive ordering on our persistence diagrams.}  Thus we can consider the category of persistence diagrams \\(**PDiag**(G)\\) into the  group \\(G\\) where the objects are persistence diagrams \\(Y: **Dgm** \\to G\\) and morphisms  as described above. As we stated before, these morphisms make this category  into a partial ordering.</p> <p>Define the mapping </p> \\[\\begin{align*} **Grow**_\\epsilon: **Dgm** &amp;\\to **Dgm**\\\\ [p, q) &amp;\\mapsto [p - \\epsilon, q + \\epsilon] \\text{ and } [p, \\infty) \\mapsto [p - \\epsilon, \\infty). \\end{align*}\\] <p>Now consider a pair of persistence modules \\(Y_1, Y_2: **Dgm** \\to G\\).  Since they are persistence modules, we know by definition that they are  \\(S_1\\) and \\(S_2\\)-finite for some finite sets \\(S_1, S_2\\). With that said, observe that  \\(Y_1 \\circ **Grow**_\\epsilon, Y_2 \\circ **Grow**_\\epsilon: **Dgm** \\to G\\)  are again persistence modules since they \\(S_1'\\) and \\(S_2'\\) finite, where\\dots</p> <p>Therefore, we have an endofunctor on our category  of persistence modules.</p> \\[\\begin{align*} \\nabla_\\epsilon : **PDgm**(G) \\to **PDgm**(G)\\\\ Y_1: **Dgm** \\to G \\mapsto Y_1 \\circ **Grow**_\\epsilon: **Dgm** \\to G. \\end{align*}\\] <p>Note that for any persistence modules \\(Y: **Dgm** \\to G\\), we have that  \\(\\nabla_\\epsilon(Y) \\to Y\\) since for any interval \\(Y\\), </p> \\[ \\sum_{\\substack{J \\in **Dgm** \\\\ I \\subset J}}Y(J)  = Y_1 \\circ **Grow**_\\epsilon \\]"},{"location":"category_theory/Persistence%20Modules/Generalized%20Persistence%20Modules./","title":"11.2. Generalized Persistence Modules.","text":"<p> Let \\(P\\) be a preorder. Then  a generalized persistence module is a functor  \\(F: P \\to \\dd\\).   Therefore, we may view \\(D^P\\) to be the category of generalized persistence modules  on \\(P\\).  </p> <p> A translation on \\(P\\) is a functor \\(\\Gamma: P \\to P\\) such that  \\(x \\le \\Gamma(x)\\) for all \\(x\\). Equivalently, it is any functor such that there  exists a natural transformation \\(\\eta_\\Gamma: I \\to \\Gamma\\).  </p> <p>We can denote the category of translations on \\(P\\) as \\(**Trans**_P\\).  Note that this is a preorder. Since \\(P\\) is a preorder, any two natural transformations between two functors must necessarily be equal.  Moreover, every pair of translations must have a natural transformation; that is,  one (or both) of the diagrams below must commute for any \\(x \\le y\\)  in \\(P\\). </p> <p> Thus we set \\(\\Gamma \\le K\\) whenever there exists a natural transformation  \\(\\eta_{\\Gamma K}: \\Gamma \\to K\\). </p> <p>\\begin{definition} Let \\(P\\) be a preorder and \\(\\Gamma, K \\in **Trans**_P\\). Suppose  \\(F, G \\in \\dd^P\\). We say \\(F, G\\) are \\((\\Gamma, K)\\)-interleaved if there exists  a pair of natural transformations \\(\\phi: F \\to G \\circ \\Gamma\\) and  \\(\\psi: G \\to F\\circ K\\) such that  \\  \\  end{definition} Note that, given the first two commutative squares, we can stack them to create a  larger commutative square:  \\  f the two triangular diagrams did not hold, then we would we would see  that there would be two different, but not necessarily equal ways of getting from \\(F\\) to \\(F(K(\\Gamma))\\) and \\(G\\) to \\(G(\\Gamma(K(x)))\\). Note also that, if we really  wanted to, we could keep stacking these diagrams on and on.</p> <p>The interleaving of two functors satisfies the following three properties. </p> <p>[Functoriality] Let \\(\\Gamma, K\\) be translations on a preordered set \\(P\\). If \\(F, G \\in \\dd^P\\), and if \\(F, G\\) are \\((\\Gamma, K)\\)-interleaved, then \\(H\\circ F\\) and \\(H \\circ G\\) are  also \\((\\Gamma, K)\\) interleaved.  </p> <p> This is true since any functor applied to a commutative diagram will output a commutative diagram.  Thus if we compose \\(H\\) with the commutative diagrams which arise from the interleaving  of \\(F, G\\),  we get  \\  \\  The above diagrams can be reconciled with the definition of an \\((\\Gamma, K)\\)  interleaving, so that \\(H\\circ F, H\\circ G\\)  are \\((\\Gamma, K)\\) are interleaved.  </p> <p>[Monotonicity] Let \\(\\Gamma_1, \\Gamma_2, K_1, K_2\\) be translations of a preordered set \\(P\\)  such that \\(\\Gamma_1  \\le \\Gamma_2\\) and \\(K_1  \\le  K_2\\). If two persistence modules  \\(F, G \\in \\dd^{P}\\) are \\((\\Gamma_1, K_1)\\) interleaved, then they are also  \\((\\Gamma_2, K_2)\\) interleaved.  </p> <p> Since \\(\\Gamma_1 \\le \\Gamma_2\\) and \\(K_1 \\le K_2\\), there must exist  natural transformations \\(\\alpha: \\Gamma_1 \\to \\Gamma_2\\) and \\(\\beta: K_1 \\to K_2\\).  Now since \\(F, G\\) are \\((\\Gamma_1, K_1)\\)-interleaved, this means we get the usual diagrams,  but we can stack an extra layer on the bottom.  \\  Hence we can see our natural transformations of interest are  \\(G(\\alpha) \\circ \\phi: F \\to G \\circ \\Gamma_2\\) and  \\(F(\\beta)\\circ \\psi: G \\to F \\circ K_2\\). We now have to show that our  two required triangular diagrams must commute.  Towards this goal, consider the diagram below. \\  The left triangle commutes since \\(F, G\\) are a \\((\\Gamma_1, K_1)\\) interleaving,  while the rightmost commutes by the original square diagrams.  We've outlined their correspondence in colors. We almost have what we want, but  we need to make sure \\(\\textcolor{Orange}{F(K_2(\\alpha_x))}\\circ  \\textcolor{Blue}{F(\\beta_{\\Gamma_1(x)})}\\circ  F(\\eta_{\\Gamma_1(K_1(x))}) = F(\\eta_{\\Gamma_2(K_2(x))})\\).  To do this, observe that the diagram \\  must necessarily commute as it is a diagram inside of \\(P\\), a preordered set.  Therefore, the image of this diagram under \\(F\\) must produce a commutative diagram, so that we do  in fact get our desired relation. All together, we then have  \\  The same procedure can be repeated  dually to demonstrate commutativity for the other required triangular diagram. Thus we have that \\(F, G\\) are \\((\\Gamma_2, K_2)\\)-interleaved. </p> <p>[Triangle inequality.] Let \\(\\Gamma_1, \\Gamma_2, K_1, K_2\\) be translations of a preordered set \\(P\\).  Suppose \\(F, G, H \\in \\dd^P\\). Then if \\(F,G\\) are \\((\\Gamma_1,  K_1)\\)-interleaved  and \\(G, H\\) are \\((\\Gamma_2, K_2)\\)-interleaved, then \\(F,H\\) are \\((\\Gamma_2\\circ\\Gamma_1, K_1\\circ K_2)\\)-interleaved.  </p> <p> First observe that since  \\(F, G\\) are \\((\\Gamma_1, K_1)\\)-interleaved and  \\(G,H\\) are \\((\\Gamma_2, K_2)\\)-interleaved, we have the natural transformations  \\[\\begin{align*} \\phi&amp;: F \\to G \\circ \\Gamma_1  &amp;&amp;\\phi': G \\to H\\circ \\Gamma_2\\\\ \\psi&amp;: G \\to F\\circ K_1 &amp;&amp;\\psi':  H \\to G\\circ K_2 \\end{align*}\\] <p>which satisfy the required diagrams. Consider the diagrams \\  which commute by our given interleavings. Then there are natural transformations \\(\\psi'_{\\Gamma_1} \\circ \\phi: F \\to H(\\Gamma_2\\circ\\Gamma_1)\\)  and \\(\\phi'_{K_2}\\circ \\psi: H \\to F(K_1\\circ K_2)\\). We now must check they satisfy the required triangular diagrams.  We can demonstrate this for at least one; Consider the diagram  \\  The above diagram commutes by our given interleavings. The diagram  in \\textcolor{Blue}{blue} commutes since \\(F, G\\) are \\((\\Gamma_1, K_1)\\) interleaved,  while the diagram in \\textcolor{Red}{red} commutes since \\(G, H\\)  are \\((\\Gamma_2, K_2)\\)-interleaved. </p> <p></p>"},{"location":"category_theory/Persistence%20Modules/Interleaving%20Distances%20via%20Sublinear%20Projections%20and%20Superlinear%20Families/","title":"11.3. Interleaving Distances via Sublinear Projections and Superlinear Families","text":"<p> A sublinear projection is a function \\(\\omega: **Trans**_P \\to [0, \\infty]\\) which acts on the objects of \\(**Trans**_P\\) in such a way that  \\(\\omega_I = 0\\) and \\(\\omega_{\\Gamma_1\\Gamma_2} \\le \\omega_{\\Gamma_1} + \\omega{\\Gamma_2}\\).  <p>Moreover, we say a sublinear projection is monotone if whenever  \\(\\Gamma \\le K\\) we have that \\(\\omega_{\\Gamma} \\le \\omega_{K}\\).  </p> <p>Note that we can turn a sublinear projection \\(\\omega\\) into a monotone one by defining</p> \\[ \\overline{\\omega}_\\Gamma = \\inf\\{\\omega_{\\Gamma'} \\mid \\Gamma' \\ge \\Gamma \\}. \\] <p>This is monotone since, if \\(\\Gamma \\le K\\) is a pair of translations, then  one can observe that </p> \\[ \\{ \\omega_{\\Gamma'} \\mid \\Gamma' \\ge \\Gamma \\} \\supset \\{ \\omega_{\\Gamma'} \\mid \\Gamma' \\ge K \\} \\implies   \\overline{\\omega}_{\\Gamma} \\le \\overline{\\omega}_{K}. \\] <p>Also note another nice property: for every sublinear projection \\(\\omega\\), it is always  the case that \\(\\overline{\\omega}_{\\Gamma} \\le \\omega_{\\Gamma}\\) for any translation \\(\\Gamma\\). </p> <p> Suppose \\(F, G\\) are interleaved by a pair of translations \\((\\Gamma, K)\\). Then  we say \\(F, G\\) are \\(\\bm{\\epsilon}\\)-interleaved with respect to \\(\\omega\\) if  \\[ \\omega_\\Gamma, \\omega_K \\le \\epsilon. \\] <p></p> <p>Now we prove a small lemma. </p> <p> Let \\(\\omega\\) be a sublinear projection on a preorder \\(P\\), and  let \\(\\Gamma\\) be a translation of \\(P\\). Then for every \\(\\eta &gt; 0\\), there  exists a translation \\(\\Gamma' \\ge \\Gamma\\) such that  \\[ \\omega_{\\Gamma'} \\le \\overline{\\omega}_\\Gamma + \\eta. \\] <p></p> <p> Suppose the statement was false. Then this would imply the existence of some  \\(\\eta&gt; 0\\) with the property that  \\[ \\overline{\\omega}_{\\Gamma} + \\eta &lt; \\omega_{\\Gamma'} \\] <p>for all \\(\\Gamma' \\ge \\Gamma\\). Hence we would see that </p> \\[ \\overline{\\omega}_{\\Gamma} \\ne \\inf\\{\\omega_{\\Gamma'} \\mid \\Gamma'\\ge\\Gamma \\} \\] <p>which is a contradiction. </p> <p>With the definition of a sublinear projection, we can now create a (psuedo)metric between  persistence modules.</p> <p> Let \\(F, G \\in \\dd^{P}\\), and suppose \\(\\omega\\) is a sublinear projection.  Then their interleaving distance is given by  \\begin{statement}{NavyBlue!10} \\[\\begin{align} d^{\\omega}(F, G)  &amp;= \\{ \\epsilon \\in [0, \\infty) \\mid F, G \\text{ are } \\epsilon \\text{-interleaved w.r.t. } \\omega  \\}\\\\ &amp;= \\{ \\epsilon \\in [0, \\infty) \\mid F, G \\text{ are } (\\Gamma, K)\\text{-interleaved and } \\omega_\\Gamma, \\omega_K \\le \\epsilon\\}. \\end{align}\\] <p>\\end{statement} </p> <p> Let \\(\\omega\\) be a sublinear projection. Then \\(d^{\\omega}= d^{\\overline{\\omega}}\\). </p> <p> We will prove this by first showing that \\(d^{\\omega} \\ge d^{\\overline{\\omega}}\\), and then demonstrating that \\(d^{\\omega} - d^{\\overline{\\omega}} = 0\\). \\begin{description} \\item[\\(\\bm{d^{\\omega} \\ge d^{\\overline{\\omega}}}\\)] If a pair of persistence modules \\(F, G\\) are \\(\\epsilon\\)-interleaved by \\((\\Gamma, K)\\) with respect to \\(\\omega\\), then we can observe that  \\[ \\overline{\\omega}_\\Gamma  \\le \\omega_\\Gamma \\le \\epsilon \\qquad \\overline{\\omega}_K \\le \\omega_K \\le \\epsilon \\] <p>so that \\(F, G\\) are also \\(\\epsilon\\)-interleaved by \\((\\Gamma, K)\\) with respect to \\(\\overline{\\omega}\\). Therefore,</p> \\[\\begin{align*} \\{ \\epsilon \\in [0, \\infty) \\mid F, G \\text{ are } \\epsilon \\text{-interleaved w.r.t. } \\omega  \\} \\subset  \\{ \\epsilon \\in [0, \\infty) \\mid F, G \\text{ are } \\epsilon \\text{-interleaved w.r.t. } \\overline{\\omega}  \\}. \\end{align*}\\] <p>If we take the infimum of the above relation, we get that \\(d^{\\overline{\\omega}} \\le d^{\\omega}\\). \\</p> <p>\\item[\\(\\bm{d^{\\omega} - d^{\\overline{\\omega}}} = 0\\).] Let \\(\\delta&gt; 0\\). We'll show that for any persistence modules \\(F,G\\) that</p> \\[   d^\\omega(F,G) - d^{\\overline{\\omega}}(F,G) \\le \\delta \\] <p>which, in combination of the fact that \\(d^{\\overline{\\omega}} \\le d^{\\omega}\\), will then give us our result. </p> <p>Towards this goal, let \\(\\Gamma, K\\)  be an interleaving of \\(F, G\\) such that </p> \\[ \\overline{\\omega}_{\\Gamma}, \\overline{\\omega}_{K} \\le d^{\\overline{\\omega}}(F, G) + \\delta. \\] <p>Such an interleaving must exist or else \\(d^{\\overline{\\omega}}(F, G)\\)  is larger than we thought.  By the lemma we proved earlier, we know that there exist  translations \\(\\Gamma', K'\\) such that </p> \\[ \\Gamma \\le \\Gamma' \\qquad K \\le K' \\] <p>and </p> \\[\\begin{align*} \\omega_{\\Gamma'} \\le \\overline{\\omega}_{\\Gamma} \\le d^{\\omega}(F, G) + \\delta \\qquad \\omega_{K'} \\le \\overline{\\omega}_{K} \\le d^{\\omega}(F, G) + \\delta. \\end{align*}\\] <p>Note that by Monotonocity of interleavings, since \\(F, G\\) are interleaved  by \\((\\Gamma, K)\\), we know that \\(F, G\\) are interleaved by  \\((\\Gamma', K')\\). Therefore, we can conclude that since \\(\\omega_{\\Gamma'}, \\omega_{K'} \\le d^{\\overline{\\omega}} + \\delta\\), we see that </p> \\[\\begin{align*} d^{\\omega}(F, G) \\le  d^{\\overline{\\omega}}(F,G) + \\delta \\implies d^{\\omega}(F, G) - d^{\\overline{\\omega}}(F,G) \\le \\delta. \\end{align*}\\] <p>Since \\(\\delta &gt; 0\\) was arbitrary, and because \\(d^{\\omega} \\ge d^{\\overline{\\omega}}\\)  we have that they must be equal, as desired.  \\end{description} </p> <p>We now introduce an important implication of these results. </p> <p> For any sublinear translation \\(\\omega: **Trans**_P \\to [0, \\infty]\\),  The interleaving distance \\(d = d^\\omega\\) becomes an extended psuedometric on \\(\\dd^P\\).  </p> <p> To show this, we must show that \\(d(F, F) = 0\\) for any persistence  module \\(F\\), \\(d\\) is symmetric, and that \\(d\\) obeys the triangle inequality.  \\begin{description} \\item[\\(\\bm{d(F, F) = 0}\\)] Observe that \\(d(F, F) = 0\\). This is because if we denote \\(I: P \\to P\\) to be the identity translation on \\(P\\), then  \\(F\\) is \\((I, I)\\) interleaved with itself. But recall that \\(\\omega_I = 0\\).  <p>\\item[\\(\\bm{d(F, G) = d(G,F)}\\)] Now observe that \\(d(F, G) = d(G, F)\\). This is because of the inherent symmetry present in the definition of an interleaving, which allows us to  swap \\(F\\) and \\(G\\). </p> <p>\\item[Triangle Inequality] Finally, we show that \\(d\\) obeys the triangle inequality. Consider a triple of  persistence modules \\(F, G, H\\). Suppose \\(F, G\\) are \\(\\epsilon\\) interleaved,  while \\(G, H\\) are \\(\\epsilon'\\) interleaved. Regardless of whether or not  \\(\\epsilon \\le \\epsilon'\\) or vice versa, we know that there exist translations  \\(\\epsilon\\)-translations \\((\\Gamma, K)\\) which interleaved \\(F, G\\) and \\(\\epsilon'\\)-translations  \\((\\Gamma', K')\\) which interleave \\(G,H\\). By the triangle inequality of translations,  we know that this implies that \\(F, H\\) are \\((\\Gamma'\\circ \\Gamma, K \\circ K')\\)-interleaved </p> <p>Note that by sublinearity we have that </p> \\[\\begin{align*} \\omega_{\\Gamma'\\Gamma} &amp;\\le \\omega_{\\Gamma'} + \\omega_{\\Gamma} \\le \\epsilon' + \\epsilon\\\\ \\omega_{KK'} &amp;\\le \\omega_{K} + \\omega_{K'} \\le \\epsilon + \\epsilon' \\end{align*}\\] <p>Therefore, we see that </p> \\[ d(F, H) \\le \\epsilon' + \\epsilon. \\] <p>Taking the infimum over \\(\\epsilon', \\epsilon\\), we get that </p> \\[ d(F,H) \\le d(F,G) + d(G, H) \\] <p>as desired. \\end{description} </p> <p>We'll now show that this isn't the only way to invent a metric for persistence modules  in their functor category.</p> <p> Let \\(P\\) be a preorder. A superlinear family \\(\\Omega: [0, \\infty) \\to **Trans**_P\\)  is a function where  \\[ \\epsilon \\mapsto \\Omega_\\epsilon \\in **Trans**_P   \\] <p>such that \\(\\Omega_{\\epsilon_1}\\Omega_{\\epsilon_2} \\le \\Omega_{\\epsilon_1 + \\epsilon_2}\\).  </p> <p>Note that in \\(**Trans**_P\\), the identity \\(I: P \\to P\\) is an initial object.  So if \\(\\epsilon_1 \\le \\epsilon_2\\), we know that </p> \\[ I \\le \\Omega_{\\epsilon_2 - \\epsilon_1}.    \\] <p>Appending \\(\\Omega_{\\epsilon_1}\\) on the right, we get that </p> \\[ I\\Omega_{\\epsilon_1} \\le \\Omega_{\\epsilon_2 - \\epsilon_1}. \\] <p>Using the fact that \\(\\Omega_{\\epsilon_1}\\Omega_{\\epsilon_2} \\le \\Omega_{\\epsilon_1 + \\epsilon_2}\\),  we see that </p> \\[ I\\Omega_{\\epsilon_1} \\le \\Omega_{\\epsilon_2 - \\epsilon_1} \\le \\Omega_{\\epsilon_2}. \\] <p>Since \\(I\\) is the identity, we know that \\(I\\Omega_{\\epsilon_1} = \\Omega_1\\). We thus have that </p> \\[ \\Omega_{\\epsilon_1} \\le \\Omega_{\\epsilon_2} \\] <p>so that superlinear families are monotonic.</p> <p>Now, how does this turn into a metric? </p> <p> Let \\(P\\) be a preorder and \\(\\dd\\) a category. Then for \\(F,G \\in \\dd^P\\),  we define their interleaving distance \\[ d^{\\Omega}(F,G) = \\inf \\{ \\epsilon \\in [0, \\infty) \\mid F, G \\text{ are } \\Omega_\\epsilon\\text{-interleaved} \\}. \\] <p>If the above set is empty, we set \\(d^{\\Omega}(F,G) = \\infty\\).  </p> <p> The interleaving distance \\(d^{\\Omega}\\) is an extended pseudometric. </p> <p> To show this, we need to prove that for persistence modules \\(F, G\\),  \\(d(F, F) = 0\\), \\(d(F, G) = d(G, F) = 0\\), and that the metric satisfies the triangle  inequality.  \\begin{description} \\item[\\(\\bm{d(F, F) = 0}\\).] Observe that the functors  \\(F, F\\) are \\((I, I)\\)-interleaved. Given that \\(I \\le \\Omega_0\\)  since it is initial, we see that \\(d(F,  F) = 0\\).  <p>\\item[\\(\\bm{d(F, G) = d(G, F)}\\).] Observe that the definition is purely symmetric  so that this result is instant. </p> <p>\\item[Triangle inequality.] Let \\(F, G, H\\) be persistence modules and suppose \\(F, G\\) are \\(\\Omega_{\\epsilon_1}\\)-interleaved  while \\(G, H\\) are \\(\\Omega_{\\epsilon_2}\\)-interleaved. Then by the triangle property of  translations, we know that \\(F, H\\) are  \\((\\Omega_{\\epsilon_2}\\Omega_{\\epsilon_1}, \\Omega_{\\epsilon_1}\\Omega_{\\epsilon_2})\\)-interleaved. </p> <p>Observe that </p> \\[\\begin{align*} &amp;\\Omega_{\\epsilon_2}\\Omega_{\\epsilon_1} \\le \\Omega_{\\epsilon_1 + \\epsilon_2}\\\\ &amp;\\Omega_{\\epsilon_1}\\Omega_{\\epsilon_2} \\le \\Omega_{\\epsilon_1 + \\epsilon_2}. \\end{align*}\\] <p>By monotonicty of translations, this implies that \\(F, H\\) are \\(\\Omega_{\\epsilon_1 + \\epsilon_2}\\)-interleaved,  so that </p> \\[ d^{\\Omega}(F, H) \\le \\epsilon_1 + \\epsilon_2. \\] <p>Taking the infimum over \\(\\epsilon_1, \\epsilon_2\\), we get that </p> \\[ d^{\\Omega}(F, H) \\le d^{\\Omega}(F, G) + d^{\\Omega}(G, H) \\] <p>as desired.  \\end{description} </p>"},{"location":"category_theory/Persistence%20Modules/Persistence%20modules%20on%20%24%5Crr%24./","title":"11.1. Persistence modules on \\(\\rr\\).","text":"<p> Let \\(\\cc\\) be a category, and denote \\((\\rr, \\le)\\) to be the poset category on \\(\\rr\\) with respect to the natural relation  \\(\\le\\). We define a functor \\(F: (\\rr, \\le) \\to \\cc\\)  to be a persistence module.  </p> <p>Thus we can say that a persistence module is an element of the functor  category \\(\\cc^{\\rr}\\). </p> <p>A persistence module allows us to model the evolution of objects within some  category \\(\\cc\\). For example, if we have some ascending chain of vector spaces </p> <p> then we say that such a chain is a persistence module since it can  be modeled as a functor from \\(\\rr \\to **Vec**\\).  </p> <p>Let \\(S = \\{s_1, s_2, \\dots, s_n\\}\\) be a finite subset of \\(\\rr^n\\). Then we can describe an adjunction \\  s follows. First observe that since \\(S \\subset \\rr\\), there exists a restriction functor  \\(R: \\cc^{\\rr} \\to \\cc^{S}\\), which acts as a restriction (hence the naming \\(R\\)): </p> \\[ R(F: \\rr \\to \\cc) = F\\big|_{S}: S \\to \\cc. \\] <p>How can we write a functor going in the opposite direction? That is, given a persistence module which acts on \\(S\\), </p> <p>\\  s there a canonical way to extend this  to a persistence module which acts on the rest of \\(\\rr\\)?  \\  One way we may extend a persistence module \\(K: S \\to \\cc\\) in \\(\\cc^S\\) to  a persistence module in \\(\\cc^{\\rr}\\) is to define a functor \\(\\overline{K}: \\rr \\to \\cc\\)  where </p> \\[ \\overline{K}(r) =  \\begin{cases} I &amp; \\text{if } s &lt; s_1\\\\ K(r) &amp; \\text{if } s_{i} \\le r \\le s_{i+1}\\\\ K(r_n) &amp; \\text{if } r &gt; s_n \\end{cases} =  \\begin{cases} I &amp; \\text{if } r &lt; \\text{min}(S)\\\\ K(s_r) &amp; \\text{where } s_r \\text{ is the largest } s_r \\in \\text{ S such that } s_r \\le r. \\end{cases} \\] <p>Now consider a morphism  \\(\\eta: K \\to P\\) in \\(\\cc^{S}\\); that is, a natural transformation.  By our above procedure we have a way of discussing the objects \\(\\overline{K}\\)  and \\(\\overline{P}\\); but can we obtain a natural transformation  \\(\\overline{\\eta}: \\overline{K} \\to \\overline{P}\\) from \\(\\eta\\)? That is, may we  extend this relationship to a functor? </p> <p>First, observe that we may write \\(\\eta: K \\to P\\) as follows.  \\  he top and bottom rows come about by functoriality of \\(K\\) and \\(P\\), while the upward arrows are the family of morphisms created by the existence  of a natural transformation. </p> <p>We can extend this to a natural transformation \\(\\overline{\\eta}: \\overline{K} \\to \\overline{P}\\)  by stating </p> \\[ \\overline{\\eta}_r =  \\begin{cases} 1_I &amp; \\text{if } r &lt; s_1  \\text{, where } I \\text{ is initial}\\\\ \\eta_{s_r} &amp; \\text{where } s_r \\text{ is the largest } s_r \\in S \\text{ such that } s_r \\le r.  \\end{cases} \\] <p>\\subsection*{Adjoint Functors}</p> <p>Thus we see that we really do have a functor \\(\\cc^{S} \\to \\cc^{\\rr}\\) on our hands If we denote this as a functor \\(E: \\cc^{S} \\to \\cc^{\\rr}\\),  where \\(E\\) can be read as extends, then we overall have  \\  e can now demonstrate that this pair of functors gives rise to an adjunction; there  a few ways to do this. We'll demonstrate that </p> \\[ \\hom_{\\cc^{S}}(K, P_S) \\cong \\hom_{\\cc^{\\rr}}(\\overline{K}, P) \\] <p>is natural, where \\(P_S = \\text{R}(P)\\) and \\(\\overline{K} = E(K)\\). Towards this  goal, consider a morphism \\(\\eta: K \\to P_S\\). Then we have something like this  again  \\  ow we seek a natural transformation \\(\\eta': \\overline{K} \\to P\\). Since \\(\\overline{K}\\)  is constructed from \\(K\\), a good choice would be to write  \\(\\eta'_{s_i} = \\eta_{s_i}\\) for \\(s_i \\in S\\).  Now our concern is considering how to define \\(\\eta'_r\\) when \\(r \\not \\in S\\). That is, we want something like  \\  o define the morphism in red, we first recall that in  this situation we have \\(K(r) = K(s_i)\\). Hence we know that any morphism from \\(K(r)\\) must originate from \\(K(s_i)\\); one such morphism we already know  about is \\(\\eta_{s_i}: K(s_i) \\to P_s(s_i)\\). Now, \\(P_s(s_i) = P(s_i)\\);  and in our case the desired target for \\(\\eta'\\) is \\(P(r)\\), not \\(P(s_i)\\). However,  we can compose this with the morphism \\(P(j): P(s_i)  \\to P(r)\\). where \\(j : s_i \\to r\\). \\  herefore, in this case we define </p> \\[ \\eta'_{r} := P(j) \\circ \\eta_{s_i}. \\] <p>which necessarily forces commutativity, and hence demonstrating  naturality of \\(\\eta'\\). Now what if \\(r &lt; s_1\\) or \\(s_n &lt; s\\)? In the first case,  \\(K(r) = I\\), and \\(\\eta'_r\\) becomes the unique morphism from \\(I \\to P(r)\\).  \\textcolor{NavyBlue}{This presents one benefit of adding the criteria  \\(K(r) = I\\) if \\(r &lt; s_1\\)}. By uniqueness of this morphism we get a commutative square.  In the second case, we proceed as above. Therefore </p> \\[ \\eta'_r =  \\begin{cases} i_{P(r)}: I \\to P(r) &amp; \\text{if } r &lt; s_1\\\\ P(j: s_i \\to r) \\circ \\eta_{s_i} &amp; \\text{where } s_i \\text{ is the largest } s \\in S \\text{ such that } s \\le r.  \\end{cases} \\] <p>Therefore, we can define a map \\(\\textcolor{Blue}{\\phi}: \\hom_{\\cc^S}(K,P_S) \\to \\hom_{\\cc^{\\rr}}(\\overline{K}, P)\\)  where</p> \\[ \\phi(\\eta: K \\to P_S) = \\eta': \\overline{K} \\to P. \\] <p>Consider the map \\(\\psi: \\hom_{\\cc^{\\rr}}(\\overline{K}, P) \\to \\hom_{\\cc^S}(K, P_S)\\) where </p> \\[ \\psi(\\sigma: \\overline{K} \\to P) = \\sigma': K \\to P_S \\] <p>where we set \\(\\sigma'_s = \\sigma_s\\). While this map is particularly boring,  we're discussing it because we can now see that \\(\\psi\\) and \\(\\phi\\) are inverses of  each other. Therefore, we see that we have a bijection between the hom-sets,  as desired. </p> <p>\\subsection*{Naturality.}</p> <p>Finally, we must demonstrate naturality. So suppose we have a natural transformation  \\(\\alpha: K \\to K'\\) between two persistence modules \\(K, K' : S \\to \\cc\\). Consider the squares below, which we do not yet know commutes.  \\  Note that on one hand, </p> \\[ \\overline{\\alpha}_r =  \\begin{cases} 1_I &amp; \\text{if } r &lt; s_1  \\text{, where } I \\text{ is initial}\\\\ \\alpha_{s_r} &amp; \\text{where } s_r \\text{ is the largest } s_r \\in S \\text{ such that } s_r \\le r.  \\end{cases} \\] <p>and </p> \\[ \\eta'_r =  \\begin{cases} i_{P(r)}: I \\to P(r) &amp; \\text{if } r &lt; s_1\\\\ P(j: s_i \\to r) \\circ \\eta_{s_i} &amp; \\text{where } s_i \\text{ is the largest } s \\in S \\text{ such that } s \\le r.  \\end{cases} \\] <p>so that </p> \\[\\begin{align*} (\\eta' \\circ \\overline{\\alpha})_r &amp;=      \\begin{cases} i_{P(r)}: I \\to P(r) &amp; \\text{if } r &lt; s_1\\\\ \\big(P(j: s_i \\to r ) \\circ \\eta \\big)\\circ\\alpha &amp; \\text{where } s_r \\text{ is the largest } s_r \\in S \\text{ such that } s_r \\le r.  \\end{cases}\\\\ &amp;= \\begin{cases} i_{P(r)}: I \\to P(r) &amp; \\text{if } r &lt; s_1\\\\ P(j: s_i \\to r ) \\circ (\\eta \\circ\\alpha) &amp; \\text{where } s_r \\text{ is the largest } s_r \\in S \\text{ such that } s_r \\le r.  \\end{cases}\\\\ &amp;=   (\\eta \\circ \\alpha)'_r. \\end{align*}\\] <p>Since we know that \\(\\big(P(j: s_i \\to r)\\circ \\eta \\big)\\circ\\alpha  = P(j: s_i \\to r) \\circ (\\eta \\circ \\alpha)\\).  Thus we see that the previous squares we discussed do in fact commute. </p> <p>Now suppose we have a natural transformation \\(\\sigma: P \\to P'\\) between  two functors \\(P, P': \\rr \\to \\cc\\). Consider the diagrams below, which we will show are commutative. \\  o show this, observe that </p> \\[\\begin{align*} \\sigma \\circ \\eta'  &amp;=  \\begin{cases} \\sigma_r \\circ i_{P(r)}: I \\to P'(r) &amp; \\text{if } r &lt; s_1\\\\ \\sigma_{r}\\circ P(j: s_i \\to r) \\circ \\eta_{s_i} &amp; \\text{where } s_i \\text{ is the largest } s \\in S \\text{ such that } s \\le r.  \\end{cases}\\\\ &amp;= \\begin{cases} \\textcolor{Purple}{i_{P'(r)}: I \\to P'(r)} &amp; \\text{if } r &lt; s_1\\\\ \\textcolor{OliveGreen}{P'(j: s_i \\to r)} \\circ (\\sigma \\circ \\eta )_{s_i} &amp; \\text{where } s_i \\text{ is the largest } s \\in S \\text{ such that } s \\le r. \\end{cases}\\\\ &amp;= \\begin{cases} i_{P'(r)}: I \\to P'(r) &amp; \\text{if } r &lt; s_1\\\\ P'(j: s_i \\to r) \\circ (\\textcolor{Red}{\\sigma'} \\circ \\eta )_{s_i} &amp; \\text{where } s_i \\text{ is the largest } s \\in S \\text{ such that } s \\le r. \\end{cases}\\\\ &amp;= (\\sigma' \\circ \\eta)'. \\end{align*}\\] <p>The diagrams below can assist to seeing why this is the case. First,  the change in \\textcolor{Purple}{purple} occurs by commutativity of the diagram on the left; the commutativity  results due to the universal nature of morphisms originating from the initial object \\(I\\). Second,  the changes in \\textcolor{OliveGreen}{green} and \\textcolor{Red}{red} occur by commutativity of the diagram on the right.  \\  hus we see that our original squares are commutative. At this point, we can conclude that  we do in fact have an adjunction  \\  s desired. </p>"},{"location":"category_theory/Sheaves/Abstracting%20Sheaves/","title":"10.2. Abstracting Sheaves","text":"<p>We will now take a more categorical approach to extract the key  properties of a sheaf, so that we may generalize our logic. Towards that goal, we'll introduce a second definition of a sheaf,  one which is equivalent to what the reader has already seen; it will offer a new  perspective. To motivate this perspective, we will again use our canonical sheaf of continuous functions: </p> \\[ C: **Open**(X)\\op \\to **Set** \\qquad C(U) = \\{f: U \\to Y \\mid f \\text{ is continuous}\\} \\] <p>Consider an open set \\(U\\) of \\(X\\), and let \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\) be an open cover  of \\(U\\). Let us make a few nontrivial observations.  The reader is strongly encouraged to move forward with pen and paper in hand and to  draw lots of pictures.</p> <ul> <li> <p>A family of continuous functions \\(h_i: U_i \\to Y\\) can be  viewed as an element \\((h_i)_{i \\in \\lambda}\\) of the  product \\(\\prod_{i \\in \\lambda} C(U_i)\\).</p> </li> <li> <p>Using our open cover \\(\\mathcal{U}\\), we can define for each pair \\(k,\\ell \\in \\lambda\\) the functions </p> </li> </ul> \\[ p_{k,\\ell},  q_{k, \\ell} : \\prod_{i \\in \\lambda}C(U_i) \\to   C(U_{k} \\cap U_{\\ell}) \\] <p>where </p> \\[ p_{k, \\ell}\\Big((h_i)_{i \\in \\lambda}\\Big)  = h_k\\big|_{U_k \\cap U_\\ell} \\qquad \\text{and} \\qquad q_{k, \\ell}\\Big((h_i)_{i \\in \\lambda}\\Big) = h_\\ell\\big|_{U_k \\cap U_\\ell}. \\] <p>With a lot of notation, a picture may help.</p> <p></p> <ul> <li>The fact that the functions \\(p_{k, \\ell}, q_{k, \\ell}\\) exist for all \\(k, \\ell \\in \\lambda\\) implies  the existence of \\(p\\) and \\(q\\) below which make the diagram commute. (This  is just applying the universal property of the product \\(\\prod_{i, j}F(U_i \\cap U_j)\\).) These two functions are rather important. \\  end{itemize}</li> </ul> <p>Now consider the set of all \\((h_i)_{i \\in \\lambda} \\in \\prod_{i \\in \\lambda}F(U_i)\\)  such that they agree on overlaps; i.e., such that \\(h_i\\big|_{U_i \\cap U_j} =  h_j\\big|_{U_i \\cap U_j}\\) for all \\(i, j \\in \\lambda\\). We call this set \\(\\text{Eq}(p,  q)\\):</p> \\[ \\text{Eq}(p, q) = \\Big\\{ (h_i)_{i \\in \\lambda} \\in \\prod_{i \\in \\lambda}F(U_i) \\mid p\\Big( (h_i)_{i \\in \\lambda} \\Big) = q\\Big( (h_i)_{i \\in \\lambda} \\Big) \\Big\\}. \\] <p>However, since \\(C\\) is a sheaf, we know that for every such \\((h_i)_{i \\in \\lambda}\\) in \\(\\text{Eq}(p, q)\\) there exists a unique \\(h: U \\to Y\\) such that \\(h|_{U_i} = h_i\\). Therefore, we see  that </p> \\[ \\text{Eq}(p, q) \\cong C(U) \\] <p>Okay, so that's just a slightly more complicated way of expressing \\(C(U)\\).  What's interesting about this, however, is that \\(\\text{Eq}(p, q)\\) is quite literally  the equalizer of \\(p\\) and \\(q\\) (hence the naming we chose for the set). \\  This is the motivation behind the following definition of a sheaf, which is exactly equivalent to  our previous one.</p> <p> A sheaf (of sets) on a topological space \\(X\\) is a functor  \\[ F: **Open**(X)\\op \\to **Set** \\] <p>with the following property: If \\(U\\) is an open set  and \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\) an open cover of \\(U\\), then \\(F(U)\\) is an equalizer of \\(p\\), \\(q\\), constructed using \\(\\mathcal{U}\\)  as above. The equalizer diagram is below: \\  end{definition}</p> <p>We remark two comments on this definition.</p> <p>\\begin{itemize}</p> <ul> <li> <p>It is more important to understand the philosophy of the above  definition rather than the literal text of it (of course, that's necessary). For example, a topological  space does in fact speak of families of sets which are closed under  arbitrary union and finite intersection. But that's a literal definition,  and not the philosophy of a topological space.</p> </li> <li> <p>There are many ways to state the definition of a sheaf. The one  offered above is very powerful because it allows us to quickly  capture many useful situations and it is useful for proofs.</p> </li> </ul> <p>Now before we move on, we are going to briefly introduce a new concept.  \\begin{definition} Let \\(\\cc\\) be a category and \\(C\\) an object of \\(\\cc\\).  A sieve on \\(C\\) is a set \\(S\\) which is a subset of all morphisms with codomain \\(C\\):</p> \\[ S \\subset \\{f \\mid f: B \\to C \\text{ and } f \\text{ is a morphism of }\\cc \\} \\] <p>with following property. \\begin{description} \\item[(SV1)] If \\(f\\) is in \\(S\\), then \\(f \\circ h\\) is in \\(S\\) for any composable  \\(h\\). \\end{description} </p> <p>We will demonstrate an example of this concept, specifically to capture why we care  about it. </p> <p> Let \\(X\\) be a topological space, and consider the category \\(**Open**(X)\\).  Let \\(U\\) be an open set of \\(X\\). To speak of a sieve on \\(U\\),  we must first realize that the set of all objects with codomain \\(U\\) is  simply the set  \\[ \\Omega_U = \\{V \\subset U \\mid V \\text{ is open}\\} \\] <p>This set may actually be treated as the object set of the full subcategory \\(**Open**(U)\\)  of \\(**Open**(X)\\).</p> <p>So, what is a sieve in this case? It is any \\(S \\subset \\Omega_U\\) such that  \\begin{description} \\item[(SV1)] If \\(V \\in S\\), \\(V'\\) is open, and \\(V' \\subset V\\), them \\(V' \\in S\\).  \\end{description} Take note that this is a bit of subtle concept; it's a very versatile definition.  For example, considering \\(\\rr^2\\) with its standard topology, the following  (blue) open sets create sieves on the same open set (the open disk at the origin). \\  On the left, we consider the set of all open sets contained in \\(V_1\\) and \\(V_2\\); this is a  sieve on the open disk (which we call \\(U\\) to be consistent with our notation and discussion).  On the right, we consider the set of all open sets contained in the weirdly shaped \\(V\\); this is  also a sieve on \\(U\\).  </p> <p>Some important facts about sieves on topological spaces that will be of interest to us.</p> <ul> <li> <p>Every open set \\(V \\subset U\\) corresponds to a sieve, which we call a principal sieve.  This sieve is simply the set of all open \\(V'\\) contained in \\(V\\). In the previous example,  the weirdly shaped region inside the open disk at the origin is a principal sieve.</p> </li> <li> <p>Every open cover of \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\) creates a covering sieve \\(S_{\\mathcal{U}}\\). This sieve is the set of all open \\(V\\) such that \\(V \\subset U_i\\)  for some \\(i\\), and where \\(V' \\subset V\\) implies \\(V'\\) is also in the set. </p> </li> </ul> <p>Additionally, a covering sieve induces a (fairly stupid) functor \\(\\mathcal{S}\\), where:</p> \\[ \\mathcal{S}: **Open**(X)\\op \\to **Set** \\qquad \\mathcal{S}(V)  =  \\begin{cases} \\{\\bullet\\} &amp; \\text{If } V \\in S_{\\mathcal{U}}\\\\ \\varnothing &amp; \\text{ otherwise.} \\end{cases} \\] <p>We are now prepared to continue our discussion. Our goal now will be to express  the equalizer \\(E\\) of \\(p, q\\) categorically (i.e., without reference to its elements). Let \\(P: \\mathcal{O}(X)\\op \\to **Set**\\) be a presheaf. Given an open set  \\(U\\) with open cover \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\), we may construct  \\(p, q\\) using \\(\\mathcal{U}\\) as before, and take their equalizer \\(E\\): \\  We now prove the following result. </p> <p> Let \\(E\\) be the equalizer of \\(p, q\\) constructed using an open cover \\(\\mathcal{U}\\) of \\(U\\).  Let \\(\\mathcal{S}\\) be the sieve functor induced by \\(\\mathcal{U}\\). Then \\[ E \\cong \\hom(\\mathcal{S}, P) \\quad \\text{ or, in alternate notation, } \\quad E \\cong \\text{Nat}(\\mathcal{S}, P) \\] <p>That is, there is a bijection between \\(E\\) and all natural transformations between \\(\\mathcal{S}\\) and \\(P\\).  </p> <p> We know that  \\[ E = \\Bigg\\{(h_i)_{i \\in \\lambda} \\Bigm\\vert h_i|_{U_i \\cap U_j} = h_j|_{U_i \\cap U_j} \\text{ for all } i, j \\Bigg\\}. \\] <p>We'll show that every \\((h_i)_{i \\in \\lambda}\\) can be used to build  a natural transformation between \\(\\mathcal{S} \\to P\\). Showing the other direction  is not hard.</p> <p>Let \\(S_{\\mathcal{U}}\\) be our covering sieve induced by \\(\\mathcal{U}\\). Consider an element \\((h_i)_{i \\in \\lambda}\\) in \\(E\\). For each \\(V \\in S_{\\mathcal{U}}\\),  we define \\(h_V \\in P(V)\\) as</p> \\[ h_V = h_i|_{V} \\] <p>where \\(i\\) is the index such that \\(V \\subset U_i\\). Of course at least one index exists,  but it might not be the only index. Thus, a natural objection to this definition is the following  question: What if \\(V\\) is contained in \\(U_i\\) and \\(U_j\\) for distinct \\(i\\), \\(j\\)?  In this case, how do we define \\(h_V\\)?</p> <p>\\  If \\((h_i)_{i \\in \\lambda} \\in E\\), then we know that  agreement on the overlaps is guaranteed and so we may unambiguously  write \\(h_i|_V = h_j|_V = h_V\\).  Hence, each \\(V \\in S_{\\mathcal{U}}\\) corresponds to some unique \\(h_V \\in P(V)\\) for every \\((h_i)_{i \\in \\lambda} \\in E\\). Furthermore,  we know that if \\(V' \\subset V\\), then \\(h_V|_{V'} = h_{V'}\\).</p> <p>These facts allow us to create the following natural transformation  \\(\\theta: \\mathcal{S} \\to P\\) using an element \\((h_i)_{i \\in \\lambda}\\) of \\(E\\),  as follows. </p> <ul> <li>If \\(V \\in S_{\\mathcal{U}}\\), we write \\(\\theta_V: \\{\\bullet\\} \\to P(V)\\)  where \\(\\theta_V(\\bullet) = h_V\\), the unique \\(h_V\\) we already know exists. </li> </ul> <p>This allows us to create the function </p> \\[ \\phi: E \\to \\hom(\\mathcal{S}, P) \\qquad \\phi\\Big( (h_i)_{i \\in \\lambda} \\Big) \\mapsto (\\theta: \\mathcal{S} \\to P) \\] <p>It is not difficult to show that every natural transformation between \\(\\mathcal{S}\\) and \\(P\\)  corresponds to a unique element in \\(E\\), thereby giving us an inverse to this function. Thus we have  our result. </p> <p>The above result is key to the the following proposition, which is what allows us  to speak of a sheaf more abstractly. Before we introduce the proposition, we make a few  comments.</p> <ul> <li>Let </li> </ul> <p> Let \\(P: **Open**(X)\\op \\to **Set**\\) be a presheaf.  Then \\(P\\) is a presheaf if and only if for every open set \\(U\\) and covering sieve \\(S\\) of \\(U\\),  </p>"},{"location":"category_theory/Sheaves/Stalks%20and%20Germs/","title":"10.3. Stalks and Germs","text":"<p>Let \\((I, \\le)\\) be a partially ordered set, and suppose we have a functor  \\(F: I \\to **Set**\\). With this functor, denote \\(F(i) = A_i\\) and  when \\(i \\le j\\), \\(F(i \\le j) = f_{ij}: A_i\\to A_j\\).  The limit of this functor \\(\\displaystyle \\Limto_{i \\in I} F\\) will be a set  \\(A\\) equipped with functions \\(\\phi_i: A_i \\to A\\) with the universal property displayed below. </p> <p> We may naively suppose that \\(\\displaystyle A = \\coprod_{i \\in I}A_i = \\{(a, i) \\mid a \\in A_i, i \\in I \\}\\), since such a set  admits a family of functions \\(\\displaystyle \\text{inc}_i: A_i \\to \\coprod_{i \\in I}A_i\\).  However, we cannot guarantee that this the triangle  \\  ill commute. In fact, it will never commute, since it would imply that  for each \\(a \\in A_i\\), \\((a, i) = (f_{ij}(a), j)\\), which cannot happen as the  tuples are mismatched. Since it is too strong to demand equality, we can define  an equivalence relation \\(\\sim\\) on \\(\\displaystyle \\coprod_{i \\in I}A_i\\)  as follows: For \\(i \\le  j\\), we say \\((a, i) \\sim (b, j)\\) if  \\(b = f_{ij}(a)\\). We can then set </p> \\[ A = \\coprod_{i \\in I}A_i \\Big/\\sim   \\] <p>and define a family of maps \\(\\phi_i: A_i \\to A\\) which maps each \\(a \\in A_i\\) to  its equivalence class under this relation. This then allows the desired triangle  to commute and satisfies the universal property necessary for it to be the limit. </p> <p>We now apply this construction to our story with sheaves. </p> <p> Let \\(X\\) be a topological  space and \\(F: \\mathcal{O}(X) \\to **Set**\\) a sheaf. For any point \\(x \\in X\\),  we define the stalk of \\(F\\) in \\(x\\), denoted \\(F_x\\),  as the colimit  \\[ \\Limto_{x \\in U} F(U). \\] <p> The above notation is a bit informal, but many people use it, so we  will stick with it and explicitly describe this limit as follows. Each point \\(x \\in X\\)  induces a functor \\(F^{(x)}: \\mathcal{O}(X)_x \\to **Set**\\) where  \\(\\mathcal{O}_x\\) is the category of open sets of \\(X\\) containing \\(x\\),  and \\(F^{(x)}(U) = F(U)\\). We can then more formally say </p> \\[ \\Limto_{x \\in U} F(U) =  \\Limto_{U \\in \\mathcal{O}(X)_x}F^{(x)}. \\] <p>Therefore, we can say that </p> \\[ \\Limto_{x \\in U} F(U) = \\coprod_{U \\mid x \\in U}  F(U)\\Big/\\sim \\] <p>where \\(\\sim\\) is the equivalence relation described previously. In this instance, the equivalence relation translates  as follows. Let \\(U_1 \\subset U_2\\) be two open sets. Then we say  \\((f, U_1) \\sim (g, U_2)\\) if \\(g\\Big|_{U_1} = f\\). </p> <p>We can make this more refined as follows. Let \\(U_1, U_2\\) be more generally  any two open sets such that \\(V = U_1 \\cap U_2 \\ne \\varnothing\\).  Then clearly \\(V \\subset U_1\\) and \\(V \\subset U_2\\).  Now suppose, \\((f, V) \\sim (g_1, U_1)\\) for some \\(f, g_1\\), and  \\((f, V) \\sim (g_2, U_2)\\). Then we now have that </p> \\[ (g_1, U_1) \\sim (g_2, U_2) \\iff g_1\\Big|_{V} = g_2\\Big|_{V}.  \\] <p>Thus we have translated our original equivalence relation into a more  useful one. To summarize, we have that our stalk is the set</p> \\[ \\Limto_{x \\in U} F(U) = \\Bigg\\{ \\,[(f, U)] \\mid x \\in U \\text{ open}, f \\in F(U), \\Bigg\\} \\] <p>where \\((f, U)\\) is a representative of its equivalence class \\([(f, U)]\\),  described explicitly as</p> \\[ [(f, U)] = \\Big\\{ (g, V) \\mid g \\in F(U), x \\in V \\text{ open} \\text{ and } g\\Big|_{V} = f\\Big|_{V}  \\Big\\}. \\] <p>The above line leads to our next definition. </p> <p> Let \\(U\\) be an open set containing \\(x\\). There naturally exists projection map  \\[ \\pi_U: F(U) \\to F_x \\qquad f \\mapsto [(f, U)]. \\] <p>Therefore, for each \\(f \\in F(U)\\), we define the germ of \\(f\\) in \\(x\\) to be the equivalence class  \\([(f, U)]\\) in the stalk \\(F_x\\).  </p>"},{"location":"category_theory/Sheaves/Topological%20Presheaves%20and%20Sheaves/","title":"10.1. Topological Presheaves and Sheaves","text":"<p>Let \\(X\\) be a topological space. Denote the set of open subsets of \\(X\\) as \\(**Open**(X)\\). We can impose the structure of a \\hyperref[definition:thin-category]{\\textcolor{Blue}{thin category}} on this set by declaring that, for two open sets \\(U\\) and \\(V\\), </p> \\[ \\hom_{**Open**(X)}(U, V) =  \\begin{cases} \\{\\bullet\\} &amp; \\text{if } U \\subset V\\\\ \\varnothing &amp; \\text{otherwise } \\end{cases} \\] <p>That is, we allow a single morphism from \\(U\\) to \\(V\\) if and only if  \\(U \\subset V\\).  Now suppose \\(Y\\) is another topological space. Then for each open subset  \\(U\\) of \\(X\\) we may construct the set </p> \\[ C(U) = \\{ f: U \\to Y \\mid f \\text{ is continuous } \\}.     \\] <p>Observe that if \\(U \\subset V \\subset X\\) are open sets, then  there is function </p> \\[ \\rho_U^V: C(V) \\to C(U) \\] <p>where each \\(f: V \\to Y\\) is mapped to its restriction \\(f|_U: U \\to Y\\). What follows is an important observation: If we have a chain of three open subsets \\(U \\subset V \\subset W\\),  then any continuous function \\(f: W \\to Y\\) can be restricted to \\(f|_V: V \\to Y\\),  which can then be restricted to \\(f|_V|_U: U \\to Y\\). However, we obtain the same  result if we instead just restrict \\(f\\) to \\(U\\) in the first place. That is,  \\(f|_V|_U = f|_U\\). In our notation, this implies that </p> \\[ \\rho_V^W \\circ \\rho_U^V = \\rho_U^W.  \\] <p>What we have on our hands is a contravariant functor (since the relation  \\(U \\subset V\\) induces a function \\(C(V) \\to C(U)\\)). As covariant functors  are easier to think about, we can equivalently express this as a covariant functor:</p> \\[ C: **Open**(X)\\op \\to **Set** \\] <p>which is an example of the concept of a presheaf. </p> <p> A presheaf (of sets on a topological space \\(X\\))  is a covariant functor \\(F: **Open**(X)\\op \\to **Set**\\).  We spell out the details: A presheaf consists of  \\begin{description} \\itemsep 0.25cm \\item[(PS1)] an assignment of open sets \\(U \\subset X\\) to sets \\(F(U)\\) \\item[(PS2)] a function \\(\\rho_{U}^{V}: F(V) \\to F(U)\\) whenever \\(U \\subset V\\) such that\\ [-0.3cm] \\begin{description} \\itemsep 0.25cm \\item[(Identity)] \\(\\rho_{U}^{U}: F(U) \\to F(U)\\) is the identity \\item[(Composition)] \\(\\rho_V^W \\circ \\rho_U^V = \\rho_U^W\\) whenever \\(U \\subset V \\subset W\\) \\end{description} \\end{description} \\noindent A morphism of presheaves is a natural transformation between presheaves. </p> <p>A few comments are to be made about this definition. </p> <ul> <li> <p>About Set. The codomain of a presheaf doesn't have to be \\(**Set**\\). Usually, the value of our presheaves are sets of functions, but sometimes such sets have additional  structure. Therefore, the codomain could be \\(**Ab**\\), \\(**Ring**\\), or another category  where the objects are sets plus some mathematical structure. In these cases, we'd obtain a presheaf of abelian groups, a presheaf of rings, and so forth.</p> </li> <li> <p>About the naming. The only reason this is called a presheaf is because,  as the reader may guess, this idea is a precursor to the concept of a sheaf. </p> </li> <li> <p>The fact that we can formulate morphisms between presheaves prompts us to  define the category of presheaves (of sets) on \\(\\cc\\) which we denote as \\(**Psh**(X, **Set**)\\).</p> </li> </ul> <p>We now offer some examples of presheaves. The examples we offer will be topological presheaves,  i.e., presheaves on \\(**Open**(X)\\) for some topological space \\(X\\). This is because  many interesting and useful examples of presheaves appear in this way. This is also done  so that we can offer our first definition of sheaf with as littel confusion as possible. </p> <p> Consider the introductory example of this section, and instead take \\(Y = \\rr\\).  Then in this case,  [ C(U) = {f: U \\to \\rr \\mid f \\text{ is continuous}}. ] <p>However, observe that \\(C(U)\\) is actually an \\(\\rr\\)-module:  if \\(f, g: U \\to \\rr\\) are continuous, then so is \\(f + g: U \\to \\rr\\). Moreover,  if \\(a \\in \\rr\\), then \\(a\\cdot f: U \\to \\rr\\) is continuous. These operations  satisfy the criteria for \\(C(U)\\) to be an \\(\\rr\\)-module. Therefore, when \\(Y = \\rr\\),  we obtain a presheaf on \\(\\rr\\)-Mod, and we may write</p> \\[ C: **Open**(X)\\op \\to **$\\rr$-Mod**. \\] <p>We will return to this example later on. </p> <p> For every open set \\(D\\) of the complex plane \\(\\mathbb{C}\\), define the  set  \\[ H(D) = \\{f: D \\to \\mathbb{C} \\mid f \\text{ is holomorphic. }\\} \\] <p>Observe that this induces a functor \\(H: **Open**(\\mathbb{C})\\op \\to **Set**\\),  and hence we have a presheaf of sets. Moreover, this is actually a \\(\\mathbb{C}\\)-module, so  that what we have is actually a presheaf of \\(\\mathbb{C}\\)-modules; hence we  write \\(H: **Open**(\\mathbb{C})\\op \\to \\mathbb{C}**-Mod**\\). </p> <p> Let \\(X\\) be a topological space, and consider the functor \\(B: **Open**(X)\\op \\to **\\)\\rr\\(-Mod**\\),  defined as follows. For an open subset \\(U \\subset X\\), we define  \\[ B(U) = \\{f: U \\to \\rr \\mid f \\text{ is bounded}\\}. \\] <p>By bounded, we mean that \\(f: U \\to \\rr\\) is bounded if there exists a constant  \\(M \\in \\rr\\) such that, for all \\(x \\in U\\), \\(|f(x)| \\le M\\).  This example becomes important later, specifically in that it is an example  of a presheaf which is not a sheaf (yet to be defined).  </p> <p>Our next goal is to offer our first definition of a sheaf. To motivate the definition, we will  consider our introductory example. </p> <p>Recall our presheaf \\(C: **Open**(X)\\op \\to **Set**\\). Consider an open set  \\(U\\) with an open cover \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\). Then every  \\(f: U \\to \\rr\\) in \\(C(U)\\) corresponds to an element of \\(F(U_i)\\) for all \\(i\\); it is simply  the restriction \\(f|_{U_i} \\to \\rr\\). </p> <p>A natural question is the converse: If I have such an open cover \\(\\mathcal{U}\\) of \\(U\\),  and a family of continuous functions \\(f_i: U_i \\to Y\\), is there a continuous  function \\(f: U \\to \\rr\\) such that \\(f|_{U_i} = f_i\\) for all \\(i\\)? </p> <p>Immediately, the answer is no: simply take a family in which the functions disagree on their overlaps.  Thus, what if our family does agree on their overlaps? This would mean that, for every  pair \\(i, j\\), </p> \\[ f_{i}|_{U_i \\cap U_j} = f_j|_{U_i \\cap U_j}. \\] <p>(Of course, \\(U_i \\cap U_j\\) could be empty; but we don't know in general,  so we just play it safe and consider all pairs \\(i, j \\in \\lambda\\).) The answer now is affirmative, there is an fact a \\(f: U \\to \\rr\\) where \\(f|_{U_i} = f_i\\)  for all \\(i\\).  Thus we see that \\(C: **Open**(X)\\op \\to **Set**\\) is a rather special type  of presheaf, and we call this kind of functor a sheaf.</p> <p> Let \\(X\\) be a topological space. A topological sheaf (of sets) on \\(X\\) is a presheaf  \\(F: **Open**(X)\\op \\to **Set**\\) such that, for every open  set \\(U\\) and any open cover \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\) of \\(U\\),  the following two properties hold: \\begin{description} \\itemsep 0.25cm \\item[(SH1)] If \\(f\\), \\(g \\in F(U)\\) are such that \\(f|_{U_i} = g|_{U_i}\\) for all  \\(i \\in \\lambda\\), then \\(f = g\\).  <p>\\item[(SH2)]  Suppose that for all \\(i \\in \\lambda\\), we have \\(h_i \\in F(U_i)\\)  such that \\(h_i|_{U_i \\cap U_j} = h_j|_{U_i \\cap U_i}\\) (i.e., a family  of \\(h_i\\) which agree on all possible overlaps). Then there exists a  \\(h \\in F(U)\\) such that \\(h|_{U_i} = h_i\\) for all \\(i \\in \\lambda\\). </p> <p>\\end{description} A few comments about this definition:</p> <ul> <li> <p>In our definition, SH2 is our main axiom of focus. We add  SH1 so that the given \\(h \\in F(U)\\) in SH2 is necessarily  unique.</p> </li> <li> <p>Once again, the codomain of our sheaf does not have to \\(**Set**\\).  We will see this in a few examples. </p> </li> <li> <p>With the notion of a morphism of sheaves, we can define the \\textbf{category  of topological sheaves (of Sets)}, denoted \\(**Sh**(X, **Set**)\\), to be the category with objects sheaves and morphisms with natural transformations.</p> </li> </ul> <p>We end this definition by defining a morphism of sheaves; it is simply  a natural transformation between sheaves.  </p> <p>We now offer a few examples of topological sheaves.</p> <p> Consider again the introductory example \\(C: **Open**(X)\\op \\to \\rr**-Mod**\\).  We show that this is a sheaf. Towards that goal, let \\(U\\) be an open with open cover \\(\\mathcal{U} = \\{U_i\\}_{i \\in \\lambda}\\). <p>\\begin{description} \\itemsep 0.25cm \\item[(SH1)] Suppose \\(f, g: U \\to \\rr\\) are continuous functions which agree on the  overlaps of the open cover. Then in this case it's clear that \\(f = g\\). </p> <p>\\item[(SH2)] Suppose \\(f_i: U_i \\to \\rr\\) is a family of continuous functions such that  \\(f_i|_{U_i \\cap U_j} = f_j|_{U_i \\cap U_j}\\) for all \\(i, j \\in \\lambda\\).  Construct a function \\(\\phi: U \\to \\rr\\) pointwise as follows: Given a \\(p \\in U\\),  there exists some \\(k \\in \\lambda\\) such that \\(p \\in U_k\\). Therefore, let \\(\\phi(p) = f_k(p)\\); agreement on overlaps  makes this well defined.</p> <p>We show that \\(\\phi\\) is continuous. For an open set \\(V\\) of \\(\\rr\\), define \\(\\phi^{-1}(V) = \\bigcup_{i \\in \\lambda}f_i^{-1}(V)\\). As this is a union of open sets, \\(\\phi^{-1}(V)\\) is open and hence \\(\\phi\\) is continuous. \\end{description} As we've satisfied SH1 and SH2, we see that this is a sheaf. </p> <p>A reader familiar with topology will note that our work towards the axiom  SH2 in the last example is nothing more than the standard proof of the  Pasting Lemma from topology. </p> <p> Consider the presheaf \\(H: **Open**(\\mathbb{C}) \\to \\mathbb{C}**-Mod**\\)  which sends open sets of \\(\\mathbb{C}\\) to the \\(\\mathbb{C}\\)-module of holomorphic  functions defined on them. <p>This is also a sheaf, which we verify. Let \\(U\\) be an open set of \\(\\mathbb{C}\\)  and \\(\\mathcal{U}=\\{U_i\\}_{i\\in\\lambda}\\) an open cover. \\begin{description} \\itemsep 0.25cm \\item[(SH1)] This is true in the same was as the last example. </p> <p>\\item[(SH2)]  Let \\(f_i: U_i \\to \\mathbb{C}\\) be a family of holomorphic functions such that each \\(f_i\\) agree on all possible overlaps. Define \\(f: U \\to \\mathbb{C}\\) in  the obvious way. To show that \\(f\\) is holomorphic on \\(U\\), pick any \\(p \\in U\\). Then \\(p \\in U_k\\) for some \\(k\\), and hence  there exists an open set \\(D_k\\) of \\(p\\) such that  \\(f_k(z) = \\sum_{n = 1}^{\\infty}a_n(z - p)^n\\), i.e., it has a power series  representation. This then gives us a well-defined power series representation  for \\(f\\), so that \\(f\\) is holomorphic. \\end{description}</p> <p></p> <p>We now offer an example of a presheaf which is not a sheaf.</p> <p> Consider the presheaf \\(B: **Open**(X) \\to \\rr**-Mod**\\) where \\(B(U)\\) is the set of all bounded functions \\(f: U \\to \\rr\\).  <p>In general, this is not a sheaf; axiom SH2 is usually  broken. For example, take \\(X = \\rr\\), and consider  the open set \\((0, 1)\\), with the open cover given by  the sets \\(\\{ U_i = \\left(\\frac{1}{i}, 1 \\right) \\mid i = 1, 2, \\dots \\}\\). Observe that the functions </p> \\[ f_i(x): \\left(\\frac{1}{i},\\, 1 \\right) \\to \\rr \\qquad f_i(x) = \\frac{1}{x} \\] <p>agree on their overlaps, but clearly there is no bounded function  \\(f: (0, 1) \\to \\rr\\) such that \\(f|_{U_i} = f_i\\) for all \\(i\\).  Hence,  this is not a sheaf. </p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Arbitrary%20Products%20and%20Coproducts%20in%20Categories/","title":"3.5. Arbitrary Products and Coproducts in Categories","text":"<p>In this section, we perform a construction that allows us to have finite products and coproducts  in a category. Once we achieve that construction, we easily generalize our work  to obtain a definition for arbitrary products and coproducts in a category.</p> <p> Let \\(\\mathcal{D}_n\\) be the discrete category with \\(n\\)-many objects (we use the letter \\(\\dd\\) for \"discrete\").  We will often visualize \\(\\mathcal{D}_n\\) as below.   \\[\\begin{align} \\begin{tikzpicture} \\filldraw[yellow!30, rounded corners] (-4.3, -0.7) rectangle (4.3,0.7); \\node at (0,0){ \\begin{tikzcd}[ampersand replacement= \\&amp;] \\bullet_1 \\arrow[out=120,in=60,looseness=3,loop] \\&amp; \\bullet_2 \\arrow[out=120,in=60,looseness=3,loop] \\&amp; \\bullet_3 \\arrow[out=120,in=60,looseness=3,loop] \\&amp; \\cdots  \\&amp;  \\bullet_n \\arrow[scale = 1, out=123,in=57,looseness = 3, loop] \\end{tikzcd} }; \\end{tikzpicture} \\end{align}\\] <p>Note that  a functor \\(F: \\dd_n \\to \\cc\\) is one which simply picks out \\(n\\) different objects \\(A_1\\), \\(A_2,  \\dots\\), \\(A_n\\) of \\(\\cc\\):</p> \\[ F(\\bullet_1) = A_1, \\quad F(\\bullet_2) = A_2, \\quad  \\dots, \\quad F(\\bullet_n) = A_n. \\] <p>This category allows us to make the following definition. </p> <p>\\begin{definition} Let \\(\\cc\\) be a category. The \\(n\\)-th diagonal functor \\(\\Delta_n: \\cc \\to \\cc^{n}\\), is the functor defined as follows.  \\begin{description} \\item[On Objects.] For an object \\(C\\), we have that \\(\\Delta_n(C) = (\\overbrace{C, C, \\dots, C}^{n\\text{-many copies}})\\). \\item[On Morphisms.] For a morphism \\(f: A \\to B\\) in \\(\\cc\\), we have that  </p> \\[ \\Delta_n(f: A \\to B) = (f, f, \\dots, f): \\Delta_n(A) \\to \\Delta_n(B). \\] <p>\\end{description}  The diagonal  functor is also sometimes informally called the  \"copy\" functor, since it is literally just copying data. We now  make some observations. </p> <ul> <li>[(1)] For each object \\(C \\in \\cc\\),  we can interpret the object \\(\\Delta_n(C) \\in \\cc^n\\) as a functor</li> </ul> \\[ \\Delta_n(C): \\mathcal{D}_n \\to \\cc \\] <p>where \\(\\Delta_n(C)\\) sends \\(\\bullet_i\\) to  \\(C\\) for all \\(i= 1, 2, \\dots, n\\).</p> <ul> <li>[(2)] Thus, we may also regard the \\(n\\)-th diagonal functor as a functor as below.</li> </ul> \\[ \\Delta_n: \\cc \\to \\text{Fun}(\\mathcal{D}_n, \\cc) \\qquad C \\mapsto (\\Delta_n(C) :\\mathcal{D}_n \\to \\cc). \\] <p>In this interpretation, every morphism \\(f:C \\to C'\\) is interpreted as a natural  transformation \\(\\Delta_n(f): \\Delta_n(C) \\to \\Delta_n(C')\\). </p> <ul> <li>[(3)] Consider a functor \\(F: \\dd_n \\to \\cc\\) such that \\(F(\\bullet_i) = A_i \\in \\cc\\).  For each \\(C \\in \\cc\\), a natural transformation </li> </ul> \\[ \\eta: \\Delta_n(C) \\to F \\] <p>will simply correspond to \\(n\\)-many morphisms \\(\\eta_1, \\dots,  \\eta_n\\) where</p> \\[ \\eta_i: \\Delta_n(C)(\\bullet_i) \\to  F(\\bullet_i)  \\implies  \\eta_i: C \\to A_i. \\] <p>\\end{definition} With this notation clarified, we now can propose  our  definition of a  product. </p> <p>[Finite Product and Coproduct Definition] Let \\(\\cc\\) be a category. Let \\(A_1\\), \\(A_2, \\dots\\), \\(A_n\\) be objects of \\(\\cc\\). Let  \\(F: \\mathcal{D}_n \\to \\cc\\) be the functor such that \\(F(\\bullet_i) = A_i\\). <ul> <li> <p>The product of \\(A_1, A_2, \\dots, A_n\\)  is an object \\(P\\) of \\(\\cc\\) equipped with a (natural transformation) $ \\displaystyle p: \\Delta_n\\left( P \\right) \\to F$  such that  \\begin{center} \\(\\displaystyle \\left( P, p: \\Delta_n\\left( P \\right) \\to F \\right)\\) is universal from  \\hyperref[definition:universal_morphism_from_D_to_F]{\\textcolor{blue}{\\(\\Delta_n\\) to \\(P\\)}.} \\end{center}        In the case where the product \\(P\\) exists, we write \\(P = \\prod_{i = 1}^{n} A_i\\). </p> </li> <li> <p>The coproduct of \\(A_1, A_2, \\dots, A_n\\) is an object \\(C\\) of \\(\\cc\\) equipped with  a (natural transformation) \\(i: F \\to \\Delta_n(C)\\) such that  \\begin{center} \\((C, i: F \\to \\Delta_n(C))\\) is universal from \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\(C\\) to \\(\\Delta_n\\)}}. \\end{center}       In the case where the coproduct \\(C\\) exists, we write \\(C = \\coprod_{i = 1}^{n} A_i\\) </p> </li> </ul> <p></p> <p> By Observation (3) as above, if \\(\\displaystyle p: \\Delta_n\\left( \\prod_{i=1}^{n}A_i \\right) \\to F\\) is a natural transformation, then it corresponds to \\(n\\)-many morphisms  \\[ p_k: \\prod_{i=1}^{n}A_i \\to A_k \\qquad k = 1, 2, \\dots, n \\] <p>This matches our intuition: A product of \\(n\\)-objects should always have  \\(n\\)-many morphisms between the product and each of its factors. </p> <p>Similarly, a natural transformation \\(i: F \\to \\Delta_n\\left( \\coprod_{i = 1}^{n}A_i \\right)\\) corresponds to  \\(n\\)-many morphisms </p> \\[ i_k: A_k \\to \\coprod_{i = 1}^{n}A_i \\qquad k = 1, 2, \\dots, n \\] <p>which again matches our intuition: A coproduct of \\(n\\)-objects should have  \\(n\\)-many morphisms between each of its factors and the coproduct. </p> <p>We now have everything we need to define arbitrary products and coproducts, including  infinite ones. We just need to specify some notation that we will use. Towards that goal, let \\(\\lambda\\) be some indexing set.  </p> <ul> <li> <p>Define \\(\\dd_{\\lambda}\\) to be the discrete category consisting of one object \\(\\bullet_i\\) for each \\(i \\in  \\lambda\\). (In particular, \\(\\dd_{\\lambda}\\) is now possibly infinite.)</p> </li> <li> <p>Define the \\(\\lambda\\)-diagonal functor to be the functor \\(\\Delta_{\\lambda}: \\cc \\to \\text{Fun}(\\dd_{\\lambda}, \\cc)\\) where \\(\\Delta_{\\lambda}(C): \\dd_{\\lambda} \\to \\cc\\)  sends each \\(\\bullet_i\\) to \\(C\\) for all \\(i \\in \\lambda\\).</p> </li> </ul> <p>[Arbitrary Product and Coproduct Definition] Let \\(\\cc\\) be a category, and consider an arbitrary set of objects \\(\\{A_i\\}_{i \\in \\lambda}\\) of \\(\\cc\\), \\(\\lambda\\) some indexing set. Let \\(F: \\dd_{\\lambda} \\to \\cc\\) be the functor such that \\(F(\\bullet_{i}) = A_i\\) for \\(i \\in \\lambda\\). <ul> <li> <p>The product of \\(\\{A_i\\}_{i \\in \\lambda}\\) is the object  \\(P\\) of \\(\\cc\\) equipped with a (natural transformation) \\(\\displaystyle p: \\Delta_{\\lambda}\\left( P\\right) \\to F\\)  such that  \\begin{center} \\(\\left( P, \\Delta_{\\lambda}\\left( P \\right) \\to F \\right)\\) is universal from  \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\(\\Delta_{\\lambda}\\) to \\(P\\)}.} \\end{center}           In the case where the product \\(P\\) exists, we write \\(P = \\prod_{i \\in \\lambda}A_i.\\)</p> </li> <li> <p>The coproduct of \\(\\{A_i\\}_{i \\in \\lambda}\\) is the object  \\(C\\) of \\(\\cc\\) equipped with a (natural transformation) \\(i: F \\to \\Delta_{\\lambda}(C)\\) such that  \\begin{center} \\((C, i:  F \\to \\Delta_{\\lambda}(C))\\) is universal from  \\hyperref[definition:universal_morphism_from_D_to_F]{\\textcolor{blue}{\\(C\\) to \\(\\Delta_{\\lambda}\\)}}. \\end{center}       </p> </li> </ul> <p></p> <p> Notice the inherent duality present in the definition of a product and coproduct.  This is one of the reasons category theory is nice; one now has a new perspective of  understanding, for example, the free product operation and the  group product operation in Grp; they're dual concepts! </p> <p>Since products and coproducts of objects are universal objects, we obtain some  nice results since we already know how universal objects operate. Before introduce  such results, we require the following lemma. </p> <p> Let \\(\\cc\\) be a locally small category, and let \\(\\{A_i\\}_{i \\in \\lambda}\\)  be objects of \\(\\cc\\). Suppose their product exists in \\(\\cc\\). Then the functor  \\[ \\prod_{i \\in \\lambda}\\hom_{\\cc}(-, A_i): \\cc \\to **Set** \\] <p>which sends an object \\(C\\) to the set \\(\\prod_{i \\in \\lambda}\\hom_{\\cc}(C, A_i)\\)  is representable by the functor </p> \\[ \\hom_{\\text{Fun}(\\dd_{\\lambda}, \\cc)}(\\Delta_{\\lambda}(-), F): \\cc \\to **Set**. \\] <p></p> <p>The proof is left as an exercise. It is not difficult to show; it simply requires realizing that there  is a natural bijection between \\(\\prod_{i \\in \\lambda}\\hom_{\\cc}(C, A_i)\\) and  \\(\\hom_{\\text{Fun}(\\dd_{\\lambda}, \\cc)}(\\Delta_{\\lambda}(C), F)\\) for each \\(C \\in \\cc\\).</p> <p>Using all of our previous work we now have the following proposition.</p> <p> Let \\(\\cc\\) be a locally small category, and let \\(\\{A_i\\}_{i \\in \\lambda}\\) be a set of objects in \\(\\cc\\).  Denote \\(F: \\dd_{\\lambda} \\to \\cc\\) where \\(F(\\bullet_i) = A_i\\) for all \\(i \\in \\lambda\\). <ul> <li>If the product \\(\\prod_{i \\in \\lambda}A_i\\)  exists in \\(\\cc\\), then for each object  \\(C\\) of \\(\\cc\\), we have the natural bijection</li> </ul> \\[ \\prod_{i \\in \\lambda}\\hom_{\\cc}(C, A_i) \\cong \\hom_{\\cc}\\left(C,\\, \\prod_{i \\in \\lambda}A_i\\right) \\] <ul> <li>If the coproduct \\(\\coprod_{i \\in \\lambda}A_i\\) exists  in \\(\\cc\\), then for each object \\(C\\) of \\(\\cc\\), we have the natural bijection </li> </ul> \\[ \\prod_{i \\in \\lambda}\\hom(A_i, C) \\cong  \\hom_{\\cc}\\left( \\coprod_{i\\in \\lambda}A_i, \\, C \\right). \\] <p></p> <p> We only prove the first result, since the second follows similarly.  Since \\(\\prod_{i \\in \\lambda}A_i\\) exists in \\(\\cc\\), we know that this implies  \\(\\prod_{i \\in \\lambda}A_i\\) is equipped with a natural transformation  \\(p: \\Delta_{\\lambda}\\left(\\prod_{i \\in \\lambda}A_i\\right) \\to F\\) such that  \\((\\prod_{i \\in \\lambda}A_i, p)\\) is universal from \\(\\Delta_{\\lambda}\\) to \\(P\\).  <p>From this perspective, we can apply the result of Exercise \\hyperref[exercise:universality_bijection]{3.2.1}  to conclude that, for each object \\(C\\), we have the natural bijection below. </p> \\[ \\hom_{\\cc}\\left(C, \\,\\prod_{i \\in \\lambda}A_i\\right) \\cong \\hom_{\\text{Fun}(\\dd_{\\lambda}, \\cc)}(\\Delta_{\\lambda}(C), F). \\] <p>However, we know from Lemma \\ref{lemma:product_of_hom_sets} that there is a natural bijection </p> \\[ \\hom_{\\text{Fun}(\\dd_{\\lambda}, \\cc)}(\\Delta_{\\lambda}(C), F) \\cong  \\prod_{i \\in \\lambda}\\hom_{\\cc}(C, A_i). \\] <p>Thus we have a natural bijection</p> \\[ \\prod_{i \\in \\lambda}\\hom_{\\cc}(C, A_i) \\cong \\hom_{\\cc}\\left(C, \\,\\prod_{i \\in \\lambda}A_i\\right) \\] <p>as desired.</p> <p>The second result is left as an exercise (we outline the steps for the reader).</p> <p></p> <p> Note that the above proposition is saying something very deep and beautiful  about products and coproducts as a concept. Moreover, also note that a direct proof would have been very long-winded  and complicated, but that our previous work made it possible to give a proof consisting  of a few lines. Thus, a categorical perspective is evidently sometimes useful.  </p> <p>We now introduce the following interesting property. This property becomes  an important observation when we begin look at abelian categories. </p> <p> Let \\(\\cc\\) be a category and let \\(\\{A_i\\}_{i \\in \\lambda}\\) be a set of objects in \\(\\cc\\). Suppose the product \\(\\prod_{i \\in \\lambda}A_i\\) and coproduct \\(\\coprod_{i \\in \\lambda}A_i\\) exist in \\(\\cc\\). Then there is a canonical morphism  \\[ \\phi:\\prod_{i \\in \\lambda}A_i \\to \\coprod_{i \\in \\lambda}A_i  \\] <p>in \\(\\cc\\). </p> <p> Let \\(F: \\dd_{\\lambda} \\to \\cc\\) be the functor where \\(F(\\bullet_i) = A_i\\). Then the product and coproduct are equipped with the natural transformations as below.  \\[ \\Delta_{\\lambda}\\left( \\prod_{i \\in \\lambda}A_i \\right) \\to F \\quad \\quad F \\to \\Delta_{\\lambda}\\left( \\coprod_{i \\in \\lambda}A_i \\right) \\] <p>Then we can compose them to obtain the natural transformation</p> \\[ \\Delta_{\\lambda}\\left( \\prod_{i \\in \\lambda}A_i \\right) \\to \\Delta_{\\lambda}\\left( \\coprod_{i \\in \\lambda}A_i \\right). \\] <p>By the universal property of the coproduct, this implies a unique \\(\\phi: \\prod_{i \\in \\lambda}A_i \\to \\coprod_{i \\in \\lambda}A_i\\) such that the diagram below commutes. </p> <p></p> <p> Here is one of our first uses of the word \"canonical.\" This is not an adjective  that adds detail to our morphism (e.g., an extra mathematical property), but it is a word we superfluously wrote to emphasize to the reader  that morphisms of a given form cannot always be found in categories.  <p>The word \"canonical\" is often used in category theory language, but it is never really defined  because its always secretly assumed that everyone knows what it means.  It's a useful word, so we will use it later on, but again: it means nothing  more than \"There exists an obvious morphism of a given form.\" </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] Prove Lemma \\ref{lemma:product_of_hom_sets}. (Note: the notation and statement may make  it look harder than it actually is.)</p> </li> <li> <p>[2.] Complete the proof of Proposition \\ref{proposition:product_coproduct_natural_bijection} as follows.  \\begin{itemize}</p> </li> <li> <p>[i.] Show that the functor </p> </li> </ul> \\[ \\prod_{i \\in \\lambda} \\hom_{\\cc}(A_i, -): \\cc \\to **Set** \\] <p>is representable by the functor </p> \\[ \\hom_{\\text{Fun}(\\dd_{\\lambda}, \\cc)}(F, \\Delta_{\\lambda}(-)): \\cc \\to **Set** \\] <ul> <li>[ii.] Using (i), Proposition \\ref{proposition:universality_bijection}, and  interpreting coproducts as universal objects, prove that </li> </ul> \\[ \\prod_{i \\in \\lambda}\\hom(A_i, C) \\cong  \\hom_{\\cc}\\left( \\coprod_{i\\in \\lambda}A_i, \\, C \\right). \\] <ul> <li> <p>[3.] Let \\(P\\) be a preorder with binary relation \\(\\le\\).  Consider a subset \\(A \\subset P\\) where \\(A = \\{a_i \\in P \\mid i \\in \\lambda\\}\\)  with \\(\\lambda\\) some indexing set. \\begin{itemize}</p> </li> <li> <p>[(i.)] Regarding \\(P\\) as a thin category, prove that the product \\(p = \\displaystyle \\prod_{i \\in \\lambda}a_i\\), when it exists,  is the supremum of \\(A\\).  \\ Hint: Recall that, if \\(X\\) is a preorder,  the supremum of a set \\(S \\subset X\\) is the element \\(s \\in X\\)  such that if \\(a_i \\le s'\\) for all \\(i \\in \\lambda\\), then  \\(s \\le s'\\).</p> </li> <li> <p>[(ii.)]  We know that the dual of the product is the coproduct.  Can you guess what the coproduct \\(\\displaystyle \\coprod_{i \\in \\lambda}a_i\\) in \\(P\\) is  in this case? Prove it. </p> </li> </ul> <p>\\item[4.] Let \\(\\cc\\) and \\(\\dd\\) be categories.  Consider the functor category \\(**Fun**(\\cc, \\dd)\\). What is a product in  this category? What conditions do we need to place on \\(\\cc\\) and \\(\\dd\\) for this  product to exist? \\end{itemize}       \\end{itemize}</p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Equalizers%20and%20Coequalizers/","title":"3.7. Equalizers and Coequalizers","text":"<p>We introduce equalizers and coequalizers as further examples of limits, and therefore examples of universal  morphisms. Equalizers and coequalizers are important constructions  that are useful for proofs and definitions that we will encounter  later on. We first introduce examples of equalizers and coequalizers. </p> <p> Let \\(G\\) and \\(H\\) be groups, and consider a pair of homomorphisms \\(\\phi\\) and \\(\\psi\\) as below. <p> Now consider the homomorphism \\(\\phi - \\psi:G \\to H\\).  Then observe that </p> \\[ \\ker(\\phi - \\psi) = \\bigg\\{ g \\in G \\;\\bigg|\\; (\\phi - \\psi)(g) = 0   \\bigg\\} \\] <p>and note that this is also the set of all \\(g \\in G\\) in which \\(\\phi\\) and \\(\\psi\\) agree. In fact, it is the smallest such set, a notion we can make precise  by the following observation: If \\(G'\\) is another group with  \\(\\vartheta: G' \\to G\\) another map such that \\(\\phi \\circ \\vartheta = \\psi \\circ \\vartheta\\),  then there exists a unique \\(i: G' \\to \\ker(\\phi - \\psi)\\)  such that the diagram below commutes. \\  Note above that \\(i: \\ker(\\phi - \\psi) \\to G\\) is the inclusion morphism.  Also note that this construction is possible for any two parallel  group homomorphisms.  </p> <p> In Set, equalizers always exist. Simply let \\(D = \\{x \\in A \\mid f(x) = g(x)\\}\\), and let \\(e: D \\to A\\) by the inclusion morphism into \\(A\\). Clearly we'll have that \\(f \\circ e = g \\circ e\\).  <p>Now for any \\(h: C \\to A\\) such that \\(f \\circ h = g \\circ h\\), we see that the image of \\(h\\) must be a subset of \\(D\\). Hence there exists a unique inclusion morphism \\(i: C \\to D\\), which shows that \\(e\\) in fact is the equalizer in Set for any \\(f, g: A \\to B\\).        </p> <p>[Nice Equalizer Definition] Let \\(\\cc\\) be a category and consider a pair of parallel morphisms  \\(f, g: A \\to B\\). The equalizer of \\(f\\) and \\(g\\) is a pair  \\((E, e: E \\to A)\\) such that \\(f \\circ e = g \\circ e\\) with the  following property. For any other morphism \\(h: C \\to A\\) such that \\(f \\circ h = g \\circ h\\), there exists a unique morphism \\(f': C \\to E\\) such that the following commutes. \\  </p> <p>[Equalizer as a Limit] Let \\(\\cc\\) be a category and consider a pair of parallel morphisms  \\(f, g: A \\to B\\). Let \\(J\\) be the category with two elements and two nontrivial morphisms as below. \\  and let \\(F: J \\to \\cc\\) be the functor such that  \\begin{tikzcd} F(\\textcolor{Blue}{\\bullet} \\arrow[r, shift right = -0.5ex] \\arrow[r, shift right = 0.5ex] &amp; \\textcolor{Orange}{\\bullet})       \\end{tikzcd} \\(=\\) \\begin{tikzcd} A \\arrow[r, shift right = -0.5ex, \"f\"] \\arrow[r, shift right = 0.5ex, swap, \"g\"] &amp; B \\end{tikzcd} We define the equalizer of \\(f\\) and \\(g\\) to be  limit \\((\\Lim F, e: \\Delta(\\Lim F) \\to F)\\) of \\(F\\).  </p> <p> Let \\(\\cc\\) be a category, and suppose \\(e: D \\to A\\) is an equalizer for a pair of morphisms \\(f, g: A \\to B\\). Then \\(e\\) is monic.  </p> <p> Consider any pair \\(f_1, f_2: C \\to D\\) such that \\(e \\circ f_1 = e \\circ f_2\\). Then we have that  \\  Since \\(e \\circ f_1 = e \\circ f_2\\), we see that  \\[\\begin{align*} f \\circ e = g \\circ e &amp;\\implies f \\circ (e \\circ f_1) = g \\circ  (e \\circ f_1)\\\\ &amp;\\implies f \\circ (e \\circ f_1) = g \\circ (e \\circ f_2). \\end{align*}\\] <p>Hence we see \\(e \\circ f_1 = e \\circ f_2 : C \\to D\\) is another morphism which is equalized by \\(f\\) and \\(g\\).  \\  By the universality of the equalizer \\(e: D \\to A\\), we know that there must exist a unique morphism \\(f': C \\to D\\) such that </p> \\[ e \\circ f' = e \\circ f_1 = e \\circ f_2. \\] <p>Since \\(f'\\) is unique, we are forced to conclude that \\(f_1 = f_2\\). Hence \\(e \\circ f_1 = e \\circ f_2 \\implies f_1 = f_2\\), so that \\(e: D \\to A\\) is monic. </p> <p> Let \\(\\cc\\) be a category with a zero object \\(Z\\) of \\(\\cc\\). That is, an object which is both initial and terminal, such that for any objects \\(A, B\\) of \\(\\cc\\) there exists a unique pair of morphisms \\(f, g\\) such that  \\  Denote \\(f \\circ g = 0\\) as the zero arrow (any morphism which passes through \\(z\\) is a zero arrow). <p>Now we define the cokernel a morphism \\(f: A \\to B\\) to be an arrow \\(u:B \\to C\\) where </p> <ul> <li> <p>[1.] \\(u \\circ f = 0: A \\to C\\) </p> </li> <li> <p>[2.] If \\(h: B \\to D\\) has the property that \\(h \\circ f = 0\\), then \\(h = h' \\circ u\\) for a unique arrow \\(h': B \\to D\\).</p> </li> </ul> <p>Visually, this becomes  \\ </p> <p>  The cokernel is a special object in Ab, as it plays a role in the concept of exact sequences and hence homology as well. The cokernel of a homomorphism \\(f: G \\to H\\) is the projection \\(H \\to H/\\im(G)\\), a quotient group of \\(B\\). This is often written as </p> \\[ \\text{coker}(f) = H/\\im(G). \\] <p>\\subsection*{\\underline{Coequalizers.}}</p> <p> Let \\(\\cc\\) be a category and consider two morphisms \\(f, g: A \\to B\\) in \\(\\cc\\). The coequalizer of \\((f, g)\\) is a morphism \\(u: B \\to D\\) such that  <ul> <li> <p>[1.] \\(u \\circ f = u \\circ h\\)</p> </li> <li> <p>[2.] If \\(h: B \\to C\\) has the property that \\(h \\circ f = h \\circ g\\), then there exists a unique morphism \\(h':D \\to C\\) such that  \\(h = h' \\circ u\\).  </p> </li> </ul> <p>This may not always exist.  We can represent this with the following commutative diagram.\\  \\textcolor{NavyBlue}{Note that we can interpret a coequalizers as a morphism which uniquely \"flattens\" morphisms, and for any other morphism which also \"flattens\" is related to the original coequalizer.} \\  </p> <p>With coequalizers, we get the following nice result. </p> <p> All coequalizers are epimorphisms. </p> <p>Coequalizers can also be realized as universal arrows. First consider the category 2, containing two objects and two nontrivial morphisms. Since there are only two objects, the two nontrivial morphisms have the same domain and codomain. Now consider the functor category \\(\\cc^**2**\\) where</p> <ul> <li> <p>[1.] Objects are functors \\(F: **2** \\to \\cc\\), whose image is therefore a pair of morphism \\(f, g: A \\to B\\) in \\(\\cc\\)</p> </li> <li> <p>[2.] Morphisms are natural transformations, which are therefore a pair of arrows \\(h : A \\to A'\\) and \\(k: B \\to B'\\) so that</p> </li> </ul> <p>\\  is a commutative diagram. Finally consider the diagonal functor \\(\\Delta: \\cc \\to \\cc^**2**\\) where </p> \\[\\begin{align*} C &amp;\\longmapsto (1_C, 1_C)\\\\ r: C \\to C' &amp;\\longmapsto (r, r). \\end{align*}\\] <p>Now consider a pair \\(f, g: A \\to B\\) in \\(\\cc^{**2**}\\). If we have a morphism \\(h: B \\to C\\) such that \\(h\\circ f = h \\circ g\\), then this is the same thing as a morphism \\((hf, hg): (f, g) \\to (1_C, 1_C)\\) in \\(\\cc^{**2**}\\). Therefore a coequalizer \\(u: B \\to C\\) is a universal arrow from \\((f, g)\\) to \\(\\Delta\\). </p> <p> In the category Ab, the coequalizer of two group homomorphisms \\(\\phi, \\psi: G \\to H\\) is the homomorphism  \\[ \\pi: H \\to H/\\im(\\phi - \\psi). \\] <p>where \\(g' \\in H\\) maps to the coset \\(g' + \\im(\\phi - \\psi)\\). We show this as follows. \\begin{description} \\item[\\(\\bm{\\pi \\circ \\phi = \\pi \\circ \\psi}\\).] First let \\(g \\in G\\), and consider the elements </p> \\[\\begin{align*} \\pi\\circ \\phi(g) &amp;= \\phi(g) + \\im(\\phi - \\psi)\\\\ \\pi\\circ \\psi(g) &amp;= \\psi(g) + \\im(\\phi - \\psi). \\end{align*}\\] <p>If we subtract these two quantities, we get that </p> \\[\\begin{align*} \\pi \\circ \\phi(g) - \\pi \\circ \\psi(g)  &amp;=  \\big[\\phi(g) + \\im(\\phi- \\psi)\\big] -  \\big[\\psi(g) + \\im(\\phi- \\psi)\\big]\\\\ &amp;=  (\\phi(g) - \\psi(g)) + \\im(\\phi- \\psi)\\\\ &amp;= 0 + \\im(\\phi - \\psi). \\end{align*}\\] <p>Since their difference is zero, we see that they're equal. Hence \\(\\pi \\circ \\phi = \\pi \\circ \\psi\\). </p> <p>\\item[Universality.] Let \\(f: H \\to H'\\) be another group homomorphism such that \\(f \\circ \\phi = f \\circ \\psi\\). Then construct the morphism \\(f': H/\\im(\\phi - \\psi) \\to H'\\) where </p> \\[ h + \\im(\\phi - \\psi) \\longmapsto f(h). \\] <p>Clearly this is well defined, since  if  \\(h + \\im(\\phi - \\psi) = h' + \\im(\\phi - \\psi)\\), then this means that \\(h = h' + (\\phi - \\psi)(g)\\), so that  </p> \\[\\begin{align*} f'(h + \\im(\\phi-  \\psi)) &amp;= f(h)\\\\ &amp;= f(h' + \\phi(g) - \\psi(g))\\\\ &amp;= f(h') + f\\circ\\phi(g) - f\\circ\\psi(g)\\\\ &amp;= f(h') \\end{align*}\\] <p>where in the last step we used the fact  that \\(f \\circ \\phi = f \\circ \\psi\\). Thus we see that \\(f'\\) is a well-defined group homomorphism. Furthermore, note that \\(f = f' \\circ \\pi\\). To finally show that \\(f'\\) is unique, we suppose there exists another group homomorphism \\(k:H/\\im(\\phi-\\psi) \\to H'\\) such that \\(f = k\\circ \\pi\\). Then we see that \\(f' \\circ \\pi = k \\circ \\pi\\), which implies that \\(f' = k\\).  \\end{description} What we've shown is that for any \\(f: H \\to H'\\) such that \\(f \\circ \\phi = f \\circ \\psi\\), there exists a unique morphism \\(f' : H/\\im(\\phi - \\psi) \\to H'\\) such that \\(f = f' \\circ \\pi\\). Thus we see that \\(\\pi\\) has the universal property of being a coequalizer.  </p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Finite%20Coproducts/","title":"3.4. Finite Coproducts","text":"<p>We now move onto the concept of coproducts in categories. We will see that this concept  is an instance of a colimit, which is yet to be defined. We build intuition  on the concept with the special concept of coproducts by introducing examples. </p> <p> Let \\((G, \\textcolor{NavyBlue}{\\bigcdot})\\) and \\((H, \\textcolor{Orange}{\\bigcdot})\\) be two groups with group operations $\\textcolor{NavyBlue}{\\bigcdot}: G \\times G \\to G $ and \\(\\textcolor{Orange}{\\bigcdot}:  H \\times H \\to H\\). The  free product of \\(G\\) and \\(H\\) is the group  \\[ (G \\* H, \\bigcdot) = \\bigg\\{ g_1h_1g_2h_2\\cdots g_kh_k \\;\\bigg|\\; g_i \\in G, h_i \\in H \\bigg\\} \\] <p>with the following operation. If \\(g_1h_1\\cdots g_kh_k\\) and \\(g'_1h'_1\\cdots g'_{\\ell}h'_{\\ell}\\) are two elements of \\(G \\* H\\), then </p> \\[ (g_1h_1\\cdots g_kh_k) \\mathbin{\\bigcdot} (g'_1h'_1\\cdots g'_{\\ell } h'_{\\ell } ) =  g_1h_1\\cdots g_kh_k g'_1h'_1 \\cdots g'_{\\ell}h'_{\\ell}. \\] <p>We require the group operation to obey the following two rules. Let \\(g_1h_1\\cdots g_kh_k \\in G \\* H\\).</p> <ul> <li>If \\(g \\in G\\), then </li> </ul> \\[ g \\mathbin{\\bigcdot} (g_1h_1\\cdots g_kh_k) = (g \\mathbin{\\textcolor{NavyBlue}{\\bigcdot}} g_1)h_1\\cdots g_kh_k. \\] <ul> <li>If \\(h \\in H\\), then </li> </ul> \\[ (g_1h_1\\cdots g_kh_k)\\mathbin{\\bigcdot} h = g_1h_1\\cdots g_k(h_k \\mathbin{\\textcolor{Orange}{\\bigcdot}} h). \\] <p>The free product of two groups arise frequently in algebraic topology. Despite that its definition is somewhat complicated, we will see later that free products are in some sense dual to the concept of the product of groups.  The reader will also soon see that the naming \"free product\" is an unfortunate one  as it is somewhat misleading.</p> <p>Free products appear prominently in various statements of Van Kampen's theorem in topology; what  follows is a simplified version. If \\(X = U \\cup V\\) is a topological space with \\(U, V\\) open sets, and  if \\(U \\cap V \\ne \\varnothing\\) is path connected and simply connected, then </p> \\[ \\pi_1(X) \\cong \\pi_1(U)\\* \\pi_1(V) \\] <p>where \\(\\pi_1(X)\\) is the fundamental group of \\(X\\). (Note that since \\(X\\) is path connected,  it doesn't matter what basepoint for the fundamental group we select). </p> <p></p> <p>We will soon see that the free product is the coproduct in the category of Grp, although  such a statement should not make any sense the reader until we define what a coproduct is. </p> <p>\\begin{example} In Set, we can combine two different sets \\(X\\) and \\(Y\\) to create the  disjoint union \\(X \\amalg Y\\), which is defined to be the set </p> \\[ X \\amalg Y = \\bigg\\{ (x, 0), (y, 1) \\;\\bigg|\\;  x \\in X, y \\in Y  \\bigg\\}. \\] <p>In the above set, elements are tuples whos first coordinate is either  in \\(X\\) or \\(Y\\), and the second is some value which depends on whether or not the first  coordinate is in \\(X\\) or \\(Y\\).  I decided to make these values 0 and 1, but it is ultimately arbitrary.  We just need to make sure that these values are distinct so that we can determine if a tuple  has an element from \\(X\\) or \\(Y\\) based on the value in the second slot. For example, for a tuple \\((z, 0)\\),  we know that \\(z \\in X\\). If the tuple is of the form \\((z, 1)\\), we know that \\(z \\in Y\\). </p> <p>We perform a similar analysis as before with products, and we consider the following question. \\begin{center} \\begin{minipage}{0.8\\textwidth} Q: What  is the bare minimum amount of logical data that perfectly characterizes  the above disjoint union \\(X \\amalg Y\\)? \\end{minipage} \\end{center} Observe that we have the two inclusion functions</p> \\[\\begin{align*} &amp;i_1: X \\to X \\amalg Y \\qquad i_1(x) = (x, 0)\\\\ &amp;i_2: Y \\to X \\amalg Y \\qquad i_2(y) = (y, 1). \\end{align*}\\] <p>These two functions are equipped with the following remarkable property.  Let \\(Z\\) be some set, and suppose I have two functions</p> \\[\\begin{align*} &amp;f: X \\to X \\amalg Y \\\\ &amp;g: Y \\to X \\amalg Y. \\end{align*}\\] <p>Then there exists a unique function \\(h: X \\amalg Y \\to Z\\) such that the diagram below  commutes. </p> \\[\\begin{align} \\begin{tikzcd}[column sep = 1.4cm, row sep = 1.4cm ,ampersand replacement=\\&amp;] \\&amp; Z  \\arrow[&lt;-, dr,  \"g\"] \\arrow[&lt;-, dl, swap,\"f\"] \\arrow[&lt;-, d, dashed, \"h\"] \\&amp; \\\\ X  \\&amp;  \\arrow[&lt;-, l, \"i_1\"] X \\amalg Y  \\arrow[&lt;-, r, swap, \"i_2\"] \\&amp; Y \\end{tikzcd} \\qquad h(z, i) = \\begin{cases} f(z) &amp; \\text{if } i = 0\\\\  g(z) &amp; \\text{if } i = 1 \\end{cases} \\end{align}\\] <p>This definition of this unique \\(h: X \\amalg Y \\to Z\\) is described above on the right. With the above definition, one can easily see that the above diagram does in fact commute. We now have an answer to our question. \\begin{center} \\begin{minipage}{0.8\\textwidth} A: The disjoint union \\(X \\amalg Y\\) is characterized by two  inclusion functions \\(i_1: X \\to X \\amalg Y\\), \\(i_2: Y \\to X \\amalg Y\\),  such that, for any \\(f: X \\to Z\\), \\(g: Y \\to Z\\), there exists a unique \\(h: X \\amalg Y \\to Z\\) such that diagram \\ref{diagram:disjoint_union_diagram}  commutes. \\end{minipage} \\end{center}end{example}</p> <p>This now motivates the following definition of a coproduct. </p> <p>\\begin{definition}[Nice Coproduct Definition.] Let \\(\\cc\\) be a category with objects \\(A\\) and \\(B\\). The coproduct of \\(A\\) and \\(B\\)  is an object \\(A \\amalg B\\) of \\(\\cc\\) which is equipped with morphisms </p> \\[ i_A: A \\to A \\amalg B \\qquad i_B: B \\to A \\amalg B \\] <p>with the following universal property: For any object \\(Z\\) of \\(\\cc\\) with a pair  of morphisms \\(f: A \\to Z\\) and \\(g: B \\to Z\\), then there exists a unique morphism  \\(h: A \\amalg B \\to Z\\) such that the diagram below commutes.  \\  end{definition}</p> <p>It is now clear that, coproducts in Set exist; it is the disjoint union.</p> <p> Note that to utilize the above universal property, one requires a pair of morphisms  \\(f: A \\to Z\\) and \\(g: B \\to Z\\). That is, it is not true that, if I have a single  morphism \\(k: A \\to Z\\), then there exists a unique \\(h: A \\amalg B \\to Z\\) such that \\(h \\circ i_X = k\\).  That would be false in many cases.  </p> <p> Suppose \\(\\cc\\) is a category with an initial object \\(I\\) and a coproduct object \\(A \\amalg B\\) for every pair of objects \\(A\\) and \\(B\\). Then  \\begin{description} \\item[\\(\\bm{(i)}\\)] \\(\\cc\\) has finite coproducts.  \\item[\\(\\bm{(ii)}\\)] There exists a bifunctor \\(\\amalg: \\cc \\times \\cc \\to \\cc\\) where \\((A, B) \\mapsto A \\amalg B\\). <p>\\item[\\(\\bm{(iii)}\\)] For any three objects, we have an isomorphism </p> \\[ (A \\amalg B) \\amalg C \\cong A \\amalg (B \\amalg C) \\cong A  \\amalg B \\amalg C \\] <p>which is natural in \\(A, B\\) and \\(C\\) .</p> <p>\\item[\\(\\bm{iv}\\)] For any object \\(A\\), we have the isomorphism </p> \\[ I \\amalg A \\cong A \\cong I \\amalg A             \\] <p>natural in \\(A\\), where \\(T\\) is the initial object of the category. \\end{description} </p> <p>\\begin{definition}[Rigorous Coproduct Definition] Let \\(\\cc\\) be a category with objects \\(A, B\\). The  coproduct \\(A \\amalg B\\) of \\(A\\) and \\(B\\) is  a universal morphism </p> \\[ (A \\amalg B, i: (A,B) \\to \\Delta(A \\amalg B)) \\] <p>from \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\((A,B)\\) to \\(\\Delta\\)}}.  This means that, for any other pair \\((C, j: (A, B) \\to \\Delta(C))\\), there exists  a unique \\(h: A \\amalg B \\to C\\) such that the diagram below commutes.  \\  Visually, we have that  \\  end{definition}</p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Finite%20Products/","title":"3.3. Finite Products","text":"<p>In this section we will discuss products in categories, which will be  our first  encounter with  the  concept of a limit, something which has  yet to be defined. The concept of a limit, and the dual concept of a  colimit, form one of the central concepts of category theory. It will turn out that both  the limit and colimit concepts are a special case of a universal morphism.</p> <p> Let \\((G, \\textcolor{NavyBlue}{\\bigcdot})\\) and \\((H, \\textcolor{Orange}{\\bigcdot})\\) be two groups with group operations $\\textcolor{NavyBlue}{\\bigcdot}: G \\times G \\to G $ and \\(\\textcolor{Orange}{\\bigcdot}:  H \\times H \\to H\\). The product group  of \\(G,H\\) is the group  \\[ (G \\times H, \\bigcdot) = \\bigg\\{(g, h) \\;\\bigg|\\; g \\in G, \\, h \\in H \\bigg\\} \\] <p>whose group product works as </p> \\[ (g, h) \\, \\bigcdot \\, (g', h') = (g \\mathbin{\\textcolor{NavyBlue}{\\bigcdot}} g', h \\mathbin{\\textcolor{Orange}{\\bigcdot}} h'). \\] <p>One may check that this construction satisfies the definition of a group. </p> <p>If \\(G, H\\) are abelian groups, then the term \"group product\" is replaced  with the term direct sum (we will explain why later). In this case, the  product is denoted  \\((G \\oplus H, \\bigcdot)\\), and the group operation does not  change from above. </p> <p>Direct sums, or more generally products of groups, are frequently used in group theory. For example, they are necessary to describe the fundamental theorem  of finite abelian groups, which states that for any finite abelian group \\(A\\),  there exist primes \\(p_1, p_2, \\dots, p_n\\) and positive integers \\(\\alpha_1, \\alpha_2, \\dots, \\alpha_n\\)  such that </p> \\[ A \\cong \\zz_{p^{\\alpha_1}_1}\\oplus \\zz_{p^{\\alpha_2}_2} \\oplus \\cdots \\oplus \\zz_{p^{\\alpha_n}_n}. \\] <p>That is, every finite abelian group is the product of cycic groups of a prime-power  order.  </p> <p> Let \\((X, \\tau_X)\\) and \\((Y, \\tau_Y)\\) be two topological spaces. Using \\(X\\) and \\(Y\\), we can create a topological space \\((X \\times Y, \\tau_{X \\times Y})\\) where \\(\\tau_{X\\times Y}\\) is the product topology. There are many ways of  defining this topology, but in the finite case, we can write  \\(\\tau_{X\\times Y}\\) as \\[ \\tau_{X\\times Y}  = \\bigg\\{ U \\times V \\;\\bigg|\\; U \\in \\tau_X, V \\in \\tau_Y \\bigg\\} .  \\] <p>In the way we have presented this, this is actually the box topology, but the  reader may recall that they coincide when we take finite products. </p> <p></p> <p> In Set, we can always take two sets \\(X, Y\\) to create the cartesian  product \\(X \\times Y\\) defined as the set  \\[ X \\times Y  =  \\bigg\\{ (x,y)  \\;\\bigg|\\; x \\in X, y \\in Y \\bigg\\} \\] <p>Now consider the following question. \\begin{center} \\begin{minipage}{0.8\\textwidth} Q: What  is the bare minimum amount of logical data that perfectly characterizes  the above product \\(X \\times Y\\)? \\end{minipage} \\end{center}        Well, observe that for such a set, we have two \\textbf{projection  functions} </p> \\[\\begin{align*} &amp;p_1: X \\times Y \\to X \\qquad p_1(x, y) = x\\\\ &amp;p_2: X \\times Y \\to Y \\qquad p_2(x, y) = y. \\end{align*}\\] <p>Further, suppose that \\(f: Z \\to X\\) and \\(g: Z \\to Y\\) are two functions.  Then there exists a third \\(h: Z \\to X \\times Y\\) such that \\(p_1\\circ h = f\\) and \\(p_2\\circ h = g\\). By this description, we can deduce that  \\(h(z) = (f(z), g(z))\\).</p> \\[\\begin{align} \\begin{tikzcd}[column sep = 1.4cm, row sep = 1.4cm ,ampersand replacement=\\&amp;] \\&amp; Z  \\arrow[dr,  \"g\"] \\arrow[dl, swap,\"f\"] \\arrow[d, dashed, \"h\"] \\&amp; \\\\ X  \\&amp;  \\arrow[l, \"p_1\"] X \\times Y  \\arrow[r, swap, \"p_2\"] \\&amp; Y \\end{tikzcd} \\hspace{0.5cm} \\begin{tikzcd}[column sep = 1.4cm, row sep = 1.4cm ,ampersand replacement=\\&amp;] \\&amp; z \\arrow[dr, maps to] \\arrow[dl, maps to] \\arrow[d, maps to] \\&amp; \\\\ f(z)  \\&amp;  \\arrow[l, maps to] (f(z), g(z)) \\arrow[r, maps to] \\&amp; g(z) \\end{tikzcd} \\end{align}\\] <p>Moreover, this \\(h\\) is unique with respect to \\(f\\) and \\(g\\); Showing this is the bulk of  Exercise \\ref{section:universal_morphisms}.\\exerciseCartesianProduct. We now have an answer to our question.</p> <p>\\begin{center} \\begin{minipage}{0.8\\textwidth} A: The product \\(X \\times Y\\) is  characterized by the following data: two projection functions  \\(p_1: X\\times Y \\to X, p_2: X \\times Y \\to Y\\), such that for any  pair of functions \\(f: Z \\to X, g: Z \\to Y\\), there exists a unique third \\(h: Z \\to X \\times Y\\) such that diagram \\ref{diagram:cartesian_product} commutes. \\end{minipage} \\end{center}   </p> <p>With the above example in mind, we now introduce our first definition of a product.</p> <p>[Nice Product Definition.] Let \\(\\cc\\) be a category with objects \\(A\\) and \\(B\\).  The product of \\(A\\) and \\(B\\) is an object \\(A \\times B\\)  equipped with morphisms \\[ \\pi_A: A \\times B \\to A \\qquad  \\pi_B: A \\times B \\to B             \\] <p>with the following universal property: For any object \\(Z\\) of \\(\\cc\\) with morphisms \\(f: Z \\to A\\), \\(g: Z \\to B\\), there exists a unique morphism \\(h: Z \\to A \\times B\\)  such that the diagram below commutes.  \\  </p> <p> Note that to utilize the above universal property, one requires a pair of morphisms  \\(f: Z \\to A\\) and \\(g: Z \\to B\\). That is, it is not true that, if I have a single  morphism \\(f: Z \\to A\\), then there exists a unique \\(h: Z \\to A \\times B\\) such that \\(\\pi_A \\circ h = k\\).  That would be false in many cases.  </p> <p>The above definition is a very nice one. For example, it returns the  concepts of products of groups or topological spaces when it is imposed  in Grp and Top. However, keep in mind the products don't  always exist. For example, it does not work in Fld,  the category of Fields (that is, there is no field which satisfies the universal property).  We will eventually explain why.</p> <p> Consider Ring, the category of rings. We can create products in this category  as follows: Let \\((R, \\mathbin{\\textcolor{NavyBlue}{+}}, \\mathbin{\\textcolor{NavyBlue}{\\bigcdot}})\\) and \\((S, \\mathbin{\\textcolor{Orange}{+}}, \\textcolor{Orange}{\\bigcdot})\\) be two rings  with zeros \\(0_R, 0_S\\) and units \\(1_R, 1_S\\).  Then we may form the product ring of \\(R\\) and \\(S\\) to be the ring  \\[ (R \\times S, +, \\bigcdot) = \\bigg\\{(r, s) \\;\\bigg| \\; r \\in R, s \\in S \\bigg\\} \\] <p>where for all pairs \\((r_1, s_1)\\) and \\((r_2, s_2)\\) in \\(R \\times S\\), we define the ring operations to behave as</p> <ul> <li> <p>\\((r_1, s_1) + (r_2, s_2) = (r_1 \\mathbin{\\textcolor{NavyBlue}{+}} r_2, s_1 \\mathbin{\\textcolor{Orange}{+}} s_2)\\) </p> </li> <li> <p>\\((r_1, s_1)\\mathbin{\\bigcdot} (r_2, s_2) = (r_1 \\mathbin{\\textcolor{NavyBlue}{\\bigcdot}} r_2, s_1 \\mathbin{\\textcolor{Orange}{\\bigcdot}} s_2)\\)</p> </li> </ul> <p>Note that with these requirements, the additive identity is \\((0_R, 0_S)\\) while the multiplicative identity  is \\((1_R, 1_S)\\). With this construction, one can show that this satisfies the universal property  of a product in Ring, so that Ring has products.  </p> <p>We make an interesting observation from the last example. For our ring \\((R \\times S, +, \\bigcdot)\\),  we surely have that \\((0_R, 1_S)\\) and \\((1_R, 0_R)\\) are elements of the product ring. However, </p> \\[ (0_R, 1_S) \\mathbin{\\bigcdot} (1_R, 0_S) = (0_R \\mathbin{\\textcolor{NavyBlue}{\\bigcdot}} 1_R, 0_S \\mathbin{\\textcolor{Orange}{\\bigcdot}} 1_S) =  (0_R, 0_S). \\] <p>Hence, even if the rings \\(R\\) and \\(S\\) are integral domains, \\(R \\times S\\) is not an integral domain.  Thus the product of two rings is never an integral domain. </p> <p> Consider the category of fields Fld. Let \\(F_1, F_2\\) be fields. Then  we would expect that the ring  \\[ F_1 \\times F_2 = \\bigg\\{ (a, b) \\;\\bigg|\\; a \\in F_1, F_2 \\bigg\\} \\] <p>to be the \"product field.\" But we just observed that this cannot  be a field because the product ring is not even an integral domain.</p> <p>However, this does not exclude the possibility that there is some kind of  other field construction which we are not considering that plays the role as  a product in Fld. We show that such a construction cannot  hold for all fields with the following simple example.</p> <p>Consider the fields \\(\\mathbb{F}_2\\) and \\(\\mathbb{F}_3\\), the fields with 2 and 3 elements, respectively.  Suppose that \\(P\\) is the product field of  \\(\\mathbb{F}_2\\) and \\(\\mathbb{F}_3\\). Then by definition, we would require  two projection field homomorphisms</p> \\[ \\pi_1: P \\to \\mathbb{F}_2 \\qquad \\pi_2: P \\to \\mathbb{F}_3 \\] <p>However, recall that two fields share a (nonzero) field homomorphism if and only if  they are of the same characteristic. Therefore, </p> <ul> <li> <p>\\(\\pi_1\\) can only exist if \\(P\\) has characteristic 2. In fact, \\(P\\) must be isomorphic to \\(\\mathbb{F}_2\\).</p> </li> <li> <p>\\(\\pi_2\\) can only exist if it has characteristic 3. In fact, \\(P\\) must be isomorphic \\(\\mathbb{F}_3\\). </p> </li> </ul> <p>Clearly, we have a contradiction. Thus we simply cannot generally take products in \\(**Fld**\\) in a logical way. </p> <p>From the previous example, we see that products don't always exist in category.  However, if they do, then we can take finitely many products. For instance, if we have  three objects \\(A, B, C\\), then we can take the products</p> \\[ A \\times (B \\times C) \\qquad (A \\times B) \\times C. \\] <p>If we have four objects, then we can create 5 products. Thus, if we can take the  product of two objects, then we all finite products consisting of objects of \\(\\cc\\) exist  in our category.</p> <p>We encapsulate this idea and include other prerequisites for a category to have finite products in the following proposition. </p> <p> Suppose \\(\\cc\\) is a category with a terminal object \\(T\\) and a product object \\(A \\times B\\) for every pair of objects \\(A\\) and \\(B\\). Then  \\begin{description} \\item[\\(\\bm{(i)}\\)] \\(\\cc\\) has finite products.  \\item[\\(\\bm{(ii)}\\)] There exists a bifunctor \\(\\prod: \\cc \\times \\cc \\to \\cc\\) where \\((A, B) \\mapsto A \\times B\\). <p>\\item[\\(\\bm{(iii)}\\)] For any three objects, we have an isomorphism </p> \\[ (A \\times B) \\times C \\cong A \\times (B \\times C) \\] <p>which is natural in \\(A, B\\) and \\(C\\) .</p> <p>\\item[\\(\\bm{iv}\\)] For any object \\(A\\), we have the isomorphism </p> \\[ T \\times A \\cong A \\cong T \\times A             \\] <p>natural in \\(A\\). \\end{description} </p> <p> To prove the first part, let \\(P(n)\\) be the following statement:  \\[ P(n) =  \\begin{cases} \\text{ For any objects } A_1, A_2, \\dots, A_n \\in \\cc,\\\\ \\text{ their product diagram in }\\cc. \\end{cases} \\] <p>\\begin{description} \\item[Base Case.] Observe that for \\(n = 0\\),  the statement is automatically true since we are given that a terminal object \\(T\\) exists. </p> <p>\\item[Inductive Step.] Suppose the statement holds for \\(n = k\\). Then for any objects \\(A_1, A_2, \\cdots, A_k\\),  we have the product diagram \\  and a unique, induced arrow \\(u\\) whenever such a \\(D \\in \\cc\\) with morphisms \\(f_i: D \\to A_i\\) exists.</p> <p>Let \\(A_{k+1}\\) be an arbitrary object of \\(\\cc\\). Then the product \\((A_1 \\times A_2 \\times \\cdots \\times A_k)\\times A_{k+1}\\) exists, since by assumption, the product of any two objects in our category must exist, and gives rise to the product diagram:  \\  whenever such an object \\(D\\) with a family of morphisms \\(g_1: D \\to A_1 \\times A_k\\) and  \\(g_2: D \\to A_{k+1}\\) exist.</p> <p>Look at the bottom of the second diagram; we have a unique morphism \\(\\pi'_1: A_1 \\times \\cdots \\times A_k \\times A_{k+1} \\to A_1 \\times \\cdots \\times A_k\\). We can extend this across the morphisms \\(\\pi_1, \\pi_2 \\cdots, \\pi_k\\) to demonstrate that there exist unique morphisms </p> \\[ \\pi_i \\circ \\pi'_1: A_1 \\times \\cdots \\times A_k \\times A_{k+1} \\to  A_i \\] <p>for \\(i = 1, 2, \\dots, k\\). Denote these as \\(\\overline{\\pi}_i\\).</p> <p>Now suppose we there exists an object \\(C\\) in \\(\\cc\\) with a family of morphisms \\(h_i: C \\to A_i\\). Then by the first diagram, there exists a unique morphism \\(u: C \\to A_1 \\times \\cdots \\times A_k\\) such that \\(h_i = \\pi_i \\circ u\\). Thus we have the diagram: \\ </p> <p>so we have a unique morphism \\(v: C \\to A_1 \\times \\cdots \\times A_{k+1}\\) such that \\(\\pi'_1 \\circ v = u\\) and \\(\\pi'_2 \\circ v = h_{k+1}\\).  However, note that </p> \\[ \\pi'_1 \\circ  v = u \\implies (\\pi_i \\circ \\pi'_1) \\circ v =  \\pi_i \\circ u  \\implies \\overline{\\pi}_i \\circ v= h_i. \\] <p>for \\(i = 1, 2, \\dots, k\\). </p> <p>Now let \\(\\overline{\\pi}_{k+1} = \\pi'_2\\).  Then we see that for such a family \\(h_i: C \\to A_i\\) for \\(i = 1,  2, \\dots, k+1\\), there exists a unique morphism \\(v: C \\to A_1 \\times \\cdots \\times A_{k+1}\\) such that </p> \\[ \\overline{\\pi}_i \\circ v = h_i \\] <p>for \\(i = 1, 2, \\dots, k + 1\\). Therefore, we have the product diagram  \\  so that the product \\(A_1 \\times A_k \\times A_{k+1}\\) exists and is well-defined in \\(\\cc\\). Hence, \\(P(n)\\) is true for \\(n = k+1\\). \\end{description} By mathematical induction, we see that all finite products must exist in \\(\\cc\\), as desired. </p> <p>To demonstrate the existence of a bifunctor, we can directly define one. Let \\(\\prod: \\cc \\times \\cc \\to \\cc\\) act as follows.  \\begin{description} \\item[Objects.] \\(\\prod(A, B) = A \\times B\\). \\item[Morphisms.] Let \\(f: A \\to A'\\) and \\(g: B \\to B'\\). Suppose we have canonical projections </p> \\[ \\pi_1: A \\times B \\to A \\qquad  \\pi_2: A \\times B \\to B   \\] <p>and </p> \\[ \\pi'_1: A' \\times B' \\to A' \\qquad  \\pi'_2: A' \\times B' \\to   B'. \\] <p>Then observe we get the diagram \\  Thus, there exists a unique morphism \\(u: A \\times B \\to A' \\times B'\\) whenever such \\(f, g\\) exist. Therefore, we can define how \\(\\prod\\) acts on morphism as</p> \\[ \\prod(f: A \\to A', g: B \\to B') = u: A \\times B \\to A' \\times B' \\] <p>where \\(u\\) is generated by the diagram above. As we just showed,  this assignment is well-defined. It's now pretty straightforward to now show that this establishes a functor (and I'm too lazy to do so). </p> <p>To establish associativity of our products, we demonstrate they're  isomorphic. Thus let \\(A \\times (B \\times C)\\) and \\((A \\times B) \\times C\\) be two products in \\(\\cc\\). Suppose we have an family of morphisms \\(h_1: D \\to A\\), \\(h_2: D \\to B\\) and \\(h_3: D \\to C\\). Then we get the following product diagrams. </p> <p>\\begin{minipage}{0.5\\textwidth} \\  \\end{minipage} \\begin{minipage}{0.5\\textwidth} \\  \\end{minipage} Since we have unique morphisms \\(v: D \\to B \\times C\\) and \\(w: D \\to A \\times B\\), we also get the product diagrams.  \\  for the products \\(A \\times (B \\times C)\\) and \\((A \\times B) \\times C\\), respectively. Thus we have the collection of morphisms </p> \\[\\begin{align*} &amp;p_1' \\circ \\pi'_1 : (A \\times B) \\times C \\to A \\quad  &amp;&amp;\\pi_1: A \\times (B \\times C) \\to A\\\\ &amp;p_2' \\circ \\pi'_1 : (A \\times B) \\times C \\to B \\quad  &amp;&amp;p_1 \\circ \\pi_2: A \\times (B \\times C) \\to B\\\\ &amp;\\pi'_2 : (A \\times B) \\times C \\to C \\quad  &amp;&amp;p_2 \\circ \\pi_2: A \\times (B \\times C) \\to C. \\end{align*}\\] <p>Now observe that </p> \\[\\begin{align} &amp;p_1' \\circ \\pi'_1 \\circ z = p'_1 \\circ w = h_1  \\quad  &amp;&amp;\\pi_1 \\circ y = h_1 \\\\  &amp;p'_2 \\circ \\pi'_1 \\circ x = p'_2 \\circ w = h_2  \\quad  &amp;&amp;p_1 \\circ \\pi_2 \\circ y = p_1 \\circ v = h_2 \\\\ &amp;\\pi'_2 \\circ z = h_3  \\quad  &amp;&amp;p_2 \\circ \\pi_2 \\circ y = p_2 \\circ v = h_3. \\end{align}\\] <p>Thus we see that our  first collection of morphisms are  projections. That is, for any family of morphisms \\(h_1: D \\to A\\), \\(h_2: D \\to B\\) and \\(h_3: D \\to C\\), there exists unique morphisms such that equations \\(y, z\\) such that equations (3), (4) and (5) hold.  What this means is that \\(A \\times  (B \\times C)\\) and \\((A \\times B) \\times C\\) are universal objects; specifically, they form universal cones. However, the original  universal cone of this construction was simply \\(A \\times B  \\times C\\) with the morphisms \\(\\overline{\\pi}_1, \\overline{\\pi}_2, \\overline{\\pi}_3\\). Thus we have that</p> \\[ A \\times (B \\times C) \\cong (A \\times B) \\times C \\cong A  \\times B \\times C \\] <p>since universal objects of the same construction are isomorphic. Showing naturality is not hard (again, too lazy to do that).</p> <p>Finally, let \\(T\\) be the terminal object of \\(\\cc\\). Denote \\(t_C: C \\to T\\) as the unique morphism from \\(C\\) to \\(T\\). Now consider the product diagram associated with the product \\(T \\times A\\):  \\  Observe that \\(t_D\\) always exists for any \\(D\\). Hence the existence of  \\(u\\) is completely dependent \\(f\\). Therefore, we can see that this diagram is equivalent  to  \\  Hence we see that \\(A\\) with the morphism \\(t_A, 1_A\\) forms a universal cone. But so does \\(T \\times A\\); hence, uniqueness guarantees they are isomorphic.  \\end{description} </p> <p>Now that we have discussed examples of products in categories, offered a rigorous definition,  and we observed an example when they do not exist, we would naturally want to generalize  this concept since it is often the case that we would like to take arbitrary products, or even  infinite products. We also want to somehow connect products to a universal morphism. To do all  of these things requires us to further abstract our definition of a product.  Before doing so, we offer a simple definition.</p> <p> Let \\(\\cc\\) be a category. Define the diagonal functor of \\(\\cc\\) as  \\(\\Delta: \\cc \\to \\cc \\times \\cc\\) where  \\begin{description} \\item[On Objects.] For \\(C\\) an object of \\(\\cc\\), we define \\(\\Delta(C) = (C, C)\\). \\item[On Morphisms.] For a morphism \\(f: A \\to B\\), we define \\(\\Delta(f) = (f, f): (A, A) \\to (B, B)\\). \\end{description} </p> <p>The above functor is a bit silly; it really doesn't do much. However, it necessary for us  to really understand what exactly a product is. It helps us realize that a product in  a category is actually a universal morphism. </p> <p>\\begin{definition}[Rigorous Product Definition.] Let \\(\\cc\\) be a category with objects \\(A, B\\). The product \\(A \\times B\\) of \\(A\\) and \\(B\\) is a universal morphism </p> \\[ \\pi: (A \\times B, u: \\Delta(A\\times B) \\to (A, B))   \\] <p>from \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\(\\Delta\\) to \\((A, B)\\)}}. This means that for any other pair \\((C, q: \\Delta(C) \\to (A, B))\\), there exists a unique  \\(h: C \\to A \\times B\\) in \\(\\cc\\) such that the diagram below commutes.  \\  end{definition}</p> <p>This definition is exactly equivalent to our previous. What this tells us is that  a product is an instance of a universal morphism. We show how this definition is equivalent  to the previous via the following example.</p> <p> To see this for the case when \\(n = 2\\), consider the product \\(A\\times B\\) of two objects \\(A, B\\) in some category \\(\\cc\\).  Then  \\  Let's spell out what's going on above; you might have seen this exposition, without even realizing, demonstrating the universality of products. Suppose there exists another object \\(C\\) with morphisms \\(f: C \\to A\\) and \\(g: C \\to B\\). Then we force the existence of a morphism \\(f': C \\to A \\times B\\). \\  When we usually do this, we simply just set  \\[ f' = (f, g)    \\] <p>so that \\(\\pi_A \\circ f' = f\\), and \\(\\pi_B \\circ f' = g\\).  </p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Introduction%20to%20Limits%20and%20Colimits/","title":"3.6. Introduction to Limits and Colimits","text":"<p>In our previous work, we learned a lot about universal morphisms and  then studied the basics of how products and coproducts behave in categories.  Such studying provides a great deal of preparation for the concepts of limits  and colimits, which we will introduce in this section. Before we do so, it will be convenient to utilize the notion of a cone. </p> <p> Let \\(\\cc\\) be a category, \\(A\\) an object of \\(\\cc\\). Let \\(F: J \\to \\cc\\) be a functor,  \\(J\\) an arbitrary category. A cone with \\(A\\) over \\(F\\) is a family  of morphisms  \\[ \\phi_i: A \\to F(i) \\qquad i \\in J \\] <p>such that, for each morphism \\(f: i \\to j\\) in \\(J\\), the diagram  below commutes. </p> <p> We denote the set of cones over \\(F\\) with apex \\(A\\) as \\(\\text{Cone}(A, F)\\).</p> <p>Dually, a cone with \\(F\\) over \\(A\\) is a family of morphisms </p> \\[ \\phi_i: F(i) \\to A \\qquad i \\in J \\] <p>such that, for each morphism \\(f: i \\to j\\) in \\(J\\), the diagram below commutes.  \\  Similarly, we define the set of cones with \\(F\\) over \\(A\\) as \\(\\text{Cone}(F, A)\\).  </p> <p>We will see that the above concept is similar to the work we have done so  far. To demonstrate this, we generalize our concept of a diagonal functor. </p> <p> Let \\(\\cc\\) and \\(J\\) be categories. The diagonal functor on \\(J\\) is the functor \\(\\Delta: \\cc \\to \\text{Fun}(J, \\cc)\\) which sends an object  \\(C\\) to the functor \\(\\Delta(C): J \\to \\cc\\), defined as follows: Each \\(i \\in J\\)  is mapped to \\(C\\), and every morphism in \\(J\\) is mapped to the identity of \\(\\cc\\).  </p> <p>Note how if we set \\(J = \\dd_n\\), the discrete category on \\(n\\)-object, or \\(J = \\dd_{\\lambda}\\),  the discrete category with objects indexed by \\(\\lambda\\), we obtain our original definitions of the  diagonal functor. </p> <p> Let \\(\\cc\\) and \\(J\\) be categories. Suppose \\(F: J \\to \\cc\\) is a functor, and let \\(A\\) be an object of \\(\\cc\\).  <ul> <li>A cone with \\(A\\) over \\(F\\) corresponds to a natural transformation  \\(\\phi: \\Delta(A) \\to F\\), and vice versa. In other words, </li> </ul> \\[ \\text{Cone}(A, F) \\cong \\nat(\\Delta(A), F). \\] <ul> <li>A cone with \\(F\\) over \\(A\\) corresponds to a natural transformation  \\(\\phi: F \\to \\Delta(A)\\), and vice versa. In other words, </li> </ul> \\[ \\text{Cone}(F,A) \\cong \\nat(F, \\Delta(A)). \\] <p></p> <p>The proof is left to the reader. The proposition is the key to mentally switching back and forth  from thinking about cones and natural transformations (between suitable functors) as equivalent constructions.</p> <p>We now define limits and colimits. </p> <p>[Limits] Let \\(F: J \\to \\cc\\) be a functor. The limit of \\(F\\)  is an object \\(\\Lim F\\) equipped with a natural transformation  \\(u: \\Delta(\\Lim F) \\to F\\) such that  \\begin{center} \\((\\Lim F, u: \\Delta(\\Lim F) \\to F)\\) is universal from  \\hyperref[definition:universal_morphism_from_F_to_D]{\\textcolor{blue}{\\(\\Delta\\) to \\(\\Lim F\\)}}. \\end{center}      *  This means that, for any other pair \\((C, v: \\Delta(C) \\to F)\\) with \\(v\\) a natural transformation and with \\(C \\in \\cc\\), there exists a unique morphism \\(h: C \\to \\Lim F\\) in \\(\\cc\\) such that  the diagram below commutes.  \\  <ul> <li>By Proposition \\ref{proposition:cones_are_natural_transformations},  the morphism \\(u: \\Delta(\\Lim F) \\to F\\) forms a cone with \\(\\Lim F\\) over \\(F\\) via  a family of morphisms \\(u_i: \\Lim F \\to F(i)\\) for all \\(i \\in J\\). </li> </ul> <p>Similarly, any other pair \\((C, v: \\Delta(C) \\to F)\\) is also a cone with \\(C\\) over \\(F\\)  via a family of morphisms \\(v_i: C \\to F(i)\\) with \\(i \\in J\\). </p> <p>Thus, the universal property, states that there exists a unique \\(h: C \\to \\Lim F\\) such that the diagram  below commutes.  \\ </p> <p></p> <p> We remind the reader that limits do not always exist for certain functors.  This is because universal objects do not always exist. We will eventually discuss  conditions for existence of limits. </p> <p>Next, we offer the definition of a limit. </p> <p>[Colimits] Let \\(F: J \\to \\cc\\) be a functor. The colimit of \\(F\\) is an object  \\(\\Colim F\\) equipped with a natural transformation \\(u: F \\to \\Delta(\\Colim F)\\)  such that  \\begin{center} \\((\\Colim F, u: F \\to \\Delta(\\Colim F))\\) is universal from  \\hyperref[definition:universal_morphism_from_D_to_F]{\\textcolor{blue}{\\(F\\) to \\(\\Delta\\)}}. \\end{center}   </p> <p>Now is a good time to use Proposition \\ref{proposition:cones_are_natural_transformations}  and reinterpret the definition of a colimit as a family of morphisms  like we did in the definition of a limit.</p> <p> We comment on the notation of a limit.  <ul> <li> <p>Many people denote the limit of a functor as \\(\\Limfrom F\\). </p> </li> <li> <p>Many people denote the colimit of a functor as \\(\\Limto F\\). </p> </li> </ul> <p>The notation makes only sense if one understand the connection between limits and colimits and universal morphisms. (Compare the direction of the arrow \\(h\\) in the universal diagrams).</p> <p>However, this then sometimes leads people to start writing \\(\\Colimto F\\) and \\(\\Colimfrom F\\).  The issue with this notation is that it seems unnecessarily complicated (perhaps I am wrong,  but I have waited for a long time  to come upon an instance for when it could be useful).  Despite these observations, this  notation is very consistently used in texts which use categorical tools, and so this  warrants a comment to the reader.</p> <p>Moving forward, I will simply write  \\(\\Lim F\\) and \\(\\Colim F\\), since I see no need to make  the notation anymore complicated than it needs to be.  </p> <p> Let \\(J = \\dd_n\\), the discrete category with \\(n\\)-objects. Let \\(F: J \\to \\cc\\) be  the functor where \\(F(\\bullet_i) = A_i\\). We then have that  <ul> <li> <p>The product \\(\\prod_{i = 1}^{n}A_i\\) is the limit of \\(F\\). </p> </li> <li> <p>The coproduct \\(\\coprod_{i = 1}^{n}A_i\\) is the colimit of \\(F\\). </p> </li> </ul> <p>When we set \\(J = \\dd_{\\lambda}\\), with \\(\\lambda\\) an arbitrary indexing set, we similarly get  that the arbitrary product and coproduct definitions are simply instances of  limits and colimits. </p> <p>Thus, universal diagrams and limits have been right in our faces for the last three sections.  </p> <p>Since limits and colimits are universal objects, we have the following proposition. This is a genearlization of Proposition \\ref{proposition:product_coproduct_natural_bijection}.</p> <p> Let \\(F: J \\to \\cc\\) be a functor.  <ul> <li>If \\(\\Lim F\\) exists, then for each object \\(C\\) of \\(\\cc\\), we have the  natural bijection </li> </ul> \\[ \\hom_{\\cc}(C, \\Lim F) \\cong \\text{Cone}(C, F) \\] <ul> <li>If \\(\\Colim F\\) exists, then for each object \\(C\\) of \\(\\cc\\),  we have the natural bijection </li> </ul> \\[ \\hom_{\\cc}(\\Colim F, C) \\cong \\text{Cone}(F, C) \\] <p></p> <p> We prove the first result. Since \\(\\Lim F\\) exists, let \\((\\Lim F, u: \\Delta(\\Lim F) \\to F)\\) be universal  from \\(\\Delta\\) to \\(F\\). Then by Exercise \\hyperref[exercise:universality_bijection]{3.2.1},  we have the natural bijection  \\[ \\hom_{\\cc}(C, \\Lim F) \\cong \\hom_{\\text{Fun}(J, \\cc)}(\\Delta(C), F) = \\nat(\\Delta(C), F). \\] <p>By Proposition \\ref{proposition:cones_are_natural_transformations}, we can rewrite this  natural bijection as </p> \\[ \\hom_{\\cc}(C, \\Lim F) \\cong \\text{Cone}(C, F). \\] <p>This proves the first result; the second follows similarly.  </p> <p>The above proposition is very useful as it gives us the following proposition,  which is our first test of whether or not a limit or colimit exists in a category. </p> <p> Let \\(F: J \\to \\cc\\) be a functor. Then we may define the functors  \\[\\begin{align*} \\cone(-, F): \\cc \\to **Set**\\\\ \\cone(F, -): \\cc \\to **Set** \\end{align*}\\] <p>We have the following two results.</p> <ul> <li> <p>\\(\\cone(-, F)\\) is representable if and only if \\(\\Lim F\\) exists in \\(\\cc\\) (in which case, this is the representing object)</p> </li> <li> <p>\\(\\cone(F, -)\\) is representable if and only if \\(\\Colim F\\) exist in \\(\\cc\\) (in which case, this is the representing object)</p> </li> </ul> <p></p> <p> For pedagogical reasons, we prove the second bullet point and leave the first as  an exercise.  <p>One direction is immediate: If \\(\\Colim F\\) exists,  then by Proposition \\ref{proposition:limit_cone_set_natural_bijection},  we obtain a natural bijection for each \\(C\\) in \\(\\cc\\)  which implies that \\(\\cone(F, -)\\) is representable.</p> <p>Conversely, suppose \\(\\cone(F, -)\\) is a representable functor with representing object  \\(R\\). We want to show \\(R = \\Lim F\\).  Now by Proposition \\ref{proposition:representable_if_and_only_if},  \\(\\cone(F, -)\\) is representable if and only if \\((R, u: \\{\\bullet\\} \\to \\cone(F, R))\\)  is universal from \\(\\{\\bullet\\}\\) to \\(\\cone(F, -)\\). </p> <p>Let us shut off our brains and blindly expand what this means. This means that for any other  pair \\((C, v: \\{\\bullet\\} \\to \\cone(F, C))\\), there exists a unique \\(h: R \\to C\\) such that  diagram below commutes. </p> <p>\\  Now let us turn our brains back on and understand what this means.  A function \\(u: \\{\\bullet\\} \\to \\cone(F, R)\\) simply picks out some cone \\(\\sigma \\in \\cone(F, R)\\) whose family we denote as \\(\\sigma_i: F(i) \\to R\\).</p> <p>Similarly, \\(v: \\{\\bullet\\} \\to \\cone(F, C)\\) picks out a cone \\(v(\\{\\bullet\\})\\), which  we may denote as \\(\\tau\\).  What the universal property then says is the following: Given any cone \\(\\tau\\) with \\(F\\) over some object \\(C\\), there exists a unique \\(h\\) such that the diagram below commutes. </p> <p>\\  This then means that \\(R = \\Colim F\\), which proves this direction. </p> <p> The above theorem is actually quite remarkable.  We have linked the existence of our limit  to the representability of a particular  functor (one which we understand fairly well). This tells us the concept of a cone is very intimately linked to that of a  limit and colimit. </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <ul> <li> <p>[1.] Let \\(F, G: J \\to \\cc\\) be two functors, and suppose \\(F \\cong G\\) (i.e., there is a natural isomorphism between them).  Show that  \\begin{itemize}</p> </li> <li> <p>[(i.)] If \\(\\Lim F\\) exists, then \\(\\Lim G\\) exists and \\(\\Lim F \\cong \\Lim G\\).</p> </li> <li> <p>[(ii.)] If \\(\\Colim F\\) exists, then \\(\\Colim G\\) exists and \\(\\Colim F \\cong \\Colim G\\). </p> </li> </ul> <p>Thus, limits and colimits are invariant up to isomorphism.</p> <p>\\item[2.] Prove Proposition \\ref{proposition:cones_are_natural_transformations}.</p> <p>\\item[3.] Expand Definition \\ref{definition:colimit_of_a_functor}, the definition of a colimit, in a similar fashion to how we expanded Definition \\ref{definition:limit_of_a_functor}, the definition of a limit.</p> <p>\\item[4.] Use Proposition \\ref{proposition:cones_are_natural_transformations} and Proposition \\ref{proposition:universality_bijection} to show that  if \\(\\Colim F\\) exists for a functor \\(F: J \\to \\cc\\), then we have a natural bijection</p> \\[ \\hom_{\\cc}(\\Colim F, C) \\cong \\cone(F, C). \\] <p>This then completes the proof of Proposition \\ref{proposition:limit_cone_set_natural_bijection}.</p> <p>\\item[5.] Use Proposition \\ref{proposition:limit_cone_set_natural_bijection} (the proof of which you just completed) to prove the first bullet point of  Proposition \\ref{proposition:representable_if_and_only_if}: The functor \\(\\cone(-, F): \\cc \\to **Set**\\) is representable if and only if  \\(\\Lim F\\) exists. Use the following steps. </p> <ul> <li>[(i.)] Let \\(\\cc\\) be a category, \\(F: J \\to \\cc\\) a functor. Recall that we  may define the functor </li> </ul> \\[ \\overline{F}: J \\to \\cc\\op \\] <p>which acts the same as \\(F\\) on objects, but if \\(f: i \\to j\\) is a morphism in \\(J\\), then \\(\\overline{F}(f) = F(f)\\op\\).</p> <p>Show that \\(\\Lim F\\) exists in \\(\\cc\\) if and only if \\(\\Colim \\overline{F}\\) exists in \\(\\cc\\op\\).</p> <ul> <li>[ii.] Show that</li> </ul> \\[ \\cone(-, F) \\cong \\cone(\\overline{F}, -). \\] <p>Then use (i) and the second bullet point of  Proposition \\ref{proposition:representable_if_and_only_if} to complete the proof.</p> <p>\\end{itemize}</p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Pullbacks%20and%20Pushouts/","title":"3.8. Pullbacks and Pushouts","text":"<p>\\section*{Pullbacks.}</p> <p> Let \\(f: A \\to C\\) and \\(g: B \\to C\\) be two morphisms. Then we say a pullback of \\(f, g\\) is a commutative square on the left <p> such that for any commutative square in the the middle, the diagram on the right commutes, and \\(f'\\) is unique.  </p> <p>Another way we can describe this is using the language of limits, and hence show that pullbacks are simply limit objects. Let \\(J\\) be the category of three objects with the following shape: \\  The numbers 1, 2, and 3 here mean nothing; they are simply place holders for some distinct objects. So any functor \\(F: J \\to \\cc\\) simply corresponds to a triple of object and a pair of morphisms in \\(\\cc\\):  \\  if we have \\(F(1) = A\\), \\(F(2) = C\\) and \\(F(3) = B\\). Now we can equivalently describe a pullback as follows: </p> <p> If \\(J\\) is the category with the shape \\(\\begin{tikzcd} 1 \\arrow[r] &amp; 2 &amp; 3 \\arrow[l] \\end{tikzcd}\\), and \\(F: J \\to \\cc\\) is a functor,  then a pullback is a universal arrow \\((D, u: \\Delta(D) \\to F)\\) from \\(\\Delta\\) to \\(F\\).   First, observe that this shows that a pullback is a limit. But how are our two definitions equivalent? </p> <p>Consider the morphism \\(u: \\Delta(D) \\to F\\). This is simply a natural transformation between the two functors \\(\\Delta(D): J \\to \\cc\\) and \\(F: J \\to \\cc\\). Now \\(\\Delta(D)(i) = D\\) for all objects \\(i = 1, 2, 3 \\in J\\). On the other hand, \\(F(1) = A\\), \\(F(2) = C\\) and \\(F(3) = B\\).  Thus we see that \\(\\Delta(R) \\to F\\) induces a family of morphisms:</p> \\[\\begin{align*} u_1: \\Delta(D)(1) \\to F(1) \\implies u_1: D \\to A\\\\ u_2: \\Delta(D)(2) \\to F(2) \\implies u_2: D \\to C\\\\ u_3: \\Delta(D)(3) \\to F(3) \\implies u_3: D \\to B \\end{align*}\\] <p>which arrange themselves in \\(\\cc\\) into the following diagram: \\  and if we \"tip\" this diagram over, and force the arrows \\(f\\) and \\(g\\) meeting at \\(C\\) into a 90 degree angle, we get the following cone: \\ </p> <p>Note that we removed the morphism \\(u_2\\) because it's redundant, unnecessary information; after all \\(u_2 = f \\circ u_1 = g \\circ u_3\\); which is information already captured in both the original diagram and the commutative square. </p> <p>Thus, we see that whenever we have an object \\(E\\) and morphism \\(v:\\Delta(E) \\to F\\), we have a commutative square! In other words, whenever we have a cone over \\(F\\), we have a commutative square! And in even other words, whenever we have a family of morphisms \\(v_i: E \\to F(i)\\) for \\(i=1,2,3\\), we have a commutative square!  \\ </p> <p>So, how do we connect the universality of \\((D, u: \\Delta(D) \\to F)\\) with the universality of the pullback? Well, since this object is universal, we know that for any other pair \\((E, v: \\Delta(E) \\to F)\\), there exists a morphism \\(f': E \\to D\\) such that the following diagram commutes.  \\  The commutativity of the top left diagram gives us the relation that \\(u \\circ \\Delta(f') = v\\), which implies that \\(u_1 \\circ f' = v_1\\) and \\(u_3 \\circ f' = v_3\\). We then have that  \\  which is just the pullback. Thus the pullback is in fact a limit object, and we understand just exactly how it is a limit object of the functor \\(F: J \\to \\cc\\). </p> <p> Let \\(\\cc\\) be a category, and consider a pair of morphism \\(f: A \\to B\\), \\(g:A \\to C\\) in \\(\\cc\\). A pushout of \\((f, g)\\) is the commutative diagram on the left \\  such that for every commutative square as on the right, there exists a unique morphism \\(t: R \\to S\\) such that \\(t \\circ u = h\\) and \\(t \\circ v = k\\). We can actually summarize this information more compactly \\  where the diagram is commutative. \\textcolor{NavyBlue}{One way to imagine a pushout is a commutative diagram which swallows every other commutative diagram which contains the morphisms \\(f, g\\).} </p> <p>As you might suspect, the pushout can in fact be related as the universal arrow of a functor. Consider the category 3, which contains 3 objects and two nontrivial morphisms.  \\  Now construct the functor category \\(\\cc^**3**\\), where </p> <ul> <li> <p>[1.] Objects are functors \\(F: **3** \\to \\cc\\), which is equivalent to pairs of morphisms \\((f, g)\\) where \\(f: A \\to B\\) and \\(g: A \\to C\\) in \\(\\cc\\) </p> </li> <li> <p>[2.] Morphisms are natural transformations, which in this case simply reduce to a triple of morphisms \\((h, l, k)\\) where  \\ </p> </li> </ul> <p>Now construct the functor \\(\\Delta: \\cc \\to \\cc^**3**\\) where \\(C \\longmapsto (1_C, 1_C)\\) where \\(1_C: C \\to C\\) is the identity morphism. Suppose there exists a natural transformation \\(\\eta_S: (f, g) \\to \\Delta(S)\\), which we can represent as follows: \\  If we have a pushout associated with the object \\(R\\) in \\(\\cc\\), the existence of these commutative squares implies the existence of a morphism \\(t: R \\to S\\), so that we have  \\  Hence we see that a pushout is a universal arrow from \\((f, g)\\) to \\(\\Delta\\).</p> <p>\\chapterimage{chapter4_pic/chapt4head.pdf}</p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Representable%20Functors%20and%20Yoneda%27s%20Lemma/","title":"3.2. Representable Functors and Yoneda's Lemma","text":"<p>This is probably the most important section out of these entire set  of notes. The propositions proved here will allows us to perform slick  proofs of interesting results later on. We will also use results that are left as exercises for the  reader (since it is important for the reader to do them).  Now before we introduce the Yoneda lemma, we prove some propositions concerning the concept of universality.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor. Then a pair \\((R, u: D \\to F(R))\\) is universal from \\universalDToF{\\(D\\) to \\(F\\)} if and only if for each \\(C \\in \\cc\\) we have the natural bijection \\[ \\hom_{\\cc}(R, C) \\cong \\hom_{\\dd}(D, F(C)). \\] <p>That is, any isomorphism, natural in \\(C\\) as above, is determined by a unique morphism \\(u: D \\to F(R)\\) so that \\((R, u)\\) is a universal arrow from \\(D\\) to \\(F\\).  </p> <p> Suppose that \\(u: D \\to F(R)\\) is a universal morphism from \\(D\\) to \\(F\\).  Then by definition, we have the relation <p></p> <p>Each \\(h: D \\to F(C)\\) uniquely corresponds to  a morphism \\(f: R \\to C\\), while conversely, any \\(f: R \\to C\\) can be precomposed  with \\(u\\) to obtain a morphism \\(F(f) \\circ u : D \\to F(C)\\). Hence we see the  we have a bijective correspondence</p> \\[ \\hom_{\\dd}(R, C) \\cong \\hom_{\\cc}(D, F(C)). \\] <p>Now to demonstrate naturality, we consider a morphism \\(k: C \\to C'\\) and we check that the diagram below commutes. \\  \\begin{minipage}{0.5\\textwidth} \\hspace{-1cm} \\  \\end{minipage} \\hspace{-0.4cm} \\begin{minipage}{0.5\\textwidth}</p> <ul> <li> <p>[\\textcolor{Red}{\\textbullet}] Beginning with a morphism \\(f: R \\to C\\), we travel right  to obtain the morphism \\(F(f) \\circ u\\). Going down, we obtain  the morphism \\(F(k) \\circ (F(f) \\circ u)\\).  </p> </li> <li> <p>[\\textcolor{Blue!80}{\\textbullet}] Consider the same morphism \\(f: R \\to C\\). If we instead first traveled down,  we'd obtain the morphism \\(k \\circ f\\). Traveling right would then  send us to the morphism \\(F(k \\circ f) \\circ u\\). </p> </li> </ul> <p>\\end{minipage} However, it is certainly the case that </p> \\[ F(k) \\circ (F(f) \\circ u)  =  F(k \\circ f) \\circ u \\] <p>so that these paths are equivalent. The proof could also be given immediately  by considering the diagram on the left, which is supplied here to give a better  understanding of what's going on.</p> <p>To prove the other direction, suppose that we have such a natural bijection given by some \\(\\phi\\).</p> \\[ \\phi_{C}: \\hom_{\\dd}(R, C) \\isomarrow \\hom_{\\cc}(D, F(C)) \\] <p>Then in particular  we have that \\(\\hom_{\\dd}(R, R) \\cong \\hom_{\\cc}(D, F(R))\\).  Consider \\(\\phi(1_R): D \\to F(R)\\); we denote this special  morphism as \\(u: D \\to F(R)\\). </p> <p>Now for any \\(f: R \\to C\\), the diagram on the bottom left commutes by naturality;  however, we are more interested in following the element  \\(1_R \\in \\hom_{\\dd}(R, R)\\).  \\  We see that any such \\(\\phi\\) must act on \\(\\hom_{\\dd}(R, C)\\)  by bijectively send \\(f: R \\to C\\) to \\(F(f) \\circ u\\). What this  means is that any \\(h \\in \\hom_{\\cc}(D, F(C))\\) corresponds uniquely  to some \\(f: R \\to C\\) such that \\(h = F(f) \\circ u\\), which is exactly the  definition for \\(u: D \\to F(R)\\) to be universal from \\(D\\) to \\(F\\). This  completes the proof. </p> <p>In the proof we demonstrated above, we did something weird. That is, we  discussed this so-called natural isomorphism </p> \\[ \\phi_C: \\hom_{\\dd}(R, C) \\to \\hom_{\\cc}(D, F(C)). \\] <p>However, at this point we've only really seen natural isomorphisms between functors. Does this mean what we really had was a  natural transformation between two functors? The answer is yes; the proof inadvertently derived the natural isomorphism</p> \\[ \\phi: \\hom_{\\dd}(R, -) \\to \\hom_{\\cc}(D, F(-))    \\] <p>which, by the proposition above, exists only when  we have a universal morphism \\(u: D \\to F(R)\\) from \\(D\\) to \\(F\\). For such functors, we call them representable.</p> <p> Let \\(\\cc\\) have small hom-sets. We say a functor  \\(K: \\cc \\to **Set**\\) is representable when there exists an object \\(R\\) and a natural isomorphism \\[ \\psi: \\hom_{\\dd}(R, -) \\to K. \\] <p>The object \\(R\\) here is said to be the representing object for \\(K\\).  </p> <p> Consider the forgetful functor \\(U: **Grp** \\to **Set**\\).  One way to describe this functor is simply with words: each group \\(G\\) is  sent to its underlying set in Set. Another approach is to literally  express the groups in terms of its elements, for this then tells us where it  is sent in Set. A simple way to do this is to consider the maps  \\[ \\hom_{**Grp**}(\\zz, G) = \\{\\text{Group homomorphisms } \\phi: \\zz \\to G\\}. \\] <p>This works since each such map \\(\\phi: \\zz \\to G\\) firstly  picks out some element \\(a\\) so that \\(\\phi(1) = a\\). As this is a group homomorphism  we then see that \\(\\phi(n) = a^n\\). Hence the collection of all these maps  picks out all of the elements of \\(G\\), so that we can say</p> \\[ U(G) \\cong \\hom_{**Grp**}(\\zz, G). \\] <p>We use an isomorphism since an equality is not exactly correct; we just know that  the two sets are going to have the same cardinality, and hence be isomorphic in Set.  Now, what this in the end means is that the forgetful functor is a representable,  since we have that </p> \\[ U:**Grp** \\to **Set** \\cong \\hom(\\zz, -) : **Grp** \\to **Set**. \\] <p>This construction works due to the key property of the group homomorphism, so  that this can be repeated for Ring, \\(R\\)-Mod, etc. Hence many forgetful functors  are representable functors. We will see in Chapter 5 what this really means.  </p> <p> Let \\((R, +, \\cdot)\\) be a ring and \\((k, +, \\cdot)\\) a field. Suppose further that \\(R\\) is \\(k\\)-algebra.  Recall that we can create the affine \\(n\\)-space of \\(R\\) \\[ A^n(R) = \\{(x_1, \\dots, x_n) \\mid x_i \\in R\\}. \\] <p>Now suppose \\(\\phi: R \\to S\\) is a morphism of \\(k\\)-algebras. Then  this induces a mapping </p> \\[ A^n(\\phi):  A^n(R) \\to A^n(S) \\qquad (r_1, \\cdots, r_n) \\mapsto (\\phi(r_1), \\dots, \\phi(r_n)). \\] <p>What we can realize now is that we have a functor on our hands (by of  course verifying the other necessary properties) between  \\(**Alg**_k\\) and Set. </p> \\[ A^n: **Alg**_{k} \\to **Set**.           \\] <p>Now recall from Example 2.\\ref{example_free_algebra_universal} that if \\(F: **Set** \\to **Alg**_k\\) is the free functor  assigning \\(X \\mapsto k\\{X\\}\\), the free algebra, and \\(U: **Alg**_k \\to **Set**\\) is the forgetful functor, then for each set \\(X\\) we have a universal morphism  \\((F(X), i: X \\to U(F(X)))\\) from \\(X\\) to the forgetful functor \\(U\\).  By Proposition \\ref{prop_universality_bijection}, we thus have the  isomorphism</p> \\[ \\hom_{**Alg**_k}(F(X), R)  \\cong  \\hom_{**Set**}(X, U(R)). \\] <p>natural for all \\(R \\in **Alg**_k\\).  However, notice that if \\(X = \\{x_1, \\dots, x_n\\}\\), \\(\\hom_{**Set**}(X, U(R))\\) is nothing  more than the set of all functions which pick out \\(n\\) elements of  \\(R\\). In other words, </p> \\[ \\hom_{**Set**}(X, U(R)) \\cong A^n(R).     \\] <p>One can verify the naturality of the above bijection (I won't it's not too bad). Therefore we have that </p> \\[ \\hom_{**Alg**_k}(F(X), R) \\cong A^n(R) \\implies  \\hom_{**Alg**_k}(K\\{X\\}, R) \\cong A^n(R).    \\] <p>so that we have a natural isomorphism between functors </p> \\[ \\hom_{**Alg**_k}(K\\{X\\}, -) \\cong A^n(-). \\] <p>What this then means is that \\(A^n(-)\\) is a representable functor.        </p> <p> Let \\(X\\) be a topological space. Recall from Example ?? that we can consider the  set \\(\\text{Path}(X)\\) consisting of all paths in the topological space \\(X\\).  If we recall that a path in \\(X\\) can be represented by  a continuous function \\(f: [0,1] \\to X\\), we see that  \\[ \\text{Path}(X) = \\{f:[0,1] \\to X \\mid f \\text{ is continuous}\\} = \\hom_{**Top**}([0,1], X). \\] <p>Hence we see that \\(\\text{Path}: **Top** \\to **Set**\\) is a functor;  moreover, it is clearly representable since \\(\\text{Path}(-) = \\hom_{**Top**}([0,1], -)\\). </p> <p>This example, however, can be taken even further: What about \\(n\\)-dimensional  \"paths?\" To generalize this we can use simplicies. Denote \\(\\Delta^n\\)  as the \\(n\\)-simplex. Then we can establish the family of functors </p> \\[ \\hom_{**Top**}(\\Delta^n, -): **Top** \\to **Set** \\] <p>which map simplicies to topological spaces; such continuous  functions provide  the foundation for singularly homology theory,  and each functor above is representable . Note that we get back \\(\\text{Path}\\)  when \\(n = 1\\).  </p> <p>As we have just seen, representable functors not only occur very frequently  but they also arise naturally to yield consturctions which we actually care about. </p> <p>A natural question to ask at this point is the following: When exactly do we have a representable  functor on our hands? The next proposition answers that question.</p> <p> Let \\(\\cc\\) be a locally small category, and suppose \\(K: \\cc \\to **Set**\\) is a functor. Then \\(K\\) is a representable functor (with representing object \\(R\\))  if and only if  \\((R, u: \\{\\bullet\\} \\to K(R))\\) is universal from \\universalDToF{\\(\\{\\bullet\\}\\) to \\(K\\)} \\ for some object \\(R \\in \\cc\\).  </p> <p>Note here that \\(\\{\\bullet\\}\\) is the one-point set whose single element is denoted as \\(\\bullet\\).</p> <p> The forward direction is similar to Example \\ref{example_affine_representable}, while  the backwards direction is similar to the proof of Proposition \\ref{prop_universality_bijection}. <p>First let's interpret what it means for \\(u: \\{\\bullet\\} \\to K(R)\\) to  be universal. This means that for any other \\(f: \\{\\bullet\\} \\to K(C')\\), there  exists a unique morphism \\(h: R \\to C'\\) such that the diagram below commutes.  \\  By Proposition \\ref{prop_universality_bijection} we also have the natural  bijection </p> \\[ \\hom_{\\cc}(R, C) \\cong \\hom_{**Set**}(\\{\\bullet\\}, K(C))    \\] <p>which is enough to establish a natural isomorphism  \\(\\phi: \\hom_{\\cc}(R, -) \\cong \\hom_{**Set**}(\\{\\bullet\\}, K(-))\\). </p> <p>Now observe that for a given \\(C'\\), each \\(f \\in \\hom_{**Set**}(\\{\\bullet\\}, K(-))\\) is just a function \\(f: \\{\\bullet\\} \\to K(R)\\). Thus, each function can be represented  uniquely by an element \\(c \\in K(C)\\), which establishes the bijection </p> \\[ \\hom_{**Set**}(\\{\\bullet\\}, K(C)) \\cong K(C) \\] <p>for each \\(C\\). In fact, it's not difficult to show that this bijection is natural. Therefore we see that we can connect our natural  bijections together</p> \\[ \\hom_{\\cc}(R, -) \\cong \\hom_{**Set**}(\\{\\bullet\\}, K(-)) \\cong K(-) \\] <p>which demonstrates that \\(K: \\cc \\to **Set**\\) is a representable functor. </p> <p>Conversely, suppose that \\(K: \\cc \\to **Set**\\) is representable. Specifically, suppose  \\(\\phi: \\hom_{\\cc}(R, -) \\isomarrow K(-)\\) is our natural isomorphism between the functors.  Then in particular, for any \\(h: R \\to C\\), naturality guarantees that the following  diagram commutes.  \\  Now take a step back; define the morphism \\(u: \\{\\bullet\\} \\to K(R)\\) where  \\(u(\\bullet) = \\phi(1_R)\\), and suppose \\(f: \\{\\bullet\\} \\to K(C)\\) is some morphism. Then  because \\(\\phi: \\hom_{\\cc}(R, C) \\to K(C)\\) is a bijection, this means that  \\(f(\\bullet) = \\phi(h: R \\to C)\\) for some unique morphism \\(h: R \\to C\\). In particular, the  above diagram tells us that </p> \\[ K(h)\\big(\\phi(1_R)\\big) = \\phi(h) \\implies K(h)(u(\\bullet)) = f(\\bullet). \\] <p>In other words, we have that given any \\(f:\\{\\bullet\\} \\to K(C)\\),  there exists a unique \\(h: R \\to C\\) such that the diagram commutes.  \\  Therefore, the fact that \\(K\\) is representable gives  rise to a \\(u: \\{\\bullet\\} \\to K(R)\\) which is universal, which is what we set  out to show. </p> <p>We are now ready to introduce the well-known lemma due to Nobuo Yoneda. The Yoneda lemma is simply a convenient result that occurs when one encounters  situations with the functors \\(\\hom_{\\cc}(R, -): \\cc \\to **Set**\\). While this  might not seem that relevant, it applicability expands when we combine the result with our  previous work on representable functors in this section.</p> <p>[ (Yoneda \"Lemma\")] Let \\(K: \\cc \\to **Set**\\) be a functor. Then for every object \\(R\\) of \\(\\cc\\), we have that  \\[ \\hom_{**Set**^\\cc}\\big(\\hom_{\\cc}(R, -), K \\big) \\cong K(R) \\implies \\nat(\\hom_{\\cc}(R, -), K) \\cong K(R) \\] <p>where \\(\\nat(F, G)\\) denotes the set of all natural transformations between functors \\(F, G\\). </p> <p> To demonstrate bijectivity, we construct two maps from each set and  demonstrate that they are inverses.  <p>Suppose we have a natural transformation \\(\\eta: \\hom_{\\cc}(R, -) \\to K\\).  Then for every \\(C \\in \\cc\\), the diagram below on the left commutes. \\  With this diagram, we can follow what happens to the identity morphism \\(1_R \\in \\hom_{\\cc}(R, R)\\). As above, denote \\(\\eta_R(1_R) = u \\in K(R)\\). The commutativity of the diagram above then tells us that </p> \\[ \\eta_C(f: R \\to C) = K(f)(u). \\] <p>This is great! This tells us the exact formula for every \\(\\eta \\in \\nat(\\hom_{\\cc}(R, -), K)\\). Moreover, each  formula is uniquely determined by some \\(u \\in K(R)\\).  This then motivates us to construct the mapping </p> \\[ y: \\text{Nat}(\\hom_{\\cc}(R, -), K) \\to K(R) \\qquad  \\eta \\mapsto u \\] <p>where \\(u\\) is the unique member of \\(K(R)\\) such that \\(\\eta_C(f: R \\to C) = K(f)(u)\\).</p> <p>Now consider any arbitrary member \\(r \\in K(R)\\). For each \\(C \\in  \\cc\\), construct the mapping </p> \\[ \\epsilon_C: \\hom_{\\cc}(R, C) \\to K(R)  \\qquad  \\epsilon_C(f: R \\to C) = K(f)(r) \\] <p>This defines a natural transformation, so that what we've constructed is a  mapping </p> \\[ y': K(R) \\to \\nat(\\hom_{\\cc}(R, -), K) \\qquad r \\mapsto \\epsilon_C \\] <p>where \\(\\epsilon_C(f: R \\to C) = K(f)(u)\\). </p> <p>Now given any \\(\\eta \\in \\nat(\\hom_{\\cc}(R, -), K)\\) we clearly have that \\(y' \\circ y(\\eta) = \\eta\\) and for any \\(r \\in K(r)\\)  we have that \\(y \\circ y'(r) = r\\). Hence we have a bijection between sets, so we  may conclude that </p> \\[ \\nat(\\hom_{\\cc}(R, -), K) \\cong K(R) \\] <p>as desired. </p> <p> As the Yoneda lemma is a bit mysterious when one first  encounters it, we can perform a  simple sanity check as follows. For any category \\(\\cc\\), consider the objects \\(A,B \\in \\cc\\), which we can use to build the functors \\(\\hom(A, -), \\hom(B,-): \\cc \\to **Set**\\). What is a natural transformation \\(\\eta: \\hom(A, -) \\to \\hom(B, -)\\)? It is a family of functions, indexed by all objects in  \\(\\cc\\), such that for each \\(f: C \\to D\\) the diagram below commutes. \\  <p>We see that these functions must satisfy the  property outlined in yellow for all \\(C,D\\). So  what functions do this? An immediate source of such functions that  assemble into natural transformations which we  seek arise when we take any \\(\\phi \\in \\hom(B,A)\\) and set  each \\(\\eta_C : \\hom(A,C) \\to \\hom(B,C)\\) equal to </p> \\[ (-) \\circ \\phi : \\hom(A,C) \\to \\hom(B,C) \\] <p>for each \\(C \\in \\cc\\). This clearly checks out since we have that, for any \\(f: C \\to D\\) and \\(k: A\\to C\\),  </p> \\[ (f \\circ k)\\circ \\phi = f  \\circ (k \\circ \\phi). \\] <p>The question now is: Is every natural transformation  derived from some \\(\\phi  \\in  \\hom(B,A)\\)? We know that the answer  is yes! This is an exercise in Section \\ref{section:natural_transformations}.  The work of that exercise is proving this; however,  we  immediately get the  result  by the Yoneda Lemma since we can just observe that</p> \\[ \\text{Nat}(\\hom(A, -), \\hom(B, -)) \\cong \\hom(B, A).    \\] <p>Therefore, each  such natural transformation is created from  some \\(\\phi  \\in  \\hom(B,A)\\),  which is what we'd expect, so the Yoneda lemma passes our sanity check.</p> <p></p> <p>We now introduce the following definition to ease our discussion. </p> <p> Let \\(\\cc\\) be a category. A functor of the form \\(F: \\cc\\op \\to **Set**\\)  is called a presheaf\\footnote{The name \"presheaf\" is due to the  fact that this concept is a  precursor to the concept of a sheaf, which is outside of our scope  for the moment. }. As a presheaf may be viewed as an element  of the functor category \\(\\text{Fun}(\\cc\\op, **Set**)\\), we can define such a category  as the category of presheaves over \\(\\cc\\).  </p> <p>A natural source of presheaves is one which we are already familiar with.  Given any locally small category \\(\\cc\\), we can take any  object \\(A\\) of \\(\\cc\\) to produce the functor </p> \\[ \\hom_{\\cc}(-, A): \\cc\\op \\to **Set**. \\] <p>This process itself induces a functor known as the Yoneda embedding.</p> <p> Let \\(\\cc\\) be a locally small category. The Yoneda embedding on \\(\\cc\\)  is the functor \\(\\bm{y}: \\cc \\to \\text{Fun}(\\cc\\op, **Set**)\\) where  for each object \\(A\\) \\[ \\bm{y}(A) = \\hom_{\\cc}(-, A): \\cc\\op \\to **Set**.   \\] <p> The reason why this is called the Yoneda embedding is because of the functor's  relationship with the Yoneda embedding, which should become clear in proving the  following proposition. </p> <p> The Yoneda embedding \\(\\bm{y}: \\cc \\to \\text{Fun}(\\cc\\op, **Set**)\\) is a full and faithful functor.  The proof of this proposition is left as an exercise. However, the Yoneda  embedding arises naturally in many calculations within category. It is used  to prove the following important proposition. </p> <p> Every small category \\(\\cc\\) is concrete.  </p> <p> Recall that a concrete category \\(\\cc\\) is one which has a faithful functor \\(F: \\cc \\to **Set**\\). To demonstrate this  for small categories, first define the functor  \\[ C: \\text{Fun}(\\cc\\op, **Set**) \\to **Set** \\] <p>where a presheaf \\(P: \\cc\\op \\to **Set**\\) is mapped as </p> \\[ (P: \\cc\\op \\to **Set**) \\mapsto \\coprod_{A \\in \\text{Ob}(\\cc)} P(A). \\] <p>Note that the indexing of the disjoint union is where we use locally smallness.  This functor is fully faithful (exercise). As it is fully faithful, and the Yoneda  embedding \\(\\bm{y}: \\cc \\to \\text{Fun}(\\cc\\op, **Set**)\\) is faithful,  the composite functor </p> \\[ C \\circ \\bm{y}: \\cc \\to **Set** \\] <p>must be faithful. Hence we see that \\(\\cc\\) is concrete.  </p> <p>Finally, we end this section with a curious connection to group theory.  It turns out that Yoneda's Lemma can actually be used in the proof of Cayley's Theorem. Sometimes this statement is taken too literally by others  and they think ``Yoneda's Lemma is a generalization of  Cayley's Theorem'' but that is simply  not true, so the reader is warned to not believe someone when they hear that. Put simply, Yoneda's Lemma offers a bijection on sets which, with a little extra separate  work, extends to an isomorphism of groups.</p> <p>(Cayley's Theorem.) Let \\((G,  \\cdot)\\) be a group. Then \\(G\\) is isomorphic to a subgroup of \\(\\text{Perm}(G)\\). </p> <p> Recall that a group \\((G, \\cdot)\\) can be regarded as a category \\(\\cc\\); specifically,  we construct a category with one object \\(\\bullet\\) and set \\(\\hom_{\\cc}(\\bullet, \\bullet) = U(G)\\),  where \\(U: **Grp** \\to **Set**\\) is the forgetful functor. For each \\(g \\in G\\), a morphism is represented as \\(f_g: \\bullet \\to \\bullet\\), and we have that \\(f_g \\circ f_{g'} = f_{g'\\cdot g}\\). <p>Now consider the functor \\(\\hom_{\\cc}(\\bullet, -): \\cc \\to **Set**\\).  Such a functor produces the following data:</p> <ul> <li> <p>We have that \\(\\hom_{\\cc}(\\bullet, \\bullet) = U(G)\\)</p> </li> <li> <p>We also get a family of bijections \\(\\phi_g: U(G) \\to U(G)\\) such that \\(\\phi_g \\circ \\phi_{g'} = \\phi_{g'\\cdot g}\\).</p> </li> </ul> <p>In other words, the functor imposes an action of \\(G\\) on its  underlying set of elements \\(U(G)\\) in Set. Specifically, we may write \\(\\phi_{g'}(g) = g' \\cdot g\\) for each \\(g \\in G\\). Now what's a natural transformation \\(\\eta\\) between two functors?</p> \\[ \\eta: \\hom_{\\cc}(\\bullet, -) \\to \\hom_{\\cc}(\\bullet, -). \\] <p>Since there is only one object of \\(\\cc\\), a natural transformation is one function  \\(\\eta: U(G) \\to U(G)\\) such that for each \\(g'\\in G\\),  the diagram below commutes.  \\  Now, Yoneda's Lemma gives us the bijection below, which we may denote as  \\(\\psi\\),</p> \\[ \\text{Nat}(\\hom_{\\cc}(\\bullet, -), \\hom_{\\cc}(\\bullet, -)) \\cong \\hom_{\\cc}(\\bullet, \\bullet) = U(G). \\] <p>If we now observe that </p> <ul> <li> <p>The collection of such natural transformations is a group under composition, with identity \\(1_{U(G)}: U(G) \\to U(G)\\), which we may denote as \\((P, \\circ)\\)</p> </li> <li> <p>\\((P,  \\circ) \\subset \\text{Perm}(G)\\) </p> </li> </ul> <p>then we can extend the isomorphism \\(\\psi: P \\to U(G)\\) to a group isomorphism </p> \\[ \\psi: (P, \\circ) \\isomarrow (G, \\cdot)  \\] <p>which is the statement of Cayley's Theorem. </p> <p>{\\large Exercises \\vspace{0.5cm}}</p> <p>The first two exercises are very important. We (in fact you! The reader!) will  use these results later on. </p> <ul> <li>[1.]  Prove the following dual counterpart to Proposition \\ref{proposition:universality_bijection}:  Let \\(F: \\cc \\to \\dd\\) be a functor. Then a pair \\((R, u: F(R) \\to D)\\)  is universal from \\universalFToD{\\(F\\) to \\(D\\)} if and only if for each \\(C \\in \\cc\\), we have the  natural bijection </li> </ul> \\[ \\hom_{\\cc}(C, R) \\cong \\hom_{\\dd}(F(C), D). \\] <ul> <li>[2.]  Prove the following dual counterpart to Proposition \\ref{proposition:representable_if_and_only_if}: Let \\(\\cc\\) be a locally small category, and suppose \\(K: \\cc \\to **Set**\\)  is a functor. Then \\(K\\) is corepresentable, with representing object \\(R\\), if and only if  \\((R, u: K(R) \\to \\{\\bullet\\})\\) is universal from \\universalFToD{\\(K\\) to \\(\\{\\bullet\\}\\)} for some object \\(R\\). </li> </ul> <p>Hint: Because \\(K\\) is corepresentable, it is a contravariant functor. Thus, this  should be very similar to the proof of Proposition \\ref{proposition:representable_if_and_only_if},  except with one twist.</p>"},{"location":"category_theory/Universal%20Constructions%20and%20Limits/Universal%20Morphisms/","title":"3.1. Universal Morphisms","text":"<p>This chapter is probably the most important chapter in these notes.  In an ideal world, this chapter would be the first chapter.  However, that would pedagogically go over terribly. The discussion requires  categories, functors, and natural transformations; we need the language  these concepts offer  to even begin to rigorously define what a universal construction even is. </p> <p>But at this point, we are in fact equipped with the fundamentals. So we can now  go on and define what a universal construction is, and demonstrate its prevalence  in mathematics and therefore the usefulness of category theory as a convenient language  to discuss these concepts. </p> <p>To begin, we will motivate with a few examples.</p> <p>Let \\(\\phi, \\psi: (G, \\cdot)  \\to (H, +)\\) be a pair of abelian\\footnote{The abelian-ness becomes important later.}  group homomorphisms. We now ask the question: \\begin{center} \\textcolor{NavyBlue}{What is the set of all \\(g \\in G\\) such that \\(\\phi(g) = \\psi(g)\\)? Is it a subgroup of \\(G\\)?} \\end{center}    To determine this, it is equivalent to asking when \\(\\phi(g) - \\psi(g) = 0  \\implies (\\phi - \\psi)(g) = 0\\). Hence every such \\(g \\in G\\) lies in the kernel of  \\(\\phi - \\psi: G \\to H\\), and every element in the kernel is such a desired element; so  we've answered the first question. The kernel is a subgroup of \\(G\\), so we've answered the last question.  Now because this is a kernel, it has an inclusion  homomorphism \\(i: \\ker(\\phi - \\psi) \\to G\\). So far, our picture looks like this: \\  and clearly \\(\\phi \\circ i = \\psi \\circ i\\).  Now suppose that \\(\\sigma: K \\to G\\) is another group homomorphism with the property  that \\(\\phi \\circ \\sigma = \\psi \\circ \\sigma\\). Then by our previous work, this means  that for each \\(k \\in K\\), we have that \\(\\sigma(k)\\in\\ker(\\phi - \\psi)\\). That is, </p> \\[ \\im(\\sigma) \\subset \\ker(\\phi - \\psi)   \\] <p>Hence instead of mapping \\(K\\) into \\(G\\), we can instead map \\(K\\) into \\(\\ker(\\phi - \\psi)\\),  and then travel back to \\(G\\) using \\(i\\). So, there is a unique morphism \\(\\tau: K \\to \\ker(\\phi - \\psi)\\) such that the diagram below commutes (Prove it is unique; it shouldn't be too bad). \\ </p> <p>What's really going on? This is an example of a universal construction. We have a  \"supreme\" morphism \\(i: \\ker(\\phi -\\psi) \\to G\\) with the property that \\(\\phi \\circ i = \\psi \\circ i\\). Any other morphism \\(\\sigma: K \\to G\\) with the same property that \\(\\phi\\circ\\sigma = \\psi \\circ \\sigma\\)  must factor through the \"supreme\" morphism \\(i\\) in a unique way. Uniqueness here is  very important. </p> <p>Now, if you haven't seen this definition before, it's going to sting a little,  and you'll probably have to read it 20 times and do many, many examples (not just  look at examples, you have to do some yourself) to  achieve true understanding. But here we go:</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor and \\(D\\) an object of \\(\\dd\\). Define a **universal morphism from \\(\\bm{D**\\) to \\(\\bm{F}\\)}  to be a morphism \\[ \\textcolor{red}{u}: D \\to F(\\textcolor{NavyBlue}{C}) \\] <p>with \\(\\textcolor{NavyBlue}{C} \\in \\ob(\\cc)\\) and  \\(\\textcolor{Red}{u}\\) a morphism in \\(\\dd\\) equipped with the universal property: \\  \\end{center} The arrow \\(h\\) is dashed, and should be read as  \"there exists an \\(h\\).'' This is a practice  that we will continue to use throughout this text.  </p> <p> To the beginner, this definition will most likely make zero sense.  The only way that it will make sense is to see the definition in action. </p> <p>A universal arrow can also be thought of as a pair \\((\\textcolor{NavyBlue}{C}, \\textcolor{red}{u}: D \\to F(\\textcolor{NavyBlue}{C}))\\). This just emphasizes that \\(\\textcolor{NavyBlue}{C}\\) is special.  This isn't really useful for us to imagine in this way right now.  So you don't have to think of it as a pair, so long as you remember you're mapping to \\(F(\\textcolor{NavyBlue}{C})\\). </p> <p>The point is that any arrow of the form \\(f: D \\to F(C')\\) forces the unique existence of an arrow \\(f' : C \\to C'\\) such that \\(F(h) \\circ \\textcolor{Red}{u} = f\\).  </p> <p> Let \\(V\\), \\(W\\) be finite-dimensional vector spaces over a field \\(k\\). Denote their  bases as \\(\\{v_1,  v_2, \\dots, v_n\\}\\) and \\(\\{w_1, w_2, \\dots, w_m\\}\\).  \\begin{center} \\begin{minipage}{0.8\\textwidth} Q: What does it take for a function \\(T: V \\to W\\) to be a linear transformation?  \\end{minipage} \\end{center}       Well, suppose we have a linear transformation. Since each element of  \\(V\\) may be written as \\(c_1v_1 + \\cdots + c_nv_n\\)  for \\(c_i \\in k\\), we  see that  \\[ T(c_1v_1 + \\cdots + c_nv_n) = c_1T(v_1) + \\cdots + c_nT(v_n). \\] <p>Thus we have an answer.  \\begin{center} \\begin{minipage}{0.8\\textwidth} A: To define a linear transformation \\(T: V \\to W\\), it suffices to specify  where we want \\(T\\) to send the basis elements \\(v_1, \\dots, v_n\\). \\end{minipage} \\end{center}       An illustration of this fact is below.  \\  \\noindent This observation helps us build our first example of universality.</p> <p>Let \\(X\\) be a (possibly infinite) set. For a field \\(k\\), we can generate a vector space \\(\\textcolor{NavyBlue}{V_x}\\) (Note  the color-coding here corresponds to the color-coding in the definition of a universal  morphism) whose basis elements are \\(x \\in X\\). Specifically,</p> \\[ \\textcolor{NavyBlue}{V_x} = \\left\\{ \\sum_{x \\in X}c_x x \\;\\middle|\\; c_x = 0 \\text{ for all but finitely many }x \\right\\}. \\] <p>Now let \\(**Vect**_{k}\\) be the category of vector spaces over the field \\(k\\). Let \\(U: **Vect**_{k} \\to **Set**\\) be the forgetful functor which sends the vector space \\(V\\) to the set containing all its elements. For any set \\(X\\),  then there is an inclusion map </p> \\[ i: X \\mathbin{\\textcolor{red}{\\to}} U(\\textcolor{NavyBlue}{V_x}) \\qquad x \\mapsto x.            \\] <p>This inclusion map has the following property. Let \\(W\\) be any vector space, and suppose that we have a function \\(f: X \\mathbin{\\textcolor{red}{\\to}} U(W)\\).  This is kind of funny. A map \\(f: X \\mathbin{\\textcolor{red}{\\to}} U(W)\\)  simply picks out a \\(w_x \\in W\\) for each \\(x \\in X\\). Since \\(X\\) is a basis for \\(\\textcolor{NavyBlue}{V_x}\\),  this \"picking out\" defines a linear transformation \\(T: V \\to W\\). That is, such an  \\(f: X \\to U(W)\\) allows us to define a linear transformation where for each basis element \\(x \\in X\\)</p> \\[ T(x) = f(x). \\] <p>Since we know where the basis elements go, we see that such a linear transformation is well defined. Moreover, we see that our construction makes the diagram below commute.  \\  Therefore, we see that a universal morphism from \\(X\\) to the forgetful functor  \\(U: **Vect**_k \\to **Set**\\) is its inclusion morphism \\(i: X \\to U(V_x)\\)  into the vector space \\(V_x\\) generated by \\(X\\). </p> <p>Several key concepts in topology are secretly universal properties in disguise.  This is because in some sense, the problem of universality is an optimization problem. And in elementary topology, we are often trying to optimize a given topological  space with a desired property. For example, the closure of a topological  space \\(X\\) is the \"largest closed set\" containing \\(X\\). We'll elaborate more on this.</p> <p> Let \\(X\\) be a topological space. In topology, it is often of interest  to consider a compactification of the space \\(X\\). Such a story goes  like this: Given \\(X\\), we seek a compact space \\(X^*\\) such that \\(X\\) embeds as  a dense subspace of \\(X^*\\). In other words, we want a compact \\(X^*\\)  which has a dense subspace \\(S \\subset X^*\\) that is homeomorphic to \\(X\\). We can then identify \\(X\\) with \\(S\\) and work within \\(X^*\\), which is a nicer  space to work inside of. \\  <p>We can, however, do even better. We can compactify \\(X\\) into a space that is  not only compact, but is also Hausdorff. The optimal compactification  for this situation is the Stone-\u010cech Compactification, which  is defined as follows. Given a topological space \\(X\\), the Stone-\u010cech compactification is the compact, Hausdorff  space \\(\\beta X\\), equipped with a dense embedding  \\(i_X: X \\to \\beta X\\) such that, for any other compact, Hausdorff space \\(K\\) equipped  with a continuous map \\(f: X \\to K\\), there exists a unique continuous function  \\(\\beta f: \\beta X \\to K\\) such that  \\  This universal property is what demonstrates that the Stone-\u010cech compactification  \\(\\beta X\\) is the \"most compact, Hausdorff\" space we can densely embed \\(X\\) into.  However, in the language of category theory we see that this is just another example of  a universal morphism. To see this, let \\(I: **CHaus** \\to **Top**\\)  be the inclusion functor from compact Hausdorff spaces into topological spaces.  Then we can rewrite the diagram as  \\  Of course, in practice, we'd never actually write it like this; but this is just for  us to be able to see that the dense embedding \\(i_X: X \\to \\beta X\\) is universal  from \\(X\\) to the the inclusion functor \\(I:  **CHaus** \\to **Top**\\), so that the  Stone-\u010cech compactification is truly an example of a universal morphism.  </p> <p> Consider the free monoid functor \\(F: **Set** \\to **Mon**\\) which  sends a set \\(X\\) to the free monoid generated by \\(X\\). Specifically,  \\[ F(X) = \\{x_1x_2 \\dots x_n \\mid x_i \\in X \\} \\cup \\{e\\} \\] <p>The set consists of all strings using elements of \\(X\\), and an identity \\(e\\);  the monoid product is concatenation.</p> <p>Suppose I have a monoid homomorphism \\(\\phi: F(X) \\to M\\) where \\(M\\) is another monoid.  Then for any two \\(x_1, x_2 \\in X\\), we have that  \\(\\phi(x_1x_2) = \\phi(x_1)\\phi(x_2)\\). More generally, for any \\(x_1\\cdots x_n \\in F(X)\\),  we have that </p> \\[ \\phi(x_1 \\cdots x_n) = \\phi(x_1)\\cdots \\phi(x_n). \\] <p>We thus see the following: To define a monoid homomorphism, we just need to know where  to send every individual \\(x \\in X\\). This is achieved by  defining a set function \\(\\phi_0: X \\to U(M)\\), and by setting  \\(\\phi(x) = \\phi_0(x)\\). This makes the  diagram below commutative. \\  We thus see that \\((X, i_X: X \\to F(X))\\) is universal from \\(X\\) to \\(U: **Mon** \\to **Set**\\).  </p> <p> Let \\((R, +, \\cdot)\\) be a ring and \\(k\\) a field. Suppose further that  \\(R\\) is a \\(k\\)-algebra. Then for any set \\(X = \\{x_1, \\dots, x_n\\}\\) of indeterminates,  we can create a free algebra generated by \\(X\\), denoted as  \\(k\\{X\\}\\). One can show that this defines a functor  \\[ F: **Set** \\to **Alg**_k \\] <p>mapping sets \\(X\\) it \\(k\\{X\\}\\) and functions \\(f: X \\to Y\\) to  the \\(k\\)-algebra morphism \\(\\phi: k\\{X\\} \\to k\\{Y\\}\\) where \\(\\phi\\)  is defined linearly by its action on the basis elements sending each \\(x \\to f(x)\\).  On the other hand, note that we can also create a forgetful functor </p> \\[ U:**Alg**_k \\to **Set** \\] <p>which simply reinterprets each \\(k\\)-algebra as a set and each \\(k\\)-algebraic morphism as a function. </p> <p>Now consider a mapping \\(f: X \\to U(R)\\) in Set. Because we  also have a mapping \\(i: X \\to U(F(X))\\), which acts an inclusion function, we see that we can create a mapping \\(h: F(X) \\to A\\) such that the diagram below  commutes.  \\  The way we do this is we defined \\(h: F(X) \\to A\\) to act linearly  on the basis elements, sending \\(x \\mapsto g(x)\\). This defines a \\(k\\)-algebraic  morphism and makes the above diagram commute.  In this case, we say that \\((F(X), i: X \\to U(F(X)))\\) is universal from  \\(X\\) to the forgetful functor \\(U: **Alg**_k \\to **Set**\\).  </p> <p>When we discuss a universal morphism from \\(D\\) to \\(F: \\cc \\to \\dd\\),  we are particularly discussing a morphism  \\(u: D \\to F(C)\\) and a special object \\(C\\). Hence, we can actually write  a universal morphism as a pair \\((C, u: D \\to F(C))\\). Does this look familiar?  This is an object of the category \\((D \\downarrow F)\\)!  Hence, universal morphisms can actually be thought of as elements in a comma category.  Under this intepretation, what does the universal property translate to? The next proposition answers our question. </p> <p> Let \\(F:\\cc \\to \\dd\\) be a functor. A morphism \\(u: D \\to F(C)\\) is universal  from \\(D\\) to \\(F\\) if and only if \\((C, u:D \\to F(C))\\) is an initial object of the  comma category \\((D \\downarrow F)\\) </p> <p>So, as we will see, the universal property of a universal morphism \\(u: D \\to F(C)\\) translates to \\((C, u: D \\to F(C))\\) being an initial object in some comma category.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor, and \\(D\\) an object of \\(\\dd\\). Recall that  the category \\((D \\downarrow \\dd)\\) is the category where  \\begin{description} \\item[Objects.] Pairs \\((C, f: D \\to F(C))\\) with \\(C \\in \\cc\\) and \\(f: D \\to \\dd\\) a morphism in \\(\\dd\\).  \\item[Morphisms.] Morphisms between two objects \\((C, f: D \\to F(C))\\) and \\((C', f: D \\to F(C'))\\)  are given by morphisms \\(h: C \\to C'\\) such that the diagram below commutes.  \\  \\end{description} Suppose \\((\\textcolor{NavyBlue}{A}, u: D \\mathbin{\\textcolor{Red}{\\to}} F(\\textcolor{NavyBlue}{A}))\\) is an initial object in \\((D \\downarrow F)\\). Then for every  other pair \\((A, f: D \\to F(A'))\\), there exists a unique morphism \\(h: A \\to A'\\)  such that the diagram on the bottom left commutes.  \\  However, if we rearrange this we see that this is just the universal property in disguise! Conversely, any pair \\((A, f:  A \\to F(A))\\) being a universal morphism can be demonstrated to be an initial object in \\((D, \\downarrow F)\\) by reversing the  above proof.  </p> <p>Now, we didn't do this just for fun. The interpretation of a universal morphism  as an initial object of a comma category theory will serve to be very useful, just not now.  As of now it does not really grant us much. But when we are deep into the chapter on Limits,  this intrepretation will become useful.</p> <p>One thing that the interpretation does grant us for now is the following theorem,  which requires essentially no proof if we understand a universal morphism is an initial object  of a comma category. This theorem explains ultimately why we care about universal morphisms; they're  like categorical invariants!</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor and \\(D \\in \\dd\\).  Suppose \\(u: D \\to F(C)\\) is universal from \\(D\\) to \\(F\\)  for some object \\(C \\in \\cc\\). If \\(u': D \\to F(C')\\) is also universal from \\(D\\) to \\(F\\), then  \\(C \\cong C'\\).  </p> <p> Universal morphisms \\(u: D \\to F(C)\\) are initial objects  in the comma category \\((D \\downarrow F)\\), and initial objects are always unique up to isomorphism. Hence \\((C, u:D \\to F(C))\\) with the universal property is unique. </p> <p>However, the direct proof, where we do not use the interpretation of a comma category,  is left as an exercise. It's actually very important to see and understand the direct proof. </p> <p>As with most constructions within category theory, there is a dual construction.  That, is there is another form of universality which is equally as important  as the one we originally introduced.  So, in general, there are two forms of universality.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor and \\(C\\) an object of \\(\\cc\\). A **universal arrow from \\(\\bm{F**\\) to \\(\\bm{C}\\)} is a morphism  \\[ \\textcolor{Red}{v}: F(\\textcolor{NavyBlue}{C}) \\to D \\] <p>equipped with the universal property: \\   Note that this is basically the previous definition of a universal arrow from an object to a functor, except the direction of the arrows have been flipped. This is why we called this the \"dual\" definition of the previous one. This motivates the following  statement which requires no effort to prove. </p> <p> Let \\(\\cc\\) be a category and \\(F: \\cc \\to \\dd\\) be a functor.  If \\(\\cc\\) has a universal morphism from \\(D\\) to \\(F\\),  then \\(\\cc\\op\\) has a universal morphism from \\(F\\) to \\(D\\).  </p> <p>So we see that the two notions of unviversality we've introduced really  are dual concepts. Both are equally important, and we will see that they both arise  as very deep concepts in mathematics. Not just in the examples we've provided,  but in deeper pure category theory. </p> <p>Anyways, we can repeat the propositions we worked on.</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor.  A morphism \\(u: F(C) \\to D\\) is universal from \\(F\\) to \\(D\\) if \\((C, u: F(C) \\to D)\\) is a terminal object of the comma category \\((F \\downarrow D)\\).  </p> <p>This is left as an exercise, and should be similar to our proof from before. And as before, we get our second important theorem:</p> <p> Let \\(F: \\cc \\to \\dd\\) be a functor, with \\(u: F(C) \\to D\\) universal  from \\(F\\) to \\(D\\). Then if \\(u': F(C') \\to D\\) is also universal from \\(F\\) to \\(D\\), then  \\(C \\cong C'\\).  </p> <p> Universal morphisms from \\(F\\) to \\(D\\) are terminal objects in a comma category, and terminal  objects are always unique up to isomorphism. </p> <p>The direct proof is also an exercise.  \\vspace{0.5cm}</p> <p>{\\large Exercises \\vspace{0.5cm}} \\def\\exerciseCartesianProduct{4} </p> <ul> <li> <p>[1.] Prove Theorem \\ref{theorem:universal_elements_are_isomorphic} directly, and dualize your proof to prove Theorem \\ref{theorem:couniversal_elements_isomorphic} directly. </p> </li> <li> <p>[2.] Prove Proposition \\ref{proposition:universal_elements_in_comma_cats}.</p> </li> <li> <p>[3.] For each ring \\(R\\), we may construct the single-variable  polynomial ring \\(R[x]\\). This process defines a functor \\(**Poly**: **Ring** \\to **Ring**\\).</p> </li> </ul> <p>Show that for each ring \\(R\\), the inclusion ring homomorphism \\(i: R \\to R[X]\\)  is a universal morphism from \\(R\\) to Poly. </p> <ul> <li> <p>[4.]  Let \\(X\\) and \\(Y\\) be two sets, and consider their product \\(X \\times Y\\). Recall that with  any product, we have \"projection maps\" \\(\\pi_1: X \\times Y \\to X\\) and \\(\\pi_2: X \\times Y \\to Y\\) where \\(\\pi_1(x,y) = x\\) and \\(\\pi_2(x,y) = y\\).  \\begin{itemize}</p> </li> <li> <p>[\\(i.\\)] Suppose we have functions \\(f: Z \\to X\\) and \\(g: Z \\to Y\\).  Show how this gives us a map \\(h: Z \\to X\\times Y\\), and show that  this map is unique (to the pair \\(f\\) and \\(g\\)). </p> </li> <li> <p>[\\(ii.\\)] Using your map \\(h: Z \\to X \\times Y\\), show that the  diagram on the left commutes, and that the diagram on the right is  equivalent. \\  To be clear, the diagram on the right is in the category  \\(**Set**\\times **Set**\\). </p> </li> <li> <p>[\\(iii.\\)] Let \\(\\Delta: **Set**\\to **Set**\\times**Set**\\) be  the \"copy functor\" which sends \\(X \\mapsto (X, X)\\). Then the above diagram  translates to  \\  Deduce how the product \\((\\pi_1, \\pi_2): \\Delta(X\\times Y) \\to (X,Y)\\) is  universal from \\((X,Y)\\) to \\(\\Delta\\).  This is an important fact that we'll build upon later.</p> </li> </ul> <p>\\item[4.] Let \\(X\\) and \\(Y\\) be two sets, and consider the coproduct</p> \\[ X \\amalg Y =  \\{(x, 1), (y, 2) \\mid x \\in X, y \\in Y\\}\\footnote{Note that I  arbitrarily chose the numbers 1 and 2. I could have put anything I wanted.  For a coproduct, we just need to create two separate tuples that contain  $x$ values and $y$-values. Hence 1 and 2 work perfectly fine.} \\] <p>Recall that with  any coproduct, we'll have \"injection maps\" \\(i_1: X \\to X \\amalg Y\\) and \\(i_2: Y \\to X \\amalg Y\\) where  \\(i_1(x) = (x, 1)\\) and \\(i_2(y) = (y, 2)\\). Repeat (\\(i\\)-\\(iii\\))  as in the previous exercise to demonstrate that  \\((i_1,i_2): (X,Y) \\to \\Delta(X\\amalg Y)\\) is universal from  \\(\\Delta\\) to \\((X,Y)\\).  \\end{itemize}</p>"}]}